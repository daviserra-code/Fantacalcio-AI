{"generated_at":"2025-08-17T20:04:18.057896Z","root":"/home/runner/workspace","git":{"head":"7b7135163eb4227ef98e4c2d7b4ab78ea306bd73","branch":"main","status":" M corrections.db\n?? app_changes.json\n?? export_changes.py\n"},"filters":{"git_range":null,"since":null,"include_ext":[".cfg",".css",".env",".htm",".html",".ini",".jinja",".jinja2",".js",".json",".md",".py",".toml",".ts",".yaml",".yml"],"exclude_dirs":[".git",".ipynb_checkpoints",".mypy_cache",".pytest_cache",".pythonlibs",".venv","__pycache__","cache","chroma_db","data/exports","node_modules","venv"],"exclude_globs":["*.bmp","*.db","*.feather","*.gif","*.gz","*.ico","*.jpeg","*.jpg","*.jsonl","*.lock","*.log","*.parquet","*.png","*.sqlite","*.sqlite3","*.tar","*.webp","*.zip"],"max_file_bytes":400000},"summary":{"file_count":876,"total_bytes":9265289},"files":[{"path":".cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/.no_exist/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/adapter_config.json","size":0,"sha1":"da39a3ee5e6b4b0d3255bfef95601890afd80709","mtime":1755353037,"is_binary":false,"encoding":"utf-8","content":""},{"path":".cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/.no_exist/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/added_tokens.json","size":0,"sha1":"da39a3ee5e6b4b0d3255bfef95601890afd80709","mtime":1754414189,"is_binary":false,"encoding":"utf-8","content":""},{"path":".cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/.no_exist/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/chat_template.jinja","size":0,"sha1":"da39a3ee5e6b4b0d3255bfef95601890afd80709","mtime":1754414189,"is_binary":false,"encoding":"utf-8","content":""},{"path":".cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/1_Pooling/config.json","size":190,"sha1":"ef7a6acbe77cc2656a60b5da3ecd43776c73056d","mtime":1754414189,"is_binary":false,"encoding":"utf-8","content":"{\n  \"word_embedding_dimension\": 384,\n  \"pooling_mode_cls_token\": false,\n  \"pooling_mode_mean_tokens\": true,\n  \"pooling_mode_max_tokens\": false,\n  \"pooling_mode_mean_sqrt_len_tokens\": false\n}"},{"path":".cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/README.md","size":10454,"sha1":"ecf35787e8cb94996e990cc029f85f7113e76a3d","mtime":1754414185,"is_binary":false,"encoding":"utf-8","content":"---\nlanguage: en\nlicense: apache-2.0\nlibrary_name: sentence-transformers\ntags:\n- sentence-transformers\n- feature-extraction\n- sentence-similarity\n- transformers\ndatasets:\n- s2orc\n- flax-sentence-embeddings/stackexchange_xml\n- ms_marco\n- gooaq\n- yahoo_answers_topics\n- code_search_net\n- search_qa\n- eli5\n- snli\n- multi_nli\n- wikihow\n- natural_questions\n- trivia_qa\n- embedding-data/sentence-compression\n- embedding-data/flickr30k-captions\n- embedding-data/altlex\n- embedding-data/simple-wiki\n- embedding-data/QQP\n- embedding-data/SPECTER\n- embedding-data/PAQ_pairs\n- embedding-data/WikiAnswers\npipeline_tag: sentence-similarity\n---\n\n\n# all-MiniLM-L6-v2\nThis is a [sentence-transformers](https://www.SBERT.net) model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.\n\n## Usage (Sentence-Transformers)\nUsing this model becomes easy when you have [sentence-transformers](https://www.SBERT.net) installed:\n\n```\npip install -U sentence-transformers\n```\n\nThen you can use the model like this:\n```python\nfrom sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n\nmodel = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\nembeddings = model.encode(sentences)\nprint(embeddings)\n```\n\n## Usage (HuggingFace Transformers)\nWithout [sentence-transformers](https://www.SBERT.net), you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\nimport torch.nn.functional as F\n\n#Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n\n# Sentences we want sentence embeddings for\nsentences = ['This is an example sentence', 'Each sentence is converted']\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\nmodel = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n\n# Perform pooling\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n\n# Normalize embeddings\nsentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\n```\n\n------\n\n## Background\n\nThe project aims to train sentence embedding models on very large sentence level datasets using a self-supervised \ncontrastive learning objective. We used the pretrained [`nreimers/MiniLM-L6-H384-uncased`](https://huggingface.co/nreimers/MiniLM-L6-H384-uncased) model and fine-tuned in on a \n1B sentence pairs dataset. We use a contrastive learning objective: given a sentence from the pair, the model should predict which out of a set of randomly sampled other sentences, was actually paired with it in our dataset.\n\nWe developed this model during the \n[Community week using JAX/Flax for NLP & CV](https://discuss.huggingface.co/t/open-to-the-community-community-week-using-jax-flax-for-nlp-cv/7104), \norganized by Hugging Face. We developed this model as part of the project:\n[Train the Best Sentence Embedding Model Ever with 1B Training Pairs](https://discuss.huggingface.co/t/train-the-best-sentence-embedding-model-ever-with-1b-training-pairs/7354). We benefited from efficient hardware infrastructure to run the project: 7 TPUs v3-8, as well as intervention from Googles Flax, JAX, and Cloud team member about efficient deep learning frameworks.\n\n## Intended uses\n\nOur model is intended to be used as a sentence and short paragraph encoder. Given an input text, it outputs a vector which captures \nthe semantic information. The sentence vector may be used for information retrieval, clustering or sentence similarity tasks.\n\nBy default, input text longer than 256 word pieces is truncated.\n\n\n## Training procedure\n\n### Pre-training \n\nWe use the pretrained [`nreimers/MiniLM-L6-H384-uncased`](https://huggingface.co/nreimers/MiniLM-L6-H384-uncased) model. Please refer to the model card for more detailed information about the pre-training procedure.\n\n### Fine-tuning \n\nWe fine-tune the model using a contrastive objective. Formally, we compute the cosine similarity from each possible sentence pairs from the batch.\nWe then apply the cross entropy loss by comparing with true pairs.\n\n#### Hyper parameters\n\nWe trained our model on a TPU v3-8. We train the model during 100k steps using a batch size of 1024 (128 per TPU core).\nWe use a learning rate warm up of 500. The sequence length was limited to 128 tokens. We used the AdamW optimizer with\na 2e-5 learning rate. The full training script is accessible in this current repository: `train_script.py`.\n\n#### Training data\n\nWe use the concatenation from multiple datasets to fine-tune our model. The total number of sentence pairs is above 1 billion sentences.\nWe sampled each dataset given a weighted probability which configuration is detailed in the `data_config.json` file.\n\n\n| Dataset                                                  | Paper                                    | Number of training tuples  |\n|--------------------------------------------------------|:----------------------------------------:|:--------------------------:|\n| [Reddit comments (2015-2018)](https://github.com/PolyAI-LDN/conversational-datasets/tree/master/reddit) | [paper](https://arxiv.org/abs/1904.06472) | 726,484,430 |\n| [S2ORC](https://github.com/allenai/s2orc) Citation pairs (Abstracts) | [paper](https://aclanthology.org/2020.acl-main.447/) | 116,288,806 |\n| [WikiAnswers](https://github.com/afader/oqa#wikianswers-corpus) Duplicate question pairs | [paper](https://doi.org/10.1145/2623330.2623677) | 77,427,422 |\n| [PAQ](https://github.com/facebookresearch/PAQ) (Question, Answer) pairs | [paper](https://arxiv.org/abs/2102.07033) | 64,371,441 |\n| [S2ORC](https://github.com/allenai/s2orc) Citation pairs (Titles) | [paper](https://aclanthology.org/2020.acl-main.447/) | 52,603,982 |\n| [S2ORC](https://github.com/allenai/s2orc) (Title, Abstract) | [paper](https://aclanthology.org/2020.acl-main.447/) | 41,769,185 |\n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) (Title, Body) pairs  | - | 25,316,456 |\n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) (Title+Body, Answer) pairs  | - | 21,396,559 |\n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) (Title, Answer) pairs  | - | 21,396,559 |\n| [MS MARCO](https://microsoft.github.io/msmarco/) triplets | [paper](https://doi.org/10.1145/3404835.3462804) | 9,144,553 |\n| [GOOAQ: Open Question Answering with Diverse Answer Types](https://github.com/allenai/gooaq) | [paper](https://arxiv.org/pdf/2104.08727.pdf) | 3,012,496 |\n| [Yahoo Answers](https://www.kaggle.com/soumikrakshit/yahoo-answers-dataset) (Title, Answer) | [paper](https://proceedings.neurips.cc/paper/2015/hash/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html) | 1,198,260 |\n| [Code Search](https://huggingface.co/datasets/code_search_net) | - | 1,151,414 |\n| [COCO](https://cocodataset.org/#home) Image captions | [paper](https://link.springer.com/chapter/10.1007%2F978-3-319-10602-1_48) | 828,395|\n| [SPECTER](https://github.com/allenai/specter) citation triplets | [paper](https://doi.org/10.18653/v1/2020.acl-main.207) | 684,100 |\n| [Yahoo Answers](https://www.kaggle.com/soumikrakshit/yahoo-answers-dataset) (Question, Answer) | [paper](https://proceedings.neurips.cc/paper/2015/hash/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html) | 681,164 |\n| [Yahoo Answers](https://www.kaggle.com/soumikrakshit/yahoo-answers-dataset) (Title, Question) | [paper](https://proceedings.neurips.cc/paper/2015/hash/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html) | 659,896 |\n| [SearchQA](https://huggingface.co/datasets/search_qa) | [paper](https://arxiv.org/abs/1704.05179) | 582,261 |\n| [Eli5](https://huggingface.co/datasets/eli5) | [paper](https://doi.org/10.18653/v1/p19-1346) | 325,475 |\n| [Flickr 30k](https://shannon.cs.illinois.edu/DenotationGraph/) | [paper](https://transacl.org/ojs/index.php/tacl/article/view/229/33) | 317,695 |\n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) Duplicate questions (titles) | | 304,525 |\n| AllNLI ([SNLI](https://nlp.stanford.edu/projects/snli/) and [MultiNLI](https://cims.nyu.edu/~sbowman/multinli/) | [paper SNLI](https://doi.org/10.18653/v1/d15-1075), [paper MultiNLI](https://doi.org/10.18653/v1/n18-1101) | 277,230 | \n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) Duplicate questions (bodies) | | 250,519 |\n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) Duplicate questions (titles+bodies) | | 250,460 |\n| [Sentence Compression](https://github.com/google-research-datasets/sentence-compression) | [paper](https://www.aclweb.org/anthology/D13-1155/) | 180,000 |\n| [Wikihow](https://github.com/pvl/wikihow_pairs_dataset) | [paper](https://arxiv.org/abs/1810.09305) | 128,542 |\n| [Altlex](https://github.com/chridey/altlex/) | [paper](https://aclanthology.org/P16-1135.pdf) | 112,696 |\n| [Quora Question Triplets](https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs) | - | 103,663 |\n| [Simple Wikipedia](https://cs.pomona.edu/~dkauchak/simplification/) | [paper](https://www.aclweb.org/anthology/P11-2117/) | 102,225 |\n| [Natural Questions (NQ)](https://ai.google.com/research/NaturalQuestions) | [paper](https://transacl.org/ojs/index.php/tacl/article/view/1455) | 100,231 |\n| [SQuAD2.0](https://rajpurkar.github.io/SQuAD-explorer/) | [paper](https://aclanthology.org/P18-2124.pdf) | 87,599 |\n| [TriviaQA](https://huggingface.co/datasets/trivia_qa) | - | 73,346 |\n| **Total** | | **1,170,060,424** |"},{"path":".cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json","size":612,"sha1":"50b71cc587a8c3357c437e4e27cf4df0126215a3","mtime":1754414186,"is_binary":false,"encoding":"utf-8","content":"{\n  \"_name_or_path\": \"nreimers/MiniLM-L6-H384-uncased\",\n  \"architectures\": [\n    \"BertModel\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 384,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 1536,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 6,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.8.2\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n"},{"path":".cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json","size":116,"sha1":"eb8ba72c3fa9548045fd7691131d70865b03ec8d","mtime":1754414185,"is_binary":false,"encoding":"utf-8","content":"{\n  \"__version__\": {\n    \"sentence_transformers\": \"2.0.0\",\n    \"transformers\": \"4.6.1\",\n    \"pytorch\": \"1.8.1\"\n  }\n}"},{"path":".cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json","size":349,"sha1":"cf2bdd275c2fe4bc323a4d6a04f1a78f6d4736dc","mtime":1754414185,"is_binary":false,"encoding":"utf-8","content":"[\n  {\n    \"idx\": 0,\n    \"name\": \"0\",\n    \"path\": \"\",\n    \"type\": \"sentence_transformers.models.Transformer\"\n  },\n  {\n    \"idx\": 1,\n    \"name\": \"1\",\n    \"path\": \"1_Pooling\",\n    \"type\": \"sentence_transformers.models.Pooling\"\n  },\n  {\n    \"idx\": 2,\n    \"name\": \"2\",\n    \"path\": \"2_Normalize\",\n    \"type\": \"sentence_transformers.models.Normalize\"\n  }\n]"},{"path":".cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/sentence_bert_config.json","size":53,"sha1":"45d8d9bed21c44d986cf5c6cd002f7c2365d92f1","mtime":1754414185,"is_binary":false,"encoding":"utf-8","content":"{\n  \"max_seq_length\": 256,\n  \"do_lower_case\": false\n}"},{"path":".cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/special_tokens_map.json","size":112,"sha1":"8e98c044a834804677adc5ae1b7884a2dff23f41","mtime":1754414189,"is_binary":false,"encoding":"utf-8","content":"{\"unk_token\": \"[UNK]\", \"sep_token\": \"[SEP]\", \"pad_token\": \"[PAD]\", \"cls_token\": \"[CLS]\", \"mask_token\": \"[MASK]\"}"},{"path":".cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json","size":350,"sha1":"1a708c9e87238b1e204ea47d267d7dd1b404ddd7","mtime":1754414188,"is_binary":false,"encoding":"utf-8","content":"{\"do_lower_case\": true, \"unk_token\": \"[UNK]\", \"sep_token\": \"[SEP]\", \"pad_token\": \"[PAD]\", \"cls_token\": \"[CLS]\", \"mask_token\": \"[MASK]\", \"tokenize_chinese_chars\": true, \"strip_accents\": null, \"name_or_path\": \"nreimers/MiniLM-L6-H384-uncased\", \"do_basic_tokenize\": true, \"never_split\": null, \"tokenizer_class\": \"BertTokenizer\", \"model_max_length\": 512}"},{"path":".cache/huggingface/models--sentence-transformers--all-MiniLM-L6-v2/.no_exist/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/adapter_config.json","size":0,"sha1":"da39a3ee5e6b4b0d3255bfef95601890afd80709","mtime":1754924407,"is_binary":false,"encoding":"utf-8","content":""},{"path":".cache/huggingface/models--sentence-transformers--all-MiniLM-L6-v2/.no_exist/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/added_tokens.json","size":0,"sha1":"da39a3ee5e6b4b0d3255bfef95601890afd80709","mtime":1754921481,"is_binary":false,"encoding":"utf-8","content":""},{"path":".cache/huggingface/models--sentence-transformers--all-MiniLM-L6-v2/.no_exist/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/chat_template.jinja","size":0,"sha1":"da39a3ee5e6b4b0d3255bfef95601890afd80709","mtime":1754921481,"is_binary":false,"encoding":"utf-8","content":""},{"path":".cache/huggingface/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json","size":612,"sha1":"50b71cc587a8c3357c437e4e27cf4df0126215a3","mtime":1754921480,"is_binary":false,"encoding":"utf-8","content":"{\n  \"_name_or_path\": \"nreimers/MiniLM-L6-H384-uncased\",\n  \"architectures\": [\n    \"BertModel\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 384,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 1536,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 6,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.8.2\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n"},{"path":".cache/huggingface/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/special_tokens_map.json","size":112,"sha1":"8e98c044a834804677adc5ae1b7884a2dff23f41","mtime":1754921481,"is_binary":false,"encoding":"utf-8","content":"{\"unk_token\": \"[UNK]\", \"sep_token\": \"[SEP]\", \"pad_token\": \"[PAD]\", \"cls_token\": \"[CLS]\", \"mask_token\": \"[MASK]\"}"},{"path":".cache/huggingface/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json","size":350,"sha1":"1a708c9e87238b1e204ea47d267d7dd1b404ddd7","mtime":1754921481,"is_binary":false,"encoding":"utf-8","content":"{\"do_lower_case\": true, \"unk_token\": \"[UNK]\", \"sep_token\": \"[SEP]\", \"pad_token\": \"[PAD]\", \"cls_token\": \"[CLS]\", \"mask_token\": \"[MASK]\", \"tokenize_chinese_chars\": true, \"strip_accents\": null, \"name_or_path\": \"nreimers/MiniLM-L6-H384-uncased\", \"do_basic_tokenize\": true, \"never_split\": null, \"tokenizer_class\": \"BertTokenizer\", \"model_max_length\": 512}"},{"path":".cache/live_sources_cache.json","size":2031,"sha1":"002ca36073e6711c1074d1b3d8fa63c2dc2ed4c0","mtime":1754828875,"is_binary":false,"encoding":"utf-8","content":"{\n  \"wp:it:sandro tonali\": {\n    \"type\": \"standard\",\n    \"title\": \"Sandro Tonali\",\n    \"displaytitle\": \"<span class=\\\"mw-page-title-main\\\">Sandro Tonali</span>\",\n    \"namespace\": {\n      \"id\": 0,\n      \"text\": \"\"\n    },\n    \"wikibase_item\": \"Q43402334\",\n    \"titles\": {\n      \"canonical\": \"Sandro_Tonali\",\n      \"normalized\": \"Sandro Tonali\",\n      \"display\": \"<span class=\\\"mw-page-title-main\\\">Sandro Tonali</span>\"\n    },\n    \"pageid\": 7884744,\n    \"thumbnail\": {\n      \"source\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/f/f5/Norway_Italy_-_June_2025_B_03.jpg/330px-Norway_Italy_-_June_2025_B_03.jpg\",\n      \"width\": 320,\n      \"height\": 480\n    },\n    \"originalimage\": {\n      \"source\": \"https://upload.wikimedia.org/wikipedia/commons/f/f5/Norway_Italy_-_June_2025_B_03.jpg\",\n      \"width\": 2223,\n      \"height\": 3335\n    },\n    \"lang\": \"it\",\n    \"dir\": \"ltr\",\n    \"revision\": \"146168618\",\n    \"tid\": \"c25ade05-745d-11f0-9cdc-e2cabf770c7f\",\n    \"timestamp\": \"2025-08-08T13:44:09Z\",\n    \"description\": \"calciatore italiano (2000-)\",\n    \"description_source\": \"central\",\n    \"content_urls\": {\n      \"desktop\": {\n        \"page\": \"https://it.wikipedia.org/wiki/Sandro_Tonali\",\n        \"revisions\": \"https://it.wikipedia.org/wiki/Sandro_Tonali?action=history\",\n        \"edit\": \"https://it.wikipedia.org/wiki/Sandro_Tonali?action=edit\",\n        \"talk\": \"https://it.wikipedia.org/wiki/Discussione:Sandro_Tonali\"\n      },\n      \"mobile\": {\n        \"page\": \"https://it.m.wikipedia.org/wiki/Sandro_Tonali\",\n        \"revisions\": \"https://it.m.wikipedia.org/wiki/Special:History/Sandro_Tonali\",\n        \"edit\": \"https://it.m.wikipedia.org/wiki/Sandro_Tonali?action=edit\",\n        \"talk\": \"https://it.m.wikipedia.org/wiki/Discussione:Sandro_Tonali\"\n      }\n    },\n    \"extract\": \"Sandro Tonali è un calciatore italiano, centrocampista del Newcastle Utd e della nazionale italiana.\",\n    \"extract_html\": \"<p><b>Sandro Tonali</b> è un calciatore italiano, centrocampista del Newcastle Utd e della nazionale italiana.</p>\"\n  }\n}"},{"path":".cache/replit/__replit_disk_meta.json","size":86,"sha1":"c371d7a0ddf36cea620e0e434924d3632b4608c3","mtime":1698707472,"is_binary":false,"encoding":"utf-8","content":"{\"nonce\":3328013730386818301,\"last_updated\":{\"seconds\":1698707472,\"nanos\":411147000}}\n"},{"path":".cache/replit/env/latest.json","size":7176,"sha1":"3a195c23b085f7ed5ffc82e8b43d95ce4a084d6d","mtime":1755440703,"is_binary":false,"encoding":"utf-8","content":"{\"environment\":{\"CFLAGS\":\"-isystem /nix/store/l7jsfn0p3vyw67xbjarwjfkp22nvixnz-jq-1.6-dev/include -isystem /nix/store/0vzjvi1cg907zxzv6a25n9c7vydnnqny-gdb-13.1/include -isystem /nix/store/n0wyrb99dxinh0y6rjixmqdgvbm57fa6-mailutils-3.15/include\",\"COLORTERM\":\"truecolor\",\"DISPLAY\":\":0\",\"DOCKER_CONFIG\":\"/home/runner/workspace/.config/docker\",\"GIT_ASKPASS\":\"replit-git-askpass\",\"GIT_EDITOR\":\"replit-git-editor\",\"GI_TYPELIB_PATH\":\"\",\"GLIBC_TUNABLES\":\"glibc.rtld.optional_static_tls=10000\",\"HOME\":\"/home/runner\",\"HOSTNAME\":\"1cf9c6824163\",\"LANG\":\"en_US.UTF-8\",\"LDFLAGS\":\"-L/nix/store/0vzjvi1cg907zxzv6a25n9c7vydnnqny-gdb-13.1/lib -L/nix/store/n0wyrb99dxinh0y6rjixmqdgvbm57fa6-mailutils-3.15/lib -L/nix/store/ljjvmhl1df2g4w43jspwgzx30zqyak07-mailutils-3.15-debug/lib -L/nix/store/1cwwzfln8dbbr3hqkwqj2wx583ibk2i3-jq-1.6-lib/lib\",\"LD_AUDIT\":\"/nix/store/6y0zqxaf220r36b74hwsq9m2b2av3lw7-replit_rtld_loader-1/rtld_loader.so\",\"LIBGL_DRIVERS_PATH\":\"/nix/store/cpwib3zazj49fm0y04y53w4xkbqsgrgm-mesa-25.0.7/lib/dri\",\"LOCALE_ARCHIVE\":\"/usr/lib/locale/locale-archive\",\"NIXPKGS_ALLOW_UNFREE\":\"1\",\"NIX_CFLAGS_COMPILE\":\"-isystem /nix/store/l7jsfn0p3vyw67xbjarwjfkp22nvixnz-jq-1.6-dev/include -isystem /nix/store/0vzjvi1cg907zxzv6a25n9c7vydnnqny-gdb-13.1/include -isystem /nix/store/n0wyrb99dxinh0y6rjixmqdgvbm57fa6-mailutils-3.15/include\",\"NIX_LDFLAGS\":\"-L/nix/store/0vzjvi1cg907zxzv6a25n9c7vydnnqny-gdb-13.1/lib -L/nix/store/n0wyrb99dxinh0y6rjixmqdgvbm57fa6-mailutils-3.15/lib -L/nix/store/ljjvmhl1df2g4w43jspwgzx30zqyak07-mailutils-3.15-debug/lib -L/nix/store/1cwwzfln8dbbr3hqkwqj2wx583ibk2i3-jq-1.6-lib/lib\",\"NIX_PATH\":\"nixpkgs=/home/runner/.nix-defexpr/channels/nixpkgs-stable-23_05:/home/runner/.nix-defexpr/channels\",\"NIX_PROFILES\":\"/nix/var/nix/profiles/default /home/runner/.nix-profile\",\"NIX_PS1\":\"\\\\[\\\\033[01;34m\\\\]\\\\w\\\\[\\\\033[00m\\\\]\\\\$ \",\"PATH\":\"/nix/store/njsp9a4bq3m6l3dzx4hfgbrfn1yzvm2p-jq-1.6-bin/bin:/nix/store/0vzjvi1cg907zxzv6a25n9c7vydnnqny-gdb-13.1/bin:/nix/store/n0wyrb99dxinh0y6rjixmqdgvbm57fa6-mailutils-3.15/bin:/home/runner/workspace/.pythonlibs/bin:/nix/store/2lcqw1d28vklbk8ikiwad28iq2smwndv-python-wrapped-0.1.0/bin:/nix/store/2qx6jrl9h5hwpsry3ilbiihgmrnrvp50-pip-wrapper/bin:/nix/store/mx06wb8qp5z14na666bwnm1f2mxxwzj0-poetry-wrapper/bin:/nix/store/w4ml96z06nzljipl8hjlfvwg4pkxknyx-uv-0.5.11/bin:/nix/store/z11r6xq2v724w6fnp4hzm0ckb4phqfw6-pid1/bin:/nix/store/1yj5z39xwiram4s0dm1i2ldpw4cbmwa6-replit-runtime-path/bin:/home/runner/.nix-profile/bin:/home/runner/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\",\"PIP_CONFIG_FILE\":\"/nix/store/vrqxyrmqa494niiky8j389432lsxhylj-pip.conf\",\"PKG_CONFIG_PATH\":\"\",\"PKG_CONFIG_PATH_FOR_TARGET\":\"\",\"POETRY_CACHE_DIR\":\"/home/runner/workspace/.cache/pypoetry\",\"POETRY_CONFIG_DIR\":\"/nix/store/1fwfg40xn15b68lfai5hp18rn0hv6im1-poetry-config\",\"POETRY_DOWNLOAD_WITH_CURL\":\"1\",\"POETRY_INSTALLER_MODERN_INSTALLATION\":\"1\",\"POETRY_PIP_FROM_PATH\":\"1\",\"POETRY_PIP_NO_ISOLATE\":\"1\",\"POETRY_PIP_NO_PREFIX\":\"1\",\"POETRY_PIP_USE_PIP_CACHE\":\"1\",\"POETRY_USE_USER_SITE\":\"1\",\"POETRY_VIRTUALENVS_CREATE\":\"0\",\"PROMPT_DIRTRIM\":\"2\",\"PYTHONPATH\":\"/nix/store/zz7i75jb78idaz0rb1y1i4rzdyxq28vf-sitecustomize/lib/python/site-packages:/nix/store/yaps09f01jp3fd1405qlr0qz6haf6z03-python3.11-pip-25.0.1/lib/python3.11/site-packages\",\"PYTHONUSERBASE\":\"/home/runner/workspace/.pythonlibs\",\"REPLIT_BASHRC\":\"/nix/store/vgmsjrn5bbcdwgqxy27njrq6n8scc2nc-replit-bashrc/bashrc\",\"REPLIT_CLI\":\"/nix/store/r85pq90mjghd4lqrpr0d36b841s5c5yj-pid1-0.0.1/bin/replit\",\"REPLIT_CLUSTER\":\"worf\",\"REPLIT_CONTAINER\":\"gcr.io/marine-cycle-160323/repl-base:97433c06ca04972a1619dedd971cb3a93f4a5087\",\"REPLIT_DEV_DOMAIN\":\"cea58e98-c921-4596-a9e7-09bf69c331a7-00-3qexsmyzhs9de.worf.replit.dev\",\"REPLIT_DOMAINS\":\"cea58e98-c921-4596-a9e7-09bf69c331a7-00-3qexsmyzhs9de.worf.replit.dev\",\"REPLIT_ENVIRONMENT\":\"production\",\"REPLIT_KEEP_PACKAGE_DEV_DEPENDENCIES\":\"1\",\"REPLIT_LD_AUDIT\":\"/nix/store/6y0zqxaf220r36b74hwsq9m2b2av3lw7-replit_rtld_loader-1/rtld_loader.so\",\"REPLIT_LD_LIBRARY_PATH\":\"/nix/store/0vzjvi1cg907zxzv6a25n9c7vydnnqny-gdb-13.1/lib:/nix/store/n0wyrb99dxinh0y6rjixmqdgvbm57fa6-mailutils-3.15/lib:/nix/store/ljjvmhl1df2g4w43jspwgzx30zqyak07-mailutils-3.15-debug/lib:/nix/store/1cwwzfln8dbbr3hqkwqj2wx583ibk2i3-jq-1.6-lib/lib\",\"REPLIT_NIX_CHANNEL\":\"stable-23_05\",\"REPLIT_PID1_FLAG_PREEVALED_SYSPKGS\":\"1\",\"REPLIT_PID1_VERSION\":\"0.0.0-7c17c04\",\"REPLIT_PYTHONPATH\":\"/home/runner/workspace/.pythonlibs/lib/python3.11/site-packages:/nix/store/y0iwy8ma2m45pr23bw4r7lyz69v2b6wf-python3.11-setuptools-80.7.1/lib/python3.11/site-packages\",\"REPLIT_PYTHON_LD_LIBRARY_PATH\":\"/nix/store/gcg4y75zva03306ddxbv88s3xzyxmm90-cpplibs/lib:/nix/store/abch1r0gnbpikbp9n4x6mm8dwqwfrib6-zlib-1.3.1/lib:/nix/store/syzi2bpl8j599spgvs20xjkjzcw758as-glib-2.84.3/lib:/nix/store/pahwl2rq51dmwrn8czks27yy3sa3byg9-libX11-1.8.12/lib:/nix/store/844cgxkyzi1nrilvamxr08gs9l278gx9-libXext-1.3.6/lib:/nix/store/1ar5dnd5hqiyaxi6whj52vkd2bf0h7n8-libXinerama-1.1.5/lib:/nix/store/zgpm3jjfsfs1ljdzm1xjq502mkvxck3m-libXcursor-1.2.3/lib:/nix/store/yp7hylmnidn1mr91xsdn2dj5glhqmk7a-libXrandr-1.5.4/lib:/nix/store/474ia8w6cpw6vmhbsvbf5zx7bx5md4bv-libXi-1.8.2/lib:/nix/store/jf8lpgskxdh2v9jciy0ghqyf7lbggrxn-libXxf86vm-1.1.6/lib\",\"REPLIT_RIPPKGS_INDICES\":\"/nix/store/sic31kz2lvpf4v5wlaldrla7ivz4v2j2-rippkgs-indices\",\"REPLIT_RTLD_LOADER\":\"1\",\"REPLIT_SUBCLUSTER\":\"paid\",\"REPL_HOME\":\"/home/runner/workspace\",\"REPL_ID\":\"cea58e98-c921-4596-a9e7-09bf69c331a7\",\"REPL_IMAGE\":\"gcr.io/marine-cycle-160323/nix:bf8590a3e2f0a8b70b7ca175eeed9074dffbfca9\",\"REPL_LANGUAGE\":\"nix\",\"REPL_OWNER\":\"daviserra3\",\"REPL_OWNER_ID\":\"40913427\",\"REPL_PUBKEYS\":\"{\\\"crosis-ci\\\":\\\"7YlpcYh82oR9NSTtSYtR5jDL4onNzCGJGq6b+9CuZII=\\\",\\\"crosis-ci:1\\\":\\\"7YlpcYh82oR9NSTtSYtR5jDL4onNzCGJGq6b+9CuZII=\\\",\\\"crosis-ci:latest\\\":\\\"7YlpcYh82oR9NSTtSYtR5jDL4onNzCGJGq6b+9CuZII=\\\",\\\"prod\\\":\\\"tGsjlu/BJvWTgvMaX7acuUb7AO1dXOrRiuk7y083RFE=\\\",\\\"prod:1\\\":\\\"tGsjlu/BJvWTgvMaX7acuUb7AO1dXOrRiuk7y083RFE=\\\",\\\"prod:3\\\":\\\"9+MCOSHQSQlcodXoot8dC8NLhc862nLkx1/VMsbY2h8=\\\",\\\"prod:4\\\":\\\"8uGN+vfszlnV93/HCSHlVLG0xddMlPkir1Ni4JKT4+w=\\\",\\\"prod:5\\\":\\\"9+MCOSHQSQlcodXoot8dC8NLhc862nLkx1/VMsbY2h8=\\\",\\\"prod:latest\\\":\\\"tGsjlu/BJvWTgvMaX7acuUb7AO1dXOrRiuk7y083RFE=\\\",\\\"vault-goval-token\\\":\\\"D5jJoMx1Ml54HM92NLgXl+MzptwDqbSsfyFG6f52g9E=\\\",\\\"vault-goval-token:1\\\":\\\"D5jJoMx1Ml54HM92NLgXl+MzptwDqbSsfyFG6f52g9E=\\\",\\\"vault-goval-token:latest\\\":\\\"D5jJoMx1Ml54HM92NLgXl+MzptwDqbSsfyFG6f52g9E=\\\"}\",\"REPL_SLUG\":\"workspace\",\"USER\":\"runner\",\"UV_PROJECT_ENVIRONMENT\":\"/home/runner/workspace/.pythonlibs\",\"UV_PYTHON_DOWNLOADS\":\"never\",\"UV_PYTHON_PREFERENCE\":\"only-system\",\"XDG_CACHE_HOME\":\"/home/runner/workspace/.cache\",\"XDG_CONFIG_HOME\":\"/home/runner/workspace/.config\",\"XDG_DATA_DIRS\":\"/nix/store/d8i311x3gvckpmfxraslgspkmydy91d0-jq-1.6-man/share:/nix/store/wsw1naq191c7alsp7n2lz4yriq3742wi-jq-1.6-doc/share:/nix/store/0vzjvi1cg907zxzv6a25n9c7vydnnqny-gdb-13.1/share:/nix/store/n0wyrb99dxinh0y6rjixmqdgvbm57fa6-mailutils-3.15/share:/nix/store/1yj5z39xwiram4s0dm1i2ldpw4cbmwa6-replit-runtime-path/share\",\"XDG_DATA_HOME\":\"/home/runner/workspace/.local/share\",\"__EGL_VENDOR_LIBRARY_FILENAMES\":\"/nix/store/cpwib3zazj49fm0y04y53w4xkbqsgrgm-mesa-25.0.7/share/glvnd/egl_vendor.d/50_mesa.json\"}}"},{"path":".cache/replit/nix/dotreplitenv.json","size":2001,"sha1":"ecf0bb12dc2e1a81d23d45ad7ee2b8dc10385d03","mtime":1755075896,"is_binary":false,"encoding":"utf-8","content":"{\"channel\":\"stable-23_05\",\"channel_nix_path\":\"/nix/store/k7hndz8n8k6qkr5w46vwm3zcpzxb98rz-nixpkgs-stable-23_05-23.05.tar.gz/nixpkgs-stable-23_05\",\"env\":{\"CFLAGS\":\"-isystem /nix/store/l7jsfn0p3vyw67xbjarwjfkp22nvixnz-jq-1.6-dev/include -isystem /nix/store/0vzjvi1cg907zxzv6a25n9c7vydnnqny-gdb-13.1/include -isystem /nix/store/n0wyrb99dxinh0y6rjixmqdgvbm57fa6-mailutils-3.15/include\",\"GI_TYPELIB_PATH\":\"\",\"LDFLAGS\":\"-L/nix/store/0vzjvi1cg907zxzv6a25n9c7vydnnqny-gdb-13.1/lib -L/nix/store/n0wyrb99dxinh0y6rjixmqdgvbm57fa6-mailutils-3.15/lib -L/nix/store/ljjvmhl1df2g4w43jspwgzx30zqyak07-mailutils-3.15-debug/lib -L/nix/store/1cwwzfln8dbbr3hqkwqj2wx583ibk2i3-jq-1.6-lib/lib\",\"NIX_CFLAGS_COMPILE\":\"-isystem /nix/store/l7jsfn0p3vyw67xbjarwjfkp22nvixnz-jq-1.6-dev/include -isystem /nix/store/0vzjvi1cg907zxzv6a25n9c7vydnnqny-gdb-13.1/include -isystem /nix/store/n0wyrb99dxinh0y6rjixmqdgvbm57fa6-mailutils-3.15/include\",\"NIX_LDFLAGS\":\"-L/nix/store/0vzjvi1cg907zxzv6a25n9c7vydnnqny-gdb-13.1/lib -L/nix/store/n0wyrb99dxinh0y6rjixmqdgvbm57fa6-mailutils-3.15/lib -L/nix/store/ljjvmhl1df2g4w43jspwgzx30zqyak07-mailutils-3.15-debug/lib -L/nix/store/1cwwzfln8dbbr3hqkwqj2wx583ibk2i3-jq-1.6-lib/lib\",\"PATH\":\"/nix/store/njsp9a4bq3m6l3dzx4hfgbrfn1yzvm2p-jq-1.6-bin/bin:/nix/store/0vzjvi1cg907zxzv6a25n9c7vydnnqny-gdb-13.1/bin:/nix/store/n0wyrb99dxinh0y6rjixmqdgvbm57fa6-mailutils-3.15/bin\",\"PKG_CONFIG_PATH\":\"\",\"PKG_CONFIG_PATH_FOR_TARGET\":\"\",\"REPLIT_LD_LIBRARY_PATH\":\"/nix/store/0vzjvi1cg907zxzv6a25n9c7vydnnqny-gdb-13.1/lib:/nix/store/n0wyrb99dxinh0y6rjixmqdgvbm57fa6-mailutils-3.15/lib:/nix/store/ljjvmhl1df2g4w43jspwgzx30zqyak07-mailutils-3.15-debug/lib:/nix/store/1cwwzfln8dbbr3hqkwqj2wx583ibk2i3-jq-1.6-lib/lib\",\"XDG_DATA_DIRS\":\"/nix/store/d8i311x3gvckpmfxraslgspkmydy91d0-jq-1.6-man/share:/nix/store/wsw1naq191c7alsp7n2lz4yriq3742wi-jq-1.6-doc/share:/nix/store/0vzjvi1cg907zxzv6a25n9c7vydnnqny-gdb-13.1/share:/nix/store/n0wyrb99dxinh0y6rjixmqdgvbm57fa6-mailutils-3.15/share\"},\"packages\":[\"gdb\",\"mailutils\",\"jq\"]}"},{"path":".cache/replit/nix/env.json","size":6811,"sha1":"407356a47bf13d48ebff66771ced3d4eba58986c","mtime":1693585871,"is_binary":false,"encoding":"utf-8","content":"{\"entries\":{\"replit.nix\":{\"env\":{\"AR\":\"ar\",\"AS\":\"as\",\"CC\":\"gcc\",\"CONFIG_SHELL\":\"/nix/store/dsd5gz46hdbdk2rfdimqddhq6m8m8fqs-bash-5.1-p16/bin/bash\",\"CXX\":\"g++\",\"DETERMINISTIC_BUILD\":\"1\",\"HOST_PATH\":\"/nix/store/hd4cc9rh83j291r5539hkf6qd8lgiikb-python3-3.10.8/bin:/nix/store/8i7xcp77pg9yiv7qz8y5lbrnb94sy0m8-prybar-python310-0.0.0-dirty/bin:/nix/store/z948zad3l3myac6s5wgrf1gvdmgk819m-stderred-0.1.0/bin:/nix/store/a7gvj343m05j2s32xcnwr35v31ynlypr-coreutils-9.1/bin:/nix/store/mydc6f4k2z73xlcz7ilif3v2lcaiqvza-findutils-4.9.0/bin:/nix/store/j9p3g8472iijd50vhdprx0nmk2fqn5gv-diffutils-3.8/bin:/nix/store/89zs7rms6x00xfq4dq6m7mjnhkr8a6r4-gnused-4.8/bin:/nix/store/86bp03jkmsl6f92w0yzg4s59g5mhxwmy-gnugrep-3.7/bin:/nix/store/hwcdqw4jrjnd37wxqgsd47hd0j8bnj09-gawk-5.1.1/bin:/nix/store/cfbhw8r8ags41vwqaz47r583d0p4h4a1-gnutar-1.34/bin:/nix/store/p3m1ndl1lapwrlh698bnb5lvvxh67378-gzip-1.12/bin:/nix/store/a8mhcagrsly7c7mpjrpsnaahk4aax056-bzip2-1.0.8-bin/bin:/nix/store/mblgz65m3zv9x548a3d5m96fj2pbwr09-gnumake-4.3/bin:/nix/store/dsd5gz46hdbdk2rfdimqddhq6m8m8fqs-bash-5.1-p16/bin:/nix/store/v7ljksji50mg3w61dykaa3n3y79n6nil-patch-2.7.6/bin:/nix/store/zlcnmqq14jz5x9439jf937mvayyl63da-xz-5.2.7-bin/bin:/nix/store/y6aj732zm9m87c82fpvf103a1xb22blp-file-5.43/bin\",\"LANG\":\"en_US.UTF-8\",\"LD\":\"ld\",\"LOCALE_ARCHIVE\":\"/usr/lib/locale/locale-archive\",\"NIX_BINTOOLS\":\"/nix/store/1d6ian3r8kdzspw8hacjhl3xkp40g1lj-binutils-wrapper-2.39\",\"NIX_BINTOOLS_WRAPPER_TARGET_HOST_x86_64_unknown_linux_gnu\":\"1\",\"NIX_BUILD_CORES\":\"16\",\"NIX_BUILD_TOP\":\"/tmp\",\"NIX_CC\":\"/nix/store/dq0xwmsk1g0i2ayg6pb7y87na2knzylh-gcc-wrapper-11.3.0\",\"NIX_CC_WRAPPER_TARGET_HOST_x86_64_unknown_linux_gnu\":\"1\",\"NIX_CFLAGS_COMPILE\":\" -frandom-seed=mp64sk8hgd -isystem /nix/store/hd4cc9rh83j291r5539hkf6qd8lgiikb-python3-3.10.8/include -isystem /nix/store/hd4cc9rh83j291r5539hkf6qd8lgiikb-python3-3.10.8/include\",\"NIX_ENFORCE_NO_NATIVE\":\"1\",\"NIX_HARDENING_ENABLE\":\"fortify stackprotector pic strictoverflow format relro bindnow\",\"NIX_INDENT_MAKE\":\"1\",\"NIX_LDFLAGS\":\"-rpath /nix/store/mp64sk8hgdw9232lc63dxfpjp3s1d0fj-nix-shell/lib64 -rpath /nix/store/mp64sk8hgdw9232lc63dxfpjp3s1d0fj-nix-shell/lib  -L/nix/store/hd4cc9rh83j291r5539hkf6qd8lgiikb-python3-3.10.8/lib -L/nix/store/hd4cc9rh83j291r5539hkf6qd8lgiikb-python3-3.10.8/lib\",\"NIX_STORE\":\"/nix/store\",\"NM\":\"nm\",\"OBJCOPY\":\"objcopy\",\"OBJDUMP\":\"objdump\",\"PATH\":\"/nix/store/bap4d0lpcrhpwmpb8ayjcgkmvfj62lnq-bash-interactive-5.1-p16/bin:/nix/store/pr5n59mb4jzmfx6kanwxly0l07p861fg-patchelf-0.15.0/bin:/nix/store/dq0xwmsk1g0i2ayg6pb7y87na2knzylh-gcc-wrapper-11.3.0/bin:/nix/store/1gf2flfqnpqbr1b4p4qz2f72y42bs56r-gcc-11.3.0/bin:/nix/store/57xv61c5zi8pphjbcwxxjlgc34p61ic9-glibc-2.35-163-bin/bin:/nix/store/a7gvj343m05j2s32xcnwr35v31ynlypr-coreutils-9.1/bin:/nix/store/1d6ian3r8kdzspw8hacjhl3xkp40g1lj-binutils-wrapper-2.39/bin:/nix/store/039g378vc3pc3dvi9dzdlrd0i4q93qwf-binutils-2.39/bin:/nix/store/hd4cc9rh83j291r5539hkf6qd8lgiikb-python3-3.10.8/bin:/nix/store/8i7xcp77pg9yiv7qz8y5lbrnb94sy0m8-prybar-python310-0.0.0-dirty/bin:/nix/store/z948zad3l3myac6s5wgrf1gvdmgk819m-stderred-0.1.0/bin:/nix/store/a7gvj343m05j2s32xcnwr35v31ynlypr-coreutils-9.1/bin:/nix/store/mydc6f4k2z73xlcz7ilif3v2lcaiqvza-findutils-4.9.0/bin:/nix/store/j9p3g8472iijd50vhdprx0nmk2fqn5gv-diffutils-3.8/bin:/nix/store/89zs7rms6x00xfq4dq6m7mjnhkr8a6r4-gnused-4.8/bin:/nix/store/86bp03jkmsl6f92w0yzg4s59g5mhxwmy-gnugrep-3.7/bin:/nix/store/hwcdqw4jrjnd37wxqgsd47hd0j8bnj09-gawk-5.1.1/bin:/nix/store/cfbhw8r8ags41vwqaz47r583d0p4h4a1-gnutar-1.34/bin:/nix/store/p3m1ndl1lapwrlh698bnb5lvvxh67378-gzip-1.12/bin:/nix/store/a8mhcagrsly7c7mpjrpsnaahk4aax056-bzip2-1.0.8-bin/bin:/nix/store/mblgz65m3zv9x548a3d5m96fj2pbwr09-gnumake-4.3/bin:/nix/store/dsd5gz46hdbdk2rfdimqddhq6m8m8fqs-bash-5.1-p16/bin:/nix/store/v7ljksji50mg3w61dykaa3n3y79n6nil-patch-2.7.6/bin:/nix/store/zlcnmqq14jz5x9439jf937mvayyl63da-xz-5.2.7-bin/bin:/nix/store/y6aj732zm9m87c82fpvf103a1xb22blp-file-5.43/bin\",\"PRYBAR_PYTHON_BIN\":\"/nix/store/8i7xcp77pg9yiv7qz8y5lbrnb94sy0m8-prybar-python310-0.0.0-dirty/bin/prybar-python310\",\"PYTHONBIN\":\"/nix/store/hd4cc9rh83j291r5539hkf6qd8lgiikb-python3-3.10.8/bin/python3.10\",\"PYTHONHASHSEED\":\"0\",\"PYTHONHOME\":\"/nix/store/hd4cc9rh83j291r5539hkf6qd8lgiikb-python3-3.10.8\",\"PYTHONNOUSERSITE\":\"1\",\"PYTHONPATH\":\"/nix/store/hd4cc9rh83j291r5539hkf6qd8lgiikb-python3-3.10.8/lib/python3.10/site-packages\",\"PYTHON_LD_LIBRARY_PATH\":\"/nix/store/mdck89nsfisflwjv6xv8ydj7dj0sj2pn-gcc-11.3.0-lib/lib:/nix/store/026hln0aq1hyshaxsdvhg0kmcm6yf45r-zlib-1.2.13/lib:/nix/store/2k366jrbsra97gjfxwvrhvixjfxdach5-glib-2.74.1/lib:/nix/store/w3zzhfl4a7xp0xfflz2gawv02y8ba9z8-libX11-1.8.1/lib\",\"RANLIB\":\"ranlib\",\"READELF\":\"readelf\",\"SIZE\":\"size\",\"SOURCE_DATE_EPOCH\":\"315532800\",\"STDERREDBIN\":\"/nix/store/z948zad3l3myac6s5wgrf1gvdmgk819m-stderred-0.1.0/bin/stderred\",\"STRINGS\":\"strings\",\"STRIP\":\"strip\",\"XDG_DATA_DIRS\":\"/nix/store/pr5n59mb4jzmfx6kanwxly0l07p861fg-patchelf-0.15.0/share\",\"_\":\"/nix/store/a7gvj343m05j2s32xcnwr35v31ynlypr-coreutils-9.1/bin/env\",\"_PYTHON_HOST_PLATFORM\":\"linux-x86_64\",\"_PYTHON_SYSCONFIGDATA_NAME\":\"_sysconfigdata__linux_x86_64-linux-gnu\",\"__ETC_PROFILE_SOURCED\":\"1\",\"buildInputs\":\"/nix/store/hd4cc9rh83j291r5539hkf6qd8lgiikb-python3-3.10.8 /nix/store/8i7xcp77pg9yiv7qz8y5lbrnb94sy0m8-prybar-python310-0.0.0-dirty /nix/store/z948zad3l3myac6s5wgrf1gvdmgk819m-stderred-0.1.0\",\"buildPhase\":\"echo \\\"------------------------------------------------------------\\\" \\u003e\\u003e$out\\necho \\\" WARNING: the existence of this path is not guaranteed.\\\" \\u003e\\u003e$out\\necho \\\" It is an internal implementation detail for pkgs.mkShell.\\\"   \\u003e\\u003e$out\\necho \\\"------------------------------------------------------------\\\" \\u003e\\u003e$out\\necho \\u003e\\u003e $out\\n# Record all build inputs as runtime dependencies\\nexport \\u003e\\u003e $out\\n\",\"builder\":\"/nix/store/dsd5gz46hdbdk2rfdimqddhq6m8m8fqs-bash-5.1-p16/bin/bash\",\"cmakeFlags\":\"\",\"configureFlags\":\"\",\"depsBuildBuild\":\"\",\"depsBuildBuildPropagated\":\"\",\"depsBuildTarget\":\"\",\"depsBuildTargetPropagated\":\"\",\"depsHostHost\":\"\",\"depsHostHostPropagated\":\"\",\"depsTargetTarget\":\"\",\"depsTargetTargetPropagated\":\"\",\"doCheck\":\"\",\"doInstallCheck\":\"\",\"mesonFlags\":\"\",\"name\":\"nix-shell\",\"nativeBuildInputs\":\"\",\"out\":\"/nix/store/mp64sk8hgdw9232lc63dxfpjp3s1d0fj-nix-shell\",\"outputs\":\"out\",\"patches\":\"\",\"phases\":\"buildPhase\",\"propagatedBuildInputs\":\"\",\"propagatedNativeBuildInputs\":\"\",\"shell\":\"/nix/store/dsd5gz46hdbdk2rfdimqddhq6m8m8fqs-bash-5.1-p16/bin/bash\",\"shellHook\":\"\",\"stdenv\":\"/nix/store/kmfaajdpyyyg319vfqni5jm9wkxjmf73-stdenv-linux\",\"strictDeps\":\"\",\"system\":\"x86_64-linux\"},\"dependencies\":[{\"path\":\"replit.nix\",\"mod_time\":\"2023-02-16T01:29:45.090880113Z\"}],\"channel\":\"stable-22_11\",\"channel_nix_path\":\"/nix/store/g2zwsf4i26pngib77ask4m00n9pj2a03-nixpkgs-stable-22_11-22.11.tar.gz/nixpkgs-stable-22_11\"}}}"},{"path":".cache/replit/toolchain.json","size":1572,"sha1":"89ac808857eca105dcab7d6dc8185f2f5b6afaae","mtime":1755440703,"is_binary":false,"encoding":"utf-8","content":"{\"entrypoint\":\"main.py\", \"runs\":[{\"id\":\"module:python-3.11/runner:python\", \"name\":\"Python 3.11\", \"fileParam\":true, \"language\":\"python3\", \"fileTypeAttrs\":{}, \"displayVersion\":\"3.11.13\", \"run\":{\"command\":{\"args\":[\"sh\", \"-c\", \"/nix/store/cn7gx98bvd6qj3jrh5v21q868nlv2jv6-python3-wrapper/bin/python3 $file\"]}}, \"defaultEntrypoints\":[\"main.py\", \"app.py\", \"run.py\"]}], \"debuggers\":[{\"id\":\"module:python-3.11/debugger:dapPython\", \"name\":\"debugpy\", \"fileParam\":true, \"language\":\"python3\", \"fileTypeAttrs\":{}, \"displayVersion\":\"1.8.14\"}], \"languageServers\":[{\"id\":\"module:python-3.11/languageServer:pyright-extended\", \"name\":\"pyright-extended\", \"language\":\"python3\", \"fileTypeAttrs\":{}, \"config\":{\"startCommand\":{\"args\":[\"sh\", \"-c\", \"/nix/store/sgk3xs1ixrkrqpz6qqmnvmrwhspsk1i5-pyright-extended-2.0.13/bin/langserver.index.js --stdio\"]}}, \"displayVersion\":\"2.0.13\"}, {\"id\":\"module:replit/languageServer:dotreplit-lsp\", \"name\":\".replit LSP\", \"language\":\"dotreplit\", \"fileTypeAttrs\":{}, \"config\":{\"startCommand\":{\"args\":[\"sh\", \"-c\", \"/nix/store/25rasmkzxdlxf3w5rasswn4ypm7ibw9l-taplo-0.patched/bin/taplo lsp -c /nix/store/5ill1byfvfxm5mdi58w30rq5rmmxa2xv-taplo-config.toml stdio\"]}}}], \"packagers\":[{\"id\":\"module:python-3.11/packager:upmPython\", \"name\":\"Python packager\", \"language\":\"python3\", \"packageSearch\":true, \"guessImports\":true}], \"formatters\":[{\"id\":\"module:python-3.11/languageServer:pyright-extended\", \"name\":\"pyright-extended\", \"fileTypeAttrs\":{}, \"displayVersion\":\"2.0.13\"}, {\"id\":\"module:replit/languageServer:dotreplit-lsp\", \"name\":\".replit LSP\", \"fileTypeAttrs\":{}}]}"},{"path":".local/share/virtualenv/py_info/2/c50344fdb641c907086db860b4dc09d5da3e9415f6de454139296c6d6bfa37d7.json","size":3544,"sha1":"7f53bfa57778e8958d4239e45587312f5b186e6b","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"{\n  \"content\": {\n    \"_creators\": null,\n    \"architecture\": 64,\n    \"base_exec_prefix\": \"/nix/store/7d088dip86hlzri9sk0h78b63yfmx0a0-python3-3.11.13\",\n    \"base_prefix\": \"/nix/store/7d088dip86hlzri9sk0h78b63yfmx0a0-python3-3.11.13\",\n    \"distutils_install\": {},\n    \"exec_prefix\": \"/nix/store/7d088dip86hlzri9sk0h78b63yfmx0a0-python3-3.11.13\",\n    \"executable\": \"/nix/store/7d088dip86hlzri9sk0h78b63yfmx0a0-python3-3.11.13/bin/python3.11\",\n    \"file_system_encoding\": \"utf-8\",\n    \"free_threaded\": false,\n    \"has_venv\": true,\n    \"implementation\": \"CPython\",\n    \"max_size\": 9223372036854775807,\n    \"original_executable\": \"/nix/store/7d088dip86hlzri9sk0h78b63yfmx0a0-python3-3.11.13/bin/python3.11\",\n    \"os\": \"posix\",\n    \"path\": [\n      \"/nix/store/qkd5csfa32wmnfhgydfbb4lfdqsb87zg-poetry-in-venv/env/lib/python3.11/site-packages/virtualenv/discovery\",\n      \"/nix/store/zz7i75jb78idaz0rb1y1i4rzdyxq28vf-sitecustomize/lib/python/site-packages\",\n      \"/nix/store/yaps09f01jp3fd1405qlr0qz6haf6z03-python3.11-pip-25.0.1/lib/python3.11/site-packages\",\n      \"/nix/store/7d088dip86hlzri9sk0h78b63yfmx0a0-python3-3.11.13/lib/python311.zip\",\n      \"/nix/store/7d088dip86hlzri9sk0h78b63yfmx0a0-python3-3.11.13/lib/python3.11\",\n      \"/nix/store/7d088dip86hlzri9sk0h78b63yfmx0a0-python3-3.11.13/lib/python3.11/lib-dynload\",\n      \"/home/runner/workspace/.pythonlibs/lib/python3.11/site-packages\",\n      \"/nix/store/7d088dip86hlzri9sk0h78b63yfmx0a0-python3-3.11.13/lib/python3.11/site-packages\",\n      \"/home/runner/workspace/.pythonlibs/lib/python3.11/site-packages\",\n      \"/nix/store/y0iwy8ma2m45pr23bw4r7lyz69v2b6wf-python3.11-setuptools-80.7.1/lib/python3.11/site-packages\"\n    ],\n    \"platform\": \"linux\",\n    \"prefix\": \"/nix/store/7d088dip86hlzri9sk0h78b63yfmx0a0-python3-3.11.13\",\n    \"real_prefix\": null,\n    \"stdout_encoding\": \"utf-8\",\n    \"sysconfig\": {\n      \"makefile_filename\": \"/nix/store/7d088dip86hlzri9sk0h78b63yfmx0a0-python3-3.11.13/lib/python3.11/config-3.11-x86_64-linux-gnu/Makefile\"\n    },\n    \"sysconfig_paths\": {\n      \"data\": \"{base}\",\n      \"include\": \"{installed_base}/include/python{py_version_short}{abiflags}\",\n      \"platlib\": \"{platbase}/{platlibdir}/python{py_version_short}/site-packages\",\n      \"platstdlib\": \"{platbase}/{platlibdir}/python{py_version_short}\",\n      \"purelib\": \"{base}/lib/python{py_version_short}/site-packages\",\n      \"scripts\": \"{base}/bin\",\n      \"stdlib\": \"{installed_base}/{platlibdir}/python{py_version_short}\"\n    },\n    \"sysconfig_scheme\": \"venv\",\n    \"sysconfig_vars\": {\n      \"PYTHONFRAMEWORK\": \"\",\n      \"abiflags\": \"\",\n      \"base\": \"/nix/store/7d088dip86hlzri9sk0h78b63yfmx0a0-python3-3.11.13\",\n      \"installed_base\": \"/nix/store/7d088dip86hlzri9sk0h78b63yfmx0a0-python3-3.11.13\",\n      \"platbase\": \"/nix/store/7d088dip86hlzri9sk0h78b63yfmx0a0-python3-3.11.13\",\n      \"platlibdir\": \"lib\",\n      \"py_version_short\": \"3.11\"\n    },\n    \"system_executable\": \"/nix/store/7d088dip86hlzri9sk0h78b63yfmx0a0-python3-3.11.13/bin/python3.11\",\n    \"system_stdlib\": \"/nix/store/7d088dip86hlzri9sk0h78b63yfmx0a0-python3-3.11.13/lib/python3.11\",\n    \"system_stdlib_platform\": \"/nix/store/7d088dip86hlzri9sk0h78b63yfmx0a0-python3-3.11.13/lib/python3.11\",\n    \"version\": \"3.11.13 (main, Jun  3 2025, 18:38:25) [GCC 14.3.0]\",\n    \"version_info\": {\n      \"major\": 3,\n      \"micro\": 13,\n      \"minor\": 11,\n      \"releaselevel\": \"final\",\n      \"serial\": 0\n    },\n    \"version_nodot\": \"311\"\n  },\n  \"path\": \"/nix/store/7d088dip86hlzri9sk0h78b63yfmx0a0-python3-3.11.13/bin/python3.11\",\n  \"st_mtime\": 1.0\n}"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/__init__.py","size":357,"sha1":"f29f600700cc1e93d400aea45137949d6c093197","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"from typing import List, Optional\n\n__version__ = \"25.0.1\"\n\n\ndef main(args: Optional[List[str]] = None) -> int:\n    \"\"\"This is an internal API only meant for use by pip's own console scripts.\n\n    For additional details, see https://github.com/pypa/pip/issues/7498.\n    \"\"\"\n    from pip._internal.utils.entrypoints import _wrapper\n\n    return _wrapper(args)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/__main__.py","size":854,"sha1":"4f4087af34a52c3c155ea0274de2e4dfec45d431","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import os\nimport sys\n\n# Remove '' and current working directory from the first entry\n# of sys.path, if present to avoid using current directory\n# in pip commands check, freeze, install, list and show,\n# when invoked as python -m pip <command>\nif sys.path[0] in (\"\", os.getcwd()):\n    sys.path.pop(0)\n\n# If we are running from a wheel, add the wheel to sys.path\n# This allows the usage python pip-*.whl/pip install pip-*.whl\nif __package__ == \"\":\n    # __file__ is pip-*.whl/pip/__main__.py\n    # first dirname call strips of '/__main__.py', second strips off '/pip'\n    # Resulting path is the name of the wheel itself\n    # Add that to sys.path so we can import pip\n    path = os.path.dirname(os.path.dirname(__file__))\n    sys.path.insert(0, path)\n\nif __name__ == \"__main__\":\n    from pip._internal.cli.main import main as _main\n\n    sys.exit(_main())\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/__pip-runner__.py","size":1450,"sha1":"179f79578e4fb966fec56c8893ca632fc1ca32b1","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"Execute exactly this copy of pip, within a different environment.\n\nThis file is named as it is, to ensure that this module can't be imported via\nan import statement.\n\"\"\"\n\n# /!\\ This version compatibility check section must be Python 2 compatible. /!\\\n\nimport sys\n\n# Copied from pyproject.toml\nPYTHON_REQUIRES = (3, 8)\n\n\ndef version_str(version):  # type: ignore\n    return \".\".join(str(v) for v in version)\n\n\nif sys.version_info[:2] < PYTHON_REQUIRES:\n    raise SystemExit(\n        \"This version of pip does not support python {} (requires >={}).\".format(\n            version_str(sys.version_info[:2]), version_str(PYTHON_REQUIRES)\n        )\n    )\n\n# From here on, we can use Python 3 features, but the syntax must remain\n# Python 2 compatible.\n\nimport runpy  # noqa: E402\nfrom importlib.machinery import PathFinder  # noqa: E402\nfrom os.path import dirname  # noqa: E402\n\nPIP_SOURCES_ROOT = dirname(dirname(__file__))\n\n\nclass PipImportRedirectingFinder:\n    @classmethod\n    def find_spec(self, fullname, path=None, target=None):  # type: ignore\n        if fullname != \"pip\":\n            return None\n\n        spec = PathFinder.find_spec(fullname, [PIP_SOURCES_ROOT], target)\n        assert spec, (PIP_SOURCES_ROOT, fullname)\n        return spec\n\n\nsys.meta_path.insert(0, PipImportRedirectingFinder())\n\nassert __name__ == \"__main__\", \"Cannot run __pip-runner__.py as a non-main module\"\nrunpy.run_module(\"pip\", run_name=\"__main__\", alter_sys=True)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/__init__.py","size":513,"sha1":"c693fd7c867f18949246675b26669dcdd338481f","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"from typing import List, Optional\n\nfrom pip._internal.utils import _log\n\n# init_logging() must be called before any call to logging.getLogger()\n# which happens at import of most modules.\n_log.init_logging()\n\n\ndef main(args: Optional[List[str]] = None) -> int:\n    \"\"\"This is preserved for old console scripts that may still be referencing\n    it.\n\n    For additional details, see https://github.com/pypa/pip/issues/7498.\n    \"\"\"\n    from pip._internal.utils.entrypoints import _wrapper\n\n    return _wrapper(args)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/build_env.py","size":10700,"sha1":"fc5fb9fb0bcb86b857af0fab35cf63fc68980102","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"Build Environment used for isolation during sdist building\n\"\"\"\n\nimport logging\nimport os\nimport pathlib\nimport site\nimport sys\nimport textwrap\nfrom collections import OrderedDict\nfrom types import TracebackType\nfrom typing import TYPE_CHECKING, Iterable, List, Optional, Set, Tuple, Type, Union\n\nfrom pip._vendor.packaging.version import Version\n\nfrom pip import __file__ as pip_location\nfrom pip._internal.cli.spinners import open_spinner\nfrom pip._internal.locations import get_platlib, get_purelib, get_scheme\nfrom pip._internal.metadata import get_default_environment, get_environment\nfrom pip._internal.utils.logging import VERBOSE\nfrom pip._internal.utils.packaging import get_requirement\nfrom pip._internal.utils.subprocess import call_subprocess\nfrom pip._internal.utils.temp_dir import TempDirectory, tempdir_kinds\n\nif TYPE_CHECKING:\n    from pip._internal.index.package_finder import PackageFinder\n\nlogger = logging.getLogger(__name__)\n\n\ndef _dedup(a: str, b: str) -> Union[Tuple[str], Tuple[str, str]]:\n    return (a, b) if a != b else (a,)\n\n\nclass _Prefix:\n    def __init__(self, path: str) -> None:\n        self.path = path\n        self.setup = False\n        scheme = get_scheme(\"\", prefix=path)\n        self.bin_dir = scheme.scripts\n        self.lib_dirs = _dedup(scheme.purelib, scheme.platlib)\n\n\ndef get_runnable_pip() -> str:\n    \"\"\"Get a file to pass to a Python executable, to run the currently-running pip.\n\n    This is used to run a pip subprocess, for installing requirements into the build\n    environment.\n    \"\"\"\n    source = pathlib.Path(pip_location).resolve().parent\n\n    if not source.is_dir():\n        # This would happen if someone is using pip from inside a zip file. In that\n        # case, we can use that directly.\n        return str(source)\n\n    return os.fsdecode(source / \"__pip-runner__.py\")\n\n\ndef _get_system_sitepackages() -> Set[str]:\n    \"\"\"Get system site packages\n\n    Usually from site.getsitepackages,\n    but fallback on `get_purelib()/get_platlib()` if unavailable\n    (e.g. in a virtualenv created by virtualenv<20)\n\n    Returns normalized set of strings.\n    \"\"\"\n    if hasattr(site, \"getsitepackages\"):\n        system_sites = site.getsitepackages()\n    else:\n        # virtualenv < 20 overwrites site.py without getsitepackages\n        # fallback on get_purelib/get_platlib.\n        # this is known to miss things, but shouldn't in the cases\n        # where getsitepackages() has been removed (inside a virtualenv)\n        system_sites = [get_purelib(), get_platlib()]\n    return {os.path.normcase(path) for path in system_sites}\n\n\nclass BuildEnvironment:\n    \"\"\"Creates and manages an isolated environment to install build deps\"\"\"\n\n    def __init__(self) -> None:\n        temp_dir = TempDirectory(kind=tempdir_kinds.BUILD_ENV, globally_managed=True)\n\n        self._prefixes = OrderedDict(\n            (name, _Prefix(os.path.join(temp_dir.path, name)))\n            for name in (\"normal\", \"overlay\")\n        )\n\n        self._bin_dirs: List[str] = []\n        self._lib_dirs: List[str] = []\n        for prefix in reversed(list(self._prefixes.values())):\n            self._bin_dirs.append(prefix.bin_dir)\n            self._lib_dirs.extend(prefix.lib_dirs)\n\n        # Customize site to:\n        # - ensure .pth files are honored\n        # - prevent access to system site packages\n        system_sites = _get_system_sitepackages()\n\n        self._site_dir = os.path.join(temp_dir.path, \"site\")\n        if not os.path.exists(self._site_dir):\n            os.mkdir(self._site_dir)\n        with open(\n            os.path.join(self._site_dir, \"sitecustomize.py\"), \"w\", encoding=\"utf-8\"\n        ) as fp:\n            fp.write(\n                textwrap.dedent(\n                    \"\"\"\n                import os, site, sys\n\n                # First, drop system-sites related paths.\n                original_sys_path = sys.path[:]\n                known_paths = set()\n                for path in {system_sites!r}:\n                    site.addsitedir(path, known_paths=known_paths)\n                system_paths = set(\n                    os.path.normcase(path)\n                    for path in sys.path[len(original_sys_path):]\n                )\n                original_sys_path = [\n                    path for path in original_sys_path\n                    if os.path.normcase(path) not in system_paths\n                ]\n                sys.path = original_sys_path\n\n                # Second, add lib directories.\n                # ensuring .pth file are processed.\n                for path in {lib_dirs!r}:\n                    assert not path in sys.path\n                    site.addsitedir(path)\n                \"\"\"\n                ).format(system_sites=system_sites, lib_dirs=self._lib_dirs)\n            )\n\n    def __enter__(self) -> None:\n        self._save_env = {\n            name: os.environ.get(name, None)\n            for name in (\"PATH\", \"PYTHONNOUSERSITE\", \"PYTHONPATH\")\n        }\n\n        path = self._bin_dirs[:]\n        old_path = self._save_env[\"PATH\"]\n        if old_path:\n            path.extend(old_path.split(os.pathsep))\n\n        pythonpath = [self._site_dir]\n\n        os.environ.update(\n            {\n                \"PATH\": os.pathsep.join(path),\n                \"PYTHONNOUSERSITE\": \"1\",\n                \"PYTHONPATH\": os.pathsep.join(pythonpath),\n            }\n        )\n\n    def __exit__(\n        self,\n        exc_type: Optional[Type[BaseException]],\n        exc_val: Optional[BaseException],\n        exc_tb: Optional[TracebackType],\n    ) -> None:\n        for varname, old_value in self._save_env.items():\n            if old_value is None:\n                os.environ.pop(varname, None)\n            else:\n                os.environ[varname] = old_value\n\n    def check_requirements(\n        self, reqs: Iterable[str]\n    ) -> Tuple[Set[Tuple[str, str]], Set[str]]:\n        \"\"\"Return 2 sets:\n        - conflicting requirements: set of (installed, wanted) reqs tuples\n        - missing requirements: set of reqs\n        \"\"\"\n        missing = set()\n        conflicting = set()\n        if reqs:\n            env = (\n                get_environment(self._lib_dirs)\n                if hasattr(self, \"_lib_dirs\")\n                else get_default_environment()\n            )\n            for req_str in reqs:\n                req = get_requirement(req_str)\n                # We're explicitly evaluating with an empty extra value, since build\n                # environments are not provided any mechanism to select specific extras.\n                if req.marker is not None and not req.marker.evaluate({\"extra\": \"\"}):\n                    continue\n                dist = env.get_distribution(req.name)\n                if not dist:\n                    missing.add(req_str)\n                    continue\n                if isinstance(dist.version, Version):\n                    installed_req_str = f\"{req.name}=={dist.version}\"\n                else:\n                    installed_req_str = f\"{req.name}==={dist.version}\"\n                if not req.specifier.contains(dist.version, prereleases=True):\n                    conflicting.add((installed_req_str, req_str))\n                # FIXME: Consider direct URL?\n        return conflicting, missing\n\n    def install_requirements(\n        self,\n        finder: \"PackageFinder\",\n        requirements: Iterable[str],\n        prefix_as_string: str,\n        *,\n        kind: str,\n    ) -> None:\n        prefix = self._prefixes[prefix_as_string]\n        assert not prefix.setup\n        prefix.setup = True\n        if not requirements:\n            return\n        self._install_requirements(\n            get_runnable_pip(),\n            finder,\n            requirements,\n            prefix,\n            kind=kind,\n        )\n\n    @staticmethod\n    def _install_requirements(\n        pip_runnable: str,\n        finder: \"PackageFinder\",\n        requirements: Iterable[str],\n        prefix: _Prefix,\n        *,\n        kind: str,\n    ) -> None:\n        args: List[str] = [\n            sys.executable,\n            pip_runnable,\n            \"install\",\n            \"--ignore-installed\",\n            \"--no-user\",\n            \"--prefix\",\n            prefix.path,\n            \"--no-warn-script-location\",\n            \"--disable-pip-version-check\",\n            # The prefix specified two lines above, thus\n            # target from config file or env var should be ignored\n            \"--target\",\n            \"\",\n        ]\n        if logger.getEffectiveLevel() <= logging.DEBUG:\n            args.append(\"-vv\")\n        elif logger.getEffectiveLevel() <= VERBOSE:\n            args.append(\"-v\")\n        for format_control in (\"no_binary\", \"only_binary\"):\n            formats = getattr(finder.format_control, format_control)\n            args.extend(\n                (\n                    \"--\" + format_control.replace(\"_\", \"-\"),\n                    \",\".join(sorted(formats or {\":none:\"})),\n                )\n            )\n\n        index_urls = finder.index_urls\n        if index_urls:\n            args.extend([\"-i\", index_urls[0]])\n            for extra_index in index_urls[1:]:\n                args.extend([\"--extra-index-url\", extra_index])\n        else:\n            args.append(\"--no-index\")\n        for link in finder.find_links:\n            args.extend([\"--find-links\", link])\n\n        if finder.proxy:\n            args.extend([\"--proxy\", finder.proxy])\n        for host in finder.trusted_hosts:\n            args.extend([\"--trusted-host\", host])\n        if finder.custom_cert:\n            args.extend([\"--cert\", finder.custom_cert])\n        if finder.client_cert:\n            args.extend([\"--client-cert\", finder.client_cert])\n        if finder.allow_all_prereleases:\n            args.append(\"--pre\")\n        if finder.prefer_binary:\n            args.append(\"--prefer-binary\")\n        args.append(\"--\")\n        args.extend(requirements)\n        with open_spinner(f\"Installing {kind}\") as spinner:\n            call_subprocess(\n                args,\n                command_desc=f\"pip subprocess to install {kind}\",\n                spinner=spinner,\n            )\n\n\nclass NoOpBuildEnvironment(BuildEnvironment):\n    \"\"\"A no-op drop-in replacement for BuildEnvironment\"\"\"\n\n    def __init__(self) -> None:\n        pass\n\n    def __enter__(self) -> None:\n        pass\n\n    def __exit__(\n        self,\n        exc_type: Optional[Type[BaseException]],\n        exc_val: Optional[BaseException],\n        exc_tb: Optional[TracebackType],\n    ) -> None:\n        pass\n\n    def cleanup(self) -> None:\n        pass\n\n    def install_requirements(\n        self,\n        finder: \"PackageFinder\",\n        requirements: Iterable[str],\n        prefix_as_string: str,\n        *,\n        kind: str,\n    ) -> None:\n        raise NotImplementedError()\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/cache.py","size":10369,"sha1":"e1e8caa4533a40f97d8b8c452af3e5cc388d8457","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"Cache Management\n\"\"\"\n\nimport hashlib\nimport json\nimport logging\nimport os\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional\n\nfrom pip._vendor.packaging.tags import Tag, interpreter_name, interpreter_version\nfrom pip._vendor.packaging.utils import canonicalize_name\n\nfrom pip._internal.exceptions import InvalidWheelFilename\nfrom pip._internal.models.direct_url import DirectUrl\nfrom pip._internal.models.link import Link\nfrom pip._internal.models.wheel import Wheel\nfrom pip._internal.utils.temp_dir import TempDirectory, tempdir_kinds\nfrom pip._internal.utils.urls import path_to_url\n\nlogger = logging.getLogger(__name__)\n\nORIGIN_JSON_NAME = \"origin.json\"\n\n\ndef _hash_dict(d: Dict[str, str]) -> str:\n    \"\"\"Return a stable sha224 of a dictionary.\"\"\"\n    s = json.dumps(d, sort_keys=True, separators=(\",\", \":\"), ensure_ascii=True)\n    return hashlib.sha224(s.encode(\"ascii\")).hexdigest()\n\n\nclass Cache:\n    \"\"\"An abstract class - provides cache directories for data from links\n\n    :param cache_dir: The root of the cache.\n    \"\"\"\n\n    def __init__(self, cache_dir: str) -> None:\n        super().__init__()\n        assert not cache_dir or os.path.isabs(cache_dir)\n        self.cache_dir = cache_dir or None\n\n    def _get_cache_path_parts(self, link: Link) -> List[str]:\n        \"\"\"Get parts of part that must be os.path.joined with cache_dir\"\"\"\n\n        # We want to generate an url to use as our cache key, we don't want to\n        # just reuse the URL because it might have other items in the fragment\n        # and we don't care about those.\n        key_parts = {\"url\": link.url_without_fragment}\n        if link.hash_name is not None and link.hash is not None:\n            key_parts[link.hash_name] = link.hash\n        if link.subdirectory_fragment:\n            key_parts[\"subdirectory\"] = link.subdirectory_fragment\n\n        # Include interpreter name, major and minor version in cache key\n        # to cope with ill-behaved sdists that build a different wheel\n        # depending on the python version their setup.py is being run on,\n        # and don't encode the difference in compatibility tags.\n        # https://github.com/pypa/pip/issues/7296\n        key_parts[\"interpreter_name\"] = interpreter_name()\n        key_parts[\"interpreter_version\"] = interpreter_version()\n\n        # Encode our key url with sha224, we'll use this because it has similar\n        # security properties to sha256, but with a shorter total output (and\n        # thus less secure). However the differences don't make a lot of\n        # difference for our use case here.\n        hashed = _hash_dict(key_parts)\n\n        # We want to nest the directories some to prevent having a ton of top\n        # level directories where we might run out of sub directories on some\n        # FS.\n        parts = [hashed[:2], hashed[2:4], hashed[4:6], hashed[6:]]\n\n        return parts\n\n    def _get_candidates(self, link: Link, canonical_package_name: str) -> List[Any]:\n        can_not_cache = not self.cache_dir or not canonical_package_name or not link\n        if can_not_cache:\n            return []\n\n        path = self.get_path_for_link(link)\n        if os.path.isdir(path):\n            return [(candidate, path) for candidate in os.listdir(path)]\n        return []\n\n    def get_path_for_link(self, link: Link) -> str:\n        \"\"\"Return a directory to store cached items in for link.\"\"\"\n        raise NotImplementedError()\n\n    def get(\n        self,\n        link: Link,\n        package_name: Optional[str],\n        supported_tags: List[Tag],\n    ) -> Link:\n        \"\"\"Returns a link to a cached item if it exists, otherwise returns the\n        passed link.\n        \"\"\"\n        raise NotImplementedError()\n\n\nclass SimpleWheelCache(Cache):\n    \"\"\"A cache of wheels for future installs.\"\"\"\n\n    def __init__(self, cache_dir: str) -> None:\n        super().__init__(cache_dir)\n\n    def get_path_for_link(self, link: Link) -> str:\n        \"\"\"Return a directory to store cached wheels for link\n\n        Because there are M wheels for any one sdist, we provide a directory\n        to cache them in, and then consult that directory when looking up\n        cache hits.\n\n        We only insert things into the cache if they have plausible version\n        numbers, so that we don't contaminate the cache with things that were\n        not unique. E.g. ./package might have dozens of installs done for it\n        and build a version of 0.0...and if we built and cached a wheel, we'd\n        end up using the same wheel even if the source has been edited.\n\n        :param link: The link of the sdist for which this will cache wheels.\n        \"\"\"\n        parts = self._get_cache_path_parts(link)\n        assert self.cache_dir\n        # Store wheels within the root cache_dir\n        return os.path.join(self.cache_dir, \"wheels\", *parts)\n\n    def get(\n        self,\n        link: Link,\n        package_name: Optional[str],\n        supported_tags: List[Tag],\n    ) -> Link:\n        candidates = []\n\n        if not package_name:\n            return link\n\n        canonical_package_name = canonicalize_name(package_name)\n        for wheel_name, wheel_dir in self._get_candidates(link, canonical_package_name):\n            try:\n                wheel = Wheel(wheel_name)\n            except InvalidWheelFilename:\n                continue\n            if canonicalize_name(wheel.name) != canonical_package_name:\n                logger.debug(\n                    \"Ignoring cached wheel %s for %s as it \"\n                    \"does not match the expected distribution name %s.\",\n                    wheel_name,\n                    link,\n                    package_name,\n                )\n                continue\n            if not wheel.supported(supported_tags):\n                # Built for a different python/arch/etc\n                continue\n            candidates.append(\n                (\n                    wheel.support_index_min(supported_tags),\n                    wheel_name,\n                    wheel_dir,\n                )\n            )\n\n        if not candidates:\n            return link\n\n        _, wheel_name, wheel_dir = min(candidates)\n        return Link(path_to_url(os.path.join(wheel_dir, wheel_name)))\n\n\nclass EphemWheelCache(SimpleWheelCache):\n    \"\"\"A SimpleWheelCache that creates it's own temporary cache directory\"\"\"\n\n    def __init__(self) -> None:\n        self._temp_dir = TempDirectory(\n            kind=tempdir_kinds.EPHEM_WHEEL_CACHE,\n            globally_managed=True,\n        )\n\n        super().__init__(self._temp_dir.path)\n\n\nclass CacheEntry:\n    def __init__(\n        self,\n        link: Link,\n        persistent: bool,\n    ):\n        self.link = link\n        self.persistent = persistent\n        self.origin: Optional[DirectUrl] = None\n        origin_direct_url_path = Path(self.link.file_path).parent / ORIGIN_JSON_NAME\n        if origin_direct_url_path.exists():\n            try:\n                self.origin = DirectUrl.from_json(\n                    origin_direct_url_path.read_text(encoding=\"utf-8\")\n                )\n            except Exception as e:\n                logger.warning(\n                    \"Ignoring invalid cache entry origin file %s for %s (%s)\",\n                    origin_direct_url_path,\n                    link.filename,\n                    e,\n                )\n\n\nclass WheelCache(Cache):\n    \"\"\"Wraps EphemWheelCache and SimpleWheelCache into a single Cache\n\n    This Cache allows for gracefully degradation, using the ephem wheel cache\n    when a certain link is not found in the simple wheel cache first.\n    \"\"\"\n\n    def __init__(self, cache_dir: str) -> None:\n        super().__init__(cache_dir)\n        self._wheel_cache = SimpleWheelCache(cache_dir)\n        self._ephem_cache = EphemWheelCache()\n\n    def get_path_for_link(self, link: Link) -> str:\n        return self._wheel_cache.get_path_for_link(link)\n\n    def get_ephem_path_for_link(self, link: Link) -> str:\n        return self._ephem_cache.get_path_for_link(link)\n\n    def get(\n        self,\n        link: Link,\n        package_name: Optional[str],\n        supported_tags: List[Tag],\n    ) -> Link:\n        cache_entry = self.get_cache_entry(link, package_name, supported_tags)\n        if cache_entry is None:\n            return link\n        return cache_entry.link\n\n    def get_cache_entry(\n        self,\n        link: Link,\n        package_name: Optional[str],\n        supported_tags: List[Tag],\n    ) -> Optional[CacheEntry]:\n        \"\"\"Returns a CacheEntry with a link to a cached item if it exists or\n        None. The cache entry indicates if the item was found in the persistent\n        or ephemeral cache.\n        \"\"\"\n        retval = self._wheel_cache.get(\n            link=link,\n            package_name=package_name,\n            supported_tags=supported_tags,\n        )\n        if retval is not link:\n            return CacheEntry(retval, persistent=True)\n\n        retval = self._ephem_cache.get(\n            link=link,\n            package_name=package_name,\n            supported_tags=supported_tags,\n        )\n        if retval is not link:\n            return CacheEntry(retval, persistent=False)\n\n        return None\n\n    @staticmethod\n    def record_download_origin(cache_dir: str, download_info: DirectUrl) -> None:\n        origin_path = Path(cache_dir) / ORIGIN_JSON_NAME\n        if origin_path.exists():\n            try:\n                origin = DirectUrl.from_json(origin_path.read_text(encoding=\"utf-8\"))\n            except Exception as e:\n                logger.warning(\n                    \"Could not read origin file %s in cache entry (%s). \"\n                    \"Will attempt to overwrite it.\",\n                    origin_path,\n                    e,\n                )\n            else:\n                # TODO: use DirectUrl.equivalent when\n                # https://github.com/pypa/pip/pull/10564 is merged.\n                if origin.url != download_info.url:\n                    logger.warning(\n                        \"Origin URL %s in cache entry %s does not match download URL \"\n                        \"%s. This is likely a pip bug or a cache corruption issue. \"\n                        \"Will overwrite it with the new value.\",\n                        origin.url,\n                        cache_dir,\n                        download_info.url,\n                    )\n        origin_path.write_text(download_info.to_json(), encoding=\"utf-8\")\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/cli/__init__.py","size":132,"sha1":"c98bba03ebc076049b09e2a3168633079a3ea7b1","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"Subpackage containing all of pip's command line interface related code\n\"\"\"\n\n# This file intentionally does not import submodules\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/cli/autocompletion.py","size":6865,"sha1":"c5ab2ef81177b1de334bee14358f93012285a060","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"Logic that powers autocompletion installed by ``pip completion``.\n\"\"\"\n\nimport optparse\nimport os\nimport sys\nfrom itertools import chain\nfrom typing import Any, Iterable, List, Optional\n\nfrom pip._internal.cli.main_parser import create_main_parser\nfrom pip._internal.commands import commands_dict, create_command\nfrom pip._internal.metadata import get_default_environment\n\n\ndef autocomplete() -> None:\n    \"\"\"Entry Point for completion of main and subcommand options.\"\"\"\n    # Don't complete if user hasn't sourced bash_completion file.\n    if \"PIP_AUTO_COMPLETE\" not in os.environ:\n        return\n    # Don't complete if autocompletion environment variables\n    # are not present\n    if not os.environ.get(\"COMP_WORDS\") or not os.environ.get(\"COMP_CWORD\"):\n        return\n    cwords = os.environ[\"COMP_WORDS\"].split()[1:]\n    cword = int(os.environ[\"COMP_CWORD\"])\n    try:\n        current = cwords[cword - 1]\n    except IndexError:\n        current = \"\"\n\n    parser = create_main_parser()\n    subcommands = list(commands_dict)\n    options = []\n\n    # subcommand\n    subcommand_name: Optional[str] = None\n    for word in cwords:\n        if word in subcommands:\n            subcommand_name = word\n            break\n    # subcommand options\n    if subcommand_name is not None:\n        # special case: 'help' subcommand has no options\n        if subcommand_name == \"help\":\n            sys.exit(1)\n        # special case: list locally installed dists for show and uninstall\n        should_list_installed = not current.startswith(\"-\") and subcommand_name in [\n            \"show\",\n            \"uninstall\",\n        ]\n        if should_list_installed:\n            env = get_default_environment()\n            lc = current.lower()\n            installed = [\n                dist.canonical_name\n                for dist in env.iter_installed_distributions(local_only=True)\n                if dist.canonical_name.startswith(lc)\n                and dist.canonical_name not in cwords[1:]\n            ]\n            # if there are no dists installed, fall back to option completion\n            if installed:\n                for dist in installed:\n                    print(dist)\n                sys.exit(1)\n\n        should_list_installables = (\n            not current.startswith(\"-\") and subcommand_name == \"install\"\n        )\n        if should_list_installables:\n            for path in auto_complete_paths(current, \"path\"):\n                print(path)\n            sys.exit(1)\n\n        subcommand = create_command(subcommand_name)\n\n        for opt in subcommand.parser.option_list_all:\n            if opt.help != optparse.SUPPRESS_HELP:\n                options += [\n                    (opt_str, opt.nargs) for opt_str in opt._long_opts + opt._short_opts\n                ]\n\n        # filter out previously specified options from available options\n        prev_opts = [x.split(\"=\")[0] for x in cwords[1 : cword - 1]]\n        options = [(x, v) for (x, v) in options if x not in prev_opts]\n        # filter options by current input\n        options = [(k, v) for k, v in options if k.startswith(current)]\n        # get completion type given cwords and available subcommand options\n        completion_type = get_path_completion_type(\n            cwords,\n            cword,\n            subcommand.parser.option_list_all,\n        )\n        # get completion files and directories if ``completion_type`` is\n        # ``<file>``, ``<dir>`` or ``<path>``\n        if completion_type:\n            paths = auto_complete_paths(current, completion_type)\n            options = [(path, 0) for path in paths]\n        for option in options:\n            opt_label = option[0]\n            # append '=' to options which require args\n            if option[1] and option[0][:2] == \"--\":\n                opt_label += \"=\"\n            print(opt_label)\n    else:\n        # show main parser options only when necessary\n\n        opts = [i.option_list for i in parser.option_groups]\n        opts.append(parser.option_list)\n        flattened_opts = chain.from_iterable(opts)\n        if current.startswith(\"-\"):\n            for opt in flattened_opts:\n                if opt.help != optparse.SUPPRESS_HELP:\n                    subcommands += opt._long_opts + opt._short_opts\n        else:\n            # get completion type given cwords and all available options\n            completion_type = get_path_completion_type(cwords, cword, flattened_opts)\n            if completion_type:\n                subcommands = list(auto_complete_paths(current, completion_type))\n\n        print(\" \".join([x for x in subcommands if x.startswith(current)]))\n    sys.exit(1)\n\n\ndef get_path_completion_type(\n    cwords: List[str], cword: int, opts: Iterable[Any]\n) -> Optional[str]:\n    \"\"\"Get the type of path completion (``file``, ``dir``, ``path`` or None)\n\n    :param cwords: same as the environmental variable ``COMP_WORDS``\n    :param cword: same as the environmental variable ``COMP_CWORD``\n    :param opts: The available options to check\n    :return: path completion type (``file``, ``dir``, ``path`` or None)\n    \"\"\"\n    if cword < 2 or not cwords[cword - 2].startswith(\"-\"):\n        return None\n    for opt in opts:\n        if opt.help == optparse.SUPPRESS_HELP:\n            continue\n        for o in str(opt).split(\"/\"):\n            if cwords[cword - 2].split(\"=\")[0] == o:\n                if not opt.metavar or any(\n                    x in (\"path\", \"file\", \"dir\") for x in opt.metavar.split(\"/\")\n                ):\n                    return opt.metavar\n    return None\n\n\ndef auto_complete_paths(current: str, completion_type: str) -> Iterable[str]:\n    \"\"\"If ``completion_type`` is ``file`` or ``path``, list all regular files\n    and directories starting with ``current``; otherwise only list directories\n    starting with ``current``.\n\n    :param current: The word to be completed\n    :param completion_type: path completion type(``file``, ``path`` or ``dir``)\n    :return: A generator of regular files and/or directories\n    \"\"\"\n    directory, filename = os.path.split(current)\n    current_path = os.path.abspath(directory)\n    # Don't complete paths if they can't be accessed\n    if not os.access(current_path, os.R_OK):\n        return\n    filename = os.path.normcase(filename)\n    # list all files that start with ``filename``\n    file_list = (\n        x for x in os.listdir(current_path) if os.path.normcase(x).startswith(filename)\n    )\n    for f in file_list:\n        opt = os.path.join(current_path, f)\n        comp_file = os.path.normcase(os.path.join(directory, f))\n        # complete regular files when there is not ``<dir>`` after option\n        # complete directories when there is ``<file>``, ``<path>`` or\n        # ``<dir>``after option\n        if completion_type != \"dir\" and os.path.isfile(opt):\n            yield comp_file\n        elif os.path.isdir(opt):\n            yield os.path.join(comp_file, \"\")\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/cli/base_command.py","size":8625,"sha1":"ae00801425999a3ae90024d63037c12c2b9833ae","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"Base Command class, and related routines\"\"\"\n\nimport logging\nimport logging.config\nimport optparse\nimport os\nimport sys\nimport traceback\nfrom optparse import Values\nfrom typing import List, Optional, Tuple\n\nfrom pip._vendor.rich import reconfigure\nfrom pip._vendor.rich import traceback as rich_traceback\n\nfrom pip._internal.cli import cmdoptions\nfrom pip._internal.cli.command_context import CommandContextMixIn\nfrom pip._internal.cli.parser import ConfigOptionParser, UpdatingDefaultsHelpFormatter\nfrom pip._internal.cli.status_codes import (\n    ERROR,\n    PREVIOUS_BUILD_DIR_ERROR,\n    UNKNOWN_ERROR,\n    VIRTUALENV_NOT_FOUND,\n)\nfrom pip._internal.exceptions import (\n    BadCommand,\n    CommandError,\n    DiagnosticPipError,\n    InstallationError,\n    NetworkConnectionError,\n    PreviousBuildDirError,\n)\nfrom pip._internal.utils.deprecation import deprecated\nfrom pip._internal.utils.filesystem import check_path_owner\nfrom pip._internal.utils.logging import BrokenStdoutLoggingError, setup_logging\nfrom pip._internal.utils.misc import get_prog, normalize_path\nfrom pip._internal.utils.temp_dir import TempDirectoryTypeRegistry as TempDirRegistry\nfrom pip._internal.utils.temp_dir import global_tempdir_manager, tempdir_registry\nfrom pip._internal.utils.virtualenv import running_under_virtualenv\n\n__all__ = [\"Command\"]\n\nlogger = logging.getLogger(__name__)\n\n\nclass Command(CommandContextMixIn):\n    usage: str = \"\"\n    ignore_require_venv: bool = False\n\n    def __init__(self, name: str, summary: str, isolated: bool = False) -> None:\n        super().__init__()\n\n        self.name = name\n        self.summary = summary\n        self.parser = ConfigOptionParser(\n            usage=self.usage,\n            prog=f\"{get_prog()} {name}\",\n            formatter=UpdatingDefaultsHelpFormatter(),\n            add_help_option=False,\n            name=name,\n            description=self.__doc__,\n            isolated=isolated,\n        )\n\n        self.tempdir_registry: Optional[TempDirRegistry] = None\n\n        # Commands should add options to this option group\n        optgroup_name = f\"{self.name.capitalize()} Options\"\n        self.cmd_opts = optparse.OptionGroup(self.parser, optgroup_name)\n\n        # Add the general options\n        gen_opts = cmdoptions.make_option_group(\n            cmdoptions.general_group,\n            self.parser,\n        )\n        self.parser.add_option_group(gen_opts)\n\n        self.add_options()\n\n    def add_options(self) -> None:\n        pass\n\n    def handle_pip_version_check(self, options: Values) -> None:\n        \"\"\"\n        This is a no-op so that commands by default do not do the pip version\n        check.\n        \"\"\"\n        # Make sure we do the pip version check if the index_group options\n        # are present.\n        assert not hasattr(options, \"no_index\")\n\n    def run(self, options: Values, args: List[str]) -> int:\n        raise NotImplementedError\n\n    def _run_wrapper(self, level_number: int, options: Values, args: List[str]) -> int:\n        def _inner_run() -> int:\n            try:\n                return self.run(options, args)\n            finally:\n                self.handle_pip_version_check(options)\n\n        if options.debug_mode:\n            rich_traceback.install(show_locals=True)\n            return _inner_run()\n\n        try:\n            status = _inner_run()\n            assert isinstance(status, int)\n            return status\n        except DiagnosticPipError as exc:\n            logger.error(\"%s\", exc, extra={\"rich\": True})\n            logger.debug(\"Exception information:\", exc_info=True)\n\n            return ERROR\n        except PreviousBuildDirError as exc:\n            logger.critical(str(exc))\n            logger.debug(\"Exception information:\", exc_info=True)\n\n            return PREVIOUS_BUILD_DIR_ERROR\n        except (\n            InstallationError,\n            BadCommand,\n            NetworkConnectionError,\n        ) as exc:\n            logger.critical(str(exc))\n            logger.debug(\"Exception information:\", exc_info=True)\n\n            return ERROR\n        except CommandError as exc:\n            logger.critical(\"%s\", exc)\n            logger.debug(\"Exception information:\", exc_info=True)\n\n            return ERROR\n        except BrokenStdoutLoggingError:\n            # Bypass our logger and write any remaining messages to\n            # stderr because stdout no longer works.\n            print(\"ERROR: Pipe to stdout was broken\", file=sys.stderr)\n            if level_number <= logging.DEBUG:\n                traceback.print_exc(file=sys.stderr)\n\n            return ERROR\n        except KeyboardInterrupt:\n            logger.critical(\"Operation cancelled by user\")\n            logger.debug(\"Exception information:\", exc_info=True)\n\n            return ERROR\n        except BaseException:\n            logger.critical(\"Exception:\", exc_info=True)\n\n            return UNKNOWN_ERROR\n\n    def parse_args(self, args: List[str]) -> Tuple[Values, List[str]]:\n        # factored out for testability\n        return self.parser.parse_args(args)\n\n    def main(self, args: List[str]) -> int:\n        try:\n            with self.main_context():\n                return self._main(args)\n        finally:\n            logging.shutdown()\n\n    def _main(self, args: List[str]) -> int:\n        # We must initialize this before the tempdir manager, otherwise the\n        # configuration would not be accessible by the time we clean up the\n        # tempdir manager.\n        self.tempdir_registry = self.enter_context(tempdir_registry())\n        # Intentionally set as early as possible so globally-managed temporary\n        # directories are available to the rest of the code.\n        self.enter_context(global_tempdir_manager())\n\n        options, args = self.parse_args(args)\n\n        # Set verbosity so that it can be used elsewhere.\n        self.verbosity = options.verbose - options.quiet\n\n        reconfigure(no_color=options.no_color)\n        level_number = setup_logging(\n            verbosity=self.verbosity,\n            no_color=options.no_color,\n            user_log_file=options.log,\n        )\n\n        always_enabled_features = set(options.features_enabled) & set(\n            cmdoptions.ALWAYS_ENABLED_FEATURES\n        )\n        if always_enabled_features:\n            logger.warning(\n                \"The following features are always enabled: %s. \",\n                \", \".join(sorted(always_enabled_features)),\n            )\n\n        # Make sure that the --python argument isn't specified after the\n        # subcommand. We can tell, because if --python was specified,\n        # we should only reach this point if we're running in the created\n        # subprocess, which has the _PIP_RUNNING_IN_SUBPROCESS environment\n        # variable set.\n        if options.python and \"_PIP_RUNNING_IN_SUBPROCESS\" not in os.environ:\n            logger.critical(\n                \"The --python option must be placed before the pip subcommand name\"\n            )\n            sys.exit(ERROR)\n\n        # TODO: Try to get these passing down from the command?\n        #       without resorting to os.environ to hold these.\n        #       This also affects isolated builds and it should.\n\n        if options.no_input:\n            os.environ[\"PIP_NO_INPUT\"] = \"1\"\n\n        if options.exists_action:\n            os.environ[\"PIP_EXISTS_ACTION\"] = \" \".join(options.exists_action)\n\n        if options.require_venv and not self.ignore_require_venv:\n            # If a venv is required check if it can really be found\n            if not running_under_virtualenv():\n                logger.critical(\"Could not find an activated virtualenv (required).\")\n                sys.exit(VIRTUALENV_NOT_FOUND)\n\n        if options.cache_dir:\n            options.cache_dir = normalize_path(options.cache_dir)\n            if not check_path_owner(options.cache_dir):\n                logger.warning(\n                    \"The directory '%s' or its parent directory is not owned \"\n                    \"or is not writable by the current user. The cache \"\n                    \"has been disabled. Check the permissions and owner of \"\n                    \"that directory. If executing pip with sudo, you should \"\n                    \"use sudo's -H flag.\",\n                    options.cache_dir,\n                )\n                options.cache_dir = None\n\n        if options.no_python_version_warning:\n            deprecated(\n                reason=\"--no-python-version-warning is deprecated.\",\n                replacement=\"to remove the flag as it's a no-op\",\n                gone_in=\"25.1\",\n                issue=13154,\n            )\n\n        return self._run_wrapper(level_number, options, args)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/cli/cmdoptions.py","size":30116,"sha1":"4b433faca1cafb340503323b4a44f20b1c74aa43","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"\nshared options and groups\n\nThe principle here is to define options once, but *not* instantiate them\nglobally. One reason being that options with action='append' can carry state\nbetween parses. pip parses general options twice internally, and shouldn't\npass on state. To be consistent, all options will follow this design.\n\"\"\"\n\n# The following comment should be removed at some point in the future.\n# mypy: strict-optional=False\n\nimport importlib.util\nimport logging\nimport os\nimport textwrap\nfrom functools import partial\nfrom optparse import SUPPRESS_HELP, Option, OptionGroup, OptionParser, Values\nfrom textwrap import dedent\nfrom typing import Any, Callable, Dict, Optional, Tuple\n\nfrom pip._vendor.packaging.utils import canonicalize_name\n\nfrom pip._internal.cli.parser import ConfigOptionParser\nfrom pip._internal.exceptions import CommandError\nfrom pip._internal.locations import USER_CACHE_DIR, get_src_prefix\nfrom pip._internal.models.format_control import FormatControl\nfrom pip._internal.models.index import PyPI\nfrom pip._internal.models.target_python import TargetPython\nfrom pip._internal.utils.hashes import STRONG_HASHES\nfrom pip._internal.utils.misc import strtobool\n\nlogger = logging.getLogger(__name__)\n\n\ndef raise_option_error(parser: OptionParser, option: Option, msg: str) -> None:\n    \"\"\"\n    Raise an option parsing error using parser.error().\n\n    Args:\n      parser: an OptionParser instance.\n      option: an Option instance.\n      msg: the error text.\n    \"\"\"\n    msg = f\"{option} error: {msg}\"\n    msg = textwrap.fill(\" \".join(msg.split()))\n    parser.error(msg)\n\n\ndef make_option_group(group: Dict[str, Any], parser: ConfigOptionParser) -> OptionGroup:\n    \"\"\"\n    Return an OptionGroup object\n    group  -- assumed to be dict with 'name' and 'options' keys\n    parser -- an optparse Parser\n    \"\"\"\n    option_group = OptionGroup(parser, group[\"name\"])\n    for option in group[\"options\"]:\n        option_group.add_option(option())\n    return option_group\n\n\ndef check_dist_restriction(options: Values, check_target: bool = False) -> None:\n    \"\"\"Function for determining if custom platform options are allowed.\n\n    :param options: The OptionParser options.\n    :param check_target: Whether or not to check if --target is being used.\n    \"\"\"\n    dist_restriction_set = any(\n        [\n            options.python_version,\n            options.platforms,\n            options.abis,\n            options.implementation,\n        ]\n    )\n\n    binary_only = FormatControl(set(), {\":all:\"})\n    sdist_dependencies_allowed = (\n        options.format_control != binary_only and not options.ignore_dependencies\n    )\n\n    # Installations or downloads using dist restrictions must not combine\n    # source distributions and dist-specific wheels, as they are not\n    # guaranteed to be locally compatible.\n    if dist_restriction_set and sdist_dependencies_allowed:\n        raise CommandError(\n            \"When restricting platform and interpreter constraints using \"\n            \"--python-version, --platform, --abi, or --implementation, \"\n            \"either --no-deps must be set, or --only-binary=:all: must be \"\n            \"set and --no-binary must not be set (or must be set to \"\n            \":none:).\"\n        )\n\n    if check_target:\n        if not options.dry_run and dist_restriction_set and not options.target_dir:\n            raise CommandError(\n                \"Can not use any platform or abi specific options unless \"\n                \"installing via '--target' or using '--dry-run'\"\n            )\n\n\ndef _path_option_check(option: Option, opt: str, value: str) -> str:\n    return os.path.expanduser(value)\n\n\ndef _package_name_option_check(option: Option, opt: str, value: str) -> str:\n    return canonicalize_name(value)\n\n\nclass PipOption(Option):\n    TYPES = Option.TYPES + (\"path\", \"package_name\")\n    TYPE_CHECKER = Option.TYPE_CHECKER.copy()\n    TYPE_CHECKER[\"package_name\"] = _package_name_option_check\n    TYPE_CHECKER[\"path\"] = _path_option_check\n\n\n###########\n# options #\n###########\n\nhelp_: Callable[..., Option] = partial(\n    Option,\n    \"-h\",\n    \"--help\",\n    dest=\"help\",\n    action=\"help\",\n    help=\"Show help.\",\n)\n\ndebug_mode: Callable[..., Option] = partial(\n    Option,\n    \"--debug\",\n    dest=\"debug_mode\",\n    action=\"store_true\",\n    default=False,\n    help=(\n        \"Let unhandled exceptions propagate outside the main subroutine, \"\n        \"instead of logging them to stderr.\"\n    ),\n)\n\nisolated_mode: Callable[..., Option] = partial(\n    Option,\n    \"--isolated\",\n    dest=\"isolated_mode\",\n    action=\"store_true\",\n    default=False,\n    help=(\n        \"Run pip in an isolated mode, ignoring environment variables and user \"\n        \"configuration.\"\n    ),\n)\n\nrequire_virtualenv: Callable[..., Option] = partial(\n    Option,\n    \"--require-virtualenv\",\n    \"--require-venv\",\n    dest=\"require_venv\",\n    action=\"store_true\",\n    default=False,\n    help=(\n        \"Allow pip to only run in a virtual environment; \"\n        \"exit with an error otherwise.\"\n    ),\n)\n\noverride_externally_managed: Callable[..., Option] = partial(\n    Option,\n    \"--break-system-packages\",\n    dest=\"override_externally_managed\",\n    action=\"store_true\",\n    help=\"Allow pip to modify an EXTERNALLY-MANAGED Python installation\",\n)\n\npython: Callable[..., Option] = partial(\n    Option,\n    \"--python\",\n    dest=\"python\",\n    help=\"Run pip with the specified Python interpreter.\",\n)\n\nverbose: Callable[..., Option] = partial(\n    Option,\n    \"-v\",\n    \"--verbose\",\n    dest=\"verbose\",\n    action=\"count\",\n    default=0,\n    help=\"Give more output. Option is additive, and can be used up to 3 times.\",\n)\n\nno_color: Callable[..., Option] = partial(\n    Option,\n    \"--no-color\",\n    dest=\"no_color\",\n    action=\"store_true\",\n    default=False,\n    help=\"Suppress colored output.\",\n)\n\nversion: Callable[..., Option] = partial(\n    Option,\n    \"-V\",\n    \"--version\",\n    dest=\"version\",\n    action=\"store_true\",\n    help=\"Show version and exit.\",\n)\n\nquiet: Callable[..., Option] = partial(\n    Option,\n    \"-q\",\n    \"--quiet\",\n    dest=\"quiet\",\n    action=\"count\",\n    default=0,\n    help=(\n        \"Give less output. Option is additive, and can be used up to 3\"\n        \" times (corresponding to WARNING, ERROR, and CRITICAL logging\"\n        \" levels).\"\n    ),\n)\n\nprogress_bar: Callable[..., Option] = partial(\n    Option,\n    \"--progress-bar\",\n    dest=\"progress_bar\",\n    type=\"choice\",\n    choices=[\"on\", \"off\", \"raw\"],\n    default=\"on\",\n    help=\"Specify whether the progress bar should be used [on, off, raw] (default: on)\",\n)\n\nlog: Callable[..., Option] = partial(\n    PipOption,\n    \"--log\",\n    \"--log-file\",\n    \"--local-log\",\n    dest=\"log\",\n    metavar=\"path\",\n    type=\"path\",\n    help=\"Path to a verbose appending log.\",\n)\n\nno_input: Callable[..., Option] = partial(\n    Option,\n    # Don't ask for input\n    \"--no-input\",\n    dest=\"no_input\",\n    action=\"store_true\",\n    default=False,\n    help=\"Disable prompting for input.\",\n)\n\nkeyring_provider: Callable[..., Option] = partial(\n    Option,\n    \"--keyring-provider\",\n    dest=\"keyring_provider\",\n    choices=[\"auto\", \"disabled\", \"import\", \"subprocess\"],\n    default=\"auto\",\n    help=(\n        \"Enable the credential lookup via the keyring library if user input is allowed.\"\n        \" Specify which mechanism to use [auto, disabled, import, subprocess].\"\n        \" (default: %default)\"\n    ),\n)\n\nproxy: Callable[..., Option] = partial(\n    Option,\n    \"--proxy\",\n    dest=\"proxy\",\n    type=\"str\",\n    default=\"\",\n    help=\"Specify a proxy in the form scheme://[user:passwd@]proxy.server:port.\",\n)\n\nretries: Callable[..., Option] = partial(\n    Option,\n    \"--retries\",\n    dest=\"retries\",\n    type=\"int\",\n    default=5,\n    help=\"Maximum number of retries each connection should attempt \"\n    \"(default %default times).\",\n)\n\ntimeout: Callable[..., Option] = partial(\n    Option,\n    \"--timeout\",\n    \"--default-timeout\",\n    metavar=\"sec\",\n    dest=\"timeout\",\n    type=\"float\",\n    default=15,\n    help=\"Set the socket timeout (default %default seconds).\",\n)\n\n\ndef exists_action() -> Option:\n    return Option(\n        # Option when path already exist\n        \"--exists-action\",\n        dest=\"exists_action\",\n        type=\"choice\",\n        choices=[\"s\", \"i\", \"w\", \"b\", \"a\"],\n        default=[],\n        action=\"append\",\n        metavar=\"action\",\n        help=\"Default action when a path already exists: \"\n        \"(s)witch, (i)gnore, (w)ipe, (b)ackup, (a)bort.\",\n    )\n\n\ncert: Callable[..., Option] = partial(\n    PipOption,\n    \"--cert\",\n    dest=\"cert\",\n    type=\"path\",\n    metavar=\"path\",\n    help=(\n        \"Path to PEM-encoded CA certificate bundle. \"\n        \"If provided, overrides the default. \"\n        \"See 'SSL Certificate Verification' in pip documentation \"\n        \"for more information.\"\n    ),\n)\n\nclient_cert: Callable[..., Option] = partial(\n    PipOption,\n    \"--client-cert\",\n    dest=\"client_cert\",\n    type=\"path\",\n    default=None,\n    metavar=\"path\",\n    help=\"Path to SSL client certificate, a single file containing the \"\n    \"private key and the certificate in PEM format.\",\n)\n\nindex_url: Callable[..., Option] = partial(\n    Option,\n    \"-i\",\n    \"--index-url\",\n    \"--pypi-url\",\n    dest=\"index_url\",\n    metavar=\"URL\",\n    default=PyPI.simple_url,\n    help=\"Base URL of the Python Package Index (default %default). \"\n    \"This should point to a repository compliant with PEP 503 \"\n    \"(the simple repository API) or a local directory laid out \"\n    \"in the same format.\",\n)\n\n\ndef extra_index_url() -> Option:\n    return Option(\n        \"--extra-index-url\",\n        dest=\"extra_index_urls\",\n        metavar=\"URL\",\n        action=\"append\",\n        default=[],\n        help=\"Extra URLs of package indexes to use in addition to \"\n        \"--index-url. Should follow the same rules as \"\n        \"--index-url.\",\n    )\n\n\nno_index: Callable[..., Option] = partial(\n    Option,\n    \"--no-index\",\n    dest=\"no_index\",\n    action=\"store_true\",\n    default=False,\n    help=\"Ignore package index (only looking at --find-links URLs instead).\",\n)\n\n\ndef find_links() -> Option:\n    return Option(\n        \"-f\",\n        \"--find-links\",\n        dest=\"find_links\",\n        action=\"append\",\n        default=[],\n        metavar=\"url\",\n        help=\"If a URL or path to an html file, then parse for links to \"\n        \"archives such as sdist (.tar.gz) or wheel (.whl) files. \"\n        \"If a local path or file:// URL that's a directory, \"\n        \"then look for archives in the directory listing. \"\n        \"Links to VCS project URLs are not supported.\",\n    )\n\n\ndef trusted_host() -> Option:\n    return Option(\n        \"--trusted-host\",\n        dest=\"trusted_hosts\",\n        action=\"append\",\n        metavar=\"HOSTNAME\",\n        default=[],\n        help=\"Mark this host or host:port pair as trusted, even though it \"\n        \"does not have valid or any HTTPS.\",\n    )\n\n\ndef constraints() -> Option:\n    return Option(\n        \"-c\",\n        \"--constraint\",\n        dest=\"constraints\",\n        action=\"append\",\n        default=[],\n        metavar=\"file\",\n        help=\"Constrain versions using the given constraints file. \"\n        \"This option can be used multiple times.\",\n    )\n\n\ndef requirements() -> Option:\n    return Option(\n        \"-r\",\n        \"--requirement\",\n        dest=\"requirements\",\n        action=\"append\",\n        default=[],\n        metavar=\"file\",\n        help=\"Install from the given requirements file. \"\n        \"This option can be used multiple times.\",\n    )\n\n\ndef editable() -> Option:\n    return Option(\n        \"-e\",\n        \"--editable\",\n        dest=\"editables\",\n        action=\"append\",\n        default=[],\n        metavar=\"path/url\",\n        help=(\n            \"Install a project in editable mode (i.e. setuptools \"\n            '\"develop mode\") from a local project path or a VCS url.'\n        ),\n    )\n\n\ndef _handle_src(option: Option, opt_str: str, value: str, parser: OptionParser) -> None:\n    value = os.path.abspath(value)\n    setattr(parser.values, option.dest, value)\n\n\nsrc: Callable[..., Option] = partial(\n    PipOption,\n    \"--src\",\n    \"--source\",\n    \"--source-dir\",\n    \"--source-directory\",\n    dest=\"src_dir\",\n    type=\"path\",\n    metavar=\"dir\",\n    default=get_src_prefix(),\n    action=\"callback\",\n    callback=_handle_src,\n    help=\"Directory to check out editable projects into. \"\n    'The default in a virtualenv is \"<venv path>/src\". '\n    'The default for global installs is \"<current dir>/src\".',\n)\n\n\ndef _get_format_control(values: Values, option: Option) -> Any:\n    \"\"\"Get a format_control object.\"\"\"\n    return getattr(values, option.dest)\n\n\ndef _handle_no_binary(\n    option: Option, opt_str: str, value: str, parser: OptionParser\n) -> None:\n    existing = _get_format_control(parser.values, option)\n    FormatControl.handle_mutual_excludes(\n        value,\n        existing.no_binary,\n        existing.only_binary,\n    )\n\n\ndef _handle_only_binary(\n    option: Option, opt_str: str, value: str, parser: OptionParser\n) -> None:\n    existing = _get_format_control(parser.values, option)\n    FormatControl.handle_mutual_excludes(\n        value,\n        existing.only_binary,\n        existing.no_binary,\n    )\n\n\ndef no_binary() -> Option:\n    format_control = FormatControl(set(), set())\n    return Option(\n        \"--no-binary\",\n        dest=\"format_control\",\n        action=\"callback\",\n        callback=_handle_no_binary,\n        type=\"str\",\n        default=format_control,\n        help=\"Do not use binary packages. Can be supplied multiple times, and \"\n        'each time adds to the existing value. Accepts either \":all:\" to '\n        'disable all binary packages, \":none:\" to empty the set (notice '\n        \"the colons), or one or more package names with commas between \"\n        \"them (no colons). Note that some packages are tricky to compile \"\n        \"and may fail to install when this option is used on them.\",\n    )\n\n\ndef only_binary() -> Option:\n    format_control = FormatControl(set(), set())\n    return Option(\n        \"--only-binary\",\n        dest=\"format_control\",\n        action=\"callback\",\n        callback=_handle_only_binary,\n        type=\"str\",\n        default=format_control,\n        help=\"Do not use source packages. Can be supplied multiple times, and \"\n        'each time adds to the existing value. Accepts either \":all:\" to '\n        'disable all source packages, \":none:\" to empty the set, or one '\n        \"or more package names with commas between them. Packages \"\n        \"without binary distributions will fail to install when this \"\n        \"option is used on them.\",\n    )\n\n\nplatforms: Callable[..., Option] = partial(\n    Option,\n    \"--platform\",\n    dest=\"platforms\",\n    metavar=\"platform\",\n    action=\"append\",\n    default=None,\n    help=(\n        \"Only use wheels compatible with <platform>. Defaults to the \"\n        \"platform of the running system. Use this option multiple times to \"\n        \"specify multiple platforms supported by the target interpreter.\"\n    ),\n)\n\n\n# This was made a separate function for unit-testing purposes.\ndef _convert_python_version(value: str) -> Tuple[Tuple[int, ...], Optional[str]]:\n    \"\"\"\n    Convert a version string like \"3\", \"37\", or \"3.7.3\" into a tuple of ints.\n\n    :return: A 2-tuple (version_info, error_msg), where `error_msg` is\n        non-None if and only if there was a parsing error.\n    \"\"\"\n    if not value:\n        # The empty string is the same as not providing a value.\n        return (None, None)\n\n    parts = value.split(\".\")\n    if len(parts) > 3:\n        return ((), \"at most three version parts are allowed\")\n\n    if len(parts) == 1:\n        # Then we are in the case of \"3\" or \"37\".\n        value = parts[0]\n        if len(value) > 1:\n            parts = [value[0], value[1:]]\n\n    try:\n        version_info = tuple(int(part) for part in parts)\n    except ValueError:\n        return ((), \"each version part must be an integer\")\n\n    return (version_info, None)\n\n\ndef _handle_python_version(\n    option: Option, opt_str: str, value: str, parser: OptionParser\n) -> None:\n    \"\"\"\n    Handle a provided --python-version value.\n    \"\"\"\n    version_info, error_msg = _convert_python_version(value)\n    if error_msg is not None:\n        msg = f\"invalid --python-version value: {value!r}: {error_msg}\"\n        raise_option_error(parser, option=option, msg=msg)\n\n    parser.values.python_version = version_info\n\n\npython_version: Callable[..., Option] = partial(\n    Option,\n    \"--python-version\",\n    dest=\"python_version\",\n    metavar=\"python_version\",\n    action=\"callback\",\n    callback=_handle_python_version,\n    type=\"str\",\n    default=None,\n    help=dedent(\n        \"\"\"\\\n    The Python interpreter version to use for wheel and \"Requires-Python\"\n    compatibility checks. Defaults to a version derived from the running\n    interpreter. The version can be specified using up to three dot-separated\n    integers (e.g. \"3\" for 3.0.0, \"3.7\" for 3.7.0, or \"3.7.3\"). A major-minor\n    version can also be given as a string without dots (e.g. \"37\" for 3.7.0).\n    \"\"\"\n    ),\n)\n\n\nimplementation: Callable[..., Option] = partial(\n    Option,\n    \"--implementation\",\n    dest=\"implementation\",\n    metavar=\"implementation\",\n    default=None,\n    help=(\n        \"Only use wheels compatible with Python \"\n        \"implementation <implementation>, e.g. 'pp', 'jy', 'cp', \"\n        \" or 'ip'. If not specified, then the current \"\n        \"interpreter implementation is used.  Use 'py' to force \"\n        \"implementation-agnostic wheels.\"\n    ),\n)\n\n\nabis: Callable[..., Option] = partial(\n    Option,\n    \"--abi\",\n    dest=\"abis\",\n    metavar=\"abi\",\n    action=\"append\",\n    default=None,\n    help=(\n        \"Only use wheels compatible with Python abi <abi>, e.g. 'pypy_41'. \"\n        \"If not specified, then the current interpreter abi tag is used. \"\n        \"Use this option multiple times to specify multiple abis supported \"\n        \"by the target interpreter. Generally you will need to specify \"\n        \"--implementation, --platform, and --python-version when using this \"\n        \"option.\"\n    ),\n)\n\n\ndef add_target_python_options(cmd_opts: OptionGroup) -> None:\n    cmd_opts.add_option(platforms())\n    cmd_opts.add_option(python_version())\n    cmd_opts.add_option(implementation())\n    cmd_opts.add_option(abis())\n\n\ndef make_target_python(options: Values) -> TargetPython:\n    target_python = TargetPython(\n        platforms=options.platforms,\n        py_version_info=options.python_version,\n        abis=options.abis,\n        implementation=options.implementation,\n    )\n\n    return target_python\n\n\ndef prefer_binary() -> Option:\n    return Option(\n        \"--prefer-binary\",\n        dest=\"prefer_binary\",\n        action=\"store_true\",\n        default=False,\n        help=(\n            \"Prefer binary packages over source packages, even if the \"\n            \"source packages are newer.\"\n        ),\n    )\n\n\ncache_dir: Callable[..., Option] = partial(\n    PipOption,\n    \"--cache-dir\",\n    dest=\"cache_dir\",\n    default=USER_CACHE_DIR,\n    metavar=\"dir\",\n    type=\"path\",\n    help=\"Store the cache data in <dir>.\",\n)\n\n\ndef _handle_no_cache_dir(\n    option: Option, opt: str, value: str, parser: OptionParser\n) -> None:\n    \"\"\"\n    Process a value provided for the --no-cache-dir option.\n\n    This is an optparse.Option callback for the --no-cache-dir option.\n    \"\"\"\n    # The value argument will be None if --no-cache-dir is passed via the\n    # command-line, since the option doesn't accept arguments.  However,\n    # the value can be non-None if the option is triggered e.g. by an\n    # environment variable, like PIP_NO_CACHE_DIR=true.\n    if value is not None:\n        # Then parse the string value to get argument error-checking.\n        try:\n            strtobool(value)\n        except ValueError as exc:\n            raise_option_error(parser, option=option, msg=str(exc))\n\n    # Originally, setting PIP_NO_CACHE_DIR to a value that strtobool()\n    # converted to 0 (like \"false\" or \"no\") caused cache_dir to be disabled\n    # rather than enabled (logic would say the latter).  Thus, we disable\n    # the cache directory not just on values that parse to True, but (for\n    # backwards compatibility reasons) also on values that parse to False.\n    # In other words, always set it to False if the option is provided in\n    # some (valid) form.\n    parser.values.cache_dir = False\n\n\nno_cache: Callable[..., Option] = partial(\n    Option,\n    \"--no-cache-dir\",\n    dest=\"cache_dir\",\n    action=\"callback\",\n    callback=_handle_no_cache_dir,\n    help=\"Disable the cache.\",\n)\n\nno_deps: Callable[..., Option] = partial(\n    Option,\n    \"--no-deps\",\n    \"--no-dependencies\",\n    dest=\"ignore_dependencies\",\n    action=\"store_true\",\n    default=False,\n    help=\"Don't install package dependencies.\",\n)\n\nignore_requires_python: Callable[..., Option] = partial(\n    Option,\n    \"--ignore-requires-python\",\n    dest=\"ignore_requires_python\",\n    action=\"store_true\",\n    help=\"Ignore the Requires-Python information.\",\n)\n\nno_build_isolation: Callable[..., Option] = partial(\n    Option,\n    \"--no-build-isolation\",\n    dest=\"build_isolation\",\n    action=\"store_false\",\n    default=True,\n    help=\"Disable isolation when building a modern source distribution. \"\n    \"Build dependencies specified by PEP 518 must be already installed \"\n    \"if this option is used.\",\n)\n\ncheck_build_deps: Callable[..., Option] = partial(\n    Option,\n    \"--check-build-dependencies\",\n    dest=\"check_build_deps\",\n    action=\"store_true\",\n    default=False,\n    help=\"Check the build dependencies when PEP517 is used.\",\n)\n\n\ndef _handle_no_use_pep517(\n    option: Option, opt: str, value: str, parser: OptionParser\n) -> None:\n    \"\"\"\n    Process a value provided for the --no-use-pep517 option.\n\n    This is an optparse.Option callback for the no_use_pep517 option.\n    \"\"\"\n    # Since --no-use-pep517 doesn't accept arguments, the value argument\n    # will be None if --no-use-pep517 is passed via the command-line.\n    # However, the value can be non-None if the option is triggered e.g.\n    # by an environment variable, for example \"PIP_NO_USE_PEP517=true\".\n    if value is not None:\n        msg = \"\"\"A value was passed for --no-use-pep517,\n        probably using either the PIP_NO_USE_PEP517 environment variable\n        or the \"no-use-pep517\" config file option. Use an appropriate value\n        of the PIP_USE_PEP517 environment variable or the \"use-pep517\"\n        config file option instead.\n        \"\"\"\n        raise_option_error(parser, option=option, msg=msg)\n\n    # If user doesn't wish to use pep517, we check if setuptools and wheel are installed\n    # and raise error if it is not.\n    packages = (\"setuptools\", \"wheel\")\n    if not all(importlib.util.find_spec(package) for package in packages):\n        msg = (\n            f\"It is not possible to use --no-use-pep517 \"\n            f\"without {' and '.join(packages)} installed.\"\n        )\n        raise_option_error(parser, option=option, msg=msg)\n\n    # Otherwise, --no-use-pep517 was passed via the command-line.\n    parser.values.use_pep517 = False\n\n\nuse_pep517: Any = partial(\n    Option,\n    \"--use-pep517\",\n    dest=\"use_pep517\",\n    action=\"store_true\",\n    default=None,\n    help=\"Use PEP 517 for building source distributions \"\n    \"(use --no-use-pep517 to force legacy behaviour).\",\n)\n\nno_use_pep517: Any = partial(\n    Option,\n    \"--no-use-pep517\",\n    dest=\"use_pep517\",\n    action=\"callback\",\n    callback=_handle_no_use_pep517,\n    default=None,\n    help=SUPPRESS_HELP,\n)\n\n\ndef _handle_config_settings(\n    option: Option, opt_str: str, value: str, parser: OptionParser\n) -> None:\n    key, sep, val = value.partition(\"=\")\n    if sep != \"=\":\n        parser.error(f\"Arguments to {opt_str} must be of the form KEY=VAL\")\n    dest = getattr(parser.values, option.dest)\n    if dest is None:\n        dest = {}\n        setattr(parser.values, option.dest, dest)\n    if key in dest:\n        if isinstance(dest[key], list):\n            dest[key].append(val)\n        else:\n            dest[key] = [dest[key], val]\n    else:\n        dest[key] = val\n\n\nconfig_settings: Callable[..., Option] = partial(\n    Option,\n    \"-C\",\n    \"--config-settings\",\n    dest=\"config_settings\",\n    type=str,\n    action=\"callback\",\n    callback=_handle_config_settings,\n    metavar=\"settings\",\n    help=\"Configuration settings to be passed to the PEP 517 build backend. \"\n    \"Settings take the form KEY=VALUE. Use multiple --config-settings options \"\n    \"to pass multiple keys to the backend.\",\n)\n\nbuild_options: Callable[..., Option] = partial(\n    Option,\n    \"--build-option\",\n    dest=\"build_options\",\n    metavar=\"options\",\n    action=\"append\",\n    help=\"Extra arguments to be supplied to 'setup.py bdist_wheel'.\",\n)\n\nglobal_options: Callable[..., Option] = partial(\n    Option,\n    \"--global-option\",\n    dest=\"global_options\",\n    action=\"append\",\n    metavar=\"options\",\n    help=\"Extra global options to be supplied to the setup.py \"\n    \"call before the install or bdist_wheel command.\",\n)\n\nno_clean: Callable[..., Option] = partial(\n    Option,\n    \"--no-clean\",\n    action=\"store_true\",\n    default=False,\n    help=\"Don't clean up build directories.\",\n)\n\npre: Callable[..., Option] = partial(\n    Option,\n    \"--pre\",\n    action=\"store_true\",\n    default=False,\n    help=\"Include pre-release and development versions. By default, \"\n    \"pip only finds stable versions.\",\n)\n\ndisable_pip_version_check: Callable[..., Option] = partial(\n    Option,\n    \"--disable-pip-version-check\",\n    dest=\"disable_pip_version_check\",\n    action=\"store_true\",\n    default=False,\n    help=\"Don't periodically check PyPI to determine whether a new version \"\n    \"of pip is available for download. Implied with --no-index.\",\n)\n\nroot_user_action: Callable[..., Option] = partial(\n    Option,\n    \"--root-user-action\",\n    dest=\"root_user_action\",\n    default=\"warn\",\n    choices=[\"warn\", \"ignore\"],\n    help=\"Action if pip is run as a root user [warn, ignore] (default: warn)\",\n)\n\n\ndef _handle_merge_hash(\n    option: Option, opt_str: str, value: str, parser: OptionParser\n) -> None:\n    \"\"\"Given a value spelled \"algo:digest\", append the digest to a list\n    pointed to in a dict by the algo name.\"\"\"\n    if not parser.values.hashes:\n        parser.values.hashes = {}\n    try:\n        algo, digest = value.split(\":\", 1)\n    except ValueError:\n        parser.error(\n            f\"Arguments to {opt_str} must be a hash name \"\n            \"followed by a value, like --hash=sha256:\"\n            \"abcde...\"\n        )\n    if algo not in STRONG_HASHES:\n        parser.error(\n            \"Allowed hash algorithms for {} are {}.\".format(\n                opt_str, \", \".join(STRONG_HASHES)\n            )\n        )\n    parser.values.hashes.setdefault(algo, []).append(digest)\n\n\nhash: Callable[..., Option] = partial(\n    Option,\n    \"--hash\",\n    # Hash values eventually end up in InstallRequirement.hashes due to\n    # __dict__ copying in process_line().\n    dest=\"hashes\",\n    action=\"callback\",\n    callback=_handle_merge_hash,\n    type=\"string\",\n    help=\"Verify that the package's archive matches this \"\n    \"hash before installing. Example: --hash=sha256:abcdef...\",\n)\n\n\nrequire_hashes: Callable[..., Option] = partial(\n    Option,\n    \"--require-hashes\",\n    dest=\"require_hashes\",\n    action=\"store_true\",\n    default=False,\n    help=\"Require a hash to check each requirement against, for \"\n    \"repeatable installs. This option is implied when any package in a \"\n    \"requirements file has a --hash option.\",\n)\n\n\nlist_path: Callable[..., Option] = partial(\n    PipOption,\n    \"--path\",\n    dest=\"path\",\n    type=\"path\",\n    action=\"append\",\n    help=\"Restrict to the specified installation path for listing \"\n    \"packages (can be used multiple times).\",\n)\n\n\ndef check_list_path_option(options: Values) -> None:\n    if options.path and (options.user or options.local):\n        raise CommandError(\"Cannot combine '--path' with '--user' or '--local'\")\n\n\nlist_exclude: Callable[..., Option] = partial(\n    PipOption,\n    \"--exclude\",\n    dest=\"excludes\",\n    action=\"append\",\n    metavar=\"package\",\n    type=\"package_name\",\n    help=\"Exclude specified package from the output\",\n)\n\n\nno_python_version_warning: Callable[..., Option] = partial(\n    Option,\n    \"--no-python-version-warning\",\n    dest=\"no_python_version_warning\",\n    action=\"store_true\",\n    default=False,\n    help=\"Silence deprecation warnings for upcoming unsupported Pythons.\",\n)\n\n\n# Features that are now always on. A warning is printed if they are used.\nALWAYS_ENABLED_FEATURES = [\n    \"truststore\",  # always on since 24.2\n    \"no-binary-enable-wheel-cache\",  # always on since 23.1\n]\n\nuse_new_feature: Callable[..., Option] = partial(\n    Option,\n    \"--use-feature\",\n    dest=\"features_enabled\",\n    metavar=\"feature\",\n    action=\"append\",\n    default=[],\n    choices=[\n        \"fast-deps\",\n    ]\n    + ALWAYS_ENABLED_FEATURES,\n    help=\"Enable new functionality, that may be backward incompatible.\",\n)\n\nuse_deprecated_feature: Callable[..., Option] = partial(\n    Option,\n    \"--use-deprecated\",\n    dest=\"deprecated_features_enabled\",\n    metavar=\"feature\",\n    action=\"append\",\n    default=[],\n    choices=[\n        \"legacy-resolver\",\n        \"legacy-certs\",\n    ],\n    help=(\"Enable deprecated functionality, that will be removed in the future.\"),\n)\n\n\n##########\n# groups #\n##########\n\ngeneral_group: Dict[str, Any] = {\n    \"name\": \"General Options\",\n    \"options\": [\n        help_,\n        debug_mode,\n        isolated_mode,\n        require_virtualenv,\n        python,\n        verbose,\n        version,\n        quiet,\n        log,\n        no_input,\n        keyring_provider,\n        proxy,\n        retries,\n        timeout,\n        exists_action,\n        trusted_host,\n        cert,\n        client_cert,\n        cache_dir,\n        no_cache,\n        disable_pip_version_check,\n        no_color,\n        no_python_version_warning,\n        use_new_feature,\n        use_deprecated_feature,\n    ],\n}\n\nindex_group: Dict[str, Any] = {\n    \"name\": \"Package Index Options\",\n    \"options\": [\n        index_url,\n        extra_index_url,\n        no_index,\n        find_links,\n    ],\n}\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/cli/command_context.py","size":774,"sha1":"07cfd732dc65402c9e687dd7871ad3db39ee6b15","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"from contextlib import ExitStack, contextmanager\nfrom typing import ContextManager, Generator, TypeVar\n\n_T = TypeVar(\"_T\", covariant=True)\n\n\nclass CommandContextMixIn:\n    def __init__(self) -> None:\n        super().__init__()\n        self._in_main_context = False\n        self._main_context = ExitStack()\n\n    @contextmanager\n    def main_context(self) -> Generator[None, None, None]:\n        assert not self._in_main_context\n\n        self._in_main_context = True\n        try:\n            with self._main_context:\n                yield\n        finally:\n            self._in_main_context = False\n\n    def enter_context(self, context_provider: ContextManager[_T]) -> _T:\n        assert self._in_main_context\n\n        return self._main_context.enter_context(context_provider)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/cli/index_command.py","size":5677,"sha1":"5c20278f4078db5292d198f1e2e9fd4f92fb0557","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"\nContains command classes which may interact with an index / the network.\n\nUnlike its sister module, req_command, this module still uses lazy imports\nso commands which don't always hit the network (e.g. list w/o --outdated or\n--uptodate) don't need waste time importing PipSession and friends.\n\"\"\"\n\nimport logging\nimport os\nimport sys\nfrom optparse import Values\nfrom typing import TYPE_CHECKING, List, Optional\n\nfrom pip._vendor import certifi\n\nfrom pip._internal.cli.base_command import Command\nfrom pip._internal.cli.command_context import CommandContextMixIn\n\nif TYPE_CHECKING:\n    from ssl import SSLContext\n\n    from pip._internal.network.session import PipSession\n\nlogger = logging.getLogger(__name__)\n\n\ndef _create_truststore_ssl_context() -> Optional[\"SSLContext\"]:\n    if sys.version_info < (3, 10):\n        logger.debug(\"Disabling truststore because Python version isn't 3.10+\")\n        return None\n\n    try:\n        import ssl\n    except ImportError:\n        logger.warning(\"Disabling truststore since ssl support is missing\")\n        return None\n\n    try:\n        from pip._vendor import truststore\n    except ImportError:\n        logger.warning(\"Disabling truststore because platform isn't supported\")\n        return None\n\n    ctx = truststore.SSLContext(ssl.PROTOCOL_TLS_CLIENT)\n    ctx.load_verify_locations(certifi.where())\n    return ctx\n\n\nclass SessionCommandMixin(CommandContextMixIn):\n    \"\"\"\n    A class mixin for command classes needing _build_session().\n    \"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n        self._session: Optional[PipSession] = None\n\n    @classmethod\n    def _get_index_urls(cls, options: Values) -> Optional[List[str]]:\n        \"\"\"Return a list of index urls from user-provided options.\"\"\"\n        index_urls = []\n        if not getattr(options, \"no_index\", False):\n            url = getattr(options, \"index_url\", None)\n            if url:\n                index_urls.append(url)\n        urls = getattr(options, \"extra_index_urls\", None)\n        if urls:\n            index_urls.extend(urls)\n        # Return None rather than an empty list\n        return index_urls or None\n\n    def get_default_session(self, options: Values) -> \"PipSession\":\n        \"\"\"Get a default-managed session.\"\"\"\n        if self._session is None:\n            self._session = self.enter_context(self._build_session(options))\n            # there's no type annotation on requests.Session, so it's\n            # automatically ContextManager[Any] and self._session becomes Any,\n            # then https://github.com/python/mypy/issues/7696 kicks in\n            assert self._session is not None\n        return self._session\n\n    def _build_session(\n        self,\n        options: Values,\n        retries: Optional[int] = None,\n        timeout: Optional[int] = None,\n    ) -> \"PipSession\":\n        from pip._internal.network.session import PipSession\n\n        cache_dir = options.cache_dir\n        assert not cache_dir or os.path.isabs(cache_dir)\n\n        if \"legacy-certs\" not in options.deprecated_features_enabled:\n            ssl_context = _create_truststore_ssl_context()\n        else:\n            ssl_context = None\n\n        session = PipSession(\n            cache=os.path.join(cache_dir, \"http-v2\") if cache_dir else None,\n            retries=retries if retries is not None else options.retries,\n            trusted_hosts=options.trusted_hosts,\n            index_urls=self._get_index_urls(options),\n            ssl_context=ssl_context,\n        )\n\n        # Handle custom ca-bundles from the user\n        if options.cert:\n            session.verify = options.cert\n\n        # Handle SSL client certificate\n        if options.client_cert:\n            session.cert = options.client_cert\n\n        # Handle timeouts\n        if options.timeout or timeout:\n            session.timeout = timeout if timeout is not None else options.timeout\n\n        # Handle configured proxies\n        if options.proxy:\n            session.proxies = {\n                \"http\": options.proxy,\n                \"https\": options.proxy,\n            }\n            session.trust_env = False\n            session.pip_proxy = options.proxy\n\n        # Determine if we can prompt the user for authentication or not\n        session.auth.prompting = not options.no_input\n        session.auth.keyring_provider = options.keyring_provider\n\n        return session\n\n\ndef _pip_self_version_check(session: \"PipSession\", options: Values) -> None:\n    from pip._internal.self_outdated_check import pip_self_version_check as check\n\n    check(session, options)\n\n\nclass IndexGroupCommand(Command, SessionCommandMixin):\n    \"\"\"\n    Abstract base class for commands with the index_group options.\n\n    This also corresponds to the commands that permit the pip version check.\n    \"\"\"\n\n    def handle_pip_version_check(self, options: Values) -> None:\n        \"\"\"\n        Do the pip version check if not disabled.\n\n        This overrides the default behavior of not doing the check.\n        \"\"\"\n        # Make sure the index_group options are present.\n        assert hasattr(options, \"no_index\")\n\n        if options.disable_pip_version_check or options.no_index:\n            return\n\n        try:\n            # Otherwise, check if we're using the latest version of pip available.\n            session = self._build_session(\n                options,\n                retries=0,\n                timeout=min(5, options.timeout),\n            )\n            with session:\n                _pip_self_version_check(session, options)\n        except Exception:\n            logger.warning(\"There was an error checking the latest version of pip.\")\n            logger.debug(\"See below for error\", exc_info=True)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/cli/main.py","size":2817,"sha1":"0e1833849981e2e55ee64824968688319eb0205f","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"Primary application entrypoint.\n\"\"\"\n\nimport locale\nimport logging\nimport os\nimport sys\nimport warnings\nfrom typing import List, Optional\n\nfrom pip._internal.cli.autocompletion import autocomplete\nfrom pip._internal.cli.main_parser import parse_command\nfrom pip._internal.commands import create_command\nfrom pip._internal.exceptions import PipError\nfrom pip._internal.utils import deprecation\n\nlogger = logging.getLogger(__name__)\n\n\n# Do not import and use main() directly! Using it directly is actively\n# discouraged by pip's maintainers. The name, location and behavior of\n# this function is subject to change, so calling it directly is not\n# portable across different pip versions.\n\n# In addition, running pip in-process is unsupported and unsafe. This is\n# elaborated in detail at\n# https://pip.pypa.io/en/stable/user_guide/#using-pip-from-your-program.\n# That document also provides suggestions that should work for nearly\n# all users that are considering importing and using main() directly.\n\n# However, we know that certain users will still want to invoke pip\n# in-process. If you understand and accept the implications of using pip\n# in an unsupported manner, the best approach is to use runpy to avoid\n# depending on the exact location of this entry point.\n\n# The following example shows how to use runpy to invoke pip in that\n# case:\n#\n#     sys.argv = [\"pip\", your, args, here]\n#     runpy.run_module(\"pip\", run_name=\"__main__\")\n#\n# Note that this will exit the process after running, unlike a direct\n# call to main. As it is not safe to do any processing after calling\n# main, this should not be an issue in practice.\n\n\ndef main(args: Optional[List[str]] = None) -> int:\n    if args is None:\n        args = sys.argv[1:]\n\n    # Suppress the pkg_resources deprecation warning\n    # Note - we use a module of .*pkg_resources to cover\n    # the normal case (pip._vendor.pkg_resources) and the\n    # devendored case (a bare pkg_resources)\n    warnings.filterwarnings(\n        action=\"ignore\", category=DeprecationWarning, module=\".*pkg_resources\"\n    )\n\n    # Configure our deprecation warnings to be sent through loggers\n    deprecation.install_warning_logger()\n\n    autocomplete()\n\n    try:\n        cmd_name, cmd_args = parse_command(args)\n    except PipError as exc:\n        sys.stderr.write(f\"ERROR: {exc}\")\n        sys.stderr.write(os.linesep)\n        sys.exit(1)\n\n    # Needed for locale.getpreferredencoding(False) to work\n    # in pip._internal.utils.encoding.auto_decode\n    try:\n        locale.setlocale(locale.LC_ALL, \"\")\n    except locale.Error as e:\n        # setlocale can apparently crash if locale are uninitialized\n        logger.debug(\"Ignoring error %s when setting locale\", e)\n    command = create_command(cmd_name, isolated=(\"--isolated\" in cmd_args))\n\n    return command.main(cmd_args)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/cli/main_parser.py","size":4338,"sha1":"8a34ef596ae1821215cc580b3f5a441f668c07cd","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"A single place for constructing and exposing the main parser\n\"\"\"\n\nimport os\nimport subprocess\nimport sys\nfrom typing import List, Optional, Tuple\n\nfrom pip._internal.build_env import get_runnable_pip\nfrom pip._internal.cli import cmdoptions\nfrom pip._internal.cli.parser import ConfigOptionParser, UpdatingDefaultsHelpFormatter\nfrom pip._internal.commands import commands_dict, get_similar_commands\nfrom pip._internal.exceptions import CommandError\nfrom pip._internal.utils.misc import get_pip_version, get_prog\n\n__all__ = [\"create_main_parser\", \"parse_command\"]\n\n\ndef create_main_parser() -> ConfigOptionParser:\n    \"\"\"Creates and returns the main parser for pip's CLI\"\"\"\n\n    parser = ConfigOptionParser(\n        usage=\"\\n%prog <command> [options]\",\n        add_help_option=False,\n        formatter=UpdatingDefaultsHelpFormatter(),\n        name=\"global\",\n        prog=get_prog(),\n    )\n    parser.disable_interspersed_args()\n\n    parser.version = get_pip_version()\n\n    # add the general options\n    gen_opts = cmdoptions.make_option_group(cmdoptions.general_group, parser)\n    parser.add_option_group(gen_opts)\n\n    # so the help formatter knows\n    parser.main = True  # type: ignore\n\n    # create command listing for description\n    description = [\"\"] + [\n        f\"{name:27} {command_info.summary}\"\n        for name, command_info in commands_dict.items()\n    ]\n    parser.description = \"\\n\".join(description)\n\n    return parser\n\n\ndef identify_python_interpreter(python: str) -> Optional[str]:\n    # If the named file exists, use it.\n    # If it's a directory, assume it's a virtual environment and\n    # look for the environment's Python executable.\n    if os.path.exists(python):\n        if os.path.isdir(python):\n            # bin/python for Unix, Scripts/python.exe for Windows\n            # Try both in case of odd cases like cygwin.\n            for exe in (\"bin/python\", \"Scripts/python.exe\"):\n                py = os.path.join(python, exe)\n                if os.path.exists(py):\n                    return py\n        else:\n            return python\n\n    # Could not find the interpreter specified\n    return None\n\n\ndef parse_command(args: List[str]) -> Tuple[str, List[str]]:\n    parser = create_main_parser()\n\n    # Note: parser calls disable_interspersed_args(), so the result of this\n    # call is to split the initial args into the general options before the\n    # subcommand and everything else.\n    # For example:\n    #  args: ['--timeout=5', 'install', '--user', 'INITools']\n    #  general_options: ['--timeout==5']\n    #  args_else: ['install', '--user', 'INITools']\n    general_options, args_else = parser.parse_args(args)\n\n    # --python\n    if general_options.python and \"_PIP_RUNNING_IN_SUBPROCESS\" not in os.environ:\n        # Re-invoke pip using the specified Python interpreter\n        interpreter = identify_python_interpreter(general_options.python)\n        if interpreter is None:\n            raise CommandError(\n                f\"Could not locate Python interpreter {general_options.python}\"\n            )\n\n        pip_cmd = [\n            interpreter,\n            get_runnable_pip(),\n        ]\n        pip_cmd.extend(args)\n\n        # Set a flag so the child doesn't re-invoke itself, causing\n        # an infinite loop.\n        os.environ[\"_PIP_RUNNING_IN_SUBPROCESS\"] = \"1\"\n        returncode = 0\n        try:\n            proc = subprocess.run(pip_cmd)\n            returncode = proc.returncode\n        except (subprocess.SubprocessError, OSError) as exc:\n            raise CommandError(f\"Failed to run pip under {interpreter}: {exc}\")\n        sys.exit(returncode)\n\n    # --version\n    if general_options.version:\n        sys.stdout.write(parser.version)\n        sys.stdout.write(os.linesep)\n        sys.exit()\n\n    # pip || pip help -> print_help()\n    if not args_else or (args_else[0] == \"help\" and len(args_else) == 1):\n        parser.print_help()\n        sys.exit()\n\n    # the subcommand name\n    cmd_name = args_else[0]\n\n    if cmd_name not in commands_dict:\n        guess = get_similar_commands(cmd_name)\n\n        msg = [f'unknown command \"{cmd_name}\"']\n        if guess:\n            msg.append(f'maybe you meant \"{guess}\"')\n\n        raise CommandError(\" - \".join(msg))\n\n    # all the args without the subcommand\n    cmd_args = args[:]\n    cmd_args.remove(cmd_name)\n\n    return cmd_name, cmd_args\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/cli/parser.py","size":10825,"sha1":"30f8dc09ad523ab6897b6b0567110b8e249c2f57","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"Base option parser setup\"\"\"\n\nimport logging\nimport optparse\nimport shutil\nimport sys\nimport textwrap\nfrom contextlib import suppress\nfrom typing import Any, Dict, Generator, List, NoReturn, Optional, Tuple\n\nfrom pip._internal.cli.status_codes import UNKNOWN_ERROR\nfrom pip._internal.configuration import Configuration, ConfigurationError\nfrom pip._internal.utils.misc import redact_auth_from_url, strtobool\n\nlogger = logging.getLogger(__name__)\n\n\nclass PrettyHelpFormatter(optparse.IndentedHelpFormatter):\n    \"\"\"A prettier/less verbose help formatter for optparse.\"\"\"\n\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\n        # help position must be aligned with __init__.parseopts.description\n        kwargs[\"max_help_position\"] = 30\n        kwargs[\"indent_increment\"] = 1\n        kwargs[\"width\"] = shutil.get_terminal_size()[0] - 2\n        super().__init__(*args, **kwargs)\n\n    def format_option_strings(self, option: optparse.Option) -> str:\n        return self._format_option_strings(option)\n\n    def _format_option_strings(\n        self, option: optparse.Option, mvarfmt: str = \" <{}>\", optsep: str = \", \"\n    ) -> str:\n        \"\"\"\n        Return a comma-separated list of option strings and metavars.\n\n        :param option:  tuple of (short opt, long opt), e.g: ('-f', '--format')\n        :param mvarfmt: metavar format string\n        :param optsep:  separator\n        \"\"\"\n        opts = []\n\n        if option._short_opts:\n            opts.append(option._short_opts[0])\n        if option._long_opts:\n            opts.append(option._long_opts[0])\n        if len(opts) > 1:\n            opts.insert(1, optsep)\n\n        if option.takes_value():\n            assert option.dest is not None\n            metavar = option.metavar or option.dest.lower()\n            opts.append(mvarfmt.format(metavar.lower()))\n\n        return \"\".join(opts)\n\n    def format_heading(self, heading: str) -> str:\n        if heading == \"Options\":\n            return \"\"\n        return heading + \":\\n\"\n\n    def format_usage(self, usage: str) -> str:\n        \"\"\"\n        Ensure there is only one newline between usage and the first heading\n        if there is no description.\n        \"\"\"\n        msg = \"\\nUsage: {}\\n\".format(self.indent_lines(textwrap.dedent(usage), \"  \"))\n        return msg\n\n    def format_description(self, description: Optional[str]) -> str:\n        # leave full control over description to us\n        if description:\n            if hasattr(self.parser, \"main\"):\n                label = \"Commands\"\n            else:\n                label = \"Description\"\n            # some doc strings have initial newlines, some don't\n            description = description.lstrip(\"\\n\")\n            # some doc strings have final newlines and spaces, some don't\n            description = description.rstrip()\n            # dedent, then reindent\n            description = self.indent_lines(textwrap.dedent(description), \"  \")\n            description = f\"{label}:\\n{description}\\n\"\n            return description\n        else:\n            return \"\"\n\n    def format_epilog(self, epilog: Optional[str]) -> str:\n        # leave full control over epilog to us\n        if epilog:\n            return epilog\n        else:\n            return \"\"\n\n    def indent_lines(self, text: str, indent: str) -> str:\n        new_lines = [indent + line for line in text.split(\"\\n\")]\n        return \"\\n\".join(new_lines)\n\n\nclass UpdatingDefaultsHelpFormatter(PrettyHelpFormatter):\n    \"\"\"Custom help formatter for use in ConfigOptionParser.\n\n    This is updates the defaults before expanding them, allowing\n    them to show up correctly in the help listing.\n\n    Also redact auth from url type options\n    \"\"\"\n\n    def expand_default(self, option: optparse.Option) -> str:\n        default_values = None\n        if self.parser is not None:\n            assert isinstance(self.parser, ConfigOptionParser)\n            self.parser._update_defaults(self.parser.defaults)\n            assert option.dest is not None\n            default_values = self.parser.defaults.get(option.dest)\n        help_text = super().expand_default(option)\n\n        if default_values and option.metavar == \"URL\":\n            if isinstance(default_values, str):\n                default_values = [default_values]\n\n            # If its not a list, we should abort and just return the help text\n            if not isinstance(default_values, list):\n                default_values = []\n\n            for val in default_values:\n                help_text = help_text.replace(val, redact_auth_from_url(val))\n\n        return help_text\n\n\nclass CustomOptionParser(optparse.OptionParser):\n    def insert_option_group(\n        self, idx: int, *args: Any, **kwargs: Any\n    ) -> optparse.OptionGroup:\n        \"\"\"Insert an OptionGroup at a given position.\"\"\"\n        group = self.add_option_group(*args, **kwargs)\n\n        self.option_groups.pop()\n        self.option_groups.insert(idx, group)\n\n        return group\n\n    @property\n    def option_list_all(self) -> List[optparse.Option]:\n        \"\"\"Get a list of all options, including those in option groups.\"\"\"\n        res = self.option_list[:]\n        for i in self.option_groups:\n            res.extend(i.option_list)\n\n        return res\n\n\nclass ConfigOptionParser(CustomOptionParser):\n    \"\"\"Custom option parser which updates its defaults by checking the\n    configuration files and environmental variables\"\"\"\n\n    def __init__(\n        self,\n        *args: Any,\n        name: str,\n        isolated: bool = False,\n        **kwargs: Any,\n    ) -> None:\n        self.name = name\n        self.config = Configuration(isolated)\n\n        assert self.name\n        super().__init__(*args, **kwargs)\n\n    def check_default(self, option: optparse.Option, key: str, val: Any) -> Any:\n        try:\n            return option.check_value(key, val)\n        except optparse.OptionValueError as exc:\n            print(f\"An error occurred during configuration: {exc}\")\n            sys.exit(3)\n\n    def _get_ordered_configuration_items(\n        self,\n    ) -> Generator[Tuple[str, Any], None, None]:\n        # Configuration gives keys in an unordered manner. Order them.\n        override_order = [\"global\", self.name, \":env:\"]\n\n        # Pool the options into different groups\n        section_items: Dict[str, List[Tuple[str, Any]]] = {\n            name: [] for name in override_order\n        }\n        for section_key, val in self.config.items():\n            # ignore empty values\n            if not val:\n                logger.debug(\n                    \"Ignoring configuration key '%s' as it's value is empty.\",\n                    section_key,\n                )\n                continue\n\n            section, key = section_key.split(\".\", 1)\n            if section in override_order:\n                section_items[section].append((key, val))\n\n        # Yield each group in their override order\n        for section in override_order:\n            for key, val in section_items[section]:\n                yield key, val\n\n    def _update_defaults(self, defaults: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Updates the given defaults with values from the config files and\n        the environ. Does a little special handling for certain types of\n        options (lists).\"\"\"\n\n        # Accumulate complex default state.\n        self.values = optparse.Values(self.defaults)\n        late_eval = set()\n        # Then set the options with those values\n        for key, val in self._get_ordered_configuration_items():\n            # '--' because configuration supports only long names\n            option = self.get_option(\"--\" + key)\n\n            # Ignore options not present in this parser. E.g. non-globals put\n            # in [global] by users that want them to apply to all applicable\n            # commands.\n            if option is None:\n                continue\n\n            assert option.dest is not None\n\n            if option.action in (\"store_true\", \"store_false\"):\n                try:\n                    val = strtobool(val)\n                except ValueError:\n                    self.error(\n                        f\"{val} is not a valid value for {key} option, \"\n                        \"please specify a boolean value like yes/no, \"\n                        \"true/false or 1/0 instead.\"\n                    )\n            elif option.action == \"count\":\n                with suppress(ValueError):\n                    val = strtobool(val)\n                with suppress(ValueError):\n                    val = int(val)\n                if not isinstance(val, int) or val < 0:\n                    self.error(\n                        f\"{val} is not a valid value for {key} option, \"\n                        \"please instead specify either a non-negative integer \"\n                        \"or a boolean value like yes/no or false/true \"\n                        \"which is equivalent to 1/0.\"\n                    )\n            elif option.action == \"append\":\n                val = val.split()\n                val = [self.check_default(option, key, v) for v in val]\n            elif option.action == \"callback\":\n                assert option.callback is not None\n                late_eval.add(option.dest)\n                opt_str = option.get_opt_string()\n                val = option.convert_value(opt_str, val)\n                # From take_action\n                args = option.callback_args or ()\n                kwargs = option.callback_kwargs or {}\n                option.callback(option, opt_str, val, self, *args, **kwargs)\n            else:\n                val = self.check_default(option, key, val)\n\n            defaults[option.dest] = val\n\n        for key in late_eval:\n            defaults[key] = getattr(self.values, key)\n        self.values = None\n        return defaults\n\n    def get_default_values(self) -> optparse.Values:\n        \"\"\"Overriding to make updating the defaults after instantiation of\n        the option parser possible, _update_defaults() does the dirty work.\"\"\"\n        if not self.process_default_values:\n            # Old, pre-Optik 1.5 behaviour.\n            return optparse.Values(self.defaults)\n\n        # Load the configuration, or error out in case of an error\n        try:\n            self.config.load()\n        except ConfigurationError as err:\n            self.exit(UNKNOWN_ERROR, str(err))\n\n        defaults = self._update_defaults(self.defaults.copy())  # ours\n        for option in self._get_all_options():\n            assert option.dest is not None\n            default = defaults.get(option.dest)\n            if isinstance(default, str):\n                opt_str = option.get_opt_string()\n                defaults[option.dest] = option.check_value(opt_str, default)\n        return optparse.Values(defaults)\n\n    def error(self, msg: str) -> NoReturn:\n        self.print_usage(sys.stderr)\n        self.exit(UNKNOWN_ERROR, f\"{msg}\\n\")\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/cli/progress_bars.py","size":2717,"sha1":"e58e5c9ddb33f25e44d532f0ffd0adc637dfa4c8","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import functools\nimport sys\nfrom typing import Callable, Generator, Iterable, Iterator, Optional, Tuple\n\nfrom pip._vendor.rich.progress import (\n    BarColumn,\n    DownloadColumn,\n    FileSizeColumn,\n    Progress,\n    ProgressColumn,\n    SpinnerColumn,\n    TextColumn,\n    TimeElapsedColumn,\n    TimeRemainingColumn,\n    TransferSpeedColumn,\n)\n\nfrom pip._internal.cli.spinners import RateLimiter\nfrom pip._internal.utils.logging import get_indentation\n\nDownloadProgressRenderer = Callable[[Iterable[bytes]], Iterator[bytes]]\n\n\ndef _rich_progress_bar(\n    iterable: Iterable[bytes],\n    *,\n    bar_type: str,\n    size: Optional[int],\n) -> Generator[bytes, None, None]:\n    assert bar_type == \"on\", \"This should only be used in the default mode.\"\n\n    if not size:\n        total = float(\"inf\")\n        columns: Tuple[ProgressColumn, ...] = (\n            TextColumn(\"[progress.description]{task.description}\"),\n            SpinnerColumn(\"line\", speed=1.5),\n            FileSizeColumn(),\n            TransferSpeedColumn(),\n            TimeElapsedColumn(),\n        )\n    else:\n        total = size\n        columns = (\n            TextColumn(\"[progress.description]{task.description}\"),\n            BarColumn(),\n            DownloadColumn(),\n            TransferSpeedColumn(),\n            TextColumn(\"eta\"),\n            TimeRemainingColumn(),\n        )\n\n    progress = Progress(*columns, refresh_per_second=5)\n    task_id = progress.add_task(\" \" * (get_indentation() + 2), total=total)\n    with progress:\n        for chunk in iterable:\n            yield chunk\n            progress.update(task_id, advance=len(chunk))\n\n\ndef _raw_progress_bar(\n    iterable: Iterable[bytes],\n    *,\n    size: Optional[int],\n) -> Generator[bytes, None, None]:\n    def write_progress(current: int, total: int) -> None:\n        sys.stdout.write(f\"Progress {current} of {total}\\n\")\n        sys.stdout.flush()\n\n    current = 0\n    total = size or 0\n    rate_limiter = RateLimiter(0.25)\n\n    write_progress(current, total)\n    for chunk in iterable:\n        current += len(chunk)\n        if rate_limiter.ready() or current == total:\n            write_progress(current, total)\n            rate_limiter.reset()\n        yield chunk\n\n\ndef get_download_progress_renderer(\n    *, bar_type: str, size: Optional[int] = None\n) -> DownloadProgressRenderer:\n    \"\"\"Get an object that can be used to render the download progress.\n\n    Returns a callable, that takes an iterable to \"wrap\".\n    \"\"\"\n    if bar_type == \"on\":\n        return functools.partial(_rich_progress_bar, bar_type=bar_type, size=size)\n    elif bar_type == \"raw\":\n        return functools.partial(_raw_progress_bar, size=size)\n    else:\n        return iter  # no-op, when passed an iterator\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/cli/req_command.py","size":12250,"sha1":"3ba7a3f6190c94fc97f0d2deb135202e1b11223c","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"Contains the RequirementCommand base class.\n\nThis class is in a separate module so the commands that do not always\nneed PackageFinder capability don't unnecessarily import the\nPackageFinder machinery and all its vendored dependencies, etc.\n\"\"\"\n\nimport logging\nfrom functools import partial\nfrom optparse import Values\nfrom typing import Any, List, Optional, Tuple\n\nfrom pip._internal.cache import WheelCache\nfrom pip._internal.cli import cmdoptions\nfrom pip._internal.cli.index_command import IndexGroupCommand\nfrom pip._internal.cli.index_command import SessionCommandMixin as SessionCommandMixin\nfrom pip._internal.exceptions import CommandError, PreviousBuildDirError\nfrom pip._internal.index.collector import LinkCollector\nfrom pip._internal.index.package_finder import PackageFinder\nfrom pip._internal.models.selection_prefs import SelectionPreferences\nfrom pip._internal.models.target_python import TargetPython\nfrom pip._internal.network.session import PipSession\nfrom pip._internal.operations.build.build_tracker import BuildTracker\nfrom pip._internal.operations.prepare import RequirementPreparer\nfrom pip._internal.req.constructors import (\n    install_req_from_editable,\n    install_req_from_line,\n    install_req_from_parsed_requirement,\n    install_req_from_req_string,\n)\nfrom pip._internal.req.req_file import parse_requirements\nfrom pip._internal.req.req_install import InstallRequirement\nfrom pip._internal.resolution.base import BaseResolver\nfrom pip._internal.utils.temp_dir import (\n    TempDirectory,\n    TempDirectoryTypeRegistry,\n    tempdir_kinds,\n)\n\nlogger = logging.getLogger(__name__)\n\n\nKEEPABLE_TEMPDIR_TYPES = [\n    tempdir_kinds.BUILD_ENV,\n    tempdir_kinds.EPHEM_WHEEL_CACHE,\n    tempdir_kinds.REQ_BUILD,\n]\n\n\ndef with_cleanup(func: Any) -> Any:\n    \"\"\"Decorator for common logic related to managing temporary\n    directories.\n    \"\"\"\n\n    def configure_tempdir_registry(registry: TempDirectoryTypeRegistry) -> None:\n        for t in KEEPABLE_TEMPDIR_TYPES:\n            registry.set_delete(t, False)\n\n    def wrapper(\n        self: RequirementCommand, options: Values, args: List[Any]\n    ) -> Optional[int]:\n        assert self.tempdir_registry is not None\n        if options.no_clean:\n            configure_tempdir_registry(self.tempdir_registry)\n\n        try:\n            return func(self, options, args)\n        except PreviousBuildDirError:\n            # This kind of conflict can occur when the user passes an explicit\n            # build directory with a pre-existing folder. In that case we do\n            # not want to accidentally remove it.\n            configure_tempdir_registry(self.tempdir_registry)\n            raise\n\n    return wrapper\n\n\nclass RequirementCommand(IndexGroupCommand):\n    def __init__(self, *args: Any, **kw: Any) -> None:\n        super().__init__(*args, **kw)\n\n        self.cmd_opts.add_option(cmdoptions.no_clean())\n\n    @staticmethod\n    def determine_resolver_variant(options: Values) -> str:\n        \"\"\"Determines which resolver should be used, based on the given options.\"\"\"\n        if \"legacy-resolver\" in options.deprecated_features_enabled:\n            return \"legacy\"\n\n        return \"resolvelib\"\n\n    @classmethod\n    def make_requirement_preparer(\n        cls,\n        temp_build_dir: TempDirectory,\n        options: Values,\n        build_tracker: BuildTracker,\n        session: PipSession,\n        finder: PackageFinder,\n        use_user_site: bool,\n        download_dir: Optional[str] = None,\n        verbosity: int = 0,\n    ) -> RequirementPreparer:\n        \"\"\"\n        Create a RequirementPreparer instance for the given parameters.\n        \"\"\"\n        temp_build_dir_path = temp_build_dir.path\n        assert temp_build_dir_path is not None\n        legacy_resolver = False\n\n        resolver_variant = cls.determine_resolver_variant(options)\n        if resolver_variant == \"resolvelib\":\n            lazy_wheel = \"fast-deps\" in options.features_enabled\n            if lazy_wheel:\n                logger.warning(\n                    \"pip is using lazily downloaded wheels using HTTP \"\n                    \"range requests to obtain dependency information. \"\n                    \"This experimental feature is enabled through \"\n                    \"--use-feature=fast-deps and it is not ready for \"\n                    \"production.\"\n                )\n        else:\n            legacy_resolver = True\n            lazy_wheel = False\n            if \"fast-deps\" in options.features_enabled:\n                logger.warning(\n                    \"fast-deps has no effect when used with the legacy resolver.\"\n                )\n\n        return RequirementPreparer(\n            build_dir=temp_build_dir_path,\n            src_dir=options.src_dir,\n            download_dir=download_dir,\n            build_isolation=options.build_isolation,\n            check_build_deps=options.check_build_deps,\n            build_tracker=build_tracker,\n            session=session,\n            progress_bar=options.progress_bar,\n            finder=finder,\n            require_hashes=options.require_hashes,\n            use_user_site=use_user_site,\n            lazy_wheel=lazy_wheel,\n            verbosity=verbosity,\n            legacy_resolver=legacy_resolver,\n        )\n\n    @classmethod\n    def make_resolver(\n        cls,\n        preparer: RequirementPreparer,\n        finder: PackageFinder,\n        options: Values,\n        wheel_cache: Optional[WheelCache] = None,\n        use_user_site: bool = False,\n        ignore_installed: bool = True,\n        ignore_requires_python: bool = False,\n        force_reinstall: bool = False,\n        upgrade_strategy: str = \"to-satisfy-only\",\n        use_pep517: Optional[bool] = None,\n        py_version_info: Optional[Tuple[int, ...]] = None,\n    ) -> BaseResolver:\n        \"\"\"\n        Create a Resolver instance for the given parameters.\n        \"\"\"\n        make_install_req = partial(\n            install_req_from_req_string,\n            isolated=options.isolated_mode,\n            use_pep517=use_pep517,\n        )\n        resolver_variant = cls.determine_resolver_variant(options)\n        # The long import name and duplicated invocation is needed to convince\n        # Mypy into correctly typechecking. Otherwise it would complain the\n        # \"Resolver\" class being redefined.\n        if resolver_variant == \"resolvelib\":\n            import pip._internal.resolution.resolvelib.resolver\n\n            return pip._internal.resolution.resolvelib.resolver.Resolver(\n                preparer=preparer,\n                finder=finder,\n                wheel_cache=wheel_cache,\n                make_install_req=make_install_req,\n                use_user_site=use_user_site,\n                ignore_dependencies=options.ignore_dependencies,\n                ignore_installed=ignore_installed,\n                ignore_requires_python=ignore_requires_python,\n                force_reinstall=force_reinstall,\n                upgrade_strategy=upgrade_strategy,\n                py_version_info=py_version_info,\n            )\n        import pip._internal.resolution.legacy.resolver\n\n        return pip._internal.resolution.legacy.resolver.Resolver(\n            preparer=preparer,\n            finder=finder,\n            wheel_cache=wheel_cache,\n            make_install_req=make_install_req,\n            use_user_site=use_user_site,\n            ignore_dependencies=options.ignore_dependencies,\n            ignore_installed=ignore_installed,\n            ignore_requires_python=ignore_requires_python,\n            force_reinstall=force_reinstall,\n            upgrade_strategy=upgrade_strategy,\n            py_version_info=py_version_info,\n        )\n\n    def get_requirements(\n        self,\n        args: List[str],\n        options: Values,\n        finder: PackageFinder,\n        session: PipSession,\n    ) -> List[InstallRequirement]:\n        \"\"\"\n        Parse command-line arguments into the corresponding requirements.\n        \"\"\"\n        requirements: List[InstallRequirement] = []\n        for filename in options.constraints:\n            for parsed_req in parse_requirements(\n                filename,\n                constraint=True,\n                finder=finder,\n                options=options,\n                session=session,\n            ):\n                req_to_add = install_req_from_parsed_requirement(\n                    parsed_req,\n                    isolated=options.isolated_mode,\n                    user_supplied=False,\n                )\n                requirements.append(req_to_add)\n\n        for req in args:\n            req_to_add = install_req_from_line(\n                req,\n                comes_from=None,\n                isolated=options.isolated_mode,\n                use_pep517=options.use_pep517,\n                user_supplied=True,\n                config_settings=getattr(options, \"config_settings\", None),\n            )\n            requirements.append(req_to_add)\n\n        for req in options.editables:\n            req_to_add = install_req_from_editable(\n                req,\n                user_supplied=True,\n                isolated=options.isolated_mode,\n                use_pep517=options.use_pep517,\n                config_settings=getattr(options, \"config_settings\", None),\n            )\n            requirements.append(req_to_add)\n\n        # NOTE: options.require_hashes may be set if --require-hashes is True\n        for filename in options.requirements:\n            for parsed_req in parse_requirements(\n                filename, finder=finder, options=options, session=session\n            ):\n                req_to_add = install_req_from_parsed_requirement(\n                    parsed_req,\n                    isolated=options.isolated_mode,\n                    use_pep517=options.use_pep517,\n                    user_supplied=True,\n                    config_settings=(\n                        parsed_req.options.get(\"config_settings\")\n                        if parsed_req.options\n                        else None\n                    ),\n                )\n                requirements.append(req_to_add)\n\n        # If any requirement has hash options, enable hash checking.\n        if any(req.has_hash_options for req in requirements):\n            options.require_hashes = True\n\n        if not (args or options.editables or options.requirements):\n            opts = {\"name\": self.name}\n            if options.find_links:\n                raise CommandError(\n                    \"You must give at least one requirement to {name} \"\n                    '(maybe you meant \"pip {name} {links}\"?)'.format(\n                        **dict(opts, links=\" \".join(options.find_links))\n                    )\n                )\n            else:\n                raise CommandError(\n                    \"You must give at least one requirement to {name} \"\n                    '(see \"pip help {name}\")'.format(**opts)\n                )\n\n        return requirements\n\n    @staticmethod\n    def trace_basic_info(finder: PackageFinder) -> None:\n        \"\"\"\n        Trace basic information about the provided objects.\n        \"\"\"\n        # Display where finder is looking for packages\n        search_scope = finder.search_scope\n        locations = search_scope.get_formatted_locations()\n        if locations:\n            logger.info(locations)\n\n    def _build_package_finder(\n        self,\n        options: Values,\n        session: PipSession,\n        target_python: Optional[TargetPython] = None,\n        ignore_requires_python: Optional[bool] = None,\n    ) -> PackageFinder:\n        \"\"\"\n        Create a package finder appropriate to this requirement command.\n\n        :param ignore_requires_python: Whether to ignore incompatible\n            \"Requires-Python\" values in links. Defaults to False.\n        \"\"\"\n        link_collector = LinkCollector.create(session, options=options)\n        selection_prefs = SelectionPreferences(\n            allow_yanked=True,\n            format_control=options.format_control,\n            allow_all_prereleases=options.pre,\n            prefer_binary=options.prefer_binary,\n            ignore_requires_python=ignore_requires_python,\n        )\n\n        return PackageFinder.create(\n            link_collector=link_collector,\n            selection_prefs=selection_prefs,\n            target_python=target_python,\n        )\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/cli/spinners.py","size":5118,"sha1":"ecfe25bb7fde3149dc85fac71f6e92f923c51c17","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import contextlib\nimport itertools\nimport logging\nimport sys\nimport time\nfrom typing import IO, Generator, Optional\n\nfrom pip._internal.utils.compat import WINDOWS\nfrom pip._internal.utils.logging import get_indentation\n\nlogger = logging.getLogger(__name__)\n\n\nclass SpinnerInterface:\n    def spin(self) -> None:\n        raise NotImplementedError()\n\n    def finish(self, final_status: str) -> None:\n        raise NotImplementedError()\n\n\nclass InteractiveSpinner(SpinnerInterface):\n    def __init__(\n        self,\n        message: str,\n        file: Optional[IO[str]] = None,\n        spin_chars: str = \"-\\\\|/\",\n        # Empirically, 8 updates/second looks nice\n        min_update_interval_seconds: float = 0.125,\n    ):\n        self._message = message\n        if file is None:\n            file = sys.stdout\n        self._file = file\n        self._rate_limiter = RateLimiter(min_update_interval_seconds)\n        self._finished = False\n\n        self._spin_cycle = itertools.cycle(spin_chars)\n\n        self._file.write(\" \" * get_indentation() + self._message + \" ... \")\n        self._width = 0\n\n    def _write(self, status: str) -> None:\n        assert not self._finished\n        # Erase what we wrote before by backspacing to the beginning, writing\n        # spaces to overwrite the old text, and then backspacing again\n        backup = \"\\b\" * self._width\n        self._file.write(backup + \" \" * self._width + backup)\n        # Now we have a blank slate to add our status\n        self._file.write(status)\n        self._width = len(status)\n        self._file.flush()\n        self._rate_limiter.reset()\n\n    def spin(self) -> None:\n        if self._finished:\n            return\n        if not self._rate_limiter.ready():\n            return\n        self._write(next(self._spin_cycle))\n\n    def finish(self, final_status: str) -> None:\n        if self._finished:\n            return\n        self._write(final_status)\n        self._file.write(\"\\n\")\n        self._file.flush()\n        self._finished = True\n\n\n# Used for dumb terminals, non-interactive installs (no tty), etc.\n# We still print updates occasionally (once every 60 seconds by default) to\n# act as a keep-alive for systems like Travis-CI that take lack-of-output as\n# an indication that a task has frozen.\nclass NonInteractiveSpinner(SpinnerInterface):\n    def __init__(self, message: str, min_update_interval_seconds: float = 60.0) -> None:\n        self._message = message\n        self._finished = False\n        self._rate_limiter = RateLimiter(min_update_interval_seconds)\n        self._update(\"started\")\n\n    def _update(self, status: str) -> None:\n        assert not self._finished\n        self._rate_limiter.reset()\n        logger.info(\"%s: %s\", self._message, status)\n\n    def spin(self) -> None:\n        if self._finished:\n            return\n        if not self._rate_limiter.ready():\n            return\n        self._update(\"still running...\")\n\n    def finish(self, final_status: str) -> None:\n        if self._finished:\n            return\n        self._update(f\"finished with status '{final_status}'\")\n        self._finished = True\n\n\nclass RateLimiter:\n    def __init__(self, min_update_interval_seconds: float) -> None:\n        self._min_update_interval_seconds = min_update_interval_seconds\n        self._last_update: float = 0\n\n    def ready(self) -> bool:\n        now = time.time()\n        delta = now - self._last_update\n        return delta >= self._min_update_interval_seconds\n\n    def reset(self) -> None:\n        self._last_update = time.time()\n\n\n@contextlib.contextmanager\ndef open_spinner(message: str) -> Generator[SpinnerInterface, None, None]:\n    # Interactive spinner goes directly to sys.stdout rather than being routed\n    # through the logging system, but it acts like it has level INFO,\n    # i.e. it's only displayed if we're at level INFO or better.\n    # Non-interactive spinner goes through the logging system, so it is always\n    # in sync with logging configuration.\n    if sys.stdout.isatty() and logger.getEffectiveLevel() <= logging.INFO:\n        spinner: SpinnerInterface = InteractiveSpinner(message)\n    else:\n        spinner = NonInteractiveSpinner(message)\n    try:\n        with hidden_cursor(sys.stdout):\n            yield spinner\n    except KeyboardInterrupt:\n        spinner.finish(\"canceled\")\n        raise\n    except Exception:\n        spinner.finish(\"error\")\n        raise\n    else:\n        spinner.finish(\"done\")\n\n\nHIDE_CURSOR = \"\\x1b[?25l\"\nSHOW_CURSOR = \"\\x1b[?25h\"\n\n\n@contextlib.contextmanager\ndef hidden_cursor(file: IO[str]) -> Generator[None, None, None]:\n    # The Windows terminal does not support the hide/show cursor ANSI codes,\n    # even via colorama. So don't even try.\n    if WINDOWS:\n        yield\n    # We don't want to clutter the output with control characters if we're\n    # writing to a file, or if the user is running with --quiet.\n    # See https://github.com/pypa/pip/issues/3418\n    elif not file.isatty() or logger.getEffectiveLevel() > logging.INFO:\n        yield\n    else:\n        file.write(HIDE_CURSOR)\n        try:\n            yield\n        finally:\n            file.write(SHOW_CURSOR)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/cli/status_codes.py","size":116,"sha1":"0f5af7b27d1a9eb30efc1023917c7c50a76dd681","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"SUCCESS = 0\nERROR = 1\nUNKNOWN_ERROR = 2\nVIRTUALENV_NOT_FOUND = 3\nPREVIOUS_BUILD_DIR_ERROR = 4\nNO_MATCHES_FOUND = 23\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/commands/__init__.py","size":3882,"sha1":"22364bc467edf6a02690dcd0a6a83086aa572238","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"\nPackage containing all pip commands\n\"\"\"\n\nimport importlib\nfrom collections import namedtuple\nfrom typing import Any, Dict, Optional\n\nfrom pip._internal.cli.base_command import Command\n\nCommandInfo = namedtuple(\"CommandInfo\", \"module_path, class_name, summary\")\n\n# This dictionary does a bunch of heavy lifting for help output:\n# - Enables avoiding additional (costly) imports for presenting `--help`.\n# - The ordering matters for help display.\n#\n# Even though the module path starts with the same \"pip._internal.commands\"\n# prefix, the full path makes testing easier (specifically when modifying\n# `commands_dict` in test setup / teardown).\ncommands_dict: Dict[str, CommandInfo] = {\n    \"install\": CommandInfo(\n        \"pip._internal.commands.install\",\n        \"InstallCommand\",\n        \"Install packages.\",\n    ),\n    \"download\": CommandInfo(\n        \"pip._internal.commands.download\",\n        \"DownloadCommand\",\n        \"Download packages.\",\n    ),\n    \"uninstall\": CommandInfo(\n        \"pip._internal.commands.uninstall\",\n        \"UninstallCommand\",\n        \"Uninstall packages.\",\n    ),\n    \"freeze\": CommandInfo(\n        \"pip._internal.commands.freeze\",\n        \"FreezeCommand\",\n        \"Output installed packages in requirements format.\",\n    ),\n    \"inspect\": CommandInfo(\n        \"pip._internal.commands.inspect\",\n        \"InspectCommand\",\n        \"Inspect the python environment.\",\n    ),\n    \"list\": CommandInfo(\n        \"pip._internal.commands.list\",\n        \"ListCommand\",\n        \"List installed packages.\",\n    ),\n    \"show\": CommandInfo(\n        \"pip._internal.commands.show\",\n        \"ShowCommand\",\n        \"Show information about installed packages.\",\n    ),\n    \"check\": CommandInfo(\n        \"pip._internal.commands.check\",\n        \"CheckCommand\",\n        \"Verify installed packages have compatible dependencies.\",\n    ),\n    \"config\": CommandInfo(\n        \"pip._internal.commands.configuration\",\n        \"ConfigurationCommand\",\n        \"Manage local and global configuration.\",\n    ),\n    \"search\": CommandInfo(\n        \"pip._internal.commands.search\",\n        \"SearchCommand\",\n        \"Search PyPI for packages.\",\n    ),\n    \"cache\": CommandInfo(\n        \"pip._internal.commands.cache\",\n        \"CacheCommand\",\n        \"Inspect and manage pip's wheel cache.\",\n    ),\n    \"index\": CommandInfo(\n        \"pip._internal.commands.index\",\n        \"IndexCommand\",\n        \"Inspect information available from package indexes.\",\n    ),\n    \"wheel\": CommandInfo(\n        \"pip._internal.commands.wheel\",\n        \"WheelCommand\",\n        \"Build wheels from your requirements.\",\n    ),\n    \"hash\": CommandInfo(\n        \"pip._internal.commands.hash\",\n        \"HashCommand\",\n        \"Compute hashes of package archives.\",\n    ),\n    \"completion\": CommandInfo(\n        \"pip._internal.commands.completion\",\n        \"CompletionCommand\",\n        \"A helper command used for command completion.\",\n    ),\n    \"debug\": CommandInfo(\n        \"pip._internal.commands.debug\",\n        \"DebugCommand\",\n        \"Show information useful for debugging.\",\n    ),\n    \"help\": CommandInfo(\n        \"pip._internal.commands.help\",\n        \"HelpCommand\",\n        \"Show help for commands.\",\n    ),\n}\n\n\ndef create_command(name: str, **kwargs: Any) -> Command:\n    \"\"\"\n    Create an instance of the Command class with the given name.\n    \"\"\"\n    module_path, class_name, summary = commands_dict[name]\n    module = importlib.import_module(module_path)\n    command_class = getattr(module, class_name)\n    command = command_class(name=name, summary=summary, **kwargs)\n\n    return command\n\n\ndef get_similar_commands(name: str) -> Optional[str]:\n    \"\"\"Command name auto-correct.\"\"\"\n    from difflib import get_close_matches\n\n    name = name.lower()\n\n    close_commands = get_close_matches(name, commands_dict.keys())\n\n    if close_commands:\n        return close_commands[0]\n    else:\n        return None\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/commands/cache.py","size":8107,"sha1":"27fee02bbb6abe45c8046a84d1353967856fc14c","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import os\nimport textwrap\nfrom optparse import Values\nfrom typing import Any, List\n\nfrom pip._internal.cli.base_command import Command\nfrom pip._internal.cli.status_codes import ERROR, SUCCESS\nfrom pip._internal.exceptions import CommandError, PipError\nfrom pip._internal.utils import filesystem\nfrom pip._internal.utils.logging import getLogger\nfrom pip._internal.utils.misc import format_size\n\nlogger = getLogger(__name__)\n\n\nclass CacheCommand(Command):\n    \"\"\"\n    Inspect and manage pip's wheel cache.\n\n    Subcommands:\n\n    - dir: Show the cache directory.\n    - info: Show information about the cache.\n    - list: List filenames of packages stored in the cache.\n    - remove: Remove one or more package from the cache.\n    - purge: Remove all items from the cache.\n\n    ``<pattern>`` can be a glob expression or a package name.\n    \"\"\"\n\n    ignore_require_venv = True\n    usage = \"\"\"\n        %prog dir\n        %prog info\n        %prog list [<pattern>] [--format=[human, abspath]]\n        %prog remove <pattern>\n        %prog purge\n    \"\"\"\n\n    def add_options(self) -> None:\n        self.cmd_opts.add_option(\n            \"--format\",\n            action=\"store\",\n            dest=\"list_format\",\n            default=\"human\",\n            choices=(\"human\", \"abspath\"),\n            help=\"Select the output format among: human (default) or abspath\",\n        )\n\n        self.parser.insert_option_group(0, self.cmd_opts)\n\n    def run(self, options: Values, args: List[str]) -> int:\n        handlers = {\n            \"dir\": self.get_cache_dir,\n            \"info\": self.get_cache_info,\n            \"list\": self.list_cache_items,\n            \"remove\": self.remove_cache_items,\n            \"purge\": self.purge_cache,\n        }\n\n        if not options.cache_dir:\n            logger.error(\"pip cache commands can not function since cache is disabled.\")\n            return ERROR\n\n        # Determine action\n        if not args or args[0] not in handlers:\n            logger.error(\n                \"Need an action (%s) to perform.\",\n                \", \".join(sorted(handlers)),\n            )\n            return ERROR\n\n        action = args[0]\n\n        # Error handling happens here, not in the action-handlers.\n        try:\n            handlers[action](options, args[1:])\n        except PipError as e:\n            logger.error(e.args[0])\n            return ERROR\n\n        return SUCCESS\n\n    def get_cache_dir(self, options: Values, args: List[Any]) -> None:\n        if args:\n            raise CommandError(\"Too many arguments\")\n\n        logger.info(options.cache_dir)\n\n    def get_cache_info(self, options: Values, args: List[Any]) -> None:\n        if args:\n            raise CommandError(\"Too many arguments\")\n\n        num_http_files = len(self._find_http_files(options))\n        num_packages = len(self._find_wheels(options, \"*\"))\n\n        http_cache_location = self._cache_dir(options, \"http-v2\")\n        old_http_cache_location = self._cache_dir(options, \"http\")\n        wheels_cache_location = self._cache_dir(options, \"wheels\")\n        http_cache_size = filesystem.format_size(\n            filesystem.directory_size(http_cache_location)\n            + filesystem.directory_size(old_http_cache_location)\n        )\n        wheels_cache_size = filesystem.format_directory_size(wheels_cache_location)\n\n        message = (\n            textwrap.dedent(\n                \"\"\"\n                    Package index page cache location (pip v23.3+): {http_cache_location}\n                    Package index page cache location (older pips): {old_http_cache_location}\n                    Package index page cache size: {http_cache_size}\n                    Number of HTTP files: {num_http_files}\n                    Locally built wheels location: {wheels_cache_location}\n                    Locally built wheels size: {wheels_cache_size}\n                    Number of locally built wheels: {package_count}\n                \"\"\"  # noqa: E501\n            )\n            .format(\n                http_cache_location=http_cache_location,\n                old_http_cache_location=old_http_cache_location,\n                http_cache_size=http_cache_size,\n                num_http_files=num_http_files,\n                wheels_cache_location=wheels_cache_location,\n                package_count=num_packages,\n                wheels_cache_size=wheels_cache_size,\n            )\n            .strip()\n        )\n\n        logger.info(message)\n\n    def list_cache_items(self, options: Values, args: List[Any]) -> None:\n        if len(args) > 1:\n            raise CommandError(\"Too many arguments\")\n\n        if args:\n            pattern = args[0]\n        else:\n            pattern = \"*\"\n\n        files = self._find_wheels(options, pattern)\n        if options.list_format == \"human\":\n            self.format_for_human(files)\n        else:\n            self.format_for_abspath(files)\n\n    def format_for_human(self, files: List[str]) -> None:\n        if not files:\n            logger.info(\"No locally built wheels cached.\")\n            return\n\n        results = []\n        for filename in files:\n            wheel = os.path.basename(filename)\n            size = filesystem.format_file_size(filename)\n            results.append(f\" - {wheel} ({size})\")\n        logger.info(\"Cache contents:\\n\")\n        logger.info(\"\\n\".join(sorted(results)))\n\n    def format_for_abspath(self, files: List[str]) -> None:\n        if files:\n            logger.info(\"\\n\".join(sorted(files)))\n\n    def remove_cache_items(self, options: Values, args: List[Any]) -> None:\n        if len(args) > 1:\n            raise CommandError(\"Too many arguments\")\n\n        if not args:\n            raise CommandError(\"Please provide a pattern\")\n\n        files = self._find_wheels(options, args[0])\n\n        no_matching_msg = \"No matching packages\"\n        if args[0] == \"*\":\n            # Only fetch http files if no specific pattern given\n            files += self._find_http_files(options)\n        else:\n            # Add the pattern to the log message\n            no_matching_msg += f' for pattern \"{args[0]}\"'\n\n        if not files:\n            logger.warning(no_matching_msg)\n\n        bytes_removed = 0\n        for filename in files:\n            bytes_removed += os.stat(filename).st_size\n            os.unlink(filename)\n            logger.verbose(\"Removed %s\", filename)\n        logger.info(\"Files removed: %s (%s)\", len(files), format_size(bytes_removed))\n\n    def purge_cache(self, options: Values, args: List[Any]) -> None:\n        if args:\n            raise CommandError(\"Too many arguments\")\n\n        return self.remove_cache_items(options, [\"*\"])\n\n    def _cache_dir(self, options: Values, subdir: str) -> str:\n        return os.path.join(options.cache_dir, subdir)\n\n    def _find_http_files(self, options: Values) -> List[str]:\n        old_http_dir = self._cache_dir(options, \"http\")\n        new_http_dir = self._cache_dir(options, \"http-v2\")\n        return filesystem.find_files(old_http_dir, \"*\") + filesystem.find_files(\n            new_http_dir, \"*\"\n        )\n\n    def _find_wheels(self, options: Values, pattern: str) -> List[str]:\n        wheel_dir = self._cache_dir(options, \"wheels\")\n\n        # The wheel filename format, as specified in PEP 427, is:\n        #     {distribution}-{version}(-{build})?-{python}-{abi}-{platform}.whl\n        #\n        # Additionally, non-alphanumeric values in the distribution are\n        # normalized to underscores (_), meaning hyphens can never occur\n        # before `-{version}`.\n        #\n        # Given that information:\n        # - If the pattern we're given contains a hyphen (-), the user is\n        #   providing at least the version. Thus, we can just append `*.whl`\n        #   to match the rest of it.\n        # - If the pattern we're given doesn't contain a hyphen (-), the\n        #   user is only providing the name. Thus, we append `-*.whl` to\n        #   match the hyphen before the version, followed by anything else.\n        #\n        # PEP 427: https://www.python.org/dev/peps/pep-0427/\n        pattern = pattern + (\"*.whl\" if \"-\" in pattern else \"-*.whl\")\n\n        return filesystem.find_files(wheel_dir, pattern)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/commands/check.py","size":2268,"sha1":"719d91213d809976768eba007186ecb19f60d13e","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import logging\nfrom optparse import Values\nfrom typing import List\n\nfrom pip._internal.cli.base_command import Command\nfrom pip._internal.cli.status_codes import ERROR, SUCCESS\nfrom pip._internal.metadata import get_default_environment\nfrom pip._internal.operations.check import (\n    check_package_set,\n    check_unsupported,\n    create_package_set_from_installed,\n)\nfrom pip._internal.utils.compatibility_tags import get_supported\nfrom pip._internal.utils.misc import write_output\n\nlogger = logging.getLogger(__name__)\n\n\nclass CheckCommand(Command):\n    \"\"\"Verify installed packages have compatible dependencies.\"\"\"\n\n    ignore_require_venv = True\n    usage = \"\"\"\n      %prog [options]\"\"\"\n\n    def run(self, options: Values, args: List[str]) -> int:\n        package_set, parsing_probs = create_package_set_from_installed()\n        missing, conflicting = check_package_set(package_set)\n        unsupported = list(\n            check_unsupported(\n                get_default_environment().iter_installed_distributions(),\n                get_supported(),\n            )\n        )\n\n        for project_name in missing:\n            version = package_set[project_name].version\n            for dependency in missing[project_name]:\n                write_output(\n                    \"%s %s requires %s, which is not installed.\",\n                    project_name,\n                    version,\n                    dependency[0],\n                )\n\n        for project_name in conflicting:\n            version = package_set[project_name].version\n            for dep_name, dep_version, req in conflicting[project_name]:\n                write_output(\n                    \"%s %s has requirement %s, but you have %s %s.\",\n                    project_name,\n                    version,\n                    req,\n                    dep_name,\n                    dep_version,\n                )\n        for package in unsupported:\n            write_output(\n                \"%s %s is not supported on this platform\",\n                package.raw_name,\n                package.version,\n            )\n        if missing or conflicting or parsing_probs or unsupported:\n            return ERROR\n        else:\n            write_output(\"No broken requirements found.\")\n            return SUCCESS\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/commands/completion.py","size":4287,"sha1":"f9b51adde0442e0a259666cdd0d47130dd122086","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import sys\nimport textwrap\nfrom optparse import Values\nfrom typing import List\n\nfrom pip._internal.cli.base_command import Command\nfrom pip._internal.cli.status_codes import SUCCESS\nfrom pip._internal.utils.misc import get_prog\n\nBASE_COMPLETION = \"\"\"\n# pip {shell} completion start{script}# pip {shell} completion end\n\"\"\"\n\nCOMPLETION_SCRIPTS = {\n    \"bash\": \"\"\"\n        _pip_completion()\n        {{\n            COMPREPLY=( $( COMP_WORDS=\"${{COMP_WORDS[*]}}\" \\\\\n                           COMP_CWORD=$COMP_CWORD \\\\\n                           PIP_AUTO_COMPLETE=1 $1 2>/dev/null ) )\n        }}\n        complete -o default -F _pip_completion {prog}\n    \"\"\",\n    \"zsh\": \"\"\"\n        #compdef -P pip[0-9.]#\n        __pip() {{\n          compadd $( COMP_WORDS=\"$words[*]\" \\\\\n                     COMP_CWORD=$((CURRENT-1)) \\\\\n                     PIP_AUTO_COMPLETE=1 $words[1] 2>/dev/null )\n        }}\n        if [[ $zsh_eval_context[-1] == loadautofunc ]]; then\n          # autoload from fpath, call function directly\n          __pip \"$@\"\n        else\n          # eval/source/. command, register function for later\n          compdef __pip -P 'pip[0-9.]#'\n        fi\n    \"\"\",\n    \"fish\": \"\"\"\n        function __fish_complete_pip\n            set -lx COMP_WORDS (commandline -o) \"\"\n            set -lx COMP_CWORD ( \\\\\n                math (contains -i -- (commandline -t) $COMP_WORDS)-1 \\\\\n            )\n            set -lx PIP_AUTO_COMPLETE 1\n            string split \\\\  -- (eval $COMP_WORDS[1])\n        end\n        complete -fa \"(__fish_complete_pip)\" -c {prog}\n    \"\"\",\n    \"powershell\": \"\"\"\n        if ((Test-Path Function:\\\\TabExpansion) -and -not `\n            (Test-Path Function:\\\\_pip_completeBackup)) {{\n            Rename-Item Function:\\\\TabExpansion _pip_completeBackup\n        }}\n        function TabExpansion($line, $lastWord) {{\n            $lastBlock = [regex]::Split($line, '[|;]')[-1].TrimStart()\n            if ($lastBlock.StartsWith(\"{prog} \")) {{\n                $Env:COMP_WORDS=$lastBlock\n                $Env:COMP_CWORD=$lastBlock.Split().Length - 1\n                $Env:PIP_AUTO_COMPLETE=1\n                (& {prog}).Split()\n                Remove-Item Env:COMP_WORDS\n                Remove-Item Env:COMP_CWORD\n                Remove-Item Env:PIP_AUTO_COMPLETE\n            }}\n            elseif (Test-Path Function:\\\\_pip_completeBackup) {{\n                # Fall back on existing tab expansion\n                _pip_completeBackup $line $lastWord\n            }}\n        }}\n    \"\"\",\n}\n\n\nclass CompletionCommand(Command):\n    \"\"\"A helper command to be used for command completion.\"\"\"\n\n    ignore_require_venv = True\n\n    def add_options(self) -> None:\n        self.cmd_opts.add_option(\n            \"--bash\",\n            \"-b\",\n            action=\"store_const\",\n            const=\"bash\",\n            dest=\"shell\",\n            help=\"Emit completion code for bash\",\n        )\n        self.cmd_opts.add_option(\n            \"--zsh\",\n            \"-z\",\n            action=\"store_const\",\n            const=\"zsh\",\n            dest=\"shell\",\n            help=\"Emit completion code for zsh\",\n        )\n        self.cmd_opts.add_option(\n            \"--fish\",\n            \"-f\",\n            action=\"store_const\",\n            const=\"fish\",\n            dest=\"shell\",\n            help=\"Emit completion code for fish\",\n        )\n        self.cmd_opts.add_option(\n            \"--powershell\",\n            \"-p\",\n            action=\"store_const\",\n            const=\"powershell\",\n            dest=\"shell\",\n            help=\"Emit completion code for powershell\",\n        )\n\n        self.parser.insert_option_group(0, self.cmd_opts)\n\n    def run(self, options: Values, args: List[str]) -> int:\n        \"\"\"Prints the completion code of the given shell\"\"\"\n        shells = COMPLETION_SCRIPTS.keys()\n        shell_options = [\"--\" + shell for shell in sorted(shells)]\n        if options.shell in shells:\n            script = textwrap.dedent(\n                COMPLETION_SCRIPTS.get(options.shell, \"\").format(prog=get_prog())\n            )\n            print(BASE_COMPLETION.format(script=script, shell=options.shell))\n            return SUCCESS\n        else:\n            sys.stderr.write(\n                \"ERROR: You must pass {}\\n\".format(\" or \".join(shell_options))\n            )\n            return SUCCESS\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/commands/configuration.py","size":9766,"sha1":"4bfaf98054bbd1b027f89190b6233d4803f760fd","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import logging\nimport os\nimport subprocess\nfrom optparse import Values\nfrom typing import Any, List, Optional\n\nfrom pip._internal.cli.base_command import Command\nfrom pip._internal.cli.status_codes import ERROR, SUCCESS\nfrom pip._internal.configuration import (\n    Configuration,\n    Kind,\n    get_configuration_files,\n    kinds,\n)\nfrom pip._internal.exceptions import PipError\nfrom pip._internal.utils.logging import indent_log\nfrom pip._internal.utils.misc import get_prog, write_output\n\nlogger = logging.getLogger(__name__)\n\n\nclass ConfigurationCommand(Command):\n    \"\"\"\n    Manage local and global configuration.\n\n    Subcommands:\n\n    - list: List the active configuration (or from the file specified)\n    - edit: Edit the configuration file in an editor\n    - get: Get the value associated with command.option\n    - set: Set the command.option=value\n    - unset: Unset the value associated with command.option\n    - debug: List the configuration files and values defined under them\n\n    Configuration keys should be dot separated command and option name,\n    with the special prefix \"global\" affecting any command. For example,\n    \"pip config set global.index-url https://example.org/\" would configure\n    the index url for all commands, but \"pip config set download.timeout 10\"\n    would configure a 10 second timeout only for \"pip download\" commands.\n\n    If none of --user, --global and --site are passed, a virtual\n    environment configuration file is used if one is active and the file\n    exists. Otherwise, all modifications happen to the user file by\n    default.\n    \"\"\"\n\n    ignore_require_venv = True\n    usage = \"\"\"\n        %prog [<file-option>] list\n        %prog [<file-option>] [--editor <editor-path>] edit\n\n        %prog [<file-option>] get command.option\n        %prog [<file-option>] set command.option value\n        %prog [<file-option>] unset command.option\n        %prog [<file-option>] debug\n    \"\"\"\n\n    def add_options(self) -> None:\n        self.cmd_opts.add_option(\n            \"--editor\",\n            dest=\"editor\",\n            action=\"store\",\n            default=None,\n            help=(\n                \"Editor to use to edit the file. Uses VISUAL or EDITOR \"\n                \"environment variables if not provided.\"\n            ),\n        )\n\n        self.cmd_opts.add_option(\n            \"--global\",\n            dest=\"global_file\",\n            action=\"store_true\",\n            default=False,\n            help=\"Use the system-wide configuration file only\",\n        )\n\n        self.cmd_opts.add_option(\n            \"--user\",\n            dest=\"user_file\",\n            action=\"store_true\",\n            default=False,\n            help=\"Use the user configuration file only\",\n        )\n\n        self.cmd_opts.add_option(\n            \"--site\",\n            dest=\"site_file\",\n            action=\"store_true\",\n            default=False,\n            help=\"Use the current environment configuration file only\",\n        )\n\n        self.parser.insert_option_group(0, self.cmd_opts)\n\n    def run(self, options: Values, args: List[str]) -> int:\n        handlers = {\n            \"list\": self.list_values,\n            \"edit\": self.open_in_editor,\n            \"get\": self.get_name,\n            \"set\": self.set_name_value,\n            \"unset\": self.unset_name,\n            \"debug\": self.list_config_values,\n        }\n\n        # Determine action\n        if not args or args[0] not in handlers:\n            logger.error(\n                \"Need an action (%s) to perform.\",\n                \", \".join(sorted(handlers)),\n            )\n            return ERROR\n\n        action = args[0]\n\n        # Determine which configuration files are to be loaded\n        #    Depends on whether the command is modifying.\n        try:\n            load_only = self._determine_file(\n                options, need_value=(action in [\"get\", \"set\", \"unset\", \"edit\"])\n            )\n        except PipError as e:\n            logger.error(e.args[0])\n            return ERROR\n\n        # Load a new configuration\n        self.configuration = Configuration(\n            isolated=options.isolated_mode, load_only=load_only\n        )\n        self.configuration.load()\n\n        # Error handling happens here, not in the action-handlers.\n        try:\n            handlers[action](options, args[1:])\n        except PipError as e:\n            logger.error(e.args[0])\n            return ERROR\n\n        return SUCCESS\n\n    def _determine_file(self, options: Values, need_value: bool) -> Optional[Kind]:\n        file_options = [\n            key\n            for key, value in (\n                (kinds.USER, options.user_file),\n                (kinds.GLOBAL, options.global_file),\n                (kinds.SITE, options.site_file),\n            )\n            if value\n        ]\n\n        if not file_options:\n            if not need_value:\n                return None\n            # Default to user, unless there's a site file.\n            elif any(\n                os.path.exists(site_config_file)\n                for site_config_file in get_configuration_files()[kinds.SITE]\n            ):\n                return kinds.SITE\n            else:\n                return kinds.USER\n        elif len(file_options) == 1:\n            return file_options[0]\n\n        raise PipError(\n            \"Need exactly one file to operate upon \"\n            \"(--user, --site, --global) to perform.\"\n        )\n\n    def list_values(self, options: Values, args: List[str]) -> None:\n        self._get_n_args(args, \"list\", n=0)\n\n        for key, value in sorted(self.configuration.items()):\n            write_output(\"%s=%r\", key, value)\n\n    def get_name(self, options: Values, args: List[str]) -> None:\n        key = self._get_n_args(args, \"get [name]\", n=1)\n        value = self.configuration.get_value(key)\n\n        write_output(\"%s\", value)\n\n    def set_name_value(self, options: Values, args: List[str]) -> None:\n        key, value = self._get_n_args(args, \"set [name] [value]\", n=2)\n        self.configuration.set_value(key, value)\n\n        self._save_configuration()\n\n    def unset_name(self, options: Values, args: List[str]) -> None:\n        key = self._get_n_args(args, \"unset [name]\", n=1)\n        self.configuration.unset_value(key)\n\n        self._save_configuration()\n\n    def list_config_values(self, options: Values, args: List[str]) -> None:\n        \"\"\"List config key-value pairs across different config files\"\"\"\n        self._get_n_args(args, \"debug\", n=0)\n\n        self.print_env_var_values()\n        # Iterate over config files and print if they exist, and the\n        # key-value pairs present in them if they do\n        for variant, files in sorted(self.configuration.iter_config_files()):\n            write_output(\"%s:\", variant)\n            for fname in files:\n                with indent_log():\n                    file_exists = os.path.exists(fname)\n                    write_output(\"%s, exists: %r\", fname, file_exists)\n                    if file_exists:\n                        self.print_config_file_values(variant)\n\n    def print_config_file_values(self, variant: Kind) -> None:\n        \"\"\"Get key-value pairs from the file of a variant\"\"\"\n        for name, value in self.configuration.get_values_in_config(variant).items():\n            with indent_log():\n                write_output(\"%s: %s\", name, value)\n\n    def print_env_var_values(self) -> None:\n        \"\"\"Get key-values pairs present as environment variables\"\"\"\n        write_output(\"%s:\", \"env_var\")\n        with indent_log():\n            for key, value in sorted(self.configuration.get_environ_vars()):\n                env_var = f\"PIP_{key.upper()}\"\n                write_output(\"%s=%r\", env_var, value)\n\n    def open_in_editor(self, options: Values, args: List[str]) -> None:\n        editor = self._determine_editor(options)\n\n        fname = self.configuration.get_file_to_edit()\n        if fname is None:\n            raise PipError(\"Could not determine appropriate file.\")\n        elif '\"' in fname:\n            # This shouldn't happen, unless we see a username like that.\n            # If that happens, we'd appreciate a pull request fixing this.\n            raise PipError(\n                f'Can not open an editor for a file name containing \"\\n{fname}'\n            )\n\n        try:\n            subprocess.check_call(f'{editor} \"{fname}\"', shell=True)\n        except FileNotFoundError as e:\n            if not e.filename:\n                e.filename = editor\n            raise\n        except subprocess.CalledProcessError as e:\n            raise PipError(f\"Editor Subprocess exited with exit code {e.returncode}\")\n\n    def _get_n_args(self, args: List[str], example: str, n: int) -> Any:\n        \"\"\"Helper to make sure the command got the right number of arguments\"\"\"\n        if len(args) != n:\n            msg = (\n                f\"Got unexpected number of arguments, expected {n}. \"\n                f'(example: \"{get_prog()} config {example}\")'\n            )\n            raise PipError(msg)\n\n        if n == 1:\n            return args[0]\n        else:\n            return args\n\n    def _save_configuration(self) -> None:\n        # We successfully ran a modifying command. Need to save the\n        # configuration.\n        try:\n            self.configuration.save()\n        except Exception:\n            logger.exception(\n                \"Unable to save configuration. Please report this as a bug.\"\n            )\n            raise PipError(\"Internal Error.\")\n\n    def _determine_editor(self, options: Values) -> str:\n        if options.editor is not None:\n            return options.editor\n        elif \"VISUAL\" in os.environ:\n            return os.environ[\"VISUAL\"]\n        elif \"EDITOR\" in os.environ:\n            return os.environ[\"EDITOR\"]\n        else:\n            raise PipError(\"Could not determine editor to use.\")\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/commands/debug.py","size":6797,"sha1":"84a1c542a832baf80bc447b58d8f7bc678331b35","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import locale\nimport logging\nimport os\nimport sys\nfrom optparse import Values\nfrom types import ModuleType\nfrom typing import Any, Dict, List, Optional\n\nimport pip._vendor\nfrom pip._vendor.certifi import where\nfrom pip._vendor.packaging.version import parse as parse_version\n\nfrom pip._internal.cli import cmdoptions\nfrom pip._internal.cli.base_command import Command\nfrom pip._internal.cli.cmdoptions import make_target_python\nfrom pip._internal.cli.status_codes import SUCCESS\nfrom pip._internal.configuration import Configuration\nfrom pip._internal.metadata import get_environment\nfrom pip._internal.utils.compat import open_text_resource\nfrom pip._internal.utils.logging import indent_log\nfrom pip._internal.utils.misc import get_pip_version\n\nlogger = logging.getLogger(__name__)\n\n\ndef show_value(name: str, value: Any) -> None:\n    logger.info(\"%s: %s\", name, value)\n\n\ndef show_sys_implementation() -> None:\n    logger.info(\"sys.implementation:\")\n    implementation_name = sys.implementation.name\n    with indent_log():\n        show_value(\"name\", implementation_name)\n\n\ndef create_vendor_txt_map() -> Dict[str, str]:\n    with open_text_resource(\"pip._vendor\", \"vendor.txt\") as f:\n        # Purge non version specifying lines.\n        # Also, remove any space prefix or suffixes (including comments).\n        lines = [\n            line.strip().split(\" \", 1)[0] for line in f.readlines() if \"==\" in line\n        ]\n\n    # Transform into \"module\" -> version dict.\n    return dict(line.split(\"==\", 1) for line in lines)\n\n\ndef get_module_from_module_name(module_name: str) -> Optional[ModuleType]:\n    # Module name can be uppercase in vendor.txt for some reason...\n    module_name = module_name.lower().replace(\"-\", \"_\")\n    # PATCH: setuptools is actually only pkg_resources.\n    if module_name == \"setuptools\":\n        module_name = \"pkg_resources\"\n\n    try:\n        __import__(f\"pip._vendor.{module_name}\", globals(), locals(), level=0)\n        return getattr(pip._vendor, module_name)\n    except ImportError:\n        # We allow 'truststore' to fail to import due\n        # to being unavailable on Python 3.9 and earlier.\n        if module_name == \"truststore\" and sys.version_info < (3, 10):\n            return None\n        raise\n\n\ndef get_vendor_version_from_module(module_name: str) -> Optional[str]:\n    module = get_module_from_module_name(module_name)\n    version = getattr(module, \"__version__\", None)\n\n    if module and not version:\n        # Try to find version in debundled module info.\n        assert module.__file__ is not None\n        env = get_environment([os.path.dirname(module.__file__)])\n        dist = env.get_distribution(module_name)\n        if dist:\n            version = str(dist.version)\n\n    return version\n\n\ndef show_actual_vendor_versions(vendor_txt_versions: Dict[str, str]) -> None:\n    \"\"\"Log the actual version and print extra info if there is\n    a conflict or if the actual version could not be imported.\n    \"\"\"\n    for module_name, expected_version in vendor_txt_versions.items():\n        extra_message = \"\"\n        actual_version = get_vendor_version_from_module(module_name)\n        if not actual_version:\n            extra_message = (\n                \" (Unable to locate actual module version, using\"\n                \" vendor.txt specified version)\"\n            )\n            actual_version = expected_version\n        elif parse_version(actual_version) != parse_version(expected_version):\n            extra_message = (\n                \" (CONFLICT: vendor.txt suggests version should\"\n                f\" be {expected_version})\"\n            )\n        logger.info(\"%s==%s%s\", module_name, actual_version, extra_message)\n\n\ndef show_vendor_versions() -> None:\n    logger.info(\"vendored library versions:\")\n\n    vendor_txt_versions = create_vendor_txt_map()\n    with indent_log():\n        show_actual_vendor_versions(vendor_txt_versions)\n\n\ndef show_tags(options: Values) -> None:\n    tag_limit = 10\n\n    target_python = make_target_python(options)\n    tags = target_python.get_sorted_tags()\n\n    # Display the target options that were explicitly provided.\n    formatted_target = target_python.format_given()\n    suffix = \"\"\n    if formatted_target:\n        suffix = f\" (target: {formatted_target})\"\n\n    msg = f\"Compatible tags: {len(tags)}{suffix}\"\n    logger.info(msg)\n\n    if options.verbose < 1 and len(tags) > tag_limit:\n        tags_limited = True\n        tags = tags[:tag_limit]\n    else:\n        tags_limited = False\n\n    with indent_log():\n        for tag in tags:\n            logger.info(str(tag))\n\n        if tags_limited:\n            msg = f\"...\\n[First {tag_limit} tags shown. Pass --verbose to show all.]\"\n            logger.info(msg)\n\n\ndef ca_bundle_info(config: Configuration) -> str:\n    levels = {key.split(\".\", 1)[0] for key, _ in config.items()}\n    if not levels:\n        return \"Not specified\"\n\n    levels_that_override_global = [\"install\", \"wheel\", \"download\"]\n    global_overriding_level = [\n        level for level in levels if level in levels_that_override_global\n    ]\n    if not global_overriding_level:\n        return \"global\"\n\n    if \"global\" in levels:\n        levels.remove(\"global\")\n    return \", \".join(levels)\n\n\nclass DebugCommand(Command):\n    \"\"\"\n    Display debug information.\n    \"\"\"\n\n    usage = \"\"\"\n      %prog <options>\"\"\"\n    ignore_require_venv = True\n\n    def add_options(self) -> None:\n        cmdoptions.add_target_python_options(self.cmd_opts)\n        self.parser.insert_option_group(0, self.cmd_opts)\n        self.parser.config.load()\n\n    def run(self, options: Values, args: List[str]) -> int:\n        logger.warning(\n            \"This command is only meant for debugging. \"\n            \"Do not use this with automation for parsing and getting these \"\n            \"details, since the output and options of this command may \"\n            \"change without notice.\"\n        )\n        show_value(\"pip version\", get_pip_version())\n        show_value(\"sys.version\", sys.version)\n        show_value(\"sys.executable\", sys.executable)\n        show_value(\"sys.getdefaultencoding\", sys.getdefaultencoding())\n        show_value(\"sys.getfilesystemencoding\", sys.getfilesystemencoding())\n        show_value(\n            \"locale.getpreferredencoding\",\n            locale.getpreferredencoding(),\n        )\n        show_value(\"sys.platform\", sys.platform)\n        show_sys_implementation()\n\n        show_value(\"'cert' config value\", ca_bundle_info(self.parser.config))\n        show_value(\"REQUESTS_CA_BUNDLE\", os.environ.get(\"REQUESTS_CA_BUNDLE\"))\n        show_value(\"CURL_CA_BUNDLE\", os.environ.get(\"CURL_CA_BUNDLE\"))\n        show_value(\"pip._vendor.certifi.where()\", where())\n        show_value(\"pip._vendor.DEBUNDLED\", pip._vendor.DEBUNDLED)\n\n        show_vendor_versions()\n\n        show_tags(options)\n\n        return SUCCESS\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/commands/download.py","size":5273,"sha1":"aaa2e2e35bcc3fc34b9f83ee9b781be60ba269ae","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import logging\nimport os\nfrom optparse import Values\nfrom typing import List\n\nfrom pip._internal.cli import cmdoptions\nfrom pip._internal.cli.cmdoptions import make_target_python\nfrom pip._internal.cli.req_command import RequirementCommand, with_cleanup\nfrom pip._internal.cli.status_codes import SUCCESS\nfrom pip._internal.operations.build.build_tracker import get_build_tracker\nfrom pip._internal.req.req_install import check_legacy_setup_py_options\nfrom pip._internal.utils.misc import ensure_dir, normalize_path, write_output\nfrom pip._internal.utils.temp_dir import TempDirectory\n\nlogger = logging.getLogger(__name__)\n\n\nclass DownloadCommand(RequirementCommand):\n    \"\"\"\n    Download packages from:\n\n    - PyPI (and other indexes) using requirement specifiers.\n    - VCS project urls.\n    - Local project directories.\n    - Local or remote source archives.\n\n    pip also supports downloading from \"requirements files\", which provide\n    an easy way to specify a whole environment to be downloaded.\n    \"\"\"\n\n    usage = \"\"\"\n      %prog [options] <requirement specifier> [package-index-options] ...\n      %prog [options] -r <requirements file> [package-index-options] ...\n      %prog [options] <vcs project url> ...\n      %prog [options] <local project path> ...\n      %prog [options] <archive url/path> ...\"\"\"\n\n    def add_options(self) -> None:\n        self.cmd_opts.add_option(cmdoptions.constraints())\n        self.cmd_opts.add_option(cmdoptions.requirements())\n        self.cmd_opts.add_option(cmdoptions.no_deps())\n        self.cmd_opts.add_option(cmdoptions.global_options())\n        self.cmd_opts.add_option(cmdoptions.no_binary())\n        self.cmd_opts.add_option(cmdoptions.only_binary())\n        self.cmd_opts.add_option(cmdoptions.prefer_binary())\n        self.cmd_opts.add_option(cmdoptions.src())\n        self.cmd_opts.add_option(cmdoptions.pre())\n        self.cmd_opts.add_option(cmdoptions.require_hashes())\n        self.cmd_opts.add_option(cmdoptions.progress_bar())\n        self.cmd_opts.add_option(cmdoptions.no_build_isolation())\n        self.cmd_opts.add_option(cmdoptions.use_pep517())\n        self.cmd_opts.add_option(cmdoptions.no_use_pep517())\n        self.cmd_opts.add_option(cmdoptions.check_build_deps())\n        self.cmd_opts.add_option(cmdoptions.ignore_requires_python())\n\n        self.cmd_opts.add_option(\n            \"-d\",\n            \"--dest\",\n            \"--destination-dir\",\n            \"--destination-directory\",\n            dest=\"download_dir\",\n            metavar=\"dir\",\n            default=os.curdir,\n            help=\"Download packages into <dir>.\",\n        )\n\n        cmdoptions.add_target_python_options(self.cmd_opts)\n\n        index_opts = cmdoptions.make_option_group(\n            cmdoptions.index_group,\n            self.parser,\n        )\n\n        self.parser.insert_option_group(0, index_opts)\n        self.parser.insert_option_group(0, self.cmd_opts)\n\n    @with_cleanup\n    def run(self, options: Values, args: List[str]) -> int:\n        options.ignore_installed = True\n        # editable doesn't really make sense for `pip download`, but the bowels\n        # of the RequirementSet code require that property.\n        options.editables = []\n\n        cmdoptions.check_dist_restriction(options)\n\n        options.download_dir = normalize_path(options.download_dir)\n        ensure_dir(options.download_dir)\n\n        session = self.get_default_session(options)\n\n        target_python = make_target_python(options)\n        finder = self._build_package_finder(\n            options=options,\n            session=session,\n            target_python=target_python,\n            ignore_requires_python=options.ignore_requires_python,\n        )\n\n        build_tracker = self.enter_context(get_build_tracker())\n\n        directory = TempDirectory(\n            delete=not options.no_clean,\n            kind=\"download\",\n            globally_managed=True,\n        )\n\n        reqs = self.get_requirements(args, options, finder, session)\n        check_legacy_setup_py_options(options, reqs)\n\n        preparer = self.make_requirement_preparer(\n            temp_build_dir=directory,\n            options=options,\n            build_tracker=build_tracker,\n            session=session,\n            finder=finder,\n            download_dir=options.download_dir,\n            use_user_site=False,\n            verbosity=self.verbosity,\n        )\n\n        resolver = self.make_resolver(\n            preparer=preparer,\n            finder=finder,\n            options=options,\n            ignore_requires_python=options.ignore_requires_python,\n            use_pep517=options.use_pep517,\n            py_version_info=options.python_version,\n        )\n\n        self.trace_basic_info(finder)\n\n        requirement_set = resolver.resolve(reqs, check_supported_wheels=True)\n\n        downloaded: List[str] = []\n        for req in requirement_set.requirements.values():\n            if req.satisfied_by is None:\n                assert req.name is not None\n                preparer.save_linked_requirement(req)\n                downloaded.append(req.name)\n\n        preparer.prepare_linked_requirements_more(requirement_set.requirements.values())\n\n        if downloaded:\n            write_output(\"Successfully downloaded %s\", \" \".join(downloaded))\n\n        return SUCCESS\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/commands/freeze.py","size":3203,"sha1":"41582fd71749fae698c41dee54fd96a1262dbfad","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import sys\nfrom optparse import Values\nfrom typing import AbstractSet, List\n\nfrom pip._internal.cli import cmdoptions\nfrom pip._internal.cli.base_command import Command\nfrom pip._internal.cli.status_codes import SUCCESS\nfrom pip._internal.operations.freeze import freeze\nfrom pip._internal.utils.compat import stdlib_pkgs\n\n\ndef _should_suppress_build_backends() -> bool:\n    return sys.version_info < (3, 12)\n\n\ndef _dev_pkgs() -> AbstractSet[str]:\n    pkgs = {\"pip\"}\n\n    if _should_suppress_build_backends():\n        pkgs |= {\"setuptools\", \"distribute\", \"wheel\"}\n\n    return pkgs\n\n\nclass FreezeCommand(Command):\n    \"\"\"\n    Output installed packages in requirements format.\n\n    packages are listed in a case-insensitive sorted order.\n    \"\"\"\n\n    ignore_require_venv = True\n    usage = \"\"\"\n      %prog [options]\"\"\"\n    log_streams = (\"ext://sys.stderr\", \"ext://sys.stderr\")\n\n    def add_options(self) -> None:\n        self.cmd_opts.add_option(\n            \"-r\",\n            \"--requirement\",\n            dest=\"requirements\",\n            action=\"append\",\n            default=[],\n            metavar=\"file\",\n            help=(\n                \"Use the order in the given requirements file and its \"\n                \"comments when generating output. This option can be \"\n                \"used multiple times.\"\n            ),\n        )\n        self.cmd_opts.add_option(\n            \"-l\",\n            \"--local\",\n            dest=\"local\",\n            action=\"store_true\",\n            default=False,\n            help=(\n                \"If in a virtualenv that has global access, do not output \"\n                \"globally-installed packages.\"\n            ),\n        )\n        self.cmd_opts.add_option(\n            \"--user\",\n            dest=\"user\",\n            action=\"store_true\",\n            default=False,\n            help=\"Only output packages installed in user-site.\",\n        )\n        self.cmd_opts.add_option(cmdoptions.list_path())\n        self.cmd_opts.add_option(\n            \"--all\",\n            dest=\"freeze_all\",\n            action=\"store_true\",\n            help=(\n                \"Do not skip these packages in the output:\"\n                \" {}\".format(\", \".join(_dev_pkgs()))\n            ),\n        )\n        self.cmd_opts.add_option(\n            \"--exclude-editable\",\n            dest=\"exclude_editable\",\n            action=\"store_true\",\n            help=\"Exclude editable package from output.\",\n        )\n        self.cmd_opts.add_option(cmdoptions.list_exclude())\n\n        self.parser.insert_option_group(0, self.cmd_opts)\n\n    def run(self, options: Values, args: List[str]) -> int:\n        skip = set(stdlib_pkgs)\n        if not options.freeze_all:\n            skip.update(_dev_pkgs())\n\n        if options.excludes:\n            skip.update(options.excludes)\n\n        cmdoptions.check_list_path_option(options)\n\n        for line in freeze(\n            requirement=options.requirements,\n            local_only=options.local,\n            user_only=options.user,\n            paths=options.path,\n            isolated=options.isolated_mode,\n            skip=skip,\n            exclude_editable=options.exclude_editable,\n        ):\n            sys.stdout.write(line + \"\\n\")\n        return SUCCESS\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/commands/hash.py","size":1703,"sha1":"3ff85f8d8bee597549fa1ad996fd684d33518c27","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import hashlib\nimport logging\nimport sys\nfrom optparse import Values\nfrom typing import List\n\nfrom pip._internal.cli.base_command import Command\nfrom pip._internal.cli.status_codes import ERROR, SUCCESS\nfrom pip._internal.utils.hashes import FAVORITE_HASH, STRONG_HASHES\nfrom pip._internal.utils.misc import read_chunks, write_output\n\nlogger = logging.getLogger(__name__)\n\n\nclass HashCommand(Command):\n    \"\"\"\n    Compute a hash of a local package archive.\n\n    These can be used with --hash in a requirements file to do repeatable\n    installs.\n    \"\"\"\n\n    usage = \"%prog [options] <file> ...\"\n    ignore_require_venv = True\n\n    def add_options(self) -> None:\n        self.cmd_opts.add_option(\n            \"-a\",\n            \"--algorithm\",\n            dest=\"algorithm\",\n            choices=STRONG_HASHES,\n            action=\"store\",\n            default=FAVORITE_HASH,\n            help=\"The hash algorithm to use: one of {}\".format(\n                \", \".join(STRONG_HASHES)\n            ),\n        )\n        self.parser.insert_option_group(0, self.cmd_opts)\n\n    def run(self, options: Values, args: List[str]) -> int:\n        if not args:\n            self.parser.print_usage(sys.stderr)\n            return ERROR\n\n        algorithm = options.algorithm\n        for path in args:\n            write_output(\n                \"%s:\\n--hash=%s:%s\", path, algorithm, _hash_of_file(path, algorithm)\n            )\n        return SUCCESS\n\n\ndef _hash_of_file(path: str, algorithm: str) -> str:\n    \"\"\"Return the hash digest of a file.\"\"\"\n    with open(path, \"rb\") as archive:\n        hash = hashlib.new(algorithm)\n        for chunk in read_chunks(archive):\n            hash.update(chunk)\n    return hash.hexdigest()\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/commands/help.py","size":1132,"sha1":"9dbfb87d39f05e31e727697d166831bfe0a6673b","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"from optparse import Values\nfrom typing import List\n\nfrom pip._internal.cli.base_command import Command\nfrom pip._internal.cli.status_codes import SUCCESS\nfrom pip._internal.exceptions import CommandError\n\n\nclass HelpCommand(Command):\n    \"\"\"Show help for commands\"\"\"\n\n    usage = \"\"\"\n      %prog <command>\"\"\"\n    ignore_require_venv = True\n\n    def run(self, options: Values, args: List[str]) -> int:\n        from pip._internal.commands import (\n            commands_dict,\n            create_command,\n            get_similar_commands,\n        )\n\n        try:\n            # 'pip help' with no args is handled by pip.__init__.parseopt()\n            cmd_name = args[0]  # the command we need help for\n        except IndexError:\n            return SUCCESS\n\n        if cmd_name not in commands_dict:\n            guess = get_similar_commands(cmd_name)\n\n            msg = [f'unknown command \"{cmd_name}\"']\n            if guess:\n                msg.append(f'maybe you meant \"{guess}\"')\n\n            raise CommandError(\" - \".join(msg))\n\n        command = create_command(cmd_name)\n        command.parser.print_help()\n\n        return SUCCESS\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/commands/index.py","size":4731,"sha1":"8143789ebef3bc0aa909c030b0ec92c16c6b19be","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import logging\nfrom optparse import Values\nfrom typing import Any, Iterable, List, Optional\n\nfrom pip._vendor.packaging.version import Version\n\nfrom pip._internal.cli import cmdoptions\nfrom pip._internal.cli.req_command import IndexGroupCommand\nfrom pip._internal.cli.status_codes import ERROR, SUCCESS\nfrom pip._internal.commands.search import print_dist_installation_info\nfrom pip._internal.exceptions import CommandError, DistributionNotFound, PipError\nfrom pip._internal.index.collector import LinkCollector\nfrom pip._internal.index.package_finder import PackageFinder\nfrom pip._internal.models.selection_prefs import SelectionPreferences\nfrom pip._internal.models.target_python import TargetPython\nfrom pip._internal.network.session import PipSession\nfrom pip._internal.utils.misc import write_output\n\nlogger = logging.getLogger(__name__)\n\n\nclass IndexCommand(IndexGroupCommand):\n    \"\"\"\n    Inspect information available from package indexes.\n    \"\"\"\n\n    ignore_require_venv = True\n    usage = \"\"\"\n        %prog versions <package>\n    \"\"\"\n\n    def add_options(self) -> None:\n        cmdoptions.add_target_python_options(self.cmd_opts)\n\n        self.cmd_opts.add_option(cmdoptions.ignore_requires_python())\n        self.cmd_opts.add_option(cmdoptions.pre())\n        self.cmd_opts.add_option(cmdoptions.no_binary())\n        self.cmd_opts.add_option(cmdoptions.only_binary())\n\n        index_opts = cmdoptions.make_option_group(\n            cmdoptions.index_group,\n            self.parser,\n        )\n\n        self.parser.insert_option_group(0, index_opts)\n        self.parser.insert_option_group(0, self.cmd_opts)\n\n    def run(self, options: Values, args: List[str]) -> int:\n        handlers = {\n            \"versions\": self.get_available_package_versions,\n        }\n\n        logger.warning(\n            \"pip index is currently an experimental command. \"\n            \"It may be removed/changed in a future release \"\n            \"without prior warning.\"\n        )\n\n        # Determine action\n        if not args or args[0] not in handlers:\n            logger.error(\n                \"Need an action (%s) to perform.\",\n                \", \".join(sorted(handlers)),\n            )\n            return ERROR\n\n        action = args[0]\n\n        # Error handling happens here, not in the action-handlers.\n        try:\n            handlers[action](options, args[1:])\n        except PipError as e:\n            logger.error(e.args[0])\n            return ERROR\n\n        return SUCCESS\n\n    def _build_package_finder(\n        self,\n        options: Values,\n        session: PipSession,\n        target_python: Optional[TargetPython] = None,\n        ignore_requires_python: Optional[bool] = None,\n    ) -> PackageFinder:\n        \"\"\"\n        Create a package finder appropriate to the index command.\n        \"\"\"\n        link_collector = LinkCollector.create(session, options=options)\n\n        # Pass allow_yanked=False to ignore yanked versions.\n        selection_prefs = SelectionPreferences(\n            allow_yanked=False,\n            allow_all_prereleases=options.pre,\n            ignore_requires_python=ignore_requires_python,\n        )\n\n        return PackageFinder.create(\n            link_collector=link_collector,\n            selection_prefs=selection_prefs,\n            target_python=target_python,\n        )\n\n    def get_available_package_versions(self, options: Values, args: List[Any]) -> None:\n        if len(args) != 1:\n            raise CommandError(\"You need to specify exactly one argument\")\n\n        target_python = cmdoptions.make_target_python(options)\n        query = args[0]\n\n        with self._build_session(options) as session:\n            finder = self._build_package_finder(\n                options=options,\n                session=session,\n                target_python=target_python,\n                ignore_requires_python=options.ignore_requires_python,\n            )\n\n            versions: Iterable[Version] = (\n                candidate.version for candidate in finder.find_all_candidates(query)\n            )\n\n            if not options.pre:\n                # Remove prereleases\n                versions = (\n                    version for version in versions if not version.is_prerelease\n                )\n            versions = set(versions)\n\n            if not versions:\n                raise DistributionNotFound(\n                    f\"No matching distribution found for {query}\"\n                )\n\n            formatted_versions = [str(ver) for ver in sorted(versions, reverse=True)]\n            latest = formatted_versions[0]\n\n        write_output(f\"{query} ({latest})\")\n        write_output(\"Available versions: {}\".format(\", \".join(formatted_versions)))\n        print_dist_installation_info(query, latest)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/commands/inspect.py","size":3189,"sha1":"3091daf91c0bc06f2b92d0680904dfe46529b4a1","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import logging\nfrom optparse import Values\nfrom typing import Any, Dict, List\n\nfrom pip._vendor.packaging.markers import default_environment\nfrom pip._vendor.rich import print_json\n\nfrom pip import __version__\nfrom pip._internal.cli import cmdoptions\nfrom pip._internal.cli.base_command import Command\nfrom pip._internal.cli.status_codes import SUCCESS\nfrom pip._internal.metadata import BaseDistribution, get_environment\nfrom pip._internal.utils.compat import stdlib_pkgs\nfrom pip._internal.utils.urls import path_to_url\n\nlogger = logging.getLogger(__name__)\n\n\nclass InspectCommand(Command):\n    \"\"\"\n    Inspect the content of a Python environment and produce a report in JSON format.\n    \"\"\"\n\n    ignore_require_venv = True\n    usage = \"\"\"\n      %prog [options]\"\"\"\n\n    def add_options(self) -> None:\n        self.cmd_opts.add_option(\n            \"--local\",\n            action=\"store_true\",\n            default=False,\n            help=(\n                \"If in a virtualenv that has global access, do not list \"\n                \"globally-installed packages.\"\n            ),\n        )\n        self.cmd_opts.add_option(\n            \"--user\",\n            dest=\"user\",\n            action=\"store_true\",\n            default=False,\n            help=\"Only output packages installed in user-site.\",\n        )\n        self.cmd_opts.add_option(cmdoptions.list_path())\n        self.parser.insert_option_group(0, self.cmd_opts)\n\n    def run(self, options: Values, args: List[str]) -> int:\n        cmdoptions.check_list_path_option(options)\n        dists = get_environment(options.path).iter_installed_distributions(\n            local_only=options.local,\n            user_only=options.user,\n            skip=set(stdlib_pkgs),\n        )\n        output = {\n            \"version\": \"1\",\n            \"pip_version\": __version__,\n            \"installed\": [self._dist_to_dict(dist) for dist in dists],\n            \"environment\": default_environment(),\n            # TODO tags? scheme?\n        }\n        print_json(data=output)\n        return SUCCESS\n\n    def _dist_to_dict(self, dist: BaseDistribution) -> Dict[str, Any]:\n        res: Dict[str, Any] = {\n            \"metadata\": dist.metadata_dict,\n            \"metadata_location\": dist.info_location,\n        }\n        # direct_url. Note that we don't have download_info (as in the installation\n        # report) since it is not recorded in installed metadata.\n        direct_url = dist.direct_url\n        if direct_url is not None:\n            res[\"direct_url\"] = direct_url.to_dict()\n        else:\n            # Emulate direct_url for legacy editable installs.\n            editable_project_location = dist.editable_project_location\n            if editable_project_location is not None:\n                res[\"direct_url\"] = {\n                    \"url\": path_to_url(editable_project_location),\n                    \"dir_info\": {\n                        \"editable\": True,\n                    },\n                }\n        # installer\n        installer = dist.installer\n        if dist.installer:\n            res[\"installer\"] = installer\n        # requested\n        if dist.installed_with_dist_info:\n            res[\"requested\"] = dist.requested\n        return res\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/commands/install.py","size":29390,"sha1":"8c07e51e0c4caa9472c94c908ad97cb2eee82849","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import errno\nimport json\nimport operator\nimport os\nimport shutil\nimport site\nfrom optparse import SUPPRESS_HELP, Values\nfrom typing import List, Optional\n\nfrom pip._vendor.packaging.utils import canonicalize_name\nfrom pip._vendor.rich import print_json\n\n# Eagerly import self_outdated_check to avoid crashes. Otherwise,\n# this module would be imported *after* pip was replaced, resulting\n# in crashes if the new self_outdated_check module was incompatible\n# with the rest of pip that's already imported, or allowing a\n# wheel to execute arbitrary code on install by replacing\n# self_outdated_check.\nimport pip._internal.self_outdated_check  # noqa: F401\nfrom pip._internal.cache import WheelCache\nfrom pip._internal.cli import cmdoptions\nfrom pip._internal.cli.cmdoptions import make_target_python\nfrom pip._internal.cli.req_command import (\n    RequirementCommand,\n    with_cleanup,\n)\nfrom pip._internal.cli.status_codes import ERROR, SUCCESS\nfrom pip._internal.exceptions import CommandError, InstallationError\nfrom pip._internal.locations import get_scheme\nfrom pip._internal.metadata import get_environment\nfrom pip._internal.models.installation_report import InstallationReport\nfrom pip._internal.operations.build.build_tracker import get_build_tracker\nfrom pip._internal.operations.check import ConflictDetails, check_install_conflicts\nfrom pip._internal.req import install_given_reqs\nfrom pip._internal.req.req_install import (\n    InstallRequirement,\n    check_legacy_setup_py_options,\n)\nfrom pip._internal.utils.compat import WINDOWS\nfrom pip._internal.utils.filesystem import test_writable_dir\nfrom pip._internal.utils.logging import getLogger\nfrom pip._internal.utils.misc import (\n    check_externally_managed,\n    ensure_dir,\n    get_pip_version,\n    protect_pip_from_modification_on_windows,\n    warn_if_run_as_root,\n    write_output,\n)\nfrom pip._internal.utils.temp_dir import TempDirectory\nfrom pip._internal.utils.virtualenv import (\n    running_under_virtualenv,\n    virtualenv_no_global,\n)\nfrom pip._internal.wheel_builder import build, should_build_for_install_command\n\nlogger = getLogger(__name__)\n\n\nclass InstallCommand(RequirementCommand):\n    \"\"\"\n    Install packages from:\n\n    - PyPI (and other indexes) using requirement specifiers.\n    - VCS project urls.\n    - Local project directories.\n    - Local or remote source archives.\n\n    pip also supports installing from \"requirements files\", which provide\n    an easy way to specify a whole environment to be installed.\n    \"\"\"\n\n    usage = \"\"\"\n      %prog [options] <requirement specifier> [package-index-options] ...\n      %prog [options] -r <requirements file> [package-index-options] ...\n      %prog [options] [-e] <vcs project url> ...\n      %prog [options] [-e] <local project path> ...\n      %prog [options] <archive url/path> ...\"\"\"\n\n    def add_options(self) -> None:\n        self.cmd_opts.add_option(cmdoptions.requirements())\n        self.cmd_opts.add_option(cmdoptions.constraints())\n        self.cmd_opts.add_option(cmdoptions.no_deps())\n        self.cmd_opts.add_option(cmdoptions.pre())\n\n        self.cmd_opts.add_option(cmdoptions.editable())\n        self.cmd_opts.add_option(\n            \"--dry-run\",\n            action=\"store_true\",\n            dest=\"dry_run\",\n            default=False,\n            help=(\n                \"Don't actually install anything, just print what would be. \"\n                \"Can be used in combination with --ignore-installed \"\n                \"to 'resolve' the requirements.\"\n            ),\n        )\n        self.cmd_opts.add_option(\n            \"-t\",\n            \"--target\",\n            dest=\"target_dir\",\n            metavar=\"dir\",\n            default=None,\n            help=(\n                \"Install packages into <dir>. \"\n                \"By default this will not replace existing files/folders in \"\n                \"<dir>. Use --upgrade to replace existing packages in <dir> \"\n                \"with new versions.\"\n            ),\n        )\n        cmdoptions.add_target_python_options(self.cmd_opts)\n\n        self.cmd_opts.add_option(\n            \"--user\",\n            dest=\"use_user_site\",\n            action=\"store_true\",\n            help=(\n                \"Install to the Python user install directory for your \"\n                \"platform. Typically ~/.local/, or %APPDATA%\\\\Python on \"\n                \"Windows. (See the Python documentation for site.USER_BASE \"\n                \"for full details.)\"\n            ),\n        )\n        self.cmd_opts.add_option(\n            \"--no-user\",\n            dest=\"use_user_site\",\n            action=\"store_false\",\n            help=SUPPRESS_HELP,\n        )\n        self.cmd_opts.add_option(\n            \"--root\",\n            dest=\"root_path\",\n            metavar=\"dir\",\n            default=None,\n            help=\"Install everything relative to this alternate root directory.\",\n        )\n        self.cmd_opts.add_option(\n            \"--prefix\",\n            dest=\"prefix_path\",\n            metavar=\"dir\",\n            default=None,\n            help=(\n                \"Installation prefix where lib, bin and other top-level \"\n                \"folders are placed. Note that the resulting installation may \"\n                \"contain scripts and other resources which reference the \"\n                \"Python interpreter of pip, and not that of ``--prefix``. \"\n                \"See also the ``--python`` option if the intention is to \"\n                \"install packages into another (possibly pip-free) \"\n                \"environment.\"\n            ),\n        )\n\n        self.cmd_opts.add_option(cmdoptions.src())\n\n        self.cmd_opts.add_option(\n            \"-U\",\n            \"--upgrade\",\n            dest=\"upgrade\",\n            action=\"store_true\",\n            help=(\n                \"Upgrade all specified packages to the newest available \"\n                \"version. The handling of dependencies depends on the \"\n                \"upgrade-strategy used.\"\n            ),\n        )\n\n        self.cmd_opts.add_option(\n            \"--upgrade-strategy\",\n            dest=\"upgrade_strategy\",\n            default=\"only-if-needed\",\n            choices=[\"only-if-needed\", \"eager\"],\n            help=(\n                \"Determines how dependency upgrading should be handled \"\n                \"[default: %default]. \"\n                '\"eager\" - dependencies are upgraded regardless of '\n                \"whether the currently installed version satisfies the \"\n                \"requirements of the upgraded package(s). \"\n                '\"only-if-needed\" -  are upgraded only when they do not '\n                \"satisfy the requirements of the upgraded package(s).\"\n            ),\n        )\n\n        self.cmd_opts.add_option(\n            \"--force-reinstall\",\n            dest=\"force_reinstall\",\n            action=\"store_true\",\n            help=\"Reinstall all packages even if they are already up-to-date.\",\n        )\n\n        self.cmd_opts.add_option(\n            \"-I\",\n            \"--ignore-installed\",\n            dest=\"ignore_installed\",\n            action=\"store_true\",\n            help=(\n                \"Ignore the installed packages, overwriting them. \"\n                \"This can break your system if the existing package \"\n                \"is of a different version or was installed \"\n                \"with a different package manager!\"\n            ),\n        )\n\n        self.cmd_opts.add_option(cmdoptions.ignore_requires_python())\n        self.cmd_opts.add_option(cmdoptions.no_build_isolation())\n        self.cmd_opts.add_option(cmdoptions.use_pep517())\n        self.cmd_opts.add_option(cmdoptions.no_use_pep517())\n        self.cmd_opts.add_option(cmdoptions.check_build_deps())\n        self.cmd_opts.add_option(cmdoptions.override_externally_managed())\n\n        self.cmd_opts.add_option(cmdoptions.config_settings())\n        self.cmd_opts.add_option(cmdoptions.global_options())\n\n        self.cmd_opts.add_option(\n            \"--compile\",\n            action=\"store_true\",\n            dest=\"compile\",\n            default=True,\n            help=\"Compile Python source files to bytecode\",\n        )\n\n        self.cmd_opts.add_option(\n            \"--no-compile\",\n            action=\"store_false\",\n            dest=\"compile\",\n            help=\"Do not compile Python source files to bytecode\",\n        )\n\n        self.cmd_opts.add_option(\n            \"--no-warn-script-location\",\n            action=\"store_false\",\n            dest=\"warn_script_location\",\n            default=True,\n            help=\"Do not warn when installing scripts outside PATH\",\n        )\n        self.cmd_opts.add_option(\n            \"--no-warn-conflicts\",\n            action=\"store_false\",\n            dest=\"warn_about_conflicts\",\n            default=True,\n            help=\"Do not warn about broken dependencies\",\n        )\n        self.cmd_opts.add_option(cmdoptions.no_binary())\n        self.cmd_opts.add_option(cmdoptions.only_binary())\n        self.cmd_opts.add_option(cmdoptions.prefer_binary())\n        self.cmd_opts.add_option(cmdoptions.require_hashes())\n        self.cmd_opts.add_option(cmdoptions.progress_bar())\n        self.cmd_opts.add_option(cmdoptions.root_user_action())\n\n        index_opts = cmdoptions.make_option_group(\n            cmdoptions.index_group,\n            self.parser,\n        )\n\n        self.parser.insert_option_group(0, index_opts)\n        self.parser.insert_option_group(0, self.cmd_opts)\n\n        self.cmd_opts.add_option(\n            \"--report\",\n            dest=\"json_report_file\",\n            metavar=\"file\",\n            default=None,\n            help=(\n                \"Generate a JSON file describing what pip did to install \"\n                \"the provided requirements. \"\n                \"Can be used in combination with --dry-run and --ignore-installed \"\n                \"to 'resolve' the requirements. \"\n                \"When - is used as file name it writes to stdout. \"\n                \"When writing to stdout, please combine with the --quiet option \"\n                \"to avoid mixing pip logging output with JSON output.\"\n            ),\n        )\n\n    @with_cleanup\n    def run(self, options: Values, args: List[str]) -> int:\n        if options.use_user_site and options.target_dir is not None:\n            raise CommandError(\"Can not combine '--user' and '--target'\")\n\n        # Check whether the environment we're installing into is externally\n        # managed, as specified in PEP 668. Specifying --root, --target, or\n        # --prefix disables the check, since there's no reliable way to locate\n        # the EXTERNALLY-MANAGED file for those cases. An exception is also\n        # made specifically for \"--dry-run --report\" for convenience.\n        installing_into_current_environment = (\n            not (options.dry_run and options.json_report_file)\n            and options.root_path is None\n            and options.target_dir is None\n            and options.prefix_path is None\n        )\n        if (\n            installing_into_current_environment\n            and not options.override_externally_managed\n        ):\n            check_externally_managed()\n\n        upgrade_strategy = \"to-satisfy-only\"\n        if options.upgrade:\n            upgrade_strategy = options.upgrade_strategy\n\n        cmdoptions.check_dist_restriction(options, check_target=True)\n\n        logger.verbose(\"Using %s\", get_pip_version())\n        options.use_user_site = decide_user_install(\n            options.use_user_site,\n            prefix_path=options.prefix_path,\n            target_dir=options.target_dir,\n            root_path=options.root_path,\n            isolated_mode=options.isolated_mode,\n        )\n\n        target_temp_dir: Optional[TempDirectory] = None\n        target_temp_dir_path: Optional[str] = None\n        if options.target_dir:\n            options.ignore_installed = True\n            options.target_dir = os.path.abspath(options.target_dir)\n            if (\n                # fmt: off\n                os.path.exists(options.target_dir) and\n                not os.path.isdir(options.target_dir)\n                # fmt: on\n            ):\n                raise CommandError(\n                    \"Target path exists but is not a directory, will not continue.\"\n                )\n\n            # Create a target directory for using with the target option\n            target_temp_dir = TempDirectory(kind=\"target\")\n            target_temp_dir_path = target_temp_dir.path\n            self.enter_context(target_temp_dir)\n\n        global_options = options.global_options or []\n\n        session = self.get_default_session(options)\n\n        target_python = make_target_python(options)\n        finder = self._build_package_finder(\n            options=options,\n            session=session,\n            target_python=target_python,\n            ignore_requires_python=options.ignore_requires_python,\n        )\n        build_tracker = self.enter_context(get_build_tracker())\n\n        directory = TempDirectory(\n            delete=not options.no_clean,\n            kind=\"install\",\n            globally_managed=True,\n        )\n\n        try:\n            reqs = self.get_requirements(args, options, finder, session)\n            check_legacy_setup_py_options(options, reqs)\n\n            wheel_cache = WheelCache(options.cache_dir)\n\n            # Only when installing is it permitted to use PEP 660.\n            # In other circumstances (pip wheel, pip download) we generate\n            # regular (i.e. non editable) metadata and wheels.\n            for req in reqs:\n                req.permit_editable_wheels = True\n\n            preparer = self.make_requirement_preparer(\n                temp_build_dir=directory,\n                options=options,\n                build_tracker=build_tracker,\n                session=session,\n                finder=finder,\n                use_user_site=options.use_user_site,\n                verbosity=self.verbosity,\n            )\n            resolver = self.make_resolver(\n                preparer=preparer,\n                finder=finder,\n                options=options,\n                wheel_cache=wheel_cache,\n                use_user_site=options.use_user_site,\n                ignore_installed=options.ignore_installed,\n                ignore_requires_python=options.ignore_requires_python,\n                force_reinstall=options.force_reinstall,\n                upgrade_strategy=upgrade_strategy,\n                use_pep517=options.use_pep517,\n                py_version_info=options.python_version,\n            )\n\n            self.trace_basic_info(finder)\n\n            requirement_set = resolver.resolve(\n                reqs, check_supported_wheels=not options.target_dir\n            )\n\n            if options.json_report_file:\n                report = InstallationReport(requirement_set.requirements_to_install)\n                if options.json_report_file == \"-\":\n                    print_json(data=report.to_dict())\n                else:\n                    with open(options.json_report_file, \"w\", encoding=\"utf-8\") as f:\n                        json.dump(report.to_dict(), f, indent=2, ensure_ascii=False)\n\n            if options.dry_run:\n                would_install_items = sorted(\n                    (r.metadata[\"name\"], r.metadata[\"version\"])\n                    for r in requirement_set.requirements_to_install\n                )\n                if would_install_items:\n                    write_output(\n                        \"Would install %s\",\n                        \" \".join(\"-\".join(item) for item in would_install_items),\n                    )\n                return SUCCESS\n\n            try:\n                pip_req = requirement_set.get_requirement(\"pip\")\n            except KeyError:\n                modifying_pip = False\n            else:\n                # If we're not replacing an already installed pip,\n                # we're not modifying it.\n                modifying_pip = pip_req.satisfied_by is None\n            protect_pip_from_modification_on_windows(modifying_pip=modifying_pip)\n\n            reqs_to_build = [\n                r\n                for r in requirement_set.requirements.values()\n                if should_build_for_install_command(r)\n            ]\n\n            _, build_failures = build(\n                reqs_to_build,\n                wheel_cache=wheel_cache,\n                verify=True,\n                build_options=[],\n                global_options=global_options,\n            )\n\n            if build_failures:\n                raise InstallationError(\n                    \"Failed to build installable wheels for some \"\n                    \"pyproject.toml based projects ({})\".format(\n                        \", \".join(r.name for r in build_failures)  # type: ignore\n                    )\n                )\n\n            to_install = resolver.get_installation_order(requirement_set)\n\n            # Check for conflicts in the package set we're installing.\n            conflicts: Optional[ConflictDetails] = None\n            should_warn_about_conflicts = (\n                not options.ignore_dependencies and options.warn_about_conflicts\n            )\n            if should_warn_about_conflicts:\n                conflicts = self._determine_conflicts(to_install)\n\n            # Don't warn about script install locations if\n            # --target or --prefix has been specified\n            warn_script_location = options.warn_script_location\n            if options.target_dir or options.prefix_path:\n                warn_script_location = False\n\n            installed = install_given_reqs(\n                to_install,\n                global_options,\n                root=options.root_path,\n                home=target_temp_dir_path,\n                prefix=options.prefix_path,\n                warn_script_location=warn_script_location,\n                use_user_site=options.use_user_site,\n                pycompile=options.compile,\n            )\n\n            lib_locations = get_lib_location_guesses(\n                user=options.use_user_site,\n                home=target_temp_dir_path,\n                root=options.root_path,\n                prefix=options.prefix_path,\n                isolated=options.isolated_mode,\n            )\n            env = get_environment(lib_locations)\n\n            # Display a summary of installed packages, with extra care to\n            # display a package name as it was requested by the user.\n            installed.sort(key=operator.attrgetter(\"name\"))\n            summary = []\n            installed_versions = {}\n            for distribution in env.iter_all_distributions():\n                installed_versions[distribution.canonical_name] = distribution.version\n            for package in installed:\n                display_name = package.name\n                version = installed_versions.get(canonicalize_name(display_name), None)\n                if version:\n                    text = f\"{display_name}-{version}\"\n                else:\n                    text = display_name\n                summary.append(text)\n\n            if conflicts is not None:\n                self._warn_about_conflicts(\n                    conflicts,\n                    resolver_variant=self.determine_resolver_variant(options),\n                )\n\n            installed_desc = \" \".join(summary)\n            if installed_desc:\n                write_output(\n                    \"Successfully installed %s\",\n                    installed_desc,\n                )\n        except OSError as error:\n            show_traceback = self.verbosity >= 1\n\n            message = create_os_error_message(\n                error,\n                show_traceback,\n                options.use_user_site,\n            )\n            logger.error(message, exc_info=show_traceback)\n\n            return ERROR\n\n        if options.target_dir:\n            assert target_temp_dir\n            self._handle_target_dir(\n                options.target_dir, target_temp_dir, options.upgrade\n            )\n        if options.root_user_action == \"warn\":\n            warn_if_run_as_root()\n        return SUCCESS\n\n    def _handle_target_dir(\n        self, target_dir: str, target_temp_dir: TempDirectory, upgrade: bool\n    ) -> None:\n        ensure_dir(target_dir)\n\n        # Checking both purelib and platlib directories for installed\n        # packages to be moved to target directory\n        lib_dir_list = []\n\n        # Checking both purelib and platlib directories for installed\n        # packages to be moved to target directory\n        scheme = get_scheme(\"\", home=target_temp_dir.path)\n        purelib_dir = scheme.purelib\n        platlib_dir = scheme.platlib\n        data_dir = scheme.data\n\n        if os.path.exists(purelib_dir):\n            lib_dir_list.append(purelib_dir)\n        if os.path.exists(platlib_dir) and platlib_dir != purelib_dir:\n            lib_dir_list.append(platlib_dir)\n        if os.path.exists(data_dir):\n            lib_dir_list.append(data_dir)\n\n        for lib_dir in lib_dir_list:\n            for item in os.listdir(lib_dir):\n                if lib_dir == data_dir:\n                    ddir = os.path.join(data_dir, item)\n                    if any(s.startswith(ddir) for s in lib_dir_list[:-1]):\n                        continue\n                target_item_dir = os.path.join(target_dir, item)\n                if os.path.exists(target_item_dir):\n                    if not upgrade:\n                        logger.warning(\n                            \"Target directory %s already exists. Specify \"\n                            \"--upgrade to force replacement.\",\n                            target_item_dir,\n                        )\n                        continue\n                    if os.path.islink(target_item_dir):\n                        logger.warning(\n                            \"Target directory %s already exists and is \"\n                            \"a link. pip will not automatically replace \"\n                            \"links, please remove if replacement is \"\n                            \"desired.\",\n                            target_item_dir,\n                        )\n                        continue\n                    if os.path.isdir(target_item_dir):\n                        shutil.rmtree(target_item_dir)\n                    else:\n                        os.remove(target_item_dir)\n\n                shutil.move(os.path.join(lib_dir, item), target_item_dir)\n\n    def _determine_conflicts(\n        self, to_install: List[InstallRequirement]\n    ) -> Optional[ConflictDetails]:\n        try:\n            return check_install_conflicts(to_install)\n        except Exception:\n            logger.exception(\n                \"Error while checking for conflicts. Please file an issue on \"\n                \"pip's issue tracker: https://github.com/pypa/pip/issues/new\"\n            )\n            return None\n\n    def _warn_about_conflicts(\n        self, conflict_details: ConflictDetails, resolver_variant: str\n    ) -> None:\n        package_set, (missing, conflicting) = conflict_details\n        if not missing and not conflicting:\n            return\n\n        parts: List[str] = []\n        if resolver_variant == \"legacy\":\n            parts.append(\n                \"pip's legacy dependency resolver does not consider dependency \"\n                \"conflicts when selecting packages. This behaviour is the \"\n                \"source of the following dependency conflicts.\"\n            )\n        else:\n            assert resolver_variant == \"resolvelib\"\n            parts.append(\n                \"pip's dependency resolver does not currently take into account \"\n                \"all the packages that are installed. This behaviour is the \"\n                \"source of the following dependency conflicts.\"\n            )\n\n        # NOTE: There is some duplication here, with commands/check.py\n        for project_name in missing:\n            version = package_set[project_name][0]\n            for dependency in missing[project_name]:\n                message = (\n                    f\"{project_name} {version} requires {dependency[1]}, \"\n                    \"which is not installed.\"\n                )\n                parts.append(message)\n\n        for project_name in conflicting:\n            version = package_set[project_name][0]\n            for dep_name, dep_version, req in conflicting[project_name]:\n                message = (\n                    \"{name} {version} requires {requirement}, but {you} have \"\n                    \"{dep_name} {dep_version} which is incompatible.\"\n                ).format(\n                    name=project_name,\n                    version=version,\n                    requirement=req,\n                    dep_name=dep_name,\n                    dep_version=dep_version,\n                    you=(\"you\" if resolver_variant == \"resolvelib\" else \"you'll\"),\n                )\n                parts.append(message)\n\n        logger.critical(\"\\n\".join(parts))\n\n\ndef get_lib_location_guesses(\n    user: bool = False,\n    home: Optional[str] = None,\n    root: Optional[str] = None,\n    isolated: bool = False,\n    prefix: Optional[str] = None,\n) -> List[str]:\n    scheme = get_scheme(\n        \"\",\n        user=user,\n        home=home,\n        root=root,\n        isolated=isolated,\n        prefix=prefix,\n    )\n    return [scheme.purelib, scheme.platlib]\n\n\ndef site_packages_writable(root: Optional[str], isolated: bool) -> bool:\n    return all(\n        test_writable_dir(d)\n        for d in set(get_lib_location_guesses(root=root, isolated=isolated))\n    )\n\n\ndef decide_user_install(\n    use_user_site: Optional[bool],\n    prefix_path: Optional[str] = None,\n    target_dir: Optional[str] = None,\n    root_path: Optional[str] = None,\n    isolated_mode: bool = False,\n) -> bool:\n    \"\"\"Determine whether to do a user install based on the input options.\n\n    If use_user_site is False, no additional checks are done.\n    If use_user_site is True, it is checked for compatibility with other\n    options.\n    If use_user_site is None, the default behaviour depends on the environment,\n    which is provided by the other arguments.\n    \"\"\"\n    # In some cases (config from tox), use_user_site can be set to an integer\n    # rather than a bool, which 'use_user_site is False' wouldn't catch.\n    if (use_user_site is not None) and (not use_user_site):\n        logger.debug(\"Non-user install by explicit request\")\n        return False\n\n    if use_user_site:\n        if prefix_path:\n            raise CommandError(\n                \"Can not combine '--user' and '--prefix' as they imply \"\n                \"different installation locations\"\n            )\n        if virtualenv_no_global():\n            raise InstallationError(\n                \"Can not perform a '--user' install. User site-packages \"\n                \"are not visible in this virtualenv.\"\n            )\n        logger.debug(\"User install by explicit request\")\n        return True\n\n    # If we are here, user installs have not been explicitly requested/avoided\n    assert use_user_site is None\n\n    # user install incompatible with --prefix/--target\n    if prefix_path or target_dir:\n        logger.debug(\"Non-user install due to --prefix or --target option\")\n        return False\n\n    # If user installs are not enabled, choose a non-user install\n    if not site.ENABLE_USER_SITE:\n        logger.debug(\"Non-user install because user site-packages disabled\")\n        return False\n\n    # If we have permission for a non-user install, do that,\n    # otherwise do a user install.\n    if site_packages_writable(root=root_path, isolated=isolated_mode):\n        logger.debug(\"Non-user install because site-packages writeable\")\n        return False\n\n    logger.info(\n        \"Defaulting to user installation because normal site-packages \"\n        \"is not writeable\"\n    )\n    return True\n\n\ndef create_os_error_message(\n    error: OSError, show_traceback: bool, using_user_site: bool\n) -> str:\n    \"\"\"Format an error message for an OSError\n\n    It may occur anytime during the execution of the install command.\n    \"\"\"\n    parts = []\n\n    # Mention the error if we are not going to show a traceback\n    parts.append(\"Could not install packages due to an OSError\")\n    if not show_traceback:\n        parts.append(\": \")\n        parts.append(str(error))\n    else:\n        parts.append(\".\")\n\n    # Spilt the error indication from a helper message (if any)\n    parts[-1] += \"\\n\"\n\n    # Suggest useful actions to the user:\n    #  (1) using user site-packages or (2) verifying the permissions\n    if error.errno == errno.EACCES:\n        user_option_part = \"Consider using the `--user` option\"\n        permissions_part = \"Check the permissions\"\n\n        if not running_under_virtualenv() and not using_user_site:\n            parts.extend(\n                [\n                    user_option_part,\n                    \" or \",\n                    permissions_part.lower(),\n                ]\n            )\n        else:\n            parts.append(permissions_part)\n        parts.append(\".\\n\")\n\n    # Suggest the user to enable Long Paths if path length is\n    # more than 260\n    if (\n        WINDOWS\n        and error.errno == errno.ENOENT\n        and error.filename\n        and len(error.filename) > 260\n    ):\n        parts.append(\n            \"HINT: This error might have occurred since \"\n            \"this system does not have Windows Long Path \"\n            \"support enabled. You can find information on \"\n            \"how to enable this at \"\n            \"https://pip.pypa.io/warnings/enable-long-paths\\n\"\n        )\n\n    return \"\".join(parts).strip() + \"\\n\"\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/commands/list.py","size":12769,"sha1":"9556dbd9834748e3b2fcccb07d19c9dd966e9df0","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import json\nimport logging\nfrom optparse import Values\nfrom typing import TYPE_CHECKING, Generator, List, Optional, Sequence, Tuple, cast\n\nfrom pip._vendor.packaging.utils import canonicalize_name\nfrom pip._vendor.packaging.version import Version\n\nfrom pip._internal.cli import cmdoptions\nfrom pip._internal.cli.index_command import IndexGroupCommand\nfrom pip._internal.cli.status_codes import SUCCESS\nfrom pip._internal.exceptions import CommandError\nfrom pip._internal.metadata import BaseDistribution, get_environment\nfrom pip._internal.models.selection_prefs import SelectionPreferences\nfrom pip._internal.utils.compat import stdlib_pkgs\nfrom pip._internal.utils.misc import tabulate, write_output\n\nif TYPE_CHECKING:\n    from pip._internal.index.package_finder import PackageFinder\n    from pip._internal.network.session import PipSession\n\n    class _DistWithLatestInfo(BaseDistribution):\n        \"\"\"Give the distribution object a couple of extra fields.\n\n        These will be populated during ``get_outdated()``. This is dirty but\n        makes the rest of the code much cleaner.\n        \"\"\"\n\n        latest_version: Version\n        latest_filetype: str\n\n    _ProcessedDists = Sequence[_DistWithLatestInfo]\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass ListCommand(IndexGroupCommand):\n    \"\"\"\n    List installed packages, including editables.\n\n    Packages are listed in a case-insensitive sorted order.\n    \"\"\"\n\n    ignore_require_venv = True\n    usage = \"\"\"\n      %prog [options]\"\"\"\n\n    def add_options(self) -> None:\n        self.cmd_opts.add_option(\n            \"-o\",\n            \"--outdated\",\n            action=\"store_true\",\n            default=False,\n            help=\"List outdated packages\",\n        )\n        self.cmd_opts.add_option(\n            \"-u\",\n            \"--uptodate\",\n            action=\"store_true\",\n            default=False,\n            help=\"List uptodate packages\",\n        )\n        self.cmd_opts.add_option(\n            \"-e\",\n            \"--editable\",\n            action=\"store_true\",\n            default=False,\n            help=\"List editable projects.\",\n        )\n        self.cmd_opts.add_option(\n            \"-l\",\n            \"--local\",\n            action=\"store_true\",\n            default=False,\n            help=(\n                \"If in a virtualenv that has global access, do not list \"\n                \"globally-installed packages.\"\n            ),\n        )\n        self.cmd_opts.add_option(\n            \"--user\",\n            dest=\"user\",\n            action=\"store_true\",\n            default=False,\n            help=\"Only output packages installed in user-site.\",\n        )\n        self.cmd_opts.add_option(cmdoptions.list_path())\n        self.cmd_opts.add_option(\n            \"--pre\",\n            action=\"store_true\",\n            default=False,\n            help=(\n                \"Include pre-release and development versions. By default, \"\n                \"pip only finds stable versions.\"\n            ),\n        )\n\n        self.cmd_opts.add_option(\n            \"--format\",\n            action=\"store\",\n            dest=\"list_format\",\n            default=\"columns\",\n            choices=(\"columns\", \"freeze\", \"json\"),\n            help=(\n                \"Select the output format among: columns (default), freeze, or json. \"\n                \"The 'freeze' format cannot be used with the --outdated option.\"\n            ),\n        )\n\n        self.cmd_opts.add_option(\n            \"--not-required\",\n            action=\"store_true\",\n            dest=\"not_required\",\n            help=\"List packages that are not dependencies of installed packages.\",\n        )\n\n        self.cmd_opts.add_option(\n            \"--exclude-editable\",\n            action=\"store_false\",\n            dest=\"include_editable\",\n            help=\"Exclude editable package from output.\",\n        )\n        self.cmd_opts.add_option(\n            \"--include-editable\",\n            action=\"store_true\",\n            dest=\"include_editable\",\n            help=\"Include editable package from output.\",\n            default=True,\n        )\n        self.cmd_opts.add_option(cmdoptions.list_exclude())\n        index_opts = cmdoptions.make_option_group(cmdoptions.index_group, self.parser)\n\n        self.parser.insert_option_group(0, index_opts)\n        self.parser.insert_option_group(0, self.cmd_opts)\n\n    def handle_pip_version_check(self, options: Values) -> None:\n        if options.outdated or options.uptodate:\n            super().handle_pip_version_check(options)\n\n    def _build_package_finder(\n        self, options: Values, session: \"PipSession\"\n    ) -> \"PackageFinder\":\n        \"\"\"\n        Create a package finder appropriate to this list command.\n        \"\"\"\n        # Lazy import the heavy index modules as most list invocations won't need 'em.\n        from pip._internal.index.collector import LinkCollector\n        from pip._internal.index.package_finder import PackageFinder\n\n        link_collector = LinkCollector.create(session, options=options)\n\n        # Pass allow_yanked=False to ignore yanked versions.\n        selection_prefs = SelectionPreferences(\n            allow_yanked=False,\n            allow_all_prereleases=options.pre,\n        )\n\n        return PackageFinder.create(\n            link_collector=link_collector,\n            selection_prefs=selection_prefs,\n        )\n\n    def run(self, options: Values, args: List[str]) -> int:\n        if options.outdated and options.uptodate:\n            raise CommandError(\"Options --outdated and --uptodate cannot be combined.\")\n\n        if options.outdated and options.list_format == \"freeze\":\n            raise CommandError(\n                \"List format 'freeze' cannot be used with the --outdated option.\"\n            )\n\n        cmdoptions.check_list_path_option(options)\n\n        skip = set(stdlib_pkgs)\n        if options.excludes:\n            skip.update(canonicalize_name(n) for n in options.excludes)\n\n        packages: _ProcessedDists = [\n            cast(\"_DistWithLatestInfo\", d)\n            for d in get_environment(options.path).iter_installed_distributions(\n                local_only=options.local,\n                user_only=options.user,\n                editables_only=options.editable,\n                include_editables=options.include_editable,\n                skip=skip,\n            )\n        ]\n\n        # get_not_required must be called firstly in order to find and\n        # filter out all dependencies correctly. Otherwise a package\n        # can't be identified as requirement because some parent packages\n        # could be filtered out before.\n        if options.not_required:\n            packages = self.get_not_required(packages, options)\n\n        if options.outdated:\n            packages = self.get_outdated(packages, options)\n        elif options.uptodate:\n            packages = self.get_uptodate(packages, options)\n\n        self.output_package_listing(packages, options)\n        return SUCCESS\n\n    def get_outdated(\n        self, packages: \"_ProcessedDists\", options: Values\n    ) -> \"_ProcessedDists\":\n        return [\n            dist\n            for dist in self.iter_packages_latest_infos(packages, options)\n            if dist.latest_version > dist.version\n        ]\n\n    def get_uptodate(\n        self, packages: \"_ProcessedDists\", options: Values\n    ) -> \"_ProcessedDists\":\n        return [\n            dist\n            for dist in self.iter_packages_latest_infos(packages, options)\n            if dist.latest_version == dist.version\n        ]\n\n    def get_not_required(\n        self, packages: \"_ProcessedDists\", options: Values\n    ) -> \"_ProcessedDists\":\n        dep_keys = {\n            canonicalize_name(dep.name)\n            for dist in packages\n            for dep in (dist.iter_dependencies() or ())\n        }\n\n        # Create a set to remove duplicate packages, and cast it to a list\n        # to keep the return type consistent with get_outdated and\n        # get_uptodate\n        return list({pkg for pkg in packages if pkg.canonical_name not in dep_keys})\n\n    def iter_packages_latest_infos(\n        self, packages: \"_ProcessedDists\", options: Values\n    ) -> Generator[\"_DistWithLatestInfo\", None, None]:\n        with self._build_session(options) as session:\n            finder = self._build_package_finder(options, session)\n\n            def latest_info(\n                dist: \"_DistWithLatestInfo\",\n            ) -> Optional[\"_DistWithLatestInfo\"]:\n                all_candidates = finder.find_all_candidates(dist.canonical_name)\n                if not options.pre:\n                    # Remove prereleases\n                    all_candidates = [\n                        candidate\n                        for candidate in all_candidates\n                        if not candidate.version.is_prerelease\n                    ]\n\n                evaluator = finder.make_candidate_evaluator(\n                    project_name=dist.canonical_name,\n                )\n                best_candidate = evaluator.sort_best_candidate(all_candidates)\n                if best_candidate is None:\n                    return None\n\n                remote_version = best_candidate.version\n                if best_candidate.link.is_wheel:\n                    typ = \"wheel\"\n                else:\n                    typ = \"sdist\"\n                dist.latest_version = remote_version\n                dist.latest_filetype = typ\n                return dist\n\n            for dist in map(latest_info, packages):\n                if dist is not None:\n                    yield dist\n\n    def output_package_listing(\n        self, packages: \"_ProcessedDists\", options: Values\n    ) -> None:\n        packages = sorted(\n            packages,\n            key=lambda dist: dist.canonical_name,\n        )\n        if options.list_format == \"columns\" and packages:\n            data, header = format_for_columns(packages, options)\n            self.output_package_listing_columns(data, header)\n        elif options.list_format == \"freeze\":\n            for dist in packages:\n                if options.verbose >= 1:\n                    write_output(\n                        \"%s==%s (%s)\", dist.raw_name, dist.version, dist.location\n                    )\n                else:\n                    write_output(\"%s==%s\", dist.raw_name, dist.version)\n        elif options.list_format == \"json\":\n            write_output(format_for_json(packages, options))\n\n    def output_package_listing_columns(\n        self, data: List[List[str]], header: List[str]\n    ) -> None:\n        # insert the header first: we need to know the size of column names\n        if len(data) > 0:\n            data.insert(0, header)\n\n        pkg_strings, sizes = tabulate(data)\n\n        # Create and add a separator.\n        if len(data) > 0:\n            pkg_strings.insert(1, \" \".join(\"-\" * x for x in sizes))\n\n        for val in pkg_strings:\n            write_output(val)\n\n\ndef format_for_columns(\n    pkgs: \"_ProcessedDists\", options: Values\n) -> Tuple[List[List[str]], List[str]]:\n    \"\"\"\n    Convert the package data into something usable\n    by output_package_listing_columns.\n    \"\"\"\n    header = [\"Package\", \"Version\"]\n\n    running_outdated = options.outdated\n    if running_outdated:\n        header.extend([\"Latest\", \"Type\"])\n\n    has_editables = any(x.editable for x in pkgs)\n    if has_editables:\n        header.append(\"Editable project location\")\n\n    if options.verbose >= 1:\n        header.append(\"Location\")\n    if options.verbose >= 1:\n        header.append(\"Installer\")\n\n    data = []\n    for proj in pkgs:\n        # if we're working on the 'outdated' list, separate out the\n        # latest_version and type\n        row = [proj.raw_name, proj.raw_version]\n\n        if running_outdated:\n            row.append(str(proj.latest_version))\n            row.append(proj.latest_filetype)\n\n        if has_editables:\n            row.append(proj.editable_project_location or \"\")\n\n        if options.verbose >= 1:\n            row.append(proj.location or \"\")\n        if options.verbose >= 1:\n            row.append(proj.installer)\n\n        data.append(row)\n\n    return data, header\n\n\ndef format_for_json(packages: \"_ProcessedDists\", options: Values) -> str:\n    data = []\n    for dist in packages:\n        info = {\n            \"name\": dist.raw_name,\n            \"version\": str(dist.version),\n        }\n        if options.verbose >= 1:\n            info[\"location\"] = dist.location or \"\"\n            info[\"installer\"] = dist.installer\n        if options.outdated:\n            info[\"latest_version\"] = str(dist.latest_version)\n            info[\"latest_filetype\"] = dist.latest_filetype\n        editable_project_location = dist.editable_project_location\n        if editable_project_location:\n            info[\"editable_project_location\"] = editable_project_location\n        data.append(info)\n    return json.dumps(data)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/commands/search.py","size":5626,"sha1":"052743d68e4492a242da6b4fda3b19983fd08e7a","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import logging\nimport shutil\nimport sys\nimport textwrap\nimport xmlrpc.client\nfrom collections import OrderedDict\nfrom optparse import Values\nfrom typing import TYPE_CHECKING, Dict, List, Optional, TypedDict\n\nfrom pip._vendor.packaging.version import parse as parse_version\n\nfrom pip._internal.cli.base_command import Command\nfrom pip._internal.cli.req_command import SessionCommandMixin\nfrom pip._internal.cli.status_codes import NO_MATCHES_FOUND, SUCCESS\nfrom pip._internal.exceptions import CommandError\nfrom pip._internal.metadata import get_default_environment\nfrom pip._internal.models.index import PyPI\nfrom pip._internal.network.xmlrpc import PipXmlrpcTransport\nfrom pip._internal.utils.logging import indent_log\nfrom pip._internal.utils.misc import write_output\n\nif TYPE_CHECKING:\n\n    class TransformedHit(TypedDict):\n        name: str\n        summary: str\n        versions: List[str]\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass SearchCommand(Command, SessionCommandMixin):\n    \"\"\"Search for PyPI packages whose name or summary contains <query>.\"\"\"\n\n    usage = \"\"\"\n      %prog [options] <query>\"\"\"\n    ignore_require_venv = True\n\n    def add_options(self) -> None:\n        self.cmd_opts.add_option(\n            \"-i\",\n            \"--index\",\n            dest=\"index\",\n            metavar=\"URL\",\n            default=PyPI.pypi_url,\n            help=\"Base URL of Python Package Index (default %default)\",\n        )\n\n        self.parser.insert_option_group(0, self.cmd_opts)\n\n    def run(self, options: Values, args: List[str]) -> int:\n        if not args:\n            raise CommandError(\"Missing required argument (search query).\")\n        query = args\n        pypi_hits = self.search(query, options)\n        hits = transform_hits(pypi_hits)\n\n        terminal_width = None\n        if sys.stdout.isatty():\n            terminal_width = shutil.get_terminal_size()[0]\n\n        print_results(hits, terminal_width=terminal_width)\n        if pypi_hits:\n            return SUCCESS\n        return NO_MATCHES_FOUND\n\n    def search(self, query: List[str], options: Values) -> List[Dict[str, str]]:\n        index_url = options.index\n\n        session = self.get_default_session(options)\n\n        transport = PipXmlrpcTransport(index_url, session)\n        pypi = xmlrpc.client.ServerProxy(index_url, transport)\n        try:\n            hits = pypi.search({\"name\": query, \"summary\": query}, \"or\")\n        except xmlrpc.client.Fault as fault:\n            message = (\n                f\"XMLRPC request failed [code: {fault.faultCode}]\\n{fault.faultString}\"\n            )\n            raise CommandError(message)\n        assert isinstance(hits, list)\n        return hits\n\n\ndef transform_hits(hits: List[Dict[str, str]]) -> List[\"TransformedHit\"]:\n    \"\"\"\n    The list from pypi is really a list of versions. We want a list of\n    packages with the list of versions stored inline. This converts the\n    list from pypi into one we can use.\n    \"\"\"\n    packages: Dict[str, TransformedHit] = OrderedDict()\n    for hit in hits:\n        name = hit[\"name\"]\n        summary = hit[\"summary\"]\n        version = hit[\"version\"]\n\n        if name not in packages.keys():\n            packages[name] = {\n                \"name\": name,\n                \"summary\": summary,\n                \"versions\": [version],\n            }\n        else:\n            packages[name][\"versions\"].append(version)\n\n            # if this is the highest version, replace summary and score\n            if version == highest_version(packages[name][\"versions\"]):\n                packages[name][\"summary\"] = summary\n\n    return list(packages.values())\n\n\ndef print_dist_installation_info(name: str, latest: str) -> None:\n    env = get_default_environment()\n    dist = env.get_distribution(name)\n    if dist is not None:\n        with indent_log():\n            if dist.version == latest:\n                write_output(\"INSTALLED: %s (latest)\", dist.version)\n            else:\n                write_output(\"INSTALLED: %s\", dist.version)\n                if parse_version(latest).pre:\n                    write_output(\n                        \"LATEST:    %s (pre-release; install\"\n                        \" with `pip install --pre`)\",\n                        latest,\n                    )\n                else:\n                    write_output(\"LATEST:    %s\", latest)\n\n\ndef print_results(\n    hits: List[\"TransformedHit\"],\n    name_column_width: Optional[int] = None,\n    terminal_width: Optional[int] = None,\n) -> None:\n    if not hits:\n        return\n    if name_column_width is None:\n        name_column_width = (\n            max(\n                [\n                    len(hit[\"name\"]) + len(highest_version(hit.get(\"versions\", [\"-\"])))\n                    for hit in hits\n                ]\n            )\n            + 4\n        )\n\n    for hit in hits:\n        name = hit[\"name\"]\n        summary = hit[\"summary\"] or \"\"\n        latest = highest_version(hit.get(\"versions\", [\"-\"]))\n        if terminal_width is not None:\n            target_width = terminal_width - name_column_width - 5\n            if target_width > 10:\n                # wrap and indent summary to fit terminal\n                summary_lines = textwrap.wrap(summary, target_width)\n                summary = (\"\\n\" + \" \" * (name_column_width + 3)).join(summary_lines)\n\n        name_latest = f\"{name} ({latest})\"\n        line = f\"{name_latest:{name_column_width}} - {summary}\"\n        try:\n            write_output(line)\n            print_dist_installation_info(name, latest)\n        except UnicodeEncodeError:\n            pass\n\n\ndef highest_version(versions: List[str]) -> str:\n    return max(versions, key=parse_version)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/commands/show.py","size":7857,"sha1":"768ceb9539ecb8150fb3a592929a01678b96302f","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import logging\nfrom optparse import Values\nfrom typing import Generator, Iterable, Iterator, List, NamedTuple, Optional\n\nfrom pip._vendor.packaging.requirements import InvalidRequirement\nfrom pip._vendor.packaging.utils import canonicalize_name\n\nfrom pip._internal.cli.base_command import Command\nfrom pip._internal.cli.status_codes import ERROR, SUCCESS\nfrom pip._internal.metadata import BaseDistribution, get_default_environment\nfrom pip._internal.utils.misc import write_output\n\nlogger = logging.getLogger(__name__)\n\n\nclass ShowCommand(Command):\n    \"\"\"\n    Show information about one or more installed packages.\n\n    The output is in RFC-compliant mail header format.\n    \"\"\"\n\n    usage = \"\"\"\n      %prog [options] <package> ...\"\"\"\n    ignore_require_venv = True\n\n    def add_options(self) -> None:\n        self.cmd_opts.add_option(\n            \"-f\",\n            \"--files\",\n            dest=\"files\",\n            action=\"store_true\",\n            default=False,\n            help=\"Show the full list of installed files for each package.\",\n        )\n\n        self.parser.insert_option_group(0, self.cmd_opts)\n\n    def run(self, options: Values, args: List[str]) -> int:\n        if not args:\n            logger.warning(\"ERROR: Please provide a package name or names.\")\n            return ERROR\n        query = args\n\n        results = search_packages_info(query)\n        if not print_results(\n            results, list_files=options.files, verbose=options.verbose\n        ):\n            return ERROR\n        return SUCCESS\n\n\nclass _PackageInfo(NamedTuple):\n    name: str\n    version: str\n    location: str\n    editable_project_location: Optional[str]\n    requires: List[str]\n    required_by: List[str]\n    installer: str\n    metadata_version: str\n    classifiers: List[str]\n    summary: str\n    homepage: str\n    project_urls: List[str]\n    author: str\n    author_email: str\n    license: str\n    license_expression: str\n    entry_points: List[str]\n    files: Optional[List[str]]\n\n\ndef search_packages_info(query: List[str]) -> Generator[_PackageInfo, None, None]:\n    \"\"\"\n    Gather details from installed distributions. Print distribution name,\n    version, location, and installed files. Installed files requires a\n    pip generated 'installed-files.txt' in the distributions '.egg-info'\n    directory.\n    \"\"\"\n    env = get_default_environment()\n\n    installed = {dist.canonical_name: dist for dist in env.iter_all_distributions()}\n    query_names = [canonicalize_name(name) for name in query]\n    missing = sorted(\n        [name for name, pkg in zip(query, query_names) if pkg not in installed]\n    )\n    if missing:\n        logger.warning(\"Package(s) not found: %s\", \", \".join(missing))\n\n    def _get_requiring_packages(current_dist: BaseDistribution) -> Iterator[str]:\n        return (\n            dist.metadata[\"Name\"] or \"UNKNOWN\"\n            for dist in installed.values()\n            if current_dist.canonical_name\n            in {canonicalize_name(d.name) for d in dist.iter_dependencies()}\n        )\n\n    for query_name in query_names:\n        try:\n            dist = installed[query_name]\n        except KeyError:\n            continue\n\n        try:\n            requires = sorted(\n                # Avoid duplicates in requirements (e.g. due to environment markers).\n                {req.name for req in dist.iter_dependencies()},\n                key=str.lower,\n            )\n        except InvalidRequirement:\n            requires = sorted(dist.iter_raw_dependencies(), key=str.lower)\n\n        try:\n            required_by = sorted(_get_requiring_packages(dist), key=str.lower)\n        except InvalidRequirement:\n            required_by = [\"#N/A\"]\n\n        try:\n            entry_points_text = dist.read_text(\"entry_points.txt\")\n            entry_points = entry_points_text.splitlines(keepends=False)\n        except FileNotFoundError:\n            entry_points = []\n\n        files_iter = dist.iter_declared_entries()\n        if files_iter is None:\n            files: Optional[List[str]] = None\n        else:\n            files = sorted(files_iter)\n\n        metadata = dist.metadata\n\n        project_urls = metadata.get_all(\"Project-URL\", [])\n        homepage = metadata.get(\"Home-page\", \"\")\n        if not homepage:\n            # It's common that there is a \"homepage\" Project-URL, but Home-page\n            # remains unset (especially as PEP 621 doesn't surface the field).\n            #\n            # This logic was taken from PyPI's codebase.\n            for url in project_urls:\n                url_label, url = url.split(\",\", maxsplit=1)\n                normalized_label = (\n                    url_label.casefold().replace(\"-\", \"\").replace(\"_\", \"\").strip()\n                )\n                if normalized_label == \"homepage\":\n                    homepage = url.strip()\n                    break\n\n        yield _PackageInfo(\n            name=dist.raw_name,\n            version=dist.raw_version,\n            location=dist.location or \"\",\n            editable_project_location=dist.editable_project_location,\n            requires=requires,\n            required_by=required_by,\n            installer=dist.installer,\n            metadata_version=dist.metadata_version or \"\",\n            classifiers=metadata.get_all(\"Classifier\", []),\n            summary=metadata.get(\"Summary\", \"\"),\n            homepage=homepage,\n            project_urls=project_urls,\n            author=metadata.get(\"Author\", \"\"),\n            author_email=metadata.get(\"Author-email\", \"\"),\n            license=metadata.get(\"License\", \"\"),\n            license_expression=metadata.get(\"License-Expression\", \"\"),\n            entry_points=entry_points,\n            files=files,\n        )\n\n\ndef print_results(\n    distributions: Iterable[_PackageInfo],\n    list_files: bool,\n    verbose: bool,\n) -> bool:\n    \"\"\"\n    Print the information from installed distributions found.\n    \"\"\"\n    results_printed = False\n    for i, dist in enumerate(distributions):\n        results_printed = True\n        if i > 0:\n            write_output(\"---\")\n\n        metadata_version_tuple = tuple(map(int, dist.metadata_version.split(\".\")))\n\n        write_output(\"Name: %s\", dist.name)\n        write_output(\"Version: %s\", dist.version)\n        write_output(\"Summary: %s\", dist.summary)\n        write_output(\"Home-page: %s\", dist.homepage)\n        write_output(\"Author: %s\", dist.author)\n        write_output(\"Author-email: %s\", dist.author_email)\n        if metadata_version_tuple >= (2, 4) and dist.license_expression:\n            write_output(\"License-Expression: %s\", dist.license_expression)\n        else:\n            write_output(\"License: %s\", dist.license)\n        write_output(\"Location: %s\", dist.location)\n        if dist.editable_project_location is not None:\n            write_output(\n                \"Editable project location: %s\", dist.editable_project_location\n            )\n        write_output(\"Requires: %s\", \", \".join(dist.requires))\n        write_output(\"Required-by: %s\", \", \".join(dist.required_by))\n\n        if verbose:\n            write_output(\"Metadata-Version: %s\", dist.metadata_version)\n            write_output(\"Installer: %s\", dist.installer)\n            write_output(\"Classifiers:\")\n            for classifier in dist.classifiers:\n                write_output(\"  %s\", classifier)\n            write_output(\"Entry-points:\")\n            for entry in dist.entry_points:\n                write_output(\"  %s\", entry.strip())\n            write_output(\"Project-URLs:\")\n            for project_url in dist.project_urls:\n                write_output(\"  %s\", project_url)\n        if list_files:\n            write_output(\"Files:\")\n            if dist.files is None:\n                write_output(\"Cannot locate RECORD or installed-files.txt\")\n            else:\n                for line in dist.files:\n                    write_output(\"  %s\", line.strip())\n    return results_printed\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/commands/uninstall.py","size":3892,"sha1":"0e7b34f7cb0ffbb7cca522cfdb0b895115e11f41","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import logging\nfrom optparse import Values\nfrom typing import List\n\nfrom pip._vendor.packaging.utils import canonicalize_name\n\nfrom pip._internal.cli import cmdoptions\nfrom pip._internal.cli.base_command import Command\nfrom pip._internal.cli.index_command import SessionCommandMixin\nfrom pip._internal.cli.status_codes import SUCCESS\nfrom pip._internal.exceptions import InstallationError\nfrom pip._internal.req import parse_requirements\nfrom pip._internal.req.constructors import (\n    install_req_from_line,\n    install_req_from_parsed_requirement,\n)\nfrom pip._internal.utils.misc import (\n    check_externally_managed,\n    protect_pip_from_modification_on_windows,\n    warn_if_run_as_root,\n)\n\nlogger = logging.getLogger(__name__)\n\n\nclass UninstallCommand(Command, SessionCommandMixin):\n    \"\"\"\n    Uninstall packages.\n\n    pip is able to uninstall most installed packages. Known exceptions are:\n\n    - Pure distutils packages installed with ``python setup.py install``, which\n      leave behind no metadata to determine what files were installed.\n    - Script wrappers installed by ``python setup.py develop``.\n    \"\"\"\n\n    usage = \"\"\"\n      %prog [options] <package> ...\n      %prog [options] -r <requirements file> ...\"\"\"\n\n    def add_options(self) -> None:\n        self.cmd_opts.add_option(\n            \"-r\",\n            \"--requirement\",\n            dest=\"requirements\",\n            action=\"append\",\n            default=[],\n            metavar=\"file\",\n            help=(\n                \"Uninstall all the packages listed in the given requirements \"\n                \"file.  This option can be used multiple times.\"\n            ),\n        )\n        self.cmd_opts.add_option(\n            \"-y\",\n            \"--yes\",\n            dest=\"yes\",\n            action=\"store_true\",\n            help=\"Don't ask for confirmation of uninstall deletions.\",\n        )\n        self.cmd_opts.add_option(cmdoptions.root_user_action())\n        self.cmd_opts.add_option(cmdoptions.override_externally_managed())\n        self.parser.insert_option_group(0, self.cmd_opts)\n\n    def run(self, options: Values, args: List[str]) -> int:\n        session = self.get_default_session(options)\n\n        reqs_to_uninstall = {}\n        for name in args:\n            req = install_req_from_line(\n                name,\n                isolated=options.isolated_mode,\n            )\n            if req.name:\n                reqs_to_uninstall[canonicalize_name(req.name)] = req\n            else:\n                logger.warning(\n                    \"Invalid requirement: %r ignored -\"\n                    \" the uninstall command expects named\"\n                    \" requirements.\",\n                    name,\n                )\n        for filename in options.requirements:\n            for parsed_req in parse_requirements(\n                filename, options=options, session=session\n            ):\n                req = install_req_from_parsed_requirement(\n                    parsed_req, isolated=options.isolated_mode\n                )\n                if req.name:\n                    reqs_to_uninstall[canonicalize_name(req.name)] = req\n        if not reqs_to_uninstall:\n            raise InstallationError(\n                f\"You must give at least one requirement to {self.name} (see \"\n                f'\"pip help {self.name}\")'\n            )\n\n        if not options.override_externally_managed:\n            check_externally_managed()\n\n        protect_pip_from_modification_on_windows(\n            modifying_pip=\"pip\" in reqs_to_uninstall\n        )\n\n        for req in reqs_to_uninstall.values():\n            uninstall_pathset = req.uninstall(\n                auto_confirm=options.yes,\n                verbose=self.verbosity > 0,\n            )\n            if uninstall_pathset:\n                uninstall_pathset.commit()\n        if options.root_user_action == \"warn\":\n            warn_if_run_as_root()\n        return SUCCESS\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/commands/wheel.py","size":6414,"sha1":"a01b3310ac7ac0b8bdabf3a88a9ac8e455da0ee9","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import logging\nimport os\nimport shutil\nfrom optparse import Values\nfrom typing import List\n\nfrom pip._internal.cache import WheelCache\nfrom pip._internal.cli import cmdoptions\nfrom pip._internal.cli.req_command import RequirementCommand, with_cleanup\nfrom pip._internal.cli.status_codes import SUCCESS\nfrom pip._internal.exceptions import CommandError\nfrom pip._internal.operations.build.build_tracker import get_build_tracker\nfrom pip._internal.req.req_install import (\n    InstallRequirement,\n    check_legacy_setup_py_options,\n)\nfrom pip._internal.utils.misc import ensure_dir, normalize_path\nfrom pip._internal.utils.temp_dir import TempDirectory\nfrom pip._internal.wheel_builder import build, should_build_for_wheel_command\n\nlogger = logging.getLogger(__name__)\n\n\nclass WheelCommand(RequirementCommand):\n    \"\"\"\n    Build Wheel archives for your requirements and dependencies.\n\n    Wheel is a built-package format, and offers the advantage of not\n    recompiling your software during every install. For more details, see the\n    wheel docs: https://wheel.readthedocs.io/en/latest/\n\n    'pip wheel' uses the build system interface as described here:\n    https://pip.pypa.io/en/stable/reference/build-system/\n\n    \"\"\"\n\n    usage = \"\"\"\n      %prog [options] <requirement specifier> ...\n      %prog [options] -r <requirements file> ...\n      %prog [options] [-e] <vcs project url> ...\n      %prog [options] [-e] <local project path> ...\n      %prog [options] <archive url/path> ...\"\"\"\n\n    def add_options(self) -> None:\n        self.cmd_opts.add_option(\n            \"-w\",\n            \"--wheel-dir\",\n            dest=\"wheel_dir\",\n            metavar=\"dir\",\n            default=os.curdir,\n            help=(\n                \"Build wheels into <dir>, where the default is the \"\n                \"current working directory.\"\n            ),\n        )\n        self.cmd_opts.add_option(cmdoptions.no_binary())\n        self.cmd_opts.add_option(cmdoptions.only_binary())\n        self.cmd_opts.add_option(cmdoptions.prefer_binary())\n        self.cmd_opts.add_option(cmdoptions.no_build_isolation())\n        self.cmd_opts.add_option(cmdoptions.use_pep517())\n        self.cmd_opts.add_option(cmdoptions.no_use_pep517())\n        self.cmd_opts.add_option(cmdoptions.check_build_deps())\n        self.cmd_opts.add_option(cmdoptions.constraints())\n        self.cmd_opts.add_option(cmdoptions.editable())\n        self.cmd_opts.add_option(cmdoptions.requirements())\n        self.cmd_opts.add_option(cmdoptions.src())\n        self.cmd_opts.add_option(cmdoptions.ignore_requires_python())\n        self.cmd_opts.add_option(cmdoptions.no_deps())\n        self.cmd_opts.add_option(cmdoptions.progress_bar())\n\n        self.cmd_opts.add_option(\n            \"--no-verify\",\n            dest=\"no_verify\",\n            action=\"store_true\",\n            default=False,\n            help=\"Don't verify if built wheel is valid.\",\n        )\n\n        self.cmd_opts.add_option(cmdoptions.config_settings())\n        self.cmd_opts.add_option(cmdoptions.build_options())\n        self.cmd_opts.add_option(cmdoptions.global_options())\n\n        self.cmd_opts.add_option(\n            \"--pre\",\n            action=\"store_true\",\n            default=False,\n            help=(\n                \"Include pre-release and development versions. By default, \"\n                \"pip only finds stable versions.\"\n            ),\n        )\n\n        self.cmd_opts.add_option(cmdoptions.require_hashes())\n\n        index_opts = cmdoptions.make_option_group(\n            cmdoptions.index_group,\n            self.parser,\n        )\n\n        self.parser.insert_option_group(0, index_opts)\n        self.parser.insert_option_group(0, self.cmd_opts)\n\n    @with_cleanup\n    def run(self, options: Values, args: List[str]) -> int:\n        session = self.get_default_session(options)\n\n        finder = self._build_package_finder(options, session)\n\n        options.wheel_dir = normalize_path(options.wheel_dir)\n        ensure_dir(options.wheel_dir)\n\n        build_tracker = self.enter_context(get_build_tracker())\n\n        directory = TempDirectory(\n            delete=not options.no_clean,\n            kind=\"wheel\",\n            globally_managed=True,\n        )\n\n        reqs = self.get_requirements(args, options, finder, session)\n        check_legacy_setup_py_options(options, reqs)\n\n        wheel_cache = WheelCache(options.cache_dir)\n\n        preparer = self.make_requirement_preparer(\n            temp_build_dir=directory,\n            options=options,\n            build_tracker=build_tracker,\n            session=session,\n            finder=finder,\n            download_dir=options.wheel_dir,\n            use_user_site=False,\n            verbosity=self.verbosity,\n        )\n\n        resolver = self.make_resolver(\n            preparer=preparer,\n            finder=finder,\n            options=options,\n            wheel_cache=wheel_cache,\n            ignore_requires_python=options.ignore_requires_python,\n            use_pep517=options.use_pep517,\n        )\n\n        self.trace_basic_info(finder)\n\n        requirement_set = resolver.resolve(reqs, check_supported_wheels=True)\n\n        reqs_to_build: List[InstallRequirement] = []\n        for req in requirement_set.requirements.values():\n            if req.is_wheel:\n                preparer.save_linked_requirement(req)\n            elif should_build_for_wheel_command(req):\n                reqs_to_build.append(req)\n\n        preparer.prepare_linked_requirements_more(requirement_set.requirements.values())\n\n        # build wheels\n        build_successes, build_failures = build(\n            reqs_to_build,\n            wheel_cache=wheel_cache,\n            verify=(not options.no_verify),\n            build_options=options.build_options or [],\n            global_options=options.global_options or [],\n        )\n        for req in build_successes:\n            assert req.link and req.link.is_wheel\n            assert req.local_file_path\n            # copy from cache to target directory\n            try:\n                shutil.copy(req.local_file_path, options.wheel_dir)\n            except OSError as e:\n                logger.warning(\n                    \"Building wheel for %s failed: %s\",\n                    req.name,\n                    e,\n                )\n                build_failures.append(req)\n        if len(build_failures) != 0:\n            raise CommandError(\"Failed to build one or more wheels\")\n\n        return SUCCESS\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/configuration.py","size":14005,"sha1":"08dd42388222781a68388829a267379a48a0877f","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"Configuration management setup\n\nSome terminology:\n- name\n  As written in config files.\n- value\n  Value associated with a name\n- key\n  Name combined with it's section (section.name)\n- variant\n  A single word describing where the configuration key-value pair came from\n\"\"\"\n\nimport configparser\nimport locale\nimport os\nimport sys\nfrom typing import Any, Dict, Iterable, List, NewType, Optional, Tuple\n\nfrom pip._internal.exceptions import (\n    ConfigurationError,\n    ConfigurationFileCouldNotBeLoaded,\n)\nfrom pip._internal.utils import appdirs\nfrom pip._internal.utils.compat import WINDOWS\nfrom pip._internal.utils.logging import getLogger\nfrom pip._internal.utils.misc import ensure_dir, enum\n\nRawConfigParser = configparser.RawConfigParser  # Shorthand\nKind = NewType(\"Kind\", str)\n\nCONFIG_BASENAME = \"pip.ini\" if WINDOWS else \"pip.conf\"\nENV_NAMES_IGNORED = \"version\", \"help\"\n\n# The kinds of configurations there are.\nkinds = enum(\n    USER=\"user\",  # User Specific\n    GLOBAL=\"global\",  # System Wide\n    SITE=\"site\",  # [Virtual] Environment Specific\n    ENV=\"env\",  # from PIP_CONFIG_FILE\n    ENV_VAR=\"env-var\",  # from Environment Variables\n)\nOVERRIDE_ORDER = kinds.GLOBAL, kinds.USER, kinds.SITE, kinds.ENV, kinds.ENV_VAR\nVALID_LOAD_ONLY = kinds.USER, kinds.GLOBAL, kinds.SITE\n\nlogger = getLogger(__name__)\n\n\n# NOTE: Maybe use the optionx attribute to normalize keynames.\ndef _normalize_name(name: str) -> str:\n    \"\"\"Make a name consistent regardless of source (environment or file)\"\"\"\n    name = name.lower().replace(\"_\", \"-\")\n    if name.startswith(\"--\"):\n        name = name[2:]  # only prefer long opts\n    return name\n\n\ndef _disassemble_key(name: str) -> List[str]:\n    if \".\" not in name:\n        error_message = (\n            \"Key does not contain dot separated section and key. \"\n            f\"Perhaps you wanted to use 'global.{name}' instead?\"\n        )\n        raise ConfigurationError(error_message)\n    return name.split(\".\", 1)\n\n\ndef get_configuration_files() -> Dict[Kind, List[str]]:\n    global_config_files = [\n        os.path.join(path, CONFIG_BASENAME) for path in appdirs.site_config_dirs(\"pip\")\n    ]\n\n    site_config_file = os.path.join(sys.prefix, CONFIG_BASENAME)\n    legacy_config_file = os.path.join(\n        os.path.expanduser(\"~\"),\n        \"pip\" if WINDOWS else \".pip\",\n        CONFIG_BASENAME,\n    )\n    new_config_file = os.path.join(appdirs.user_config_dir(\"pip\"), CONFIG_BASENAME)\n    return {\n        kinds.GLOBAL: global_config_files,\n        kinds.SITE: [site_config_file],\n        kinds.USER: [legacy_config_file, new_config_file],\n    }\n\n\nclass Configuration:\n    \"\"\"Handles management of configuration.\n\n    Provides an interface to accessing and managing configuration files.\n\n    This class converts provides an API that takes \"section.key-name\" style\n    keys and stores the value associated with it as \"key-name\" under the\n    section \"section\".\n\n    This allows for a clean interface wherein the both the section and the\n    key-name are preserved in an easy to manage form in the configuration files\n    and the data stored is also nice.\n    \"\"\"\n\n    def __init__(self, isolated: bool, load_only: Optional[Kind] = None) -> None:\n        super().__init__()\n\n        if load_only is not None and load_only not in VALID_LOAD_ONLY:\n            raise ConfigurationError(\n                \"Got invalid value for load_only - should be one of {}\".format(\n                    \", \".join(map(repr, VALID_LOAD_ONLY))\n                )\n            )\n        self.isolated = isolated\n        self.load_only = load_only\n\n        # Because we keep track of where we got the data from\n        self._parsers: Dict[Kind, List[Tuple[str, RawConfigParser]]] = {\n            variant: [] for variant in OVERRIDE_ORDER\n        }\n        self._config: Dict[Kind, Dict[str, Any]] = {\n            variant: {} for variant in OVERRIDE_ORDER\n        }\n        self._modified_parsers: List[Tuple[str, RawConfigParser]] = []\n\n    def load(self) -> None:\n        \"\"\"Loads configuration from configuration files and environment\"\"\"\n        self._load_config_files()\n        if not self.isolated:\n            self._load_environment_vars()\n\n    def get_file_to_edit(self) -> Optional[str]:\n        \"\"\"Returns the file with highest priority in configuration\"\"\"\n        assert self.load_only is not None, \"Need to be specified a file to be editing\"\n\n        try:\n            return self._get_parser_to_modify()[0]\n        except IndexError:\n            return None\n\n    def items(self) -> Iterable[Tuple[str, Any]]:\n        \"\"\"Returns key-value pairs like dict.items() representing the loaded\n        configuration\n        \"\"\"\n        return self._dictionary.items()\n\n    def get_value(self, key: str) -> Any:\n        \"\"\"Get a value from the configuration.\"\"\"\n        orig_key = key\n        key = _normalize_name(key)\n        try:\n            return self._dictionary[key]\n        except KeyError:\n            # disassembling triggers a more useful error message than simply\n            # \"No such key\" in the case that the key isn't in the form command.option\n            _disassemble_key(key)\n            raise ConfigurationError(f\"No such key - {orig_key}\")\n\n    def set_value(self, key: str, value: Any) -> None:\n        \"\"\"Modify a value in the configuration.\"\"\"\n        key = _normalize_name(key)\n        self._ensure_have_load_only()\n\n        assert self.load_only\n        fname, parser = self._get_parser_to_modify()\n\n        if parser is not None:\n            section, name = _disassemble_key(key)\n\n            # Modify the parser and the configuration\n            if not parser.has_section(section):\n                parser.add_section(section)\n            parser.set(section, name, value)\n\n        self._config[self.load_only][key] = value\n        self._mark_as_modified(fname, parser)\n\n    def unset_value(self, key: str) -> None:\n        \"\"\"Unset a value in the configuration.\"\"\"\n        orig_key = key\n        key = _normalize_name(key)\n        self._ensure_have_load_only()\n\n        assert self.load_only\n        if key not in self._config[self.load_only]:\n            raise ConfigurationError(f\"No such key - {orig_key}\")\n\n        fname, parser = self._get_parser_to_modify()\n\n        if parser is not None:\n            section, name = _disassemble_key(key)\n            if not (\n                parser.has_section(section) and parser.remove_option(section, name)\n            ):\n                # The option was not removed.\n                raise ConfigurationError(\n                    \"Fatal Internal error [id=1]. Please report as a bug.\"\n                )\n\n            # The section may be empty after the option was removed.\n            if not parser.items(section):\n                parser.remove_section(section)\n            self._mark_as_modified(fname, parser)\n\n        del self._config[self.load_only][key]\n\n    def save(self) -> None:\n        \"\"\"Save the current in-memory state.\"\"\"\n        self._ensure_have_load_only()\n\n        for fname, parser in self._modified_parsers:\n            logger.info(\"Writing to %s\", fname)\n\n            # Ensure directory exists.\n            ensure_dir(os.path.dirname(fname))\n\n            # Ensure directory's permission(need to be writeable)\n            try:\n                with open(fname, \"w\") as f:\n                    parser.write(f)\n            except OSError as error:\n                raise ConfigurationError(\n                    f\"An error occurred while writing to the configuration file \"\n                    f\"{fname}: {error}\"\n                )\n\n    #\n    # Private routines\n    #\n\n    def _ensure_have_load_only(self) -> None:\n        if self.load_only is None:\n            raise ConfigurationError(\"Needed a specific file to be modifying.\")\n        logger.debug(\"Will be working with %s variant only\", self.load_only)\n\n    @property\n    def _dictionary(self) -> Dict[str, Any]:\n        \"\"\"A dictionary representing the loaded configuration.\"\"\"\n        # NOTE: Dictionaries are not populated if not loaded. So, conditionals\n        #       are not needed here.\n        retval = {}\n\n        for variant in OVERRIDE_ORDER:\n            retval.update(self._config[variant])\n\n        return retval\n\n    def _load_config_files(self) -> None:\n        \"\"\"Loads configuration from configuration files\"\"\"\n        config_files = dict(self.iter_config_files())\n        if config_files[kinds.ENV][0:1] == [os.devnull]:\n            logger.debug(\n                \"Skipping loading configuration files due to \"\n                \"environment's PIP_CONFIG_FILE being os.devnull\"\n            )\n            return\n\n        for variant, files in config_files.items():\n            for fname in files:\n                # If there's specific variant set in `load_only`, load only\n                # that variant, not the others.\n                if self.load_only is not None and variant != self.load_only:\n                    logger.debug(\"Skipping file '%s' (variant: %s)\", fname, variant)\n                    continue\n\n                parser = self._load_file(variant, fname)\n\n                # Keeping track of the parsers used\n                self._parsers[variant].append((fname, parser))\n\n    def _load_file(self, variant: Kind, fname: str) -> RawConfigParser:\n        logger.verbose(\"For variant '%s', will try loading '%s'\", variant, fname)\n        parser = self._construct_parser(fname)\n\n        for section in parser.sections():\n            items = parser.items(section)\n            self._config[variant].update(self._normalized_keys(section, items))\n\n        return parser\n\n    def _construct_parser(self, fname: str) -> RawConfigParser:\n        parser = configparser.RawConfigParser()\n        # If there is no such file, don't bother reading it but create the\n        # parser anyway, to hold the data.\n        # Doing this is useful when modifying and saving files, where we don't\n        # need to construct a parser.\n        if os.path.exists(fname):\n            locale_encoding = locale.getpreferredencoding(False)\n            try:\n                parser.read(fname, encoding=locale_encoding)\n            except UnicodeDecodeError:\n                # See https://github.com/pypa/pip/issues/4963\n                raise ConfigurationFileCouldNotBeLoaded(\n                    reason=f\"contains invalid {locale_encoding} characters\",\n                    fname=fname,\n                )\n            except configparser.Error as error:\n                # See https://github.com/pypa/pip/issues/4893\n                raise ConfigurationFileCouldNotBeLoaded(error=error)\n        return parser\n\n    def _load_environment_vars(self) -> None:\n        \"\"\"Loads configuration from environment variables\"\"\"\n        self._config[kinds.ENV_VAR].update(\n            self._normalized_keys(\":env:\", self.get_environ_vars())\n        )\n\n    def _normalized_keys(\n        self, section: str, items: Iterable[Tuple[str, Any]]\n    ) -> Dict[str, Any]:\n        \"\"\"Normalizes items to construct a dictionary with normalized keys.\n\n        This routine is where the names become keys and are made the same\n        regardless of source - configuration files or environment.\n        \"\"\"\n        normalized = {}\n        for name, val in items:\n            key = section + \".\" + _normalize_name(name)\n            normalized[key] = val\n        return normalized\n\n    def get_environ_vars(self) -> Iterable[Tuple[str, str]]:\n        \"\"\"Returns a generator with all environmental vars with prefix PIP_\"\"\"\n        for key, val in os.environ.items():\n            if key.startswith(\"PIP_\"):\n                name = key[4:].lower()\n                if name not in ENV_NAMES_IGNORED:\n                    yield name, val\n\n    # XXX: This is patched in the tests.\n    def iter_config_files(self) -> Iterable[Tuple[Kind, List[str]]]:\n        \"\"\"Yields variant and configuration files associated with it.\n\n        This should be treated like items of a dictionary. The order\n        here doesn't affect what gets overridden. That is controlled\n        by OVERRIDE_ORDER. However this does control the order they are\n        displayed to the user. It's probably most ergonomic to display\n        things in the same order as OVERRIDE_ORDER\n        \"\"\"\n        # SMELL: Move the conditions out of this function\n\n        env_config_file = os.environ.get(\"PIP_CONFIG_FILE\", None)\n        config_files = get_configuration_files()\n\n        yield kinds.GLOBAL, config_files[kinds.GLOBAL]\n\n        # per-user config is not loaded when env_config_file exists\n        should_load_user_config = not self.isolated and not (\n            env_config_file and os.path.exists(env_config_file)\n        )\n        if should_load_user_config:\n            # The legacy config file is overridden by the new config file\n            yield kinds.USER, config_files[kinds.USER]\n\n        # virtualenv config\n        yield kinds.SITE, config_files[kinds.SITE]\n\n        if env_config_file is not None:\n            yield kinds.ENV, [env_config_file]\n        else:\n            yield kinds.ENV, []\n\n    def get_values_in_config(self, variant: Kind) -> Dict[str, Any]:\n        \"\"\"Get values present in a config file\"\"\"\n        return self._config[variant]\n\n    def _get_parser_to_modify(self) -> Tuple[str, RawConfigParser]:\n        # Determine which parser to modify\n        assert self.load_only\n        parsers = self._parsers[self.load_only]\n        if not parsers:\n            # This should not happen if everything works correctly.\n            raise ConfigurationError(\n                \"Fatal Internal error [id=2]. Please report as a bug.\"\n            )\n\n        # Use the highest priority parser.\n        return parsers[-1]\n\n    # XXX: This is patched in the tests.\n    def _mark_as_modified(self, fname: str, parser: RawConfigParser) -> None:\n        file_parser_tuple = (fname, parser)\n        if file_parser_tuple not in self._modified_parsers:\n            self._modified_parsers.append(file_parser_tuple)\n\n    def __repr__(self) -> str:\n        return f\"{self.__class__.__name__}({self._dictionary!r})\"\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/distributions/__init__.py","size":858,"sha1":"cda4ca594b1ab236cb2a17fde09a59d46410ca30","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"from pip._internal.distributions.base import AbstractDistribution\nfrom pip._internal.distributions.sdist import SourceDistribution\nfrom pip._internal.distributions.wheel import WheelDistribution\nfrom pip._internal.req.req_install import InstallRequirement\n\n\ndef make_distribution_for_install_requirement(\n    install_req: InstallRequirement,\n) -> AbstractDistribution:\n    \"\"\"Returns a Distribution for the given InstallRequirement\"\"\"\n    # Editable requirements will always be source distributions. They use the\n    # legacy logic until we create a modern standard for them.\n    if install_req.editable:\n        return SourceDistribution(install_req)\n\n    # If it's a wheel, it's a WheelDistribution\n    if install_req.is_wheel:\n        return WheelDistribution(install_req)\n\n    # Otherwise, a SourceDistribution\n    return SourceDistribution(install_req)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/distributions/base.py","size":1783,"sha1":"8d155b14c9935281f5f4135116043db2bc91385e","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import abc\nfrom typing import TYPE_CHECKING, Optional\n\nfrom pip._internal.metadata.base import BaseDistribution\nfrom pip._internal.req import InstallRequirement\n\nif TYPE_CHECKING:\n    from pip._internal.index.package_finder import PackageFinder\n\n\nclass AbstractDistribution(metaclass=abc.ABCMeta):\n    \"\"\"A base class for handling installable artifacts.\n\n    The requirements for anything installable are as follows:\n\n     - we must be able to determine the requirement name\n       (or we can't correctly handle the non-upgrade case).\n\n     - for packages with setup requirements, we must also be able\n       to determine their requirements without installing additional\n       packages (for the same reason as run-time dependencies)\n\n     - we must be able to create a Distribution object exposing the\n       above metadata.\n\n     - if we need to do work in the build tracker, we must be able to generate a unique\n       string to identify the requirement in the build tracker.\n    \"\"\"\n\n    def __init__(self, req: InstallRequirement) -> None:\n        super().__init__()\n        self.req = req\n\n    @abc.abstractproperty\n    def build_tracker_id(self) -> Optional[str]:\n        \"\"\"A string that uniquely identifies this requirement to the build tracker.\n\n        If None, then this dist has no work to do in the build tracker, and\n        ``.prepare_distribution_metadata()`` will not be called.\"\"\"\n        raise NotImplementedError()\n\n    @abc.abstractmethod\n    def get_metadata_distribution(self) -> BaseDistribution:\n        raise NotImplementedError()\n\n    @abc.abstractmethod\n    def prepare_distribution_metadata(\n        self,\n        finder: \"PackageFinder\",\n        build_isolation: bool,\n        check_build_deps: bool,\n    ) -> None:\n        raise NotImplementedError()\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/distributions/installed.py","size":842,"sha1":"d324a8c68f8ae49cfd4fdfad1b873d947f9feac3","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"from typing import Optional\n\nfrom pip._internal.distributions.base import AbstractDistribution\nfrom pip._internal.index.package_finder import PackageFinder\nfrom pip._internal.metadata import BaseDistribution\n\n\nclass InstalledDistribution(AbstractDistribution):\n    \"\"\"Represents an installed package.\n\n    This does not need any preparation as the required information has already\n    been computed.\n    \"\"\"\n\n    @property\n    def build_tracker_id(self) -> Optional[str]:\n        return None\n\n    def get_metadata_distribution(self) -> BaseDistribution:\n        assert self.req.satisfied_by is not None, \"not actually installed\"\n        return self.req.satisfied_by\n\n    def prepare_distribution_metadata(\n        self,\n        finder: PackageFinder,\n        build_isolation: bool,\n        check_build_deps: bool,\n    ) -> None:\n        pass\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/distributions/sdist.py","size":6751,"sha1":"77e02a0ac12b629c8bf3a45e863823d5ad6dc0ef","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import logging\nfrom typing import TYPE_CHECKING, Iterable, Optional, Set, Tuple\n\nfrom pip._internal.build_env import BuildEnvironment\nfrom pip._internal.distributions.base import AbstractDistribution\nfrom pip._internal.exceptions import InstallationError\nfrom pip._internal.metadata import BaseDistribution\nfrom pip._internal.utils.subprocess import runner_with_spinner_message\n\nif TYPE_CHECKING:\n    from pip._internal.index.package_finder import PackageFinder\n\nlogger = logging.getLogger(__name__)\n\n\nclass SourceDistribution(AbstractDistribution):\n    \"\"\"Represents a source distribution.\n\n    The preparation step for these needs metadata for the packages to be\n    generated, either using PEP 517 or using the legacy `setup.py egg_info`.\n    \"\"\"\n\n    @property\n    def build_tracker_id(self) -> Optional[str]:\n        \"\"\"Identify this requirement uniquely by its link.\"\"\"\n        assert self.req.link\n        return self.req.link.url_without_fragment\n\n    def get_metadata_distribution(self) -> BaseDistribution:\n        return self.req.get_dist()\n\n    def prepare_distribution_metadata(\n        self,\n        finder: \"PackageFinder\",\n        build_isolation: bool,\n        check_build_deps: bool,\n    ) -> None:\n        # Load pyproject.toml, to determine whether PEP 517 is to be used\n        self.req.load_pyproject_toml()\n\n        # Set up the build isolation, if this requirement should be isolated\n        should_isolate = self.req.use_pep517 and build_isolation\n        if should_isolate:\n            # Setup an isolated environment and install the build backend static\n            # requirements in it.\n            self._prepare_build_backend(finder)\n            # Check that if the requirement is editable, it either supports PEP 660 or\n            # has a setup.py or a setup.cfg. This cannot be done earlier because we need\n            # to setup the build backend to verify it supports build_editable, nor can\n            # it be done later, because we want to avoid installing build requirements\n            # needlessly. Doing it here also works around setuptools generating\n            # UNKNOWN.egg-info when running get_requires_for_build_wheel on a directory\n            # without setup.py nor setup.cfg.\n            self.req.isolated_editable_sanity_check()\n            # Install the dynamic build requirements.\n            self._install_build_reqs(finder)\n        # Check if the current environment provides build dependencies\n        should_check_deps = self.req.use_pep517 and check_build_deps\n        if should_check_deps:\n            pyproject_requires = self.req.pyproject_requires\n            assert pyproject_requires is not None\n            conflicting, missing = self.req.build_env.check_requirements(\n                pyproject_requires\n            )\n            if conflicting:\n                self._raise_conflicts(\"the backend dependencies\", conflicting)\n            if missing:\n                self._raise_missing_reqs(missing)\n        self.req.prepare_metadata()\n\n    def _prepare_build_backend(self, finder: \"PackageFinder\") -> None:\n        # Isolate in a BuildEnvironment and install the build-time\n        # requirements.\n        pyproject_requires = self.req.pyproject_requires\n        assert pyproject_requires is not None\n\n        self.req.build_env = BuildEnvironment()\n        self.req.build_env.install_requirements(\n            finder, pyproject_requires, \"overlay\", kind=\"build dependencies\"\n        )\n        conflicting, missing = self.req.build_env.check_requirements(\n            self.req.requirements_to_check\n        )\n        if conflicting:\n            self._raise_conflicts(\"PEP 517/518 supported requirements\", conflicting)\n        if missing:\n            logger.warning(\n                \"Missing build requirements in pyproject.toml for %s.\",\n                self.req,\n            )\n            logger.warning(\n                \"The project does not specify a build backend, and \"\n                \"pip cannot fall back to setuptools without %s.\",\n                \" and \".join(map(repr, sorted(missing))),\n            )\n\n    def _get_build_requires_wheel(self) -> Iterable[str]:\n        with self.req.build_env:\n            runner = runner_with_spinner_message(\"Getting requirements to build wheel\")\n            backend = self.req.pep517_backend\n            assert backend is not None\n            with backend.subprocess_runner(runner):\n                return backend.get_requires_for_build_wheel()\n\n    def _get_build_requires_editable(self) -> Iterable[str]:\n        with self.req.build_env:\n            runner = runner_with_spinner_message(\n                \"Getting requirements to build editable\"\n            )\n            backend = self.req.pep517_backend\n            assert backend is not None\n            with backend.subprocess_runner(runner):\n                return backend.get_requires_for_build_editable()\n\n    def _install_build_reqs(self, finder: \"PackageFinder\") -> None:\n        # Install any extra build dependencies that the backend requests.\n        # This must be done in a second pass, as the pyproject.toml\n        # dependencies must be installed before we can call the backend.\n        if (\n            self.req.editable\n            and self.req.permit_editable_wheels\n            and self.req.supports_pyproject_editable\n        ):\n            build_reqs = self._get_build_requires_editable()\n        else:\n            build_reqs = self._get_build_requires_wheel()\n        conflicting, missing = self.req.build_env.check_requirements(build_reqs)\n        if conflicting:\n            self._raise_conflicts(\"the backend dependencies\", conflicting)\n        self.req.build_env.install_requirements(\n            finder, missing, \"normal\", kind=\"backend dependencies\"\n        )\n\n    def _raise_conflicts(\n        self, conflicting_with: str, conflicting_reqs: Set[Tuple[str, str]]\n    ) -> None:\n        format_string = (\n            \"Some build dependencies for {requirement} \"\n            \"conflict with {conflicting_with}: {description}.\"\n        )\n        error_message = format_string.format(\n            requirement=self.req,\n            conflicting_with=conflicting_with,\n            description=\", \".join(\n                f\"{installed} is incompatible with {wanted}\"\n                for installed, wanted in sorted(conflicting_reqs)\n            ),\n        )\n        raise InstallationError(error_message)\n\n    def _raise_missing_reqs(self, missing: Set[str]) -> None:\n        format_string = (\n            \"Some build dependencies for {requirement} are missing: {missing}.\"\n        )\n        error_message = format_string.format(\n            requirement=self.req, missing=\", \".join(map(repr, sorted(missing)))\n        )\n        raise InstallationError(error_message)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/distributions/wheel.py","size":1317,"sha1":"4b434ee9ebae5ff4a8f2c9941b9f877fcb284ac6","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"from typing import TYPE_CHECKING, Optional\n\nfrom pip._vendor.packaging.utils import canonicalize_name\n\nfrom pip._internal.distributions.base import AbstractDistribution\nfrom pip._internal.metadata import (\n    BaseDistribution,\n    FilesystemWheel,\n    get_wheel_distribution,\n)\n\nif TYPE_CHECKING:\n    from pip._internal.index.package_finder import PackageFinder\n\n\nclass WheelDistribution(AbstractDistribution):\n    \"\"\"Represents a wheel distribution.\n\n    This does not need any preparation as wheels can be directly unpacked.\n    \"\"\"\n\n    @property\n    def build_tracker_id(self) -> Optional[str]:\n        return None\n\n    def get_metadata_distribution(self) -> BaseDistribution:\n        \"\"\"Loads the metadata from the wheel file into memory and returns a\n        Distribution that uses it, not relying on the wheel file or\n        requirement.\n        \"\"\"\n        assert self.req.local_file_path, \"Set as part of preparation during download\"\n        assert self.req.name, \"Wheels are never unnamed\"\n        wheel = FilesystemWheel(self.req.local_file_path)\n        return get_wheel_distribution(wheel, canonicalize_name(self.req.name))\n\n    def prepare_distribution_metadata(\n        self,\n        finder: \"PackageFinder\",\n        build_isolation: bool,\n        check_build_deps: bool,\n    ) -> None:\n        pass\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/exceptions.py","size":26481,"sha1":"874ca7011f6b28fd16cd248da971a7aeea1219ba","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"Exceptions used throughout package.\n\nThis module MUST NOT try to import from anything within `pip._internal` to\noperate. This is expected to be importable from any/all files within the\nsubpackage and, thus, should not depend on them.\n\"\"\"\n\nimport configparser\nimport contextlib\nimport locale\nimport logging\nimport pathlib\nimport re\nimport sys\nfrom itertools import chain, groupby, repeat\nfrom typing import TYPE_CHECKING, Dict, Iterator, List, Literal, Optional, Union\n\nfrom pip._vendor.packaging.requirements import InvalidRequirement\nfrom pip._vendor.packaging.version import InvalidVersion\nfrom pip._vendor.rich.console import Console, ConsoleOptions, RenderResult\nfrom pip._vendor.rich.markup import escape\nfrom pip._vendor.rich.text import Text\n\nif TYPE_CHECKING:\n    from hashlib import _Hash\n\n    from pip._vendor.requests.models import Request, Response\n\n    from pip._internal.metadata import BaseDistribution\n    from pip._internal.req.req_install import InstallRequirement\n\nlogger = logging.getLogger(__name__)\n\n\n#\n# Scaffolding\n#\ndef _is_kebab_case(s: str) -> bool:\n    return re.match(r\"^[a-z]+(-[a-z]+)*$\", s) is not None\n\n\ndef _prefix_with_indent(\n    s: Union[Text, str],\n    console: Console,\n    *,\n    prefix: str,\n    indent: str,\n) -> Text:\n    if isinstance(s, Text):\n        text = s\n    else:\n        text = console.render_str(s)\n\n    return console.render_str(prefix, overflow=\"ignore\") + console.render_str(\n        f\"\\n{indent}\", overflow=\"ignore\"\n    ).join(text.split(allow_blank=True))\n\n\nclass PipError(Exception):\n    \"\"\"The base pip error.\"\"\"\n\n\nclass DiagnosticPipError(PipError):\n    \"\"\"An error, that presents diagnostic information to the user.\n\n    This contains a bunch of logic, to enable pretty presentation of our error\n    messages. Each error gets a unique reference. Each error can also include\n    additional context, a hint and/or a note -- which are presented with the\n    main error message in a consistent style.\n\n    This is adapted from the error output styling in `sphinx-theme-builder`.\n    \"\"\"\n\n    reference: str\n\n    def __init__(\n        self,\n        *,\n        kind: 'Literal[\"error\", \"warning\"]' = \"error\",\n        reference: Optional[str] = None,\n        message: Union[str, Text],\n        context: Optional[Union[str, Text]],\n        hint_stmt: Optional[Union[str, Text]],\n        note_stmt: Optional[Union[str, Text]] = None,\n        link: Optional[str] = None,\n    ) -> None:\n        # Ensure a proper reference is provided.\n        if reference is None:\n            assert hasattr(self, \"reference\"), \"error reference not provided!\"\n            reference = self.reference\n        assert _is_kebab_case(reference), \"error reference must be kebab-case!\"\n\n        self.kind = kind\n        self.reference = reference\n\n        self.message = message\n        self.context = context\n\n        self.note_stmt = note_stmt\n        self.hint_stmt = hint_stmt\n\n        self.link = link\n\n        super().__init__(f\"<{self.__class__.__name__}: {self.reference}>\")\n\n    def __repr__(self) -> str:\n        return (\n            f\"<{self.__class__.__name__}(\"\n            f\"reference={self.reference!r}, \"\n            f\"message={self.message!r}, \"\n            f\"context={self.context!r}, \"\n            f\"note_stmt={self.note_stmt!r}, \"\n            f\"hint_stmt={self.hint_stmt!r}\"\n            \")>\"\n        )\n\n    def __rich_console__(\n        self,\n        console: Console,\n        options: ConsoleOptions,\n    ) -> RenderResult:\n        colour = \"red\" if self.kind == \"error\" else \"yellow\"\n\n        yield f\"[{colour} bold]{self.kind}[/]: [bold]{self.reference}[/]\"\n        yield \"\"\n\n        if not options.ascii_only:\n            # Present the main message, with relevant context indented.\n            if self.context is not None:\n                yield _prefix_with_indent(\n                    self.message,\n                    console,\n                    prefix=f\"[{colour}]×[/] \",\n                    indent=f\"[{colour}]│[/] \",\n                )\n                yield _prefix_with_indent(\n                    self.context,\n                    console,\n                    prefix=f\"[{colour}]╰─>[/] \",\n                    indent=f\"[{colour}]   [/] \",\n                )\n            else:\n                yield _prefix_with_indent(\n                    self.message,\n                    console,\n                    prefix=\"[red]×[/] \",\n                    indent=\"  \",\n                )\n        else:\n            yield self.message\n            if self.context is not None:\n                yield \"\"\n                yield self.context\n\n        if self.note_stmt is not None or self.hint_stmt is not None:\n            yield \"\"\n\n        if self.note_stmt is not None:\n            yield _prefix_with_indent(\n                self.note_stmt,\n                console,\n                prefix=\"[magenta bold]note[/]: \",\n                indent=\"      \",\n            )\n        if self.hint_stmt is not None:\n            yield _prefix_with_indent(\n                self.hint_stmt,\n                console,\n                prefix=\"[cyan bold]hint[/]: \",\n                indent=\"      \",\n            )\n\n        if self.link is not None:\n            yield \"\"\n            yield f\"Link: {self.link}\"\n\n\n#\n# Actual Errors\n#\nclass ConfigurationError(PipError):\n    \"\"\"General exception in configuration\"\"\"\n\n\nclass InstallationError(PipError):\n    \"\"\"General exception during installation\"\"\"\n\n\nclass MissingPyProjectBuildRequires(DiagnosticPipError):\n    \"\"\"Raised when pyproject.toml has `build-system`, but no `build-system.requires`.\"\"\"\n\n    reference = \"missing-pyproject-build-system-requires\"\n\n    def __init__(self, *, package: str) -> None:\n        super().__init__(\n            message=f\"Can not process {escape(package)}\",\n            context=Text(\n                \"This package has an invalid pyproject.toml file.\\n\"\n                \"The [build-system] table is missing the mandatory `requires` key.\"\n            ),\n            note_stmt=\"This is an issue with the package mentioned above, not pip.\",\n            hint_stmt=Text(\"See PEP 518 for the detailed specification.\"),\n        )\n\n\nclass InvalidPyProjectBuildRequires(DiagnosticPipError):\n    \"\"\"Raised when pyproject.toml an invalid `build-system.requires`.\"\"\"\n\n    reference = \"invalid-pyproject-build-system-requires\"\n\n    def __init__(self, *, package: str, reason: str) -> None:\n        super().__init__(\n            message=f\"Can not process {escape(package)}\",\n            context=Text(\n                \"This package has an invalid `build-system.requires` key in \"\n                f\"pyproject.toml.\\n{reason}\"\n            ),\n            note_stmt=\"This is an issue with the package mentioned above, not pip.\",\n            hint_stmt=Text(\"See PEP 518 for the detailed specification.\"),\n        )\n\n\nclass NoneMetadataError(PipError):\n    \"\"\"Raised when accessing a Distribution's \"METADATA\" or \"PKG-INFO\".\n\n    This signifies an inconsistency, when the Distribution claims to have\n    the metadata file (if not, raise ``FileNotFoundError`` instead), but is\n    not actually able to produce its content. This may be due to permission\n    errors.\n    \"\"\"\n\n    def __init__(\n        self,\n        dist: \"BaseDistribution\",\n        metadata_name: str,\n    ) -> None:\n        \"\"\"\n        :param dist: A Distribution object.\n        :param metadata_name: The name of the metadata being accessed\n            (can be \"METADATA\" or \"PKG-INFO\").\n        \"\"\"\n        self.dist = dist\n        self.metadata_name = metadata_name\n\n    def __str__(self) -> str:\n        # Use `dist` in the error message because its stringification\n        # includes more information, like the version and location.\n        return f\"None {self.metadata_name} metadata found for distribution: {self.dist}\"\n\n\nclass UserInstallationInvalid(InstallationError):\n    \"\"\"A --user install is requested on an environment without user site.\"\"\"\n\n    def __str__(self) -> str:\n        return \"User base directory is not specified\"\n\n\nclass InvalidSchemeCombination(InstallationError):\n    def __str__(self) -> str:\n        before = \", \".join(str(a) for a in self.args[:-1])\n        return f\"Cannot set {before} and {self.args[-1]} together\"\n\n\nclass DistributionNotFound(InstallationError):\n    \"\"\"Raised when a distribution cannot be found to satisfy a requirement\"\"\"\n\n\nclass RequirementsFileParseError(InstallationError):\n    \"\"\"Raised when a general error occurs parsing a requirements file line.\"\"\"\n\n\nclass BestVersionAlreadyInstalled(PipError):\n    \"\"\"Raised when the most up-to-date version of a package is already\n    installed.\"\"\"\n\n\nclass BadCommand(PipError):\n    \"\"\"Raised when virtualenv or a command is not found\"\"\"\n\n\nclass CommandError(PipError):\n    \"\"\"Raised when there is an error in command-line arguments\"\"\"\n\n\nclass PreviousBuildDirError(PipError):\n    \"\"\"Raised when there's a previous conflicting build directory\"\"\"\n\n\nclass NetworkConnectionError(PipError):\n    \"\"\"HTTP connection error\"\"\"\n\n    def __init__(\n        self,\n        error_msg: str,\n        response: Optional[\"Response\"] = None,\n        request: Optional[\"Request\"] = None,\n    ) -> None:\n        \"\"\"\n        Initialize NetworkConnectionError with  `request` and `response`\n        objects.\n        \"\"\"\n        self.response = response\n        self.request = request\n        self.error_msg = error_msg\n        if (\n            self.response is not None\n            and not self.request\n            and hasattr(response, \"request\")\n        ):\n            self.request = self.response.request\n        super().__init__(error_msg, response, request)\n\n    def __str__(self) -> str:\n        return str(self.error_msg)\n\n\nclass InvalidWheelFilename(InstallationError):\n    \"\"\"Invalid wheel filename.\"\"\"\n\n\nclass UnsupportedWheel(InstallationError):\n    \"\"\"Unsupported wheel.\"\"\"\n\n\nclass InvalidWheel(InstallationError):\n    \"\"\"Invalid (e.g. corrupt) wheel.\"\"\"\n\n    def __init__(self, location: str, name: str):\n        self.location = location\n        self.name = name\n\n    def __str__(self) -> str:\n        return f\"Wheel '{self.name}' located at {self.location} is invalid.\"\n\n\nclass MetadataInconsistent(InstallationError):\n    \"\"\"Built metadata contains inconsistent information.\n\n    This is raised when the metadata contains values (e.g. name and version)\n    that do not match the information previously obtained from sdist filename,\n    user-supplied ``#egg=`` value, or an install requirement name.\n    \"\"\"\n\n    def __init__(\n        self, ireq: \"InstallRequirement\", field: str, f_val: str, m_val: str\n    ) -> None:\n        self.ireq = ireq\n        self.field = field\n        self.f_val = f_val\n        self.m_val = m_val\n\n    def __str__(self) -> str:\n        return (\n            f\"Requested {self.ireq} has inconsistent {self.field}: \"\n            f\"expected {self.f_val!r}, but metadata has {self.m_val!r}\"\n        )\n\n\nclass MetadataInvalid(InstallationError):\n    \"\"\"Metadata is invalid.\"\"\"\n\n    def __init__(self, ireq: \"InstallRequirement\", error: str) -> None:\n        self.ireq = ireq\n        self.error = error\n\n    def __str__(self) -> str:\n        return f\"Requested {self.ireq} has invalid metadata: {self.error}\"\n\n\nclass InstallationSubprocessError(DiagnosticPipError, InstallationError):\n    \"\"\"A subprocess call failed.\"\"\"\n\n    reference = \"subprocess-exited-with-error\"\n\n    def __init__(\n        self,\n        *,\n        command_description: str,\n        exit_code: int,\n        output_lines: Optional[List[str]],\n    ) -> None:\n        if output_lines is None:\n            output_prompt = Text(\"See above for output.\")\n        else:\n            output_prompt = (\n                Text.from_markup(f\"[red][{len(output_lines)} lines of output][/]\\n\")\n                + Text(\"\".join(output_lines))\n                + Text.from_markup(R\"[red]\\[end of output][/]\")\n            )\n\n        super().__init__(\n            message=(\n                f\"[green]{escape(command_description)}[/] did not run successfully.\\n\"\n                f\"exit code: {exit_code}\"\n            ),\n            context=output_prompt,\n            hint_stmt=None,\n            note_stmt=(\n                \"This error originates from a subprocess, and is likely not a \"\n                \"problem with pip.\"\n            ),\n        )\n\n        self.command_description = command_description\n        self.exit_code = exit_code\n\n    def __str__(self) -> str:\n        return f\"{self.command_description} exited with {self.exit_code}\"\n\n\nclass MetadataGenerationFailed(InstallationSubprocessError, InstallationError):\n    reference = \"metadata-generation-failed\"\n\n    def __init__(\n        self,\n        *,\n        package_details: str,\n    ) -> None:\n        super(InstallationSubprocessError, self).__init__(\n            message=\"Encountered error while generating package metadata.\",\n            context=escape(package_details),\n            hint_stmt=\"See above for details.\",\n            note_stmt=\"This is an issue with the package mentioned above, not pip.\",\n        )\n\n    def __str__(self) -> str:\n        return \"metadata generation failed\"\n\n\nclass HashErrors(InstallationError):\n    \"\"\"Multiple HashError instances rolled into one for reporting\"\"\"\n\n    def __init__(self) -> None:\n        self.errors: List[HashError] = []\n\n    def append(self, error: \"HashError\") -> None:\n        self.errors.append(error)\n\n    def __str__(self) -> str:\n        lines = []\n        self.errors.sort(key=lambda e: e.order)\n        for cls, errors_of_cls in groupby(self.errors, lambda e: e.__class__):\n            lines.append(cls.head)\n            lines.extend(e.body() for e in errors_of_cls)\n        if lines:\n            return \"\\n\".join(lines)\n        return \"\"\n\n    def __bool__(self) -> bool:\n        return bool(self.errors)\n\n\nclass HashError(InstallationError):\n    \"\"\"\n    A failure to verify a package against known-good hashes\n\n    :cvar order: An int sorting hash exception classes by difficulty of\n        recovery (lower being harder), so the user doesn't bother fretting\n        about unpinned packages when he has deeper issues, like VCS\n        dependencies, to deal with. Also keeps error reports in a\n        deterministic order.\n    :cvar head: A section heading for display above potentially many\n        exceptions of this kind\n    :ivar req: The InstallRequirement that triggered this error. This is\n        pasted on after the exception is instantiated, because it's not\n        typically available earlier.\n\n    \"\"\"\n\n    req: Optional[\"InstallRequirement\"] = None\n    head = \"\"\n    order: int = -1\n\n    def body(self) -> str:\n        \"\"\"Return a summary of me for display under the heading.\n\n        This default implementation simply prints a description of the\n        triggering requirement.\n\n        :param req: The InstallRequirement that provoked this error, with\n            its link already populated by the resolver's _populate_link().\n\n        \"\"\"\n        return f\"    {self._requirement_name()}\"\n\n    def __str__(self) -> str:\n        return f\"{self.head}\\n{self.body()}\"\n\n    def _requirement_name(self) -> str:\n        \"\"\"Return a description of the requirement that triggered me.\n\n        This default implementation returns long description of the req, with\n        line numbers\n\n        \"\"\"\n        return str(self.req) if self.req else \"unknown package\"\n\n\nclass VcsHashUnsupported(HashError):\n    \"\"\"A hash was provided for a version-control-system-based requirement, but\n    we don't have a method for hashing those.\"\"\"\n\n    order = 0\n    head = (\n        \"Can't verify hashes for these requirements because we don't \"\n        \"have a way to hash version control repositories:\"\n    )\n\n\nclass DirectoryUrlHashUnsupported(HashError):\n    \"\"\"A hash was provided for a version-control-system-based requirement, but\n    we don't have a method for hashing those.\"\"\"\n\n    order = 1\n    head = (\n        \"Can't verify hashes for these file:// requirements because they \"\n        \"point to directories:\"\n    )\n\n\nclass HashMissing(HashError):\n    \"\"\"A hash was needed for a requirement but is absent.\"\"\"\n\n    order = 2\n    head = (\n        \"Hashes are required in --require-hashes mode, but they are \"\n        \"missing from some requirements. Here is a list of those \"\n        \"requirements along with the hashes their downloaded archives \"\n        \"actually had. Add lines like these to your requirements files to \"\n        \"prevent tampering. (If you did not enable --require-hashes \"\n        \"manually, note that it turns on automatically when any package \"\n        \"has a hash.)\"\n    )\n\n    def __init__(self, gotten_hash: str) -> None:\n        \"\"\"\n        :param gotten_hash: The hash of the (possibly malicious) archive we\n            just downloaded\n        \"\"\"\n        self.gotten_hash = gotten_hash\n\n    def body(self) -> str:\n        # Dodge circular import.\n        from pip._internal.utils.hashes import FAVORITE_HASH\n\n        package = None\n        if self.req:\n            # In the case of URL-based requirements, display the original URL\n            # seen in the requirements file rather than the package name,\n            # so the output can be directly copied into the requirements file.\n            package = (\n                self.req.original_link\n                if self.req.is_direct\n                # In case someone feeds something downright stupid\n                # to InstallRequirement's constructor.\n                else getattr(self.req, \"req\", None)\n            )\n        return \"    {} --hash={}:{}\".format(\n            package or \"unknown package\", FAVORITE_HASH, self.gotten_hash\n        )\n\n\nclass HashUnpinned(HashError):\n    \"\"\"A requirement had a hash specified but was not pinned to a specific\n    version.\"\"\"\n\n    order = 3\n    head = (\n        \"In --require-hashes mode, all requirements must have their \"\n        \"versions pinned with ==. These do not:\"\n    )\n\n\nclass HashMismatch(HashError):\n    \"\"\"\n    Distribution file hash values don't match.\n\n    :ivar package_name: The name of the package that triggered the hash\n        mismatch. Feel free to write to this after the exception is raise to\n        improve its error message.\n\n    \"\"\"\n\n    order = 4\n    head = (\n        \"THESE PACKAGES DO NOT MATCH THE HASHES FROM THE REQUIREMENTS \"\n        \"FILE. If you have updated the package versions, please update \"\n        \"the hashes. Otherwise, examine the package contents carefully; \"\n        \"someone may have tampered with them.\"\n    )\n\n    def __init__(self, allowed: Dict[str, List[str]], gots: Dict[str, \"_Hash\"]) -> None:\n        \"\"\"\n        :param allowed: A dict of algorithm names pointing to lists of allowed\n            hex digests\n        :param gots: A dict of algorithm names pointing to hashes we\n            actually got from the files under suspicion\n        \"\"\"\n        self.allowed = allowed\n        self.gots = gots\n\n    def body(self) -> str:\n        return f\"    {self._requirement_name()}:\\n{self._hash_comparison()}\"\n\n    def _hash_comparison(self) -> str:\n        \"\"\"\n        Return a comparison of actual and expected hash values.\n\n        Example::\n\n               Expected sha256 abcdeabcdeabcdeabcdeabcdeabcdeabcdeabcdeabcde\n                            or 123451234512345123451234512345123451234512345\n                    Got        bcdefbcdefbcdefbcdefbcdefbcdefbcdefbcdefbcdef\n\n        \"\"\"\n\n        def hash_then_or(hash_name: str) -> \"chain[str]\":\n            # For now, all the decent hashes have 6-char names, so we can get\n            # away with hard-coding space literals.\n            return chain([hash_name], repeat(\"    or\"))\n\n        lines: List[str] = []\n        for hash_name, expecteds in self.allowed.items():\n            prefix = hash_then_or(hash_name)\n            lines.extend((f\"        Expected {next(prefix)} {e}\") for e in expecteds)\n            lines.append(\n                f\"             Got        {self.gots[hash_name].hexdigest()}\\n\"\n            )\n        return \"\\n\".join(lines)\n\n\nclass UnsupportedPythonVersion(InstallationError):\n    \"\"\"Unsupported python version according to Requires-Python package\n    metadata.\"\"\"\n\n\nclass ConfigurationFileCouldNotBeLoaded(ConfigurationError):\n    \"\"\"When there are errors while loading a configuration file\"\"\"\n\n    def __init__(\n        self,\n        reason: str = \"could not be loaded\",\n        fname: Optional[str] = None,\n        error: Optional[configparser.Error] = None,\n    ) -> None:\n        super().__init__(error)\n        self.reason = reason\n        self.fname = fname\n        self.error = error\n\n    def __str__(self) -> str:\n        if self.fname is not None:\n            message_part = f\" in {self.fname}.\"\n        else:\n            assert self.error is not None\n            message_part = f\".\\n{self.error}\\n\"\n        return f\"Configuration file {self.reason}{message_part}\"\n\n\n_DEFAULT_EXTERNALLY_MANAGED_ERROR = f\"\"\"\\\nThe Python environment under {sys.prefix} is managed externally, and may not be\nmanipulated by the user. Please use specific tooling from the distributor of\nthe Python installation to interact with this environment instead.\n\"\"\"\n\n\nclass ExternallyManagedEnvironment(DiagnosticPipError):\n    \"\"\"The current environment is externally managed.\n\n    This is raised when the current environment is externally managed, as\n    defined by `PEP 668`_. The ``EXTERNALLY-MANAGED`` configuration is checked\n    and displayed when the error is bubbled up to the user.\n\n    :param error: The error message read from ``EXTERNALLY-MANAGED``.\n    \"\"\"\n\n    reference = \"externally-managed-environment\"\n\n    def __init__(self, error: Optional[str]) -> None:\n        if error is None:\n            context = Text(_DEFAULT_EXTERNALLY_MANAGED_ERROR)\n        else:\n            context = Text(error)\n        super().__init__(\n            message=\"This environment is externally managed\",\n            context=context,\n            note_stmt=(\n                \"If you believe this is a mistake, please contact your \"\n                \"Python installation or OS distribution provider. \"\n                \"You can override this, at the risk of breaking your Python \"\n                \"installation or OS, by passing --break-system-packages.\"\n            ),\n            hint_stmt=Text(\"See PEP 668 for the detailed specification.\"),\n        )\n\n    @staticmethod\n    def _iter_externally_managed_error_keys() -> Iterator[str]:\n        # LC_MESSAGES is in POSIX, but not the C standard. The most common\n        # platform that does not implement this category is Windows, where\n        # using other categories for console message localization is equally\n        # unreliable, so we fall back to the locale-less vendor message. This\n        # can always be re-evaluated when a vendor proposes a new alternative.\n        try:\n            category = locale.LC_MESSAGES\n        except AttributeError:\n            lang: Optional[str] = None\n        else:\n            lang, _ = locale.getlocale(category)\n        if lang is not None:\n            yield f\"Error-{lang}\"\n            for sep in (\"-\", \"_\"):\n                before, found, _ = lang.partition(sep)\n                if not found:\n                    continue\n                yield f\"Error-{before}\"\n        yield \"Error\"\n\n    @classmethod\n    def from_config(\n        cls,\n        config: Union[pathlib.Path, str],\n    ) -> \"ExternallyManagedEnvironment\":\n        parser = configparser.ConfigParser(interpolation=None)\n        try:\n            parser.read(config, encoding=\"utf-8\")\n            section = parser[\"externally-managed\"]\n            for key in cls._iter_externally_managed_error_keys():\n                with contextlib.suppress(KeyError):\n                    return cls(section[key])\n        except KeyError:\n            pass\n        except (OSError, UnicodeDecodeError, configparser.ParsingError):\n            from pip._internal.utils._log import VERBOSE\n\n            exc_info = logger.isEnabledFor(VERBOSE)\n            logger.warning(\"Failed to read %s\", config, exc_info=exc_info)\n        return cls(None)\n\n\nclass UninstallMissingRecord(DiagnosticPipError):\n    reference = \"uninstall-no-record-file\"\n\n    def __init__(self, *, distribution: \"BaseDistribution\") -> None:\n        installer = distribution.installer\n        if not installer or installer == \"pip\":\n            dep = f\"{distribution.raw_name}=={distribution.version}\"\n            hint = Text.assemble(\n                \"You might be able to recover from this via: \",\n                (f\"pip install --force-reinstall --no-deps {dep}\", \"green\"),\n            )\n        else:\n            hint = Text(\n                f\"The package was installed by {installer}. \"\n                \"You should check if it can uninstall the package.\"\n            )\n\n        super().__init__(\n            message=Text(f\"Cannot uninstall {distribution}\"),\n            context=(\n                \"The package's contents are unknown: \"\n                f\"no RECORD file was found for {distribution.raw_name}.\"\n            ),\n            hint_stmt=hint,\n        )\n\n\nclass LegacyDistutilsInstall(DiagnosticPipError):\n    reference = \"uninstall-distutils-installed-package\"\n\n    def __init__(self, *, distribution: \"BaseDistribution\") -> None:\n        super().__init__(\n            message=Text(f\"Cannot uninstall {distribution}\"),\n            context=(\n                \"It is a distutils installed project and thus we cannot accurately \"\n                \"determine which files belong to it which would lead to only a partial \"\n                \"uninstall.\"\n            ),\n            hint_stmt=None,\n        )\n\n\nclass InvalidInstalledPackage(DiagnosticPipError):\n    reference = \"invalid-installed-package\"\n\n    def __init__(\n        self,\n        *,\n        dist: \"BaseDistribution\",\n        invalid_exc: Union[InvalidRequirement, InvalidVersion],\n    ) -> None:\n        installed_location = dist.installed_location\n\n        if isinstance(invalid_exc, InvalidRequirement):\n            invalid_type = \"requirement\"\n        else:\n            invalid_type = \"version\"\n\n        super().__init__(\n            message=Text(\n                f\"Cannot process installed package {dist} \"\n                + (f\"in {installed_location!r} \" if installed_location else \"\")\n                + f\"because it has an invalid {invalid_type}:\\n{invalid_exc.args[0]}\"\n            ),\n            context=(\n                \"Starting with pip 24.1, packages with invalid \"\n                f\"{invalid_type}s can not be processed.\"\n            ),\n            hint_stmt=\"To proceed this package must be uninstalled.\",\n        )\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/index/__init__.py","size":30,"sha1":"a55d1d416e674d9f4a8e0337defe350962f21f1a","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"Index interaction code\n\"\"\"\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/index/collector.py","size":16265,"sha1":"d5809b7e772c0875a2c43aa789ca4cfb5c9cb169","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"\nThe main purpose of this module is to expose LinkCollector.collect_sources().\n\"\"\"\n\nimport collections\nimport email.message\nimport functools\nimport itertools\nimport json\nimport logging\nimport os\nimport urllib.parse\nimport urllib.request\nfrom dataclasses import dataclass\nfrom html.parser import HTMLParser\nfrom optparse import Values\nfrom typing import (\n    Callable,\n    Dict,\n    Iterable,\n    List,\n    MutableMapping,\n    NamedTuple,\n    Optional,\n    Protocol,\n    Sequence,\n    Tuple,\n    Union,\n)\n\nfrom pip._vendor import requests\nfrom pip._vendor.requests import Response\nfrom pip._vendor.requests.exceptions import RetryError, SSLError\n\nfrom pip._internal.exceptions import NetworkConnectionError\nfrom pip._internal.models.link import Link\nfrom pip._internal.models.search_scope import SearchScope\nfrom pip._internal.network.session import PipSession\nfrom pip._internal.network.utils import raise_for_status\nfrom pip._internal.utils.filetypes import is_archive_file\nfrom pip._internal.utils.misc import redact_auth_from_url\nfrom pip._internal.vcs import vcs\n\nfrom .sources import CandidatesFromPage, LinkSource, build_source\n\nlogger = logging.getLogger(__name__)\n\nResponseHeaders = MutableMapping[str, str]\n\n\ndef _match_vcs_scheme(url: str) -> Optional[str]:\n    \"\"\"Look for VCS schemes in the URL.\n\n    Returns the matched VCS scheme, or None if there's no match.\n    \"\"\"\n    for scheme in vcs.schemes:\n        if url.lower().startswith(scheme) and url[len(scheme)] in \"+:\":\n            return scheme\n    return None\n\n\nclass _NotAPIContent(Exception):\n    def __init__(self, content_type: str, request_desc: str) -> None:\n        super().__init__(content_type, request_desc)\n        self.content_type = content_type\n        self.request_desc = request_desc\n\n\ndef _ensure_api_header(response: Response) -> None:\n    \"\"\"\n    Check the Content-Type header to ensure the response contains a Simple\n    API Response.\n\n    Raises `_NotAPIContent` if the content type is not a valid content-type.\n    \"\"\"\n    content_type = response.headers.get(\"Content-Type\", \"Unknown\")\n\n    content_type_l = content_type.lower()\n    if content_type_l.startswith(\n        (\n            \"text/html\",\n            \"application/vnd.pypi.simple.v1+html\",\n            \"application/vnd.pypi.simple.v1+json\",\n        )\n    ):\n        return\n\n    raise _NotAPIContent(content_type, response.request.method)\n\n\nclass _NotHTTP(Exception):\n    pass\n\n\ndef _ensure_api_response(url: str, session: PipSession) -> None:\n    \"\"\"\n    Send a HEAD request to the URL, and ensure the response contains a simple\n    API Response.\n\n    Raises `_NotHTTP` if the URL is not available for a HEAD request, or\n    `_NotAPIContent` if the content type is not a valid content type.\n    \"\"\"\n    scheme, netloc, path, query, fragment = urllib.parse.urlsplit(url)\n    if scheme not in {\"http\", \"https\"}:\n        raise _NotHTTP()\n\n    resp = session.head(url, allow_redirects=True)\n    raise_for_status(resp)\n\n    _ensure_api_header(resp)\n\n\ndef _get_simple_response(url: str, session: PipSession) -> Response:\n    \"\"\"Access an Simple API response with GET, and return the response.\n\n    This consists of three parts:\n\n    1. If the URL looks suspiciously like an archive, send a HEAD first to\n       check the Content-Type is HTML or Simple API, to avoid downloading a\n       large file. Raise `_NotHTTP` if the content type cannot be determined, or\n       `_NotAPIContent` if it is not HTML or a Simple API.\n    2. Actually perform the request. Raise HTTP exceptions on network failures.\n    3. Check the Content-Type header to make sure we got a Simple API response,\n       and raise `_NotAPIContent` otherwise.\n    \"\"\"\n    if is_archive_file(Link(url).filename):\n        _ensure_api_response(url, session=session)\n\n    logger.debug(\"Getting page %s\", redact_auth_from_url(url))\n\n    resp = session.get(\n        url,\n        headers={\n            \"Accept\": \", \".join(\n                [\n                    \"application/vnd.pypi.simple.v1+json\",\n                    \"application/vnd.pypi.simple.v1+html; q=0.1\",\n                    \"text/html; q=0.01\",\n                ]\n            ),\n            # We don't want to blindly returned cached data for\n            # /simple/, because authors generally expecting that\n            # twine upload && pip install will function, but if\n            # they've done a pip install in the last ~10 minutes\n            # it won't. Thus by setting this to zero we will not\n            # blindly use any cached data, however the benefit of\n            # using max-age=0 instead of no-cache, is that we will\n            # still support conditional requests, so we will still\n            # minimize traffic sent in cases where the page hasn't\n            # changed at all, we will just always incur the round\n            # trip for the conditional GET now instead of only\n            # once per 10 minutes.\n            # For more information, please see pypa/pip#5670.\n            \"Cache-Control\": \"max-age=0\",\n        },\n    )\n    raise_for_status(resp)\n\n    # The check for archives above only works if the url ends with\n    # something that looks like an archive. However that is not a\n    # requirement of an url. Unless we issue a HEAD request on every\n    # url we cannot know ahead of time for sure if something is a\n    # Simple API response or not. However we can check after we've\n    # downloaded it.\n    _ensure_api_header(resp)\n\n    logger.debug(\n        \"Fetched page %s as %s\",\n        redact_auth_from_url(url),\n        resp.headers.get(\"Content-Type\", \"Unknown\"),\n    )\n\n    return resp\n\n\ndef _get_encoding_from_headers(headers: ResponseHeaders) -> Optional[str]:\n    \"\"\"Determine if we have any encoding information in our headers.\"\"\"\n    if headers and \"Content-Type\" in headers:\n        m = email.message.Message()\n        m[\"content-type\"] = headers[\"Content-Type\"]\n        charset = m.get_param(\"charset\")\n        if charset:\n            return str(charset)\n    return None\n\n\nclass CacheablePageContent:\n    def __init__(self, page: \"IndexContent\") -> None:\n        assert page.cache_link_parsing\n        self.page = page\n\n    def __eq__(self, other: object) -> bool:\n        return isinstance(other, type(self)) and self.page.url == other.page.url\n\n    def __hash__(self) -> int:\n        return hash(self.page.url)\n\n\nclass ParseLinks(Protocol):\n    def __call__(self, page: \"IndexContent\") -> Iterable[Link]: ...\n\n\ndef with_cached_index_content(fn: ParseLinks) -> ParseLinks:\n    \"\"\"\n    Given a function that parses an Iterable[Link] from an IndexContent, cache the\n    function's result (keyed by CacheablePageContent), unless the IndexContent\n    `page` has `page.cache_link_parsing == False`.\n    \"\"\"\n\n    @functools.lru_cache(maxsize=None)\n    def wrapper(cacheable_page: CacheablePageContent) -> List[Link]:\n        return list(fn(cacheable_page.page))\n\n    @functools.wraps(fn)\n    def wrapper_wrapper(page: \"IndexContent\") -> List[Link]:\n        if page.cache_link_parsing:\n            return wrapper(CacheablePageContent(page))\n        return list(fn(page))\n\n    return wrapper_wrapper\n\n\n@with_cached_index_content\ndef parse_links(page: \"IndexContent\") -> Iterable[Link]:\n    \"\"\"\n    Parse a Simple API's Index Content, and yield its anchor elements as Link objects.\n    \"\"\"\n\n    content_type_l = page.content_type.lower()\n    if content_type_l.startswith(\"application/vnd.pypi.simple.v1+json\"):\n        data = json.loads(page.content)\n        for file in data.get(\"files\", []):\n            link = Link.from_json(file, page.url)\n            if link is None:\n                continue\n            yield link\n        return\n\n    parser = HTMLLinkParser(page.url)\n    encoding = page.encoding or \"utf-8\"\n    parser.feed(page.content.decode(encoding))\n\n    url = page.url\n    base_url = parser.base_url or url\n    for anchor in parser.anchors:\n        link = Link.from_element(anchor, page_url=url, base_url=base_url)\n        if link is None:\n            continue\n        yield link\n\n\n@dataclass(frozen=True)\nclass IndexContent:\n    \"\"\"Represents one response (or page), along with its URL.\n\n    :param encoding: the encoding to decode the given content.\n    :param url: the URL from which the HTML was downloaded.\n    :param cache_link_parsing: whether links parsed from this page's url\n                               should be cached. PyPI index urls should\n                               have this set to False, for example.\n    \"\"\"\n\n    content: bytes\n    content_type: str\n    encoding: Optional[str]\n    url: str\n    cache_link_parsing: bool = True\n\n    def __str__(self) -> str:\n        return redact_auth_from_url(self.url)\n\n\nclass HTMLLinkParser(HTMLParser):\n    \"\"\"\n    HTMLParser that keeps the first base HREF and a list of all anchor\n    elements' attributes.\n    \"\"\"\n\n    def __init__(self, url: str) -> None:\n        super().__init__(convert_charrefs=True)\n\n        self.url: str = url\n        self.base_url: Optional[str] = None\n        self.anchors: List[Dict[str, Optional[str]]] = []\n\n    def handle_starttag(self, tag: str, attrs: List[Tuple[str, Optional[str]]]) -> None:\n        if tag == \"base\" and self.base_url is None:\n            href = self.get_href(attrs)\n            if href is not None:\n                self.base_url = href\n        elif tag == \"a\":\n            self.anchors.append(dict(attrs))\n\n    def get_href(self, attrs: List[Tuple[str, Optional[str]]]) -> Optional[str]:\n        for name, value in attrs:\n            if name == \"href\":\n                return value\n        return None\n\n\ndef _handle_get_simple_fail(\n    link: Link,\n    reason: Union[str, Exception],\n    meth: Optional[Callable[..., None]] = None,\n) -> None:\n    if meth is None:\n        meth = logger.debug\n    meth(\"Could not fetch URL %s: %s - skipping\", link, reason)\n\n\ndef _make_index_content(\n    response: Response, cache_link_parsing: bool = True\n) -> IndexContent:\n    encoding = _get_encoding_from_headers(response.headers)\n    return IndexContent(\n        response.content,\n        response.headers[\"Content-Type\"],\n        encoding=encoding,\n        url=response.url,\n        cache_link_parsing=cache_link_parsing,\n    )\n\n\ndef _get_index_content(link: Link, *, session: PipSession) -> Optional[\"IndexContent\"]:\n    url = link.url.split(\"#\", 1)[0]\n\n    # Check for VCS schemes that do not support lookup as web pages.\n    vcs_scheme = _match_vcs_scheme(url)\n    if vcs_scheme:\n        logger.warning(\n            \"Cannot look at %s URL %s because it does not support lookup as web pages.\",\n            vcs_scheme,\n            link,\n        )\n        return None\n\n    # Tack index.html onto file:// URLs that point to directories\n    scheme, _, path, _, _, _ = urllib.parse.urlparse(url)\n    if scheme == \"file\" and os.path.isdir(urllib.request.url2pathname(path)):\n        # add trailing slash if not present so urljoin doesn't trim\n        # final segment\n        if not url.endswith(\"/\"):\n            url += \"/\"\n        # TODO: In the future, it would be nice if pip supported PEP 691\n        #       style responses in the file:// URLs, however there's no\n        #       standard file extension for application/vnd.pypi.simple.v1+json\n        #       so we'll need to come up with something on our own.\n        url = urllib.parse.urljoin(url, \"index.html\")\n        logger.debug(\" file: URL is directory, getting %s\", url)\n\n    try:\n        resp = _get_simple_response(url, session=session)\n    except _NotHTTP:\n        logger.warning(\n            \"Skipping page %s because it looks like an archive, and cannot \"\n            \"be checked by a HTTP HEAD request.\",\n            link,\n        )\n    except _NotAPIContent as exc:\n        logger.warning(\n            \"Skipping page %s because the %s request got Content-Type: %s. \"\n            \"The only supported Content-Types are application/vnd.pypi.simple.v1+json, \"\n            \"application/vnd.pypi.simple.v1+html, and text/html\",\n            link,\n            exc.request_desc,\n            exc.content_type,\n        )\n    except NetworkConnectionError as exc:\n        _handle_get_simple_fail(link, exc)\n    except RetryError as exc:\n        _handle_get_simple_fail(link, exc)\n    except SSLError as exc:\n        reason = \"There was a problem confirming the ssl certificate: \"\n        reason += str(exc)\n        _handle_get_simple_fail(link, reason, meth=logger.info)\n    except requests.ConnectionError as exc:\n        _handle_get_simple_fail(link, f\"connection error: {exc}\")\n    except requests.Timeout:\n        _handle_get_simple_fail(link, \"timed out\")\n    else:\n        return _make_index_content(resp, cache_link_parsing=link.cache_link_parsing)\n    return None\n\n\nclass CollectedSources(NamedTuple):\n    find_links: Sequence[Optional[LinkSource]]\n    index_urls: Sequence[Optional[LinkSource]]\n\n\nclass LinkCollector:\n    \"\"\"\n    Responsible for collecting Link objects from all configured locations,\n    making network requests as needed.\n\n    The class's main method is its collect_sources() method.\n    \"\"\"\n\n    def __init__(\n        self,\n        session: PipSession,\n        search_scope: SearchScope,\n    ) -> None:\n        self.search_scope = search_scope\n        self.session = session\n\n    @classmethod\n    def create(\n        cls,\n        session: PipSession,\n        options: Values,\n        suppress_no_index: bool = False,\n    ) -> \"LinkCollector\":\n        \"\"\"\n        :param session: The Session to use to make requests.\n        :param suppress_no_index: Whether to ignore the --no-index option\n            when constructing the SearchScope object.\n        \"\"\"\n        index_urls = [options.index_url] + options.extra_index_urls\n        if options.no_index and not suppress_no_index:\n            logger.debug(\n                \"Ignoring indexes: %s\",\n                \",\".join(redact_auth_from_url(url) for url in index_urls),\n            )\n            index_urls = []\n\n        # Make sure find_links is a list before passing to create().\n        find_links = options.find_links or []\n\n        search_scope = SearchScope.create(\n            find_links=find_links,\n            index_urls=index_urls,\n            no_index=options.no_index,\n        )\n        link_collector = LinkCollector(\n            session=session,\n            search_scope=search_scope,\n        )\n        return link_collector\n\n    @property\n    def find_links(self) -> List[str]:\n        return self.search_scope.find_links\n\n    def fetch_response(self, location: Link) -> Optional[IndexContent]:\n        \"\"\"\n        Fetch an HTML page containing package links.\n        \"\"\"\n        return _get_index_content(location, session=self.session)\n\n    def collect_sources(\n        self,\n        project_name: str,\n        candidates_from_page: CandidatesFromPage,\n    ) -> CollectedSources:\n        # The OrderedDict calls deduplicate sources by URL.\n        index_url_sources = collections.OrderedDict(\n            build_source(\n                loc,\n                candidates_from_page=candidates_from_page,\n                page_validator=self.session.is_secure_origin,\n                expand_dir=False,\n                cache_link_parsing=False,\n                project_name=project_name,\n            )\n            for loc in self.search_scope.get_index_urls_locations(project_name)\n        ).values()\n        find_links_sources = collections.OrderedDict(\n            build_source(\n                loc,\n                candidates_from_page=candidates_from_page,\n                page_validator=self.session.is_secure_origin,\n                expand_dir=True,\n                cache_link_parsing=True,\n                project_name=project_name,\n            )\n            for loc in self.find_links\n        ).values()\n\n        if logger.isEnabledFor(logging.DEBUG):\n            lines = [\n                f\"* {s.link}\"\n                for s in itertools.chain(find_links_sources, index_url_sources)\n                if s is not None and s.link is not None\n            ]\n            lines = [\n                f\"{len(lines)} location(s) to search \"\n                f\"for versions of {project_name}:\"\n            ] + lines\n            logger.debug(\"\\n\".join(lines))\n\n        return CollectedSources(\n            find_links=list(find_links_sources),\n            index_urls=list(index_url_sources),\n        )\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/index/package_finder.py","size":38076,"sha1":"790d5851e0bd357180892de7e52e6099365f838c","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"Routines related to PyPI, indexes\"\"\"\n\nimport enum\nimport functools\nimport itertools\nimport logging\nimport re\nfrom dataclasses import dataclass\nfrom typing import TYPE_CHECKING, FrozenSet, Iterable, List, Optional, Set, Tuple, Union\n\nfrom pip._vendor.packaging import specifiers\nfrom pip._vendor.packaging.tags import Tag\nfrom pip._vendor.packaging.utils import canonicalize_name\nfrom pip._vendor.packaging.version import InvalidVersion, _BaseVersion\nfrom pip._vendor.packaging.version import parse as parse_version\n\nfrom pip._internal.exceptions import (\n    BestVersionAlreadyInstalled,\n    DistributionNotFound,\n    InvalidWheelFilename,\n    UnsupportedWheel,\n)\nfrom pip._internal.index.collector import LinkCollector, parse_links\nfrom pip._internal.models.candidate import InstallationCandidate\nfrom pip._internal.models.format_control import FormatControl\nfrom pip._internal.models.link import Link\nfrom pip._internal.models.search_scope import SearchScope\nfrom pip._internal.models.selection_prefs import SelectionPreferences\nfrom pip._internal.models.target_python import TargetPython\nfrom pip._internal.models.wheel import Wheel\nfrom pip._internal.req import InstallRequirement\nfrom pip._internal.utils._log import getLogger\nfrom pip._internal.utils.filetypes import WHEEL_EXTENSION\nfrom pip._internal.utils.hashes import Hashes\nfrom pip._internal.utils.logging import indent_log\nfrom pip._internal.utils.misc import build_netloc\nfrom pip._internal.utils.packaging import check_requires_python\nfrom pip._internal.utils.unpacking import SUPPORTED_EXTENSIONS\n\nif TYPE_CHECKING:\n    from pip._vendor.typing_extensions import TypeGuard\n\n__all__ = [\"FormatControl\", \"BestCandidateResult\", \"PackageFinder\"]\n\n\nlogger = getLogger(__name__)\n\nBuildTag = Union[Tuple[()], Tuple[int, str]]\nCandidateSortingKey = Tuple[int, int, int, _BaseVersion, Optional[int], BuildTag]\n\n\ndef _check_link_requires_python(\n    link: Link,\n    version_info: Tuple[int, int, int],\n    ignore_requires_python: bool = False,\n) -> bool:\n    \"\"\"\n    Return whether the given Python version is compatible with a link's\n    \"Requires-Python\" value.\n\n    :param version_info: A 3-tuple of ints representing the Python\n        major-minor-micro version to check.\n    :param ignore_requires_python: Whether to ignore the \"Requires-Python\"\n        value if the given Python version isn't compatible.\n    \"\"\"\n    try:\n        is_compatible = check_requires_python(\n            link.requires_python,\n            version_info=version_info,\n        )\n    except specifiers.InvalidSpecifier:\n        logger.debug(\n            \"Ignoring invalid Requires-Python (%r) for link: %s\",\n            link.requires_python,\n            link,\n        )\n    else:\n        if not is_compatible:\n            version = \".\".join(map(str, version_info))\n            if not ignore_requires_python:\n                logger.verbose(\n                    \"Link requires a different Python (%s not in: %r): %s\",\n                    version,\n                    link.requires_python,\n                    link,\n                )\n                return False\n\n            logger.debug(\n                \"Ignoring failed Requires-Python check (%s not in: %r) for link: %s\",\n                version,\n                link.requires_python,\n                link,\n            )\n\n    return True\n\n\nclass LinkType(enum.Enum):\n    candidate = enum.auto()\n    different_project = enum.auto()\n    yanked = enum.auto()\n    format_unsupported = enum.auto()\n    format_invalid = enum.auto()\n    platform_mismatch = enum.auto()\n    requires_python_mismatch = enum.auto()\n\n\nclass LinkEvaluator:\n    \"\"\"\n    Responsible for evaluating links for a particular project.\n    \"\"\"\n\n    _py_version_re = re.compile(r\"-py([123]\\.?[0-9]?)$\")\n\n    # Don't include an allow_yanked default value to make sure each call\n    # site considers whether yanked releases are allowed. This also causes\n    # that decision to be made explicit in the calling code, which helps\n    # people when reading the code.\n    def __init__(\n        self,\n        project_name: str,\n        canonical_name: str,\n        formats: FrozenSet[str],\n        target_python: TargetPython,\n        allow_yanked: bool,\n        ignore_requires_python: Optional[bool] = None,\n    ) -> None:\n        \"\"\"\n        :param project_name: The user supplied package name.\n        :param canonical_name: The canonical package name.\n        :param formats: The formats allowed for this package. Should be a set\n            with 'binary' or 'source' or both in it.\n        :param target_python: The target Python interpreter to use when\n            evaluating link compatibility. This is used, for example, to\n            check wheel compatibility, as well as when checking the Python\n            version, e.g. the Python version embedded in a link filename\n            (or egg fragment) and against an HTML link's optional PEP 503\n            \"data-requires-python\" attribute.\n        :param allow_yanked: Whether files marked as yanked (in the sense\n            of PEP 592) are permitted to be candidates for install.\n        :param ignore_requires_python: Whether to ignore incompatible\n            PEP 503 \"data-requires-python\" values in HTML links. Defaults\n            to False.\n        \"\"\"\n        if ignore_requires_python is None:\n            ignore_requires_python = False\n\n        self._allow_yanked = allow_yanked\n        self._canonical_name = canonical_name\n        self._ignore_requires_python = ignore_requires_python\n        self._formats = formats\n        self._target_python = target_python\n\n        self.project_name = project_name\n\n    def evaluate_link(self, link: Link) -> Tuple[LinkType, str]:\n        \"\"\"\n        Determine whether a link is a candidate for installation.\n\n        :return: A tuple (result, detail), where *result* is an enum\n            representing whether the evaluation found a candidate, or the reason\n            why one is not found. If a candidate is found, *detail* will be the\n            candidate's version string; if one is not found, it contains the\n            reason the link fails to qualify.\n        \"\"\"\n        version = None\n        if link.is_yanked and not self._allow_yanked:\n            reason = link.yanked_reason or \"<none given>\"\n            return (LinkType.yanked, f\"yanked for reason: {reason}\")\n\n        if link.egg_fragment:\n            egg_info = link.egg_fragment\n            ext = link.ext\n        else:\n            egg_info, ext = link.splitext()\n            if not ext:\n                return (LinkType.format_unsupported, \"not a file\")\n            if ext not in SUPPORTED_EXTENSIONS:\n                return (\n                    LinkType.format_unsupported,\n                    f\"unsupported archive format: {ext}\",\n                )\n            if \"binary\" not in self._formats and ext == WHEEL_EXTENSION:\n                reason = f\"No binaries permitted for {self.project_name}\"\n                return (LinkType.format_unsupported, reason)\n            if \"macosx10\" in link.path and ext == \".zip\":\n                return (LinkType.format_unsupported, \"macosx10 one\")\n            if ext == WHEEL_EXTENSION:\n                try:\n                    wheel = Wheel(link.filename)\n                except InvalidWheelFilename:\n                    return (\n                        LinkType.format_invalid,\n                        \"invalid wheel filename\",\n                    )\n                if canonicalize_name(wheel.name) != self._canonical_name:\n                    reason = f\"wrong project name (not {self.project_name})\"\n                    return (LinkType.different_project, reason)\n\n                supported_tags = self._target_python.get_unsorted_tags()\n                if not wheel.supported(supported_tags):\n                    # Include the wheel's tags in the reason string to\n                    # simplify troubleshooting compatibility issues.\n                    file_tags = \", \".join(wheel.get_formatted_file_tags())\n                    reason = (\n                        f\"none of the wheel's tags ({file_tags}) are compatible \"\n                        f\"(run pip debug --verbose to show compatible tags)\"\n                    )\n                    return (LinkType.platform_mismatch, reason)\n\n                version = wheel.version\n\n        # This should be up by the self.ok_binary check, but see issue 2700.\n        if \"source\" not in self._formats and ext != WHEEL_EXTENSION:\n            reason = f\"No sources permitted for {self.project_name}\"\n            return (LinkType.format_unsupported, reason)\n\n        if not version:\n            version = _extract_version_from_fragment(\n                egg_info,\n                self._canonical_name,\n            )\n        if not version:\n            reason = f\"Missing project version for {self.project_name}\"\n            return (LinkType.format_invalid, reason)\n\n        match = self._py_version_re.search(version)\n        if match:\n            version = version[: match.start()]\n            py_version = match.group(1)\n            if py_version != self._target_python.py_version:\n                return (\n                    LinkType.platform_mismatch,\n                    \"Python version is incorrect\",\n                )\n\n        supports_python = _check_link_requires_python(\n            link,\n            version_info=self._target_python.py_version_info,\n            ignore_requires_python=self._ignore_requires_python,\n        )\n        if not supports_python:\n            reason = f\"{version} Requires-Python {link.requires_python}\"\n            return (LinkType.requires_python_mismatch, reason)\n\n        logger.debug(\"Found link %s, version: %s\", link, version)\n\n        return (LinkType.candidate, version)\n\n\ndef filter_unallowed_hashes(\n    candidates: List[InstallationCandidate],\n    hashes: Optional[Hashes],\n    project_name: str,\n) -> List[InstallationCandidate]:\n    \"\"\"\n    Filter out candidates whose hashes aren't allowed, and return a new\n    list of candidates.\n\n    If at least one candidate has an allowed hash, then all candidates with\n    either an allowed hash or no hash specified are returned.  Otherwise,\n    the given candidates are returned.\n\n    Including the candidates with no hash specified when there is a match\n    allows a warning to be logged if there is a more preferred candidate\n    with no hash specified.  Returning all candidates in the case of no\n    matches lets pip report the hash of the candidate that would otherwise\n    have been installed (e.g. permitting the user to more easily update\n    their requirements file with the desired hash).\n    \"\"\"\n    if not hashes:\n        logger.debug(\n            \"Given no hashes to check %s links for project %r: \"\n            \"discarding no candidates\",\n            len(candidates),\n            project_name,\n        )\n        # Make sure we're not returning back the given value.\n        return list(candidates)\n\n    matches_or_no_digest = []\n    # Collect the non-matches for logging purposes.\n    non_matches = []\n    match_count = 0\n    for candidate in candidates:\n        link = candidate.link\n        if not link.has_hash:\n            pass\n        elif link.is_hash_allowed(hashes=hashes):\n            match_count += 1\n        else:\n            non_matches.append(candidate)\n            continue\n\n        matches_or_no_digest.append(candidate)\n\n    if match_count:\n        filtered = matches_or_no_digest\n    else:\n        # Make sure we're not returning back the given value.\n        filtered = list(candidates)\n\n    if len(filtered) == len(candidates):\n        discard_message = \"discarding no candidates\"\n    else:\n        discard_message = \"discarding {} non-matches:\\n  {}\".format(\n            len(non_matches),\n            \"\\n  \".join(str(candidate.link) for candidate in non_matches),\n        )\n\n    logger.debug(\n        \"Checked %s links for project %r against %s hashes \"\n        \"(%s matches, %s no digest): %s\",\n        len(candidates),\n        project_name,\n        hashes.digest_count,\n        match_count,\n        len(matches_or_no_digest) - match_count,\n        discard_message,\n    )\n\n    return filtered\n\n\n@dataclass\nclass CandidatePreferences:\n    \"\"\"\n    Encapsulates some of the preferences for filtering and sorting\n    InstallationCandidate objects.\n    \"\"\"\n\n    prefer_binary: bool = False\n    allow_all_prereleases: bool = False\n\n\n@dataclass(frozen=True)\nclass BestCandidateResult:\n    \"\"\"A collection of candidates, returned by `PackageFinder.find_best_candidate`.\n\n    This class is only intended to be instantiated by CandidateEvaluator's\n    `compute_best_candidate()` method.\n\n    :param all_candidates: A sequence of all available candidates found.\n    :param applicable_candidates: The applicable candidates.\n    :param best_candidate: The most preferred candidate found, or None\n        if no applicable candidates were found.\n    \"\"\"\n\n    all_candidates: List[InstallationCandidate]\n    applicable_candidates: List[InstallationCandidate]\n    best_candidate: Optional[InstallationCandidate]\n\n    def __post_init__(self) -> None:\n        assert set(self.applicable_candidates) <= set(self.all_candidates)\n\n        if self.best_candidate is None:\n            assert not self.applicable_candidates\n        else:\n            assert self.best_candidate in self.applicable_candidates\n\n\nclass CandidateEvaluator:\n    \"\"\"\n    Responsible for filtering and sorting candidates for installation based\n    on what tags are valid.\n    \"\"\"\n\n    @classmethod\n    def create(\n        cls,\n        project_name: str,\n        target_python: Optional[TargetPython] = None,\n        prefer_binary: bool = False,\n        allow_all_prereleases: bool = False,\n        specifier: Optional[specifiers.BaseSpecifier] = None,\n        hashes: Optional[Hashes] = None,\n    ) -> \"CandidateEvaluator\":\n        \"\"\"Create a CandidateEvaluator object.\n\n        :param target_python: The target Python interpreter to use when\n            checking compatibility. If None (the default), a TargetPython\n            object will be constructed from the running Python.\n        :param specifier: An optional object implementing `filter`\n            (e.g. `packaging.specifiers.SpecifierSet`) to filter applicable\n            versions.\n        :param hashes: An optional collection of allowed hashes.\n        \"\"\"\n        if target_python is None:\n            target_python = TargetPython()\n        if specifier is None:\n            specifier = specifiers.SpecifierSet()\n\n        supported_tags = target_python.get_sorted_tags()\n\n        return cls(\n            project_name=project_name,\n            supported_tags=supported_tags,\n            specifier=specifier,\n            prefer_binary=prefer_binary,\n            allow_all_prereleases=allow_all_prereleases,\n            hashes=hashes,\n        )\n\n    def __init__(\n        self,\n        project_name: str,\n        supported_tags: List[Tag],\n        specifier: specifiers.BaseSpecifier,\n        prefer_binary: bool = False,\n        allow_all_prereleases: bool = False,\n        hashes: Optional[Hashes] = None,\n    ) -> None:\n        \"\"\"\n        :param supported_tags: The PEP 425 tags supported by the target\n            Python in order of preference (most preferred first).\n        \"\"\"\n        self._allow_all_prereleases = allow_all_prereleases\n        self._hashes = hashes\n        self._prefer_binary = prefer_binary\n        self._project_name = project_name\n        self._specifier = specifier\n        self._supported_tags = supported_tags\n        # Since the index of the tag in the _supported_tags list is used\n        # as a priority, precompute a map from tag to index/priority to be\n        # used in wheel.find_most_preferred_tag.\n        self._wheel_tag_preferences = {\n            tag: idx for idx, tag in enumerate(supported_tags)\n        }\n\n    def get_applicable_candidates(\n        self,\n        candidates: List[InstallationCandidate],\n    ) -> List[InstallationCandidate]:\n        \"\"\"\n        Return the applicable candidates from a list of candidates.\n        \"\"\"\n        # Using None infers from the specifier instead.\n        allow_prereleases = self._allow_all_prereleases or None\n        specifier = self._specifier\n\n        # We turn the version object into a str here because otherwise\n        # when we're debundled but setuptools isn't, Python will see\n        # packaging.version.Version and\n        # pkg_resources._vendor.packaging.version.Version as different\n        # types. This way we'll use a str as a common data interchange\n        # format. If we stop using the pkg_resources provided specifier\n        # and start using our own, we can drop the cast to str().\n        candidates_and_versions = [(c, str(c.version)) for c in candidates]\n        versions = set(\n            specifier.filter(\n                (v for _, v in candidates_and_versions),\n                prereleases=allow_prereleases,\n            )\n        )\n\n        applicable_candidates = [c for c, v in candidates_and_versions if v in versions]\n        filtered_applicable_candidates = filter_unallowed_hashes(\n            candidates=applicable_candidates,\n            hashes=self._hashes,\n            project_name=self._project_name,\n        )\n\n        return sorted(filtered_applicable_candidates, key=self._sort_key)\n\n    def _sort_key(self, candidate: InstallationCandidate) -> CandidateSortingKey:\n        \"\"\"\n        Function to pass as the `key` argument to a call to sorted() to sort\n        InstallationCandidates by preference.\n\n        Returns a tuple such that tuples sorting as greater using Python's\n        default comparison operator are more preferred.\n\n        The preference is as follows:\n\n        First and foremost, candidates with allowed (matching) hashes are\n        always preferred over candidates without matching hashes. This is\n        because e.g. if the only candidate with an allowed hash is yanked,\n        we still want to use that candidate.\n\n        Second, excepting hash considerations, candidates that have been\n        yanked (in the sense of PEP 592) are always less preferred than\n        candidates that haven't been yanked. Then:\n\n        If not finding wheels, they are sorted by version only.\n        If finding wheels, then the sort order is by version, then:\n          1. existing installs\n          2. wheels ordered via Wheel.support_index_min(self._supported_tags)\n          3. source archives\n        If prefer_binary was set, then all wheels are sorted above sources.\n\n        Note: it was considered to embed this logic into the Link\n              comparison operators, but then different sdist links\n              with the same version, would have to be considered equal\n        \"\"\"\n        valid_tags = self._supported_tags\n        support_num = len(valid_tags)\n        build_tag: BuildTag = ()\n        binary_preference = 0\n        link = candidate.link\n        if link.is_wheel:\n            # can raise InvalidWheelFilename\n            wheel = Wheel(link.filename)\n            try:\n                pri = -(\n                    wheel.find_most_preferred_tag(\n                        valid_tags, self._wheel_tag_preferences\n                    )\n                )\n            except ValueError:\n                raise UnsupportedWheel(\n                    f\"{wheel.filename} is not a supported wheel for this platform. It \"\n                    \"can't be sorted.\"\n                )\n            if self._prefer_binary:\n                binary_preference = 1\n            if wheel.build_tag is not None:\n                match = re.match(r\"^(\\d+)(.*)$\", wheel.build_tag)\n                assert match is not None, \"guaranteed by filename validation\"\n                build_tag_groups = match.groups()\n                build_tag = (int(build_tag_groups[0]), build_tag_groups[1])\n        else:  # sdist\n            pri = -(support_num)\n        has_allowed_hash = int(link.is_hash_allowed(self._hashes))\n        yank_value = -1 * int(link.is_yanked)  # -1 for yanked.\n        return (\n            has_allowed_hash,\n            yank_value,\n            binary_preference,\n            candidate.version,\n            pri,\n            build_tag,\n        )\n\n    def sort_best_candidate(\n        self,\n        candidates: List[InstallationCandidate],\n    ) -> Optional[InstallationCandidate]:\n        \"\"\"\n        Return the best candidate per the instance's sort order, or None if\n        no candidate is acceptable.\n        \"\"\"\n        if not candidates:\n            return None\n        best_candidate = max(candidates, key=self._sort_key)\n        return best_candidate\n\n    def compute_best_candidate(\n        self,\n        candidates: List[InstallationCandidate],\n    ) -> BestCandidateResult:\n        \"\"\"\n        Compute and return a `BestCandidateResult` instance.\n        \"\"\"\n        applicable_candidates = self.get_applicable_candidates(candidates)\n\n        best_candidate = self.sort_best_candidate(applicable_candidates)\n\n        return BestCandidateResult(\n            candidates,\n            applicable_candidates=applicable_candidates,\n            best_candidate=best_candidate,\n        )\n\n\nclass PackageFinder:\n    \"\"\"This finds packages.\n\n    This is meant to match easy_install's technique for looking for\n    packages, by reading pages and looking for appropriate links.\n    \"\"\"\n\n    def __init__(\n        self,\n        link_collector: LinkCollector,\n        target_python: TargetPython,\n        allow_yanked: bool,\n        format_control: Optional[FormatControl] = None,\n        candidate_prefs: Optional[CandidatePreferences] = None,\n        ignore_requires_python: Optional[bool] = None,\n    ) -> None:\n        \"\"\"\n        This constructor is primarily meant to be used by the create() class\n        method and from tests.\n\n        :param format_control: A FormatControl object, used to control\n            the selection of source packages / binary packages when consulting\n            the index and links.\n        :param candidate_prefs: Options to use when creating a\n            CandidateEvaluator object.\n        \"\"\"\n        if candidate_prefs is None:\n            candidate_prefs = CandidatePreferences()\n\n        format_control = format_control or FormatControl(set(), set())\n\n        self._allow_yanked = allow_yanked\n        self._candidate_prefs = candidate_prefs\n        self._ignore_requires_python = ignore_requires_python\n        self._link_collector = link_collector\n        self._target_python = target_python\n\n        self.format_control = format_control\n\n        # These are boring links that have already been logged somehow.\n        self._logged_links: Set[Tuple[Link, LinkType, str]] = set()\n\n    # Don't include an allow_yanked default value to make sure each call\n    # site considers whether yanked releases are allowed. This also causes\n    # that decision to be made explicit in the calling code, which helps\n    # people when reading the code.\n    @classmethod\n    def create(\n        cls,\n        link_collector: LinkCollector,\n        selection_prefs: SelectionPreferences,\n        target_python: Optional[TargetPython] = None,\n    ) -> \"PackageFinder\":\n        \"\"\"Create a PackageFinder.\n\n        :param selection_prefs: The candidate selection preferences, as a\n            SelectionPreferences object.\n        :param target_python: The target Python interpreter to use when\n            checking compatibility. If None (the default), a TargetPython\n            object will be constructed from the running Python.\n        \"\"\"\n        if target_python is None:\n            target_python = TargetPython()\n\n        candidate_prefs = CandidatePreferences(\n            prefer_binary=selection_prefs.prefer_binary,\n            allow_all_prereleases=selection_prefs.allow_all_prereleases,\n        )\n\n        return cls(\n            candidate_prefs=candidate_prefs,\n            link_collector=link_collector,\n            target_python=target_python,\n            allow_yanked=selection_prefs.allow_yanked,\n            format_control=selection_prefs.format_control,\n            ignore_requires_python=selection_prefs.ignore_requires_python,\n        )\n\n    @property\n    def target_python(self) -> TargetPython:\n        return self._target_python\n\n    @property\n    def search_scope(self) -> SearchScope:\n        return self._link_collector.search_scope\n\n    @search_scope.setter\n    def search_scope(self, search_scope: SearchScope) -> None:\n        self._link_collector.search_scope = search_scope\n\n    @property\n    def find_links(self) -> List[str]:\n        return self._link_collector.find_links\n\n    @property\n    def index_urls(self) -> List[str]:\n        return self.search_scope.index_urls\n\n    @property\n    def proxy(self) -> Optional[str]:\n        return self._link_collector.session.pip_proxy\n\n    @property\n    def trusted_hosts(self) -> Iterable[str]:\n        for host_port in self._link_collector.session.pip_trusted_origins:\n            yield build_netloc(*host_port)\n\n    @property\n    def custom_cert(self) -> Optional[str]:\n        # session.verify is either a boolean (use default bundle/no SSL\n        # verification) or a string path to a custom CA bundle to use. We only\n        # care about the latter.\n        verify = self._link_collector.session.verify\n        return verify if isinstance(verify, str) else None\n\n    @property\n    def client_cert(self) -> Optional[str]:\n        cert = self._link_collector.session.cert\n        assert not isinstance(cert, tuple), \"pip only supports PEM client certs\"\n        return cert\n\n    @property\n    def allow_all_prereleases(self) -> bool:\n        return self._candidate_prefs.allow_all_prereleases\n\n    def set_allow_all_prereleases(self) -> None:\n        self._candidate_prefs.allow_all_prereleases = True\n\n    @property\n    def prefer_binary(self) -> bool:\n        return self._candidate_prefs.prefer_binary\n\n    def set_prefer_binary(self) -> None:\n        self._candidate_prefs.prefer_binary = True\n\n    def requires_python_skipped_reasons(self) -> List[str]:\n        reasons = {\n            detail\n            for _, result, detail in self._logged_links\n            if result == LinkType.requires_python_mismatch\n        }\n        return sorted(reasons)\n\n    def make_link_evaluator(self, project_name: str) -> LinkEvaluator:\n        canonical_name = canonicalize_name(project_name)\n        formats = self.format_control.get_allowed_formats(canonical_name)\n\n        return LinkEvaluator(\n            project_name=project_name,\n            canonical_name=canonical_name,\n            formats=formats,\n            target_python=self._target_python,\n            allow_yanked=self._allow_yanked,\n            ignore_requires_python=self._ignore_requires_python,\n        )\n\n    def _sort_links(self, links: Iterable[Link]) -> List[Link]:\n        \"\"\"\n        Returns elements of links in order, non-egg links first, egg links\n        second, while eliminating duplicates\n        \"\"\"\n        eggs, no_eggs = [], []\n        seen: Set[Link] = set()\n        for link in links:\n            if link not in seen:\n                seen.add(link)\n                if link.egg_fragment:\n                    eggs.append(link)\n                else:\n                    no_eggs.append(link)\n        return no_eggs + eggs\n\n    def _log_skipped_link(self, link: Link, result: LinkType, detail: str) -> None:\n        # This is a hot method so don't waste time hashing links unless we're\n        # actually going to log 'em.\n        if not logger.isEnabledFor(logging.DEBUG):\n            return\n\n        entry = (link, result, detail)\n        if entry not in self._logged_links:\n            # Put the link at the end so the reason is more visible and because\n            # the link string is usually very long.\n            logger.debug(\"Skipping link: %s: %s\", detail, link)\n            self._logged_links.add(entry)\n\n    def get_install_candidate(\n        self, link_evaluator: LinkEvaluator, link: Link\n    ) -> Optional[InstallationCandidate]:\n        \"\"\"\n        If the link is a candidate for install, convert it to an\n        InstallationCandidate and return it. Otherwise, return None.\n        \"\"\"\n        result, detail = link_evaluator.evaluate_link(link)\n        if result != LinkType.candidate:\n            self._log_skipped_link(link, result, detail)\n            return None\n\n        try:\n            return InstallationCandidate(\n                name=link_evaluator.project_name,\n                link=link,\n                version=detail,\n            )\n        except InvalidVersion:\n            return None\n\n    def evaluate_links(\n        self, link_evaluator: LinkEvaluator, links: Iterable[Link]\n    ) -> List[InstallationCandidate]:\n        \"\"\"\n        Convert links that are candidates to InstallationCandidate objects.\n        \"\"\"\n        candidates = []\n        for link in self._sort_links(links):\n            candidate = self.get_install_candidate(link_evaluator, link)\n            if candidate is not None:\n                candidates.append(candidate)\n\n        return candidates\n\n    def process_project_url(\n        self, project_url: Link, link_evaluator: LinkEvaluator\n    ) -> List[InstallationCandidate]:\n        logger.debug(\n            \"Fetching project page and analyzing links: %s\",\n            project_url,\n        )\n        index_response = self._link_collector.fetch_response(project_url)\n        if index_response is None:\n            return []\n\n        page_links = list(parse_links(index_response))\n\n        with indent_log():\n            package_links = self.evaluate_links(\n                link_evaluator,\n                links=page_links,\n            )\n\n        return package_links\n\n    @functools.lru_cache(maxsize=None)\n    def find_all_candidates(self, project_name: str) -> List[InstallationCandidate]:\n        \"\"\"Find all available InstallationCandidate for project_name\n\n        This checks index_urls and find_links.\n        All versions found are returned as an InstallationCandidate list.\n\n        See LinkEvaluator.evaluate_link() for details on which files\n        are accepted.\n        \"\"\"\n        link_evaluator = self.make_link_evaluator(project_name)\n\n        collected_sources = self._link_collector.collect_sources(\n            project_name=project_name,\n            candidates_from_page=functools.partial(\n                self.process_project_url,\n                link_evaluator=link_evaluator,\n            ),\n        )\n\n        page_candidates_it = itertools.chain.from_iterable(\n            source.page_candidates()\n            for sources in collected_sources\n            for source in sources\n            if source is not None\n        )\n        page_candidates = list(page_candidates_it)\n\n        file_links_it = itertools.chain.from_iterable(\n            source.file_links()\n            for sources in collected_sources\n            for source in sources\n            if source is not None\n        )\n        file_candidates = self.evaluate_links(\n            link_evaluator,\n            sorted(file_links_it, reverse=True),\n        )\n\n        if logger.isEnabledFor(logging.DEBUG) and file_candidates:\n            paths = []\n            for candidate in file_candidates:\n                assert candidate.link.url  # we need to have a URL\n                try:\n                    paths.append(candidate.link.file_path)\n                except Exception:\n                    paths.append(candidate.link.url)  # it's not a local file\n\n            logger.debug(\"Local files found: %s\", \", \".join(paths))\n\n        # This is an intentional priority ordering\n        return file_candidates + page_candidates\n\n    def make_candidate_evaluator(\n        self,\n        project_name: str,\n        specifier: Optional[specifiers.BaseSpecifier] = None,\n        hashes: Optional[Hashes] = None,\n    ) -> CandidateEvaluator:\n        \"\"\"Create a CandidateEvaluator object to use.\"\"\"\n        candidate_prefs = self._candidate_prefs\n        return CandidateEvaluator.create(\n            project_name=project_name,\n            target_python=self._target_python,\n            prefer_binary=candidate_prefs.prefer_binary,\n            allow_all_prereleases=candidate_prefs.allow_all_prereleases,\n            specifier=specifier,\n            hashes=hashes,\n        )\n\n    @functools.lru_cache(maxsize=None)\n    def find_best_candidate(\n        self,\n        project_name: str,\n        specifier: Optional[specifiers.BaseSpecifier] = None,\n        hashes: Optional[Hashes] = None,\n    ) -> BestCandidateResult:\n        \"\"\"Find matches for the given project and specifier.\n\n        :param specifier: An optional object implementing `filter`\n            (e.g. `packaging.specifiers.SpecifierSet`) to filter applicable\n            versions.\n\n        :return: A `BestCandidateResult` instance.\n        \"\"\"\n        candidates = self.find_all_candidates(project_name)\n        candidate_evaluator = self.make_candidate_evaluator(\n            project_name=project_name,\n            specifier=specifier,\n            hashes=hashes,\n        )\n        return candidate_evaluator.compute_best_candidate(candidates)\n\n    def find_requirement(\n        self, req: InstallRequirement, upgrade: bool\n    ) -> Optional[InstallationCandidate]:\n        \"\"\"Try to find a Link matching req\n\n        Expects req, an InstallRequirement and upgrade, a boolean\n        Returns a InstallationCandidate if found,\n        Raises DistributionNotFound or BestVersionAlreadyInstalled otherwise\n        \"\"\"\n        hashes = req.hashes(trust_internet=False)\n        best_candidate_result = self.find_best_candidate(\n            req.name,\n            specifier=req.specifier,\n            hashes=hashes,\n        )\n        best_candidate = best_candidate_result.best_candidate\n\n        installed_version: Optional[_BaseVersion] = None\n        if req.satisfied_by is not None:\n            installed_version = req.satisfied_by.version\n\n        def _format_versions(cand_iter: Iterable[InstallationCandidate]) -> str:\n            # This repeated parse_version and str() conversion is needed to\n            # handle different vendoring sources from pip and pkg_resources.\n            # If we stop using the pkg_resources provided specifier and start\n            # using our own, we can drop the cast to str().\n            return (\n                \", \".join(\n                    sorted(\n                        {str(c.version) for c in cand_iter},\n                        key=parse_version,\n                    )\n                )\n                or \"none\"\n            )\n\n        if installed_version is None and best_candidate is None:\n            logger.critical(\n                \"Could not find a version that satisfies the requirement %s \"\n                \"(from versions: %s)\",\n                req,\n                _format_versions(best_candidate_result.all_candidates),\n            )\n\n            raise DistributionNotFound(f\"No matching distribution found for {req}\")\n\n        def _should_install_candidate(\n            candidate: Optional[InstallationCandidate],\n        ) -> \"TypeGuard[InstallationCandidate]\":\n            if installed_version is None:\n                return True\n            if best_candidate is None:\n                return False\n            return best_candidate.version > installed_version\n\n        if not upgrade and installed_version is not None:\n            if _should_install_candidate(best_candidate):\n                logger.debug(\n                    \"Existing installed version (%s) satisfies requirement \"\n                    \"(most up-to-date version is %s)\",\n                    installed_version,\n                    best_candidate.version,\n                )\n            else:\n                logger.debug(\n                    \"Existing installed version (%s) is most up-to-date and \"\n                    \"satisfies requirement\",\n                    installed_version,\n                )\n            return None\n\n        if _should_install_candidate(best_candidate):\n            logger.debug(\n                \"Using version %s (newest of versions: %s)\",\n                best_candidate.version,\n                _format_versions(best_candidate_result.applicable_candidates),\n            )\n            return best_candidate\n\n        # We have an existing version, and its the best version\n        logger.debug(\n            \"Installed version (%s) is most up-to-date (past versions: %s)\",\n            installed_version,\n            _format_versions(best_candidate_result.applicable_candidates),\n        )\n        raise BestVersionAlreadyInstalled\n\n\ndef _find_name_version_sep(fragment: str, canonical_name: str) -> int:\n    \"\"\"Find the separator's index based on the package's canonical name.\n\n    :param fragment: A <package>+<version> filename \"fragment\" (stem) or\n        egg fragment.\n    :param canonical_name: The package's canonical name.\n\n    This function is needed since the canonicalized name does not necessarily\n    have the same length as the egg info's name part. An example::\n\n    >>> fragment = 'foo__bar-1.0'\n    >>> canonical_name = 'foo-bar'\n    >>> _find_name_version_sep(fragment, canonical_name)\n    8\n    \"\"\"\n    # Project name and version must be separated by one single dash. Find all\n    # occurrences of dashes; if the string in front of it matches the canonical\n    # name, this is the one separating the name and version parts.\n    for i, c in enumerate(fragment):\n        if c != \"-\":\n            continue\n        if canonicalize_name(fragment[:i]) == canonical_name:\n            return i\n    raise ValueError(f\"{fragment} does not match {canonical_name}\")\n\n\ndef _extract_version_from_fragment(fragment: str, canonical_name: str) -> Optional[str]:\n    \"\"\"Parse the version string from a <package>+<version> filename\n    \"fragment\" (stem) or egg fragment.\n\n    :param fragment: The string to parse. E.g. foo-2.1\n    :param canonical_name: The canonicalized name of the package this\n        belongs to.\n    \"\"\"\n    try:\n        version_start = _find_name_version_sep(fragment, canonical_name) + 1\n    except ValueError:\n        return None\n    version = fragment[version_start:]\n    if not version:\n        return None\n    return version\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/index/sources.py","size":8632,"sha1":"5889a7db308185ad1f73d3ac5ab446f4b3a1cf9c","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import logging\nimport mimetypes\nimport os\nfrom collections import defaultdict\nfrom typing import Callable, Dict, Iterable, List, Optional, Tuple\n\nfrom pip._vendor.packaging.utils import (\n    InvalidSdistFilename,\n    InvalidWheelFilename,\n    canonicalize_name,\n    parse_sdist_filename,\n    parse_wheel_filename,\n)\n\nfrom pip._internal.models.candidate import InstallationCandidate\nfrom pip._internal.models.link import Link\nfrom pip._internal.utils.urls import path_to_url, url_to_path\nfrom pip._internal.vcs import is_url\n\nlogger = logging.getLogger(__name__)\n\nFoundCandidates = Iterable[InstallationCandidate]\nFoundLinks = Iterable[Link]\nCandidatesFromPage = Callable[[Link], Iterable[InstallationCandidate]]\nPageValidator = Callable[[Link], bool]\n\n\nclass LinkSource:\n    @property\n    def link(self) -> Optional[Link]:\n        \"\"\"Returns the underlying link, if there's one.\"\"\"\n        raise NotImplementedError()\n\n    def page_candidates(self) -> FoundCandidates:\n        \"\"\"Candidates found by parsing an archive listing HTML file.\"\"\"\n        raise NotImplementedError()\n\n    def file_links(self) -> FoundLinks:\n        \"\"\"Links found by specifying archives directly.\"\"\"\n        raise NotImplementedError()\n\n\ndef _is_html_file(file_url: str) -> bool:\n    return mimetypes.guess_type(file_url, strict=False)[0] == \"text/html\"\n\n\nclass _FlatDirectoryToUrls:\n    \"\"\"Scans directory and caches results\"\"\"\n\n    def __init__(self, path: str) -> None:\n        self._path = path\n        self._page_candidates: List[str] = []\n        self._project_name_to_urls: Dict[str, List[str]] = defaultdict(list)\n        self._scanned_directory = False\n\n    def _scan_directory(self) -> None:\n        \"\"\"Scans directory once and populates both page_candidates\n        and project_name_to_urls at the same time\n        \"\"\"\n        for entry in os.scandir(self._path):\n            url = path_to_url(entry.path)\n            if _is_html_file(url):\n                self._page_candidates.append(url)\n                continue\n\n            # File must have a valid wheel or sdist name,\n            # otherwise not worth considering as a package\n            try:\n                project_filename = parse_wheel_filename(entry.name)[0]\n            except InvalidWheelFilename:\n                try:\n                    project_filename = parse_sdist_filename(entry.name)[0]\n                except InvalidSdistFilename:\n                    continue\n\n            self._project_name_to_urls[project_filename].append(url)\n        self._scanned_directory = True\n\n    @property\n    def page_candidates(self) -> List[str]:\n        if not self._scanned_directory:\n            self._scan_directory()\n\n        return self._page_candidates\n\n    @property\n    def project_name_to_urls(self) -> Dict[str, List[str]]:\n        if not self._scanned_directory:\n            self._scan_directory()\n\n        return self._project_name_to_urls\n\n\nclass _FlatDirectorySource(LinkSource):\n    \"\"\"Link source specified by ``--find-links=<path-to-dir>``.\n\n    This looks the content of the directory, and returns:\n\n    * ``page_candidates``: Links listed on each HTML file in the directory.\n    * ``file_candidates``: Archives in the directory.\n    \"\"\"\n\n    _paths_to_urls: Dict[str, _FlatDirectoryToUrls] = {}\n\n    def __init__(\n        self,\n        candidates_from_page: CandidatesFromPage,\n        path: str,\n        project_name: str,\n    ) -> None:\n        self._candidates_from_page = candidates_from_page\n        self._project_name = canonicalize_name(project_name)\n\n        # Get existing instance of _FlatDirectoryToUrls if it exists\n        if path in self._paths_to_urls:\n            self._path_to_urls = self._paths_to_urls[path]\n        else:\n            self._path_to_urls = _FlatDirectoryToUrls(path=path)\n            self._paths_to_urls[path] = self._path_to_urls\n\n    @property\n    def link(self) -> Optional[Link]:\n        return None\n\n    def page_candidates(self) -> FoundCandidates:\n        for url in self._path_to_urls.page_candidates:\n            yield from self._candidates_from_page(Link(url))\n\n    def file_links(self) -> FoundLinks:\n        for url in self._path_to_urls.project_name_to_urls[self._project_name]:\n            yield Link(url)\n\n\nclass _LocalFileSource(LinkSource):\n    \"\"\"``--find-links=<path-or-url>`` or ``--[extra-]index-url=<path-or-url>``.\n\n    If a URL is supplied, it must be a ``file:`` URL. If a path is supplied to\n    the option, it is converted to a URL first. This returns:\n\n    * ``page_candidates``: Links listed on an HTML file.\n    * ``file_candidates``: The non-HTML file.\n    \"\"\"\n\n    def __init__(\n        self,\n        candidates_from_page: CandidatesFromPage,\n        link: Link,\n    ) -> None:\n        self._candidates_from_page = candidates_from_page\n        self._link = link\n\n    @property\n    def link(self) -> Optional[Link]:\n        return self._link\n\n    def page_candidates(self) -> FoundCandidates:\n        if not _is_html_file(self._link.url):\n            return\n        yield from self._candidates_from_page(self._link)\n\n    def file_links(self) -> FoundLinks:\n        if _is_html_file(self._link.url):\n            return\n        yield self._link\n\n\nclass _RemoteFileSource(LinkSource):\n    \"\"\"``--find-links=<url>`` or ``--[extra-]index-url=<url>``.\n\n    This returns:\n\n    * ``page_candidates``: Links listed on an HTML file.\n    * ``file_candidates``: The non-HTML file.\n    \"\"\"\n\n    def __init__(\n        self,\n        candidates_from_page: CandidatesFromPage,\n        page_validator: PageValidator,\n        link: Link,\n    ) -> None:\n        self._candidates_from_page = candidates_from_page\n        self._page_validator = page_validator\n        self._link = link\n\n    @property\n    def link(self) -> Optional[Link]:\n        return self._link\n\n    def page_candidates(self) -> FoundCandidates:\n        if not self._page_validator(self._link):\n            return\n        yield from self._candidates_from_page(self._link)\n\n    def file_links(self) -> FoundLinks:\n        yield self._link\n\n\nclass _IndexDirectorySource(LinkSource):\n    \"\"\"``--[extra-]index-url=<path-to-directory>``.\n\n    This is treated like a remote URL; ``candidates_from_page`` contains logic\n    for this by appending ``index.html`` to the link.\n    \"\"\"\n\n    def __init__(\n        self,\n        candidates_from_page: CandidatesFromPage,\n        link: Link,\n    ) -> None:\n        self._candidates_from_page = candidates_from_page\n        self._link = link\n\n    @property\n    def link(self) -> Optional[Link]:\n        return self._link\n\n    def page_candidates(self) -> FoundCandidates:\n        yield from self._candidates_from_page(self._link)\n\n    def file_links(self) -> FoundLinks:\n        return ()\n\n\ndef build_source(\n    location: str,\n    *,\n    candidates_from_page: CandidatesFromPage,\n    page_validator: PageValidator,\n    expand_dir: bool,\n    cache_link_parsing: bool,\n    project_name: str,\n) -> Tuple[Optional[str], Optional[LinkSource]]:\n    path: Optional[str] = None\n    url: Optional[str] = None\n    if os.path.exists(location):  # Is a local path.\n        url = path_to_url(location)\n        path = location\n    elif location.startswith(\"file:\"):  # A file: URL.\n        url = location\n        path = url_to_path(location)\n    elif is_url(location):\n        url = location\n\n    if url is None:\n        msg = (\n            \"Location '%s' is ignored: \"\n            \"it is either a non-existing path or lacks a specific scheme.\"\n        )\n        logger.warning(msg, location)\n        return (None, None)\n\n    if path is None:\n        source: LinkSource = _RemoteFileSource(\n            candidates_from_page=candidates_from_page,\n            page_validator=page_validator,\n            link=Link(url, cache_link_parsing=cache_link_parsing),\n        )\n        return (url, source)\n\n    if os.path.isdir(path):\n        if expand_dir:\n            source = _FlatDirectorySource(\n                candidates_from_page=candidates_from_page,\n                path=path,\n                project_name=project_name,\n            )\n        else:\n            source = _IndexDirectorySource(\n                candidates_from_page=candidates_from_page,\n                link=Link(url, cache_link_parsing=cache_link_parsing),\n            )\n        return (url, source)\n    elif os.path.isfile(path):\n        source = _LocalFileSource(\n            candidates_from_page=candidates_from_page,\n            link=Link(url, cache_link_parsing=cache_link_parsing),\n        )\n        return (url, source)\n    logger.warning(\n        \"Location '%s' is ignored: it is neither a file nor a directory.\",\n        location,\n    )\n    return (url, None)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/locations/__init__.py","size":14925,"sha1":"f4ffe215d2b76b129112b5824927561d404fadb9","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import functools\nimport logging\nimport os\nimport pathlib\nimport sys\nimport sysconfig\nfrom typing import Any, Dict, Generator, Optional, Tuple\n\nfrom pip._internal.models.scheme import SCHEME_KEYS, Scheme\nfrom pip._internal.utils.compat import WINDOWS\nfrom pip._internal.utils.deprecation import deprecated\nfrom pip._internal.utils.virtualenv import running_under_virtualenv\n\nfrom . import _sysconfig\nfrom .base import (\n    USER_CACHE_DIR,\n    get_major_minor_version,\n    get_src_prefix,\n    is_osx_framework,\n    site_packages,\n    user_site,\n)\n\n__all__ = [\n    \"USER_CACHE_DIR\",\n    \"get_bin_prefix\",\n    \"get_bin_user\",\n    \"get_major_minor_version\",\n    \"get_platlib\",\n    \"get_purelib\",\n    \"get_scheme\",\n    \"get_src_prefix\",\n    \"site_packages\",\n    \"user_site\",\n]\n\n\nlogger = logging.getLogger(__name__)\n\n\n_PLATLIBDIR: str = getattr(sys, \"platlibdir\", \"lib\")\n\n_USE_SYSCONFIG_DEFAULT = sys.version_info >= (3, 10)\n\n\ndef _should_use_sysconfig() -> bool:\n    \"\"\"This function determines the value of _USE_SYSCONFIG.\n\n    By default, pip uses sysconfig on Python 3.10+.\n    But Python distributors can override this decision by setting:\n        sysconfig._PIP_USE_SYSCONFIG = True / False\n    Rationale in https://github.com/pypa/pip/issues/10647\n\n    This is a function for testability, but should be constant during any one\n    run.\n    \"\"\"\n    return bool(getattr(sysconfig, \"_PIP_USE_SYSCONFIG\", _USE_SYSCONFIG_DEFAULT))\n\n\n_USE_SYSCONFIG = _should_use_sysconfig()\n\nif not _USE_SYSCONFIG:\n    # Import distutils lazily to avoid deprecation warnings,\n    # but import it soon enough that it is in memory and available during\n    # a pip reinstall.\n    from . import _distutils\n\n# Be noisy about incompatibilities if this platforms \"should\" be using\n# sysconfig, but is explicitly opting out and using distutils instead.\nif _USE_SYSCONFIG_DEFAULT and not _USE_SYSCONFIG:\n    _MISMATCH_LEVEL = logging.WARNING\nelse:\n    _MISMATCH_LEVEL = logging.DEBUG\n\n\ndef _looks_like_bpo_44860() -> bool:\n    \"\"\"The resolution to bpo-44860 will change this incorrect platlib.\n\n    See <https://bugs.python.org/issue44860>.\n    \"\"\"\n    from distutils.command.install import INSTALL_SCHEMES\n\n    try:\n        unix_user_platlib = INSTALL_SCHEMES[\"unix_user\"][\"platlib\"]\n    except KeyError:\n        return False\n    return unix_user_platlib == \"$usersite\"\n\n\ndef _looks_like_red_hat_patched_platlib_purelib(scheme: Dict[str, str]) -> bool:\n    platlib = scheme[\"platlib\"]\n    if \"/$platlibdir/\" in platlib:\n        platlib = platlib.replace(\"/$platlibdir/\", f\"/{_PLATLIBDIR}/\")\n    if \"/lib64/\" not in platlib:\n        return False\n    unpatched = platlib.replace(\"/lib64/\", \"/lib/\")\n    return unpatched.replace(\"$platbase/\", \"$base/\") == scheme[\"purelib\"]\n\n\n@functools.lru_cache(maxsize=None)\ndef _looks_like_red_hat_lib() -> bool:\n    \"\"\"Red Hat patches platlib in unix_prefix and unix_home, but not purelib.\n\n    This is the only way I can see to tell a Red Hat-patched Python.\n    \"\"\"\n    from distutils.command.install import INSTALL_SCHEMES\n\n    return all(\n        k in INSTALL_SCHEMES\n        and _looks_like_red_hat_patched_platlib_purelib(INSTALL_SCHEMES[k])\n        for k in (\"unix_prefix\", \"unix_home\")\n    )\n\n\n@functools.lru_cache(maxsize=None)\ndef _looks_like_debian_scheme() -> bool:\n    \"\"\"Debian adds two additional schemes.\"\"\"\n    from distutils.command.install import INSTALL_SCHEMES\n\n    return \"deb_system\" in INSTALL_SCHEMES and \"unix_local\" in INSTALL_SCHEMES\n\n\n@functools.lru_cache(maxsize=None)\ndef _looks_like_red_hat_scheme() -> bool:\n    \"\"\"Red Hat patches ``sys.prefix`` and ``sys.exec_prefix``.\n\n    Red Hat's ``00251-change-user-install-location.patch`` changes the install\n    command's ``prefix`` and ``exec_prefix`` to append ``\"/local\"``. This is\n    (fortunately?) done quite unconditionally, so we create a default command\n    object without any configuration to detect this.\n    \"\"\"\n    from distutils.command.install import install\n    from distutils.dist import Distribution\n\n    cmd: Any = install(Distribution())\n    cmd.finalize_options()\n    return (\n        cmd.exec_prefix == f\"{os.path.normpath(sys.exec_prefix)}/local\"\n        and cmd.prefix == f\"{os.path.normpath(sys.prefix)}/local\"\n    )\n\n\n@functools.lru_cache(maxsize=None)\ndef _looks_like_slackware_scheme() -> bool:\n    \"\"\"Slackware patches sysconfig but fails to patch distutils and site.\n\n    Slackware changes sysconfig's user scheme to use ``\"lib64\"`` for the lib\n    path, but does not do the same to the site module.\n    \"\"\"\n    if user_site is None:  # User-site not available.\n        return False\n    try:\n        paths = sysconfig.get_paths(scheme=\"posix_user\", expand=False)\n    except KeyError:  # User-site not available.\n        return False\n    return \"/lib64/\" in paths[\"purelib\"] and \"/lib64/\" not in user_site\n\n\n@functools.lru_cache(maxsize=None)\ndef _looks_like_msys2_mingw_scheme() -> bool:\n    \"\"\"MSYS2 patches distutils and sysconfig to use a UNIX-like scheme.\n\n    However, MSYS2 incorrectly patches sysconfig ``nt`` scheme. The fix is\n    likely going to be included in their 3.10 release, so we ignore the warning.\n    See msys2/MINGW-packages#9319.\n\n    MSYS2 MINGW's patch uses lowercase ``\"lib\"`` instead of the usual uppercase,\n    and is missing the final ``\"site-packages\"``.\n    \"\"\"\n    paths = sysconfig.get_paths(\"nt\", expand=False)\n    return all(\n        \"Lib\" not in p and \"lib\" in p and not p.endswith(\"site-packages\")\n        for p in (paths[key] for key in (\"platlib\", \"purelib\"))\n    )\n\n\ndef _fix_abiflags(parts: Tuple[str]) -> Generator[str, None, None]:\n    ldversion = sysconfig.get_config_var(\"LDVERSION\")\n    abiflags = getattr(sys, \"abiflags\", None)\n\n    # LDVERSION does not end with sys.abiflags. Just return the path unchanged.\n    if not ldversion or not abiflags or not ldversion.endswith(abiflags):\n        yield from parts\n        return\n\n    # Strip sys.abiflags from LDVERSION-based path components.\n    for part in parts:\n        if part.endswith(ldversion):\n            part = part[: (0 - len(abiflags))]\n        yield part\n\n\n@functools.lru_cache(maxsize=None)\ndef _warn_mismatched(old: pathlib.Path, new: pathlib.Path, *, key: str) -> None:\n    issue_url = \"https://github.com/pypa/pip/issues/10151\"\n    message = (\n        \"Value for %s does not match. Please report this to <%s>\"\n        \"\\ndistutils: %s\"\n        \"\\nsysconfig: %s\"\n    )\n    logger.log(_MISMATCH_LEVEL, message, key, issue_url, old, new)\n\n\ndef _warn_if_mismatch(old: pathlib.Path, new: pathlib.Path, *, key: str) -> bool:\n    if old == new:\n        return False\n    _warn_mismatched(old, new, key=key)\n    return True\n\n\n@functools.lru_cache(maxsize=None)\ndef _log_context(\n    *,\n    user: bool = False,\n    home: Optional[str] = None,\n    root: Optional[str] = None,\n    prefix: Optional[str] = None,\n) -> None:\n    parts = [\n        \"Additional context:\",\n        \"user = %r\",\n        \"home = %r\",\n        \"root = %r\",\n        \"prefix = %r\",\n    ]\n\n    logger.log(_MISMATCH_LEVEL, \"\\n\".join(parts), user, home, root, prefix)\n\n\ndef get_scheme(\n    dist_name: str,\n    user: bool = False,\n    home: Optional[str] = None,\n    root: Optional[str] = None,\n    isolated: bool = False,\n    prefix: Optional[str] = None,\n) -> Scheme:\n    new = _sysconfig.get_scheme(\n        dist_name,\n        user=user,\n        home=home,\n        root=root,\n        isolated=isolated,\n        prefix=prefix,\n    )\n    if _USE_SYSCONFIG:\n        return new\n\n    old = _distutils.get_scheme(\n        dist_name,\n        user=user,\n        home=home,\n        root=root,\n        isolated=isolated,\n        prefix=prefix,\n    )\n\n    warning_contexts = []\n    for k in SCHEME_KEYS:\n        old_v = pathlib.Path(getattr(old, k))\n        new_v = pathlib.Path(getattr(new, k))\n\n        if old_v == new_v:\n            continue\n\n        # distutils incorrectly put PyPy packages under ``site-packages/python``\n        # in the ``posix_home`` scheme, but PyPy devs said they expect the\n        # directory name to be ``pypy`` instead. So we treat this as a bug fix\n        # and not warn about it. See bpo-43307 and python/cpython#24628.\n        skip_pypy_special_case = (\n            sys.implementation.name == \"pypy\"\n            and home is not None\n            and k in (\"platlib\", \"purelib\")\n            and old_v.parent == new_v.parent\n            and old_v.name.startswith(\"python\")\n            and new_v.name.startswith(\"pypy\")\n        )\n        if skip_pypy_special_case:\n            continue\n\n        # sysconfig's ``osx_framework_user`` does not include ``pythonX.Y`` in\n        # the ``include`` value, but distutils's ``headers`` does. We'll let\n        # CPython decide whether this is a bug or feature. See bpo-43948.\n        skip_osx_framework_user_special_case = (\n            user\n            and is_osx_framework()\n            and k == \"headers\"\n            and old_v.parent.parent == new_v.parent\n            and old_v.parent.name.startswith(\"python\")\n        )\n        if skip_osx_framework_user_special_case:\n            continue\n\n        # On Red Hat and derived Linux distributions, distutils is patched to\n        # use \"lib64\" instead of \"lib\" for platlib.\n        if k == \"platlib\" and _looks_like_red_hat_lib():\n            continue\n\n        # On Python 3.9+, sysconfig's posix_user scheme sets platlib against\n        # sys.platlibdir, but distutils's unix_user incorrectly coninutes\n        # using the same $usersite for both platlib and purelib. This creates a\n        # mismatch when sys.platlibdir is not \"lib\".\n        skip_bpo_44860 = (\n            user\n            and k == \"platlib\"\n            and not WINDOWS\n            and sys.version_info >= (3, 9)\n            and _PLATLIBDIR != \"lib\"\n            and _looks_like_bpo_44860()\n        )\n        if skip_bpo_44860:\n            continue\n\n        # Slackware incorrectly patches posix_user to use lib64 instead of lib,\n        # but not usersite to match the location.\n        skip_slackware_user_scheme = (\n            user\n            and k in (\"platlib\", \"purelib\")\n            and not WINDOWS\n            and _looks_like_slackware_scheme()\n        )\n        if skip_slackware_user_scheme:\n            continue\n\n        # Both Debian and Red Hat patch Python to place the system site under\n        # /usr/local instead of /usr. Debian also places lib in dist-packages\n        # instead of site-packages, but the /usr/local check should cover it.\n        skip_linux_system_special_case = (\n            not (user or home or prefix or running_under_virtualenv())\n            and old_v.parts[1:3] == (\"usr\", \"local\")\n            and len(new_v.parts) > 1\n            and new_v.parts[1] == \"usr\"\n            and (len(new_v.parts) < 3 or new_v.parts[2] != \"local\")\n            and (_looks_like_red_hat_scheme() or _looks_like_debian_scheme())\n        )\n        if skip_linux_system_special_case:\n            continue\n\n        # MSYS2 MINGW's sysconfig patch does not include the \"site-packages\"\n        # part of the path. This is incorrect and will be fixed in MSYS.\n        skip_msys2_mingw_bug = (\n            WINDOWS and k in (\"platlib\", \"purelib\") and _looks_like_msys2_mingw_scheme()\n        )\n        if skip_msys2_mingw_bug:\n            continue\n\n        # CPython's POSIX install script invokes pip (via ensurepip) against the\n        # interpreter located in the source tree, not the install site. This\n        # triggers special logic in sysconfig that's not present in distutils.\n        # https://github.com/python/cpython/blob/8c21941ddaf/Lib/sysconfig.py#L178-L194\n        skip_cpython_build = (\n            sysconfig.is_python_build(check_home=True)\n            and not WINDOWS\n            and k in (\"headers\", \"include\", \"platinclude\")\n        )\n        if skip_cpython_build:\n            continue\n\n        warning_contexts.append((old_v, new_v, f\"scheme.{k}\"))\n\n    if not warning_contexts:\n        return old\n\n    # Check if this path mismatch is caused by distutils config files. Those\n    # files will no longer work once we switch to sysconfig, so this raises a\n    # deprecation message for them.\n    default_old = _distutils.distutils_scheme(\n        dist_name,\n        user,\n        home,\n        root,\n        isolated,\n        prefix,\n        ignore_config_files=True,\n    )\n    if any(default_old[k] != getattr(old, k) for k in SCHEME_KEYS):\n        deprecated(\n            reason=(\n                \"Configuring installation scheme with distutils config files \"\n                \"is deprecated and will no longer work in the near future. If you \"\n                \"are using a Homebrew or Linuxbrew Python, please see discussion \"\n                \"at https://github.com/Homebrew/homebrew-core/issues/76621\"\n            ),\n            replacement=None,\n            gone_in=None,\n        )\n        return old\n\n    # Post warnings about this mismatch so user can report them back.\n    for old_v, new_v, key in warning_contexts:\n        _warn_mismatched(old_v, new_v, key=key)\n    _log_context(user=user, home=home, root=root, prefix=prefix)\n\n    return old\n\n\ndef get_bin_prefix() -> str:\n    new = _sysconfig.get_bin_prefix()\n    if _USE_SYSCONFIG:\n        return new\n\n    old = _distutils.get_bin_prefix()\n    if _warn_if_mismatch(pathlib.Path(old), pathlib.Path(new), key=\"bin_prefix\"):\n        _log_context()\n    return old\n\n\ndef get_bin_user() -> str:\n    return _sysconfig.get_scheme(\"\", user=True).scripts\n\n\ndef _looks_like_deb_system_dist_packages(value: str) -> bool:\n    \"\"\"Check if the value is Debian's APT-controlled dist-packages.\n\n    Debian's ``distutils.sysconfig.get_python_lib()`` implementation returns the\n    default package path controlled by APT, but does not patch ``sysconfig`` to\n    do the same. This is similar to the bug worked around in ``get_scheme()``,\n    but here the default is ``deb_system`` instead of ``unix_local``. Ultimately\n    we can't do anything about this Debian bug, and this detection allows us to\n    skip the warning when needed.\n    \"\"\"\n    if not _looks_like_debian_scheme():\n        return False\n    if value == \"/usr/lib/python3/dist-packages\":\n        return True\n    return False\n\n\ndef get_purelib() -> str:\n    \"\"\"Return the default pure-Python lib location.\"\"\"\n    new = _sysconfig.get_purelib()\n    if _USE_SYSCONFIG:\n        return new\n\n    old = _distutils.get_purelib()\n    if _looks_like_deb_system_dist_packages(old):\n        return old\n    if _warn_if_mismatch(pathlib.Path(old), pathlib.Path(new), key=\"purelib\"):\n        _log_context()\n    return old\n\n\ndef get_platlib() -> str:\n    \"\"\"Return the default platform-shared lib location.\"\"\"\n    new = _sysconfig.get_platlib()\n    if _USE_SYSCONFIG:\n        return new\n\n    from . import _distutils\n\n    old = _distutils.get_platlib()\n    if _looks_like_deb_system_dist_packages(old):\n        return old\n    if _warn_if_mismatch(pathlib.Path(old), pathlib.Path(new), key=\"platlib\"):\n        _log_context()\n    return old\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/locations/_distutils.py","size":6013,"sha1":"78733a97f47e1d3cc2b31104fc993fbed566e61f","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"Locations where we look for configs, install stuff, etc\"\"\"\n\n# The following comment should be removed at some point in the future.\n# mypy: strict-optional=False\n\n# If pip's going to use distutils, it should not be using the copy that setuptools\n# might have injected into the environment. This is done by removing the injected\n# shim, if it's injected.\n#\n# See https://github.com/pypa/pip/issues/8761 for the original discussion and\n# rationale for why this is done within pip.\ntry:\n    __import__(\"_distutils_hack\").remove_shim()\nexcept (ImportError, AttributeError):\n    pass\n\nimport logging\nimport os\nimport sys\nfrom distutils.cmd import Command as DistutilsCommand\nfrom distutils.command.install import SCHEME_KEYS\nfrom distutils.command.install import install as distutils_install_command\nfrom distutils.sysconfig import get_python_lib\nfrom typing import Dict, List, Optional, Union\n\nfrom pip._internal.models.scheme import Scheme\nfrom pip._internal.utils.compat import WINDOWS\nfrom pip._internal.utils.virtualenv import running_under_virtualenv\n\nfrom .base import get_major_minor_version\n\nlogger = logging.getLogger(__name__)\n\n\ndef distutils_scheme(\n    dist_name: str,\n    user: bool = False,\n    home: Optional[str] = None,\n    root: Optional[str] = None,\n    isolated: bool = False,\n    prefix: Optional[str] = None,\n    *,\n    ignore_config_files: bool = False,\n) -> Dict[str, str]:\n    \"\"\"\n    Return a distutils install scheme\n    \"\"\"\n    from distutils.dist import Distribution\n\n    dist_args: Dict[str, Union[str, List[str]]] = {\"name\": dist_name}\n    if isolated:\n        dist_args[\"script_args\"] = [\"--no-user-cfg\"]\n\n    d = Distribution(dist_args)\n    if not ignore_config_files:\n        try:\n            d.parse_config_files()\n        except UnicodeDecodeError:\n            paths = d.find_config_files()\n            logger.warning(\n                \"Ignore distutils configs in %s due to encoding errors.\",\n                \", \".join(os.path.basename(p) for p in paths),\n            )\n    obj: Optional[DistutilsCommand] = None\n    obj = d.get_command_obj(\"install\", create=True)\n    assert obj is not None\n    i: distutils_install_command = obj\n    # NOTE: setting user or home has the side-effect of creating the home dir\n    # or user base for installations during finalize_options()\n    # ideally, we'd prefer a scheme class that has no side-effects.\n    assert not (user and prefix), f\"user={user} prefix={prefix}\"\n    assert not (home and prefix), f\"home={home} prefix={prefix}\"\n    i.user = user or i.user\n    if user or home:\n        i.prefix = \"\"\n    i.prefix = prefix or i.prefix\n    i.home = home or i.home\n    i.root = root or i.root\n    i.finalize_options()\n\n    scheme: Dict[str, str] = {}\n    for key in SCHEME_KEYS:\n        scheme[key] = getattr(i, \"install_\" + key)\n\n    # install_lib specified in setup.cfg should install *everything*\n    # into there (i.e. it takes precedence over both purelib and\n    # platlib).  Note, i.install_lib is *always* set after\n    # finalize_options(); we only want to override here if the user\n    # has explicitly requested it hence going back to the config\n    if \"install_lib\" in d.get_option_dict(\"install\"):\n        scheme.update({\"purelib\": i.install_lib, \"platlib\": i.install_lib})\n\n    if running_under_virtualenv():\n        if home:\n            prefix = home\n        elif user:\n            prefix = i.install_userbase\n        else:\n            prefix = i.prefix\n        scheme[\"headers\"] = os.path.join(\n            prefix,\n            \"include\",\n            \"site\",\n            f\"python{get_major_minor_version()}\",\n            dist_name,\n        )\n\n        if root is not None:\n            path_no_drive = os.path.splitdrive(os.path.abspath(scheme[\"headers\"]))[1]\n            scheme[\"headers\"] = os.path.join(root, path_no_drive[1:])\n\n    return scheme\n\n\ndef get_scheme(\n    dist_name: str,\n    user: bool = False,\n    home: Optional[str] = None,\n    root: Optional[str] = None,\n    isolated: bool = False,\n    prefix: Optional[str] = None,\n) -> Scheme:\n    \"\"\"\n    Get the \"scheme\" corresponding to the input parameters. The distutils\n    documentation provides the context for the available schemes:\n    https://docs.python.org/3/install/index.html#alternate-installation\n\n    :param dist_name: the name of the package to retrieve the scheme for, used\n        in the headers scheme path\n    :param user: indicates to use the \"user\" scheme\n    :param home: indicates to use the \"home\" scheme and provides the base\n        directory for the same\n    :param root: root under which other directories are re-based\n    :param isolated: equivalent to --no-user-cfg, i.e. do not consider\n        ~/.pydistutils.cfg (posix) or ~/pydistutils.cfg (non-posix) for\n        scheme paths\n    :param prefix: indicates to use the \"prefix\" scheme and provides the\n        base directory for the same\n    \"\"\"\n    scheme = distutils_scheme(dist_name, user, home, root, isolated, prefix)\n    return Scheme(\n        platlib=scheme[\"platlib\"],\n        purelib=scheme[\"purelib\"],\n        headers=scheme[\"headers\"],\n        scripts=scheme[\"scripts\"],\n        data=scheme[\"data\"],\n    )\n\n\ndef get_bin_prefix() -> str:\n    # XXX: In old virtualenv versions, sys.prefix can contain '..' components,\n    # so we need to call normpath to eliminate them.\n    prefix = os.path.normpath(sys.prefix)\n    if WINDOWS:\n        bin_py = os.path.join(prefix, \"Scripts\")\n        # buildout uses 'bin' on Windows too?\n        if not os.path.exists(bin_py):\n            bin_py = os.path.join(prefix, \"bin\")\n        return bin_py\n    # Forcing to use /usr/local/bin for standard macOS framework installs\n    # Also log to ~/Library/Logs/ for use with the Console.app log viewer\n    if sys.platform[:6] == \"darwin\" and prefix[:16] == \"/System/Library/\":\n        return \"/usr/local/bin\"\n    return os.path.join(prefix, \"bin\")\n\n\ndef get_purelib() -> str:\n    return get_python_lib(plat_specific=False)\n\n\ndef get_platlib() -> str:\n    return get_python_lib(plat_specific=True)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/locations/_sysconfig.py","size":7724,"sha1":"49c9f1bd1563b4174a4be0b72306875d7d38267e","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import logging\nimport os\nimport sys\nimport sysconfig\nimport typing\n\nfrom pip._internal.exceptions import InvalidSchemeCombination, UserInstallationInvalid\nfrom pip._internal.models.scheme import SCHEME_KEYS, Scheme\nfrom pip._internal.utils.virtualenv import running_under_virtualenv\n\nfrom .base import change_root, get_major_minor_version, is_osx_framework\n\nlogger = logging.getLogger(__name__)\n\n\n# Notes on _infer_* functions.\n# Unfortunately ``get_default_scheme()`` didn't exist before 3.10, so there's no\n# way to ask things like \"what is the '_prefix' scheme on this platform\". These\n# functions try to answer that with some heuristics while accounting for ad-hoc\n# platforms not covered by CPython's default sysconfig implementation. If the\n# ad-hoc implementation does not fully implement sysconfig, we'll fall back to\n# a POSIX scheme.\n\n_AVAILABLE_SCHEMES = set(sysconfig.get_scheme_names())\n\n_PREFERRED_SCHEME_API = getattr(sysconfig, \"get_preferred_scheme\", None)\n\n\ndef _should_use_osx_framework_prefix() -> bool:\n    \"\"\"Check for Apple's ``osx_framework_library`` scheme.\n\n    Python distributed by Apple's Command Line Tools has this special scheme\n    that's used when:\n\n    * This is a framework build.\n    * We are installing into the system prefix.\n\n    This does not account for ``pip install --prefix`` (also means we're not\n    installing to the system prefix), which should use ``posix_prefix``, but\n    logic here means ``_infer_prefix()`` outputs ``osx_framework_library``. But\n    since ``prefix`` is not available for ``sysconfig.get_default_scheme()``,\n    which is the stdlib replacement for ``_infer_prefix()``, presumably Apple\n    wouldn't be able to magically switch between ``osx_framework_library`` and\n    ``posix_prefix``. ``_infer_prefix()`` returning ``osx_framework_library``\n    means its behavior is consistent whether we use the stdlib implementation\n    or our own, and we deal with this special case in ``get_scheme()`` instead.\n    \"\"\"\n    return (\n        \"osx_framework_library\" in _AVAILABLE_SCHEMES\n        and not running_under_virtualenv()\n        and is_osx_framework()\n    )\n\n\ndef _infer_prefix() -> str:\n    \"\"\"Try to find a prefix scheme for the current platform.\n\n    This tries:\n\n    * A special ``osx_framework_library`` for Python distributed by Apple's\n      Command Line Tools, when not running in a virtual environment.\n    * Implementation + OS, used by PyPy on Windows (``pypy_nt``).\n    * Implementation without OS, used by PyPy on POSIX (``pypy``).\n    * OS + \"prefix\", used by CPython on POSIX (``posix_prefix``).\n    * Just the OS name, used by CPython on Windows (``nt``).\n\n    If none of the above works, fall back to ``posix_prefix``.\n    \"\"\"\n    if _PREFERRED_SCHEME_API:\n        return _PREFERRED_SCHEME_API(\"prefix\")\n    if _should_use_osx_framework_prefix():\n        return \"osx_framework_library\"\n    implementation_suffixed = f\"{sys.implementation.name}_{os.name}\"\n    if implementation_suffixed in _AVAILABLE_SCHEMES:\n        return implementation_suffixed\n    if sys.implementation.name in _AVAILABLE_SCHEMES:\n        return sys.implementation.name\n    suffixed = f\"{os.name}_prefix\"\n    if suffixed in _AVAILABLE_SCHEMES:\n        return suffixed\n    if os.name in _AVAILABLE_SCHEMES:  # On Windows, prefx is just called \"nt\".\n        return os.name\n    return \"posix_prefix\"\n\n\ndef _infer_user() -> str:\n    \"\"\"Try to find a user scheme for the current platform.\"\"\"\n    if _PREFERRED_SCHEME_API:\n        return _PREFERRED_SCHEME_API(\"user\")\n    if is_osx_framework() and not running_under_virtualenv():\n        suffixed = \"osx_framework_user\"\n    else:\n        suffixed = f\"{os.name}_user\"\n    if suffixed in _AVAILABLE_SCHEMES:\n        return suffixed\n    if \"posix_user\" not in _AVAILABLE_SCHEMES:  # User scheme unavailable.\n        raise UserInstallationInvalid()\n    return \"posix_user\"\n\n\ndef _infer_home() -> str:\n    \"\"\"Try to find a home for the current platform.\"\"\"\n    if _PREFERRED_SCHEME_API:\n        return _PREFERRED_SCHEME_API(\"home\")\n    suffixed = f\"{os.name}_home\"\n    if suffixed in _AVAILABLE_SCHEMES:\n        return suffixed\n    return \"posix_home\"\n\n\n# Update these keys if the user sets a custom home.\n_HOME_KEYS = [\n    \"installed_base\",\n    \"base\",\n    \"installed_platbase\",\n    \"platbase\",\n    \"prefix\",\n    \"exec_prefix\",\n]\nif sysconfig.get_config_var(\"userbase\") is not None:\n    _HOME_KEYS.append(\"userbase\")\n\n\ndef get_scheme(\n    dist_name: str,\n    user: bool = False,\n    home: typing.Optional[str] = None,\n    root: typing.Optional[str] = None,\n    isolated: bool = False,\n    prefix: typing.Optional[str] = None,\n) -> Scheme:\n    \"\"\"\n    Get the \"scheme\" corresponding to the input parameters.\n\n    :param dist_name: the name of the package to retrieve the scheme for, used\n        in the headers scheme path\n    :param user: indicates to use the \"user\" scheme\n    :param home: indicates to use the \"home\" scheme\n    :param root: root under which other directories are re-based\n    :param isolated: ignored, but kept for distutils compatibility (where\n        this controls whether the user-site pydistutils.cfg is honored)\n    :param prefix: indicates to use the \"prefix\" scheme and provides the\n        base directory for the same\n    \"\"\"\n    if user and prefix:\n        raise InvalidSchemeCombination(\"--user\", \"--prefix\")\n    if home and prefix:\n        raise InvalidSchemeCombination(\"--home\", \"--prefix\")\n\n    if home is not None:\n        scheme_name = _infer_home()\n    elif user:\n        scheme_name = _infer_user()\n    else:\n        scheme_name = _infer_prefix()\n\n    # Special case: When installing into a custom prefix, use posix_prefix\n    # instead of osx_framework_library. See _should_use_osx_framework_prefix()\n    # docstring for details.\n    if prefix is not None and scheme_name == \"osx_framework_library\":\n        scheme_name = \"posix_prefix\"\n\n    if home is not None:\n        variables = {k: home for k in _HOME_KEYS}\n    elif prefix is not None:\n        variables = {k: prefix for k in _HOME_KEYS}\n    else:\n        variables = {}\n\n    paths = sysconfig.get_paths(scheme=scheme_name, vars=variables)\n\n    # Logic here is very arbitrary, we're doing it for compatibility, don't ask.\n    # 1. Pip historically uses a special header path in virtual environments.\n    # 2. If the distribution name is not known, distutils uses 'UNKNOWN'. We\n    #    only do the same when not running in a virtual environment because\n    #    pip's historical header path logic (see point 1) did not do this.\n    if running_under_virtualenv():\n        if user:\n            base = variables.get(\"userbase\", sys.prefix)\n        else:\n            base = variables.get(\"base\", sys.prefix)\n        python_xy = f\"python{get_major_minor_version()}\"\n        paths[\"include\"] = os.path.join(base, \"include\", \"site\", python_xy)\n    elif not dist_name:\n        dist_name = \"UNKNOWN\"\n\n    scheme = Scheme(\n        platlib=paths[\"platlib\"],\n        purelib=paths[\"purelib\"],\n        headers=os.path.join(paths[\"include\"], dist_name),\n        scripts=paths[\"scripts\"],\n        data=paths[\"data\"],\n    )\n    if root is not None:\n        converted_keys = {}\n        for key in SCHEME_KEYS:\n            converted_keys[key] = change_root(root, getattr(scheme, key))\n        scheme = Scheme(**converted_keys)\n    return scheme\n\n\ndef get_bin_prefix() -> str:\n    # Forcing to use /usr/local/bin for standard macOS framework installs.\n    if sys.platform[:6] == \"darwin\" and sys.prefix[:16] == \"/System/Library/\":\n        return \"/usr/local/bin\"\n    return sysconfig.get_paths()[\"scripts\"]\n\n\ndef get_purelib() -> str:\n    return sysconfig.get_paths()[\"purelib\"]\n\n\ndef get_platlib() -> str:\n    return sysconfig.get_paths()[\"platlib\"]\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/locations/base.py","size":2556,"sha1":"b8e2670e06883b1ac1244f41eb9d63b50704c3ce","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import functools\nimport os\nimport site\nimport sys\nimport sysconfig\nimport typing\n\nfrom pip._internal.exceptions import InstallationError\nfrom pip._internal.utils import appdirs\nfrom pip._internal.utils.virtualenv import running_under_virtualenv\n\n# Application Directories\nUSER_CACHE_DIR = appdirs.user_cache_dir(\"pip\")\n\n# FIXME doesn't account for venv linked to global site-packages\nsite_packages: str = sysconfig.get_path(\"purelib\")\n\n\ndef get_major_minor_version() -> str:\n    \"\"\"\n    Return the major-minor version of the current Python as a string, e.g.\n    \"3.7\" or \"3.10\".\n    \"\"\"\n    return \"{}.{}\".format(*sys.version_info)\n\n\ndef change_root(new_root: str, pathname: str) -> str:\n    \"\"\"Return 'pathname' with 'new_root' prepended.\n\n    If 'pathname' is relative, this is equivalent to os.path.join(new_root, pathname).\n    Otherwise, it requires making 'pathname' relative and then joining the\n    two, which is tricky on DOS/Windows and Mac OS.\n\n    This is borrowed from Python's standard library's distutils module.\n    \"\"\"\n    if os.name == \"posix\":\n        if not os.path.isabs(pathname):\n            return os.path.join(new_root, pathname)\n        else:\n            return os.path.join(new_root, pathname[1:])\n\n    elif os.name == \"nt\":\n        (drive, path) = os.path.splitdrive(pathname)\n        if path[0] == \"\\\\\":\n            path = path[1:]\n        return os.path.join(new_root, path)\n\n    else:\n        raise InstallationError(\n            f\"Unknown platform: {os.name}\\n\"\n            \"Can not change root path prefix on unknown platform.\"\n        )\n\n\ndef get_src_prefix() -> str:\n    if running_under_virtualenv():\n        src_prefix = os.path.join(sys.prefix, \"src\")\n    else:\n        # FIXME: keep src in cwd for now (it is not a temporary folder)\n        try:\n            src_prefix = os.path.join(os.getcwd(), \"src\")\n        except OSError:\n            # In case the current working directory has been renamed or deleted\n            sys.exit(\"The folder you are executing pip from can no longer be found.\")\n\n    # under macOS + virtualenv sys.prefix is not properly resolved\n    # it is something like /path/to/python/bin/..\n    return os.path.abspath(src_prefix)\n\n\ntry:\n    # Use getusersitepackages if this is present, as it ensures that the\n    # value is initialised properly.\n    user_site: typing.Optional[str] = site.getusersitepackages()\nexcept AttributeError:\n    user_site = site.USER_SITE\n\n\n@functools.lru_cache(maxsize=None)\ndef is_osx_framework() -> bool:\n    return bool(sysconfig.get_config_var(\"PYTHONFRAMEWORK\"))\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/main.py","size":340,"sha1":"442943cd1fa0793dd0a43f75da3843ae3f9c67de","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"from typing import List, Optional\n\n\ndef main(args: Optional[List[str]] = None) -> int:\n    \"\"\"This is preserved for old console scripts that may still be referencing\n    it.\n\n    For additional details, see https://github.com/pypa/pip/issues/7498.\n    \"\"\"\n    from pip._internal.utils.entrypoints import _wrapper\n\n    return _wrapper(args)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/metadata/__init__.py","size":4337,"sha1":"a784b643a0a9284d9002c99877f926f69784b8fb","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import contextlib\nimport functools\nimport os\nimport sys\nfrom typing import TYPE_CHECKING, List, Optional, Type, cast\n\nfrom pip._internal.utils.misc import strtobool\n\nfrom .base import BaseDistribution, BaseEnvironment, FilesystemWheel, MemoryWheel, Wheel\n\nif TYPE_CHECKING:\n    from typing import Literal, Protocol\nelse:\n    Protocol = object\n\n__all__ = [\n    \"BaseDistribution\",\n    \"BaseEnvironment\",\n    \"FilesystemWheel\",\n    \"MemoryWheel\",\n    \"Wheel\",\n    \"get_default_environment\",\n    \"get_environment\",\n    \"get_wheel_distribution\",\n    \"select_backend\",\n]\n\n\ndef _should_use_importlib_metadata() -> bool:\n    \"\"\"Whether to use the ``importlib.metadata`` or ``pkg_resources`` backend.\n\n    By default, pip uses ``importlib.metadata`` on Python 3.11+, and\n    ``pkg_resources`` otherwise. This can be overridden by a couple of ways:\n\n    * If environment variable ``_PIP_USE_IMPORTLIB_METADATA`` is set, it\n      dictates whether ``importlib.metadata`` is used, regardless of Python\n      version.\n    * On Python 3.11+, Python distributors can patch ``importlib.metadata``\n      to add a global constant ``_PIP_USE_IMPORTLIB_METADATA = False``. This\n      makes pip use ``pkg_resources`` (unless the user set the aforementioned\n      environment variable to *True*).\n    \"\"\"\n    with contextlib.suppress(KeyError, ValueError):\n        return bool(strtobool(os.environ[\"_PIP_USE_IMPORTLIB_METADATA\"]))\n    if sys.version_info < (3, 11):\n        return False\n    import importlib.metadata\n\n    return bool(getattr(importlib.metadata, \"_PIP_USE_IMPORTLIB_METADATA\", True))\n\n\nclass Backend(Protocol):\n    NAME: 'Literal[\"importlib\", \"pkg_resources\"]'\n    Distribution: Type[BaseDistribution]\n    Environment: Type[BaseEnvironment]\n\n\n@functools.lru_cache(maxsize=None)\ndef select_backend() -> Backend:\n    if _should_use_importlib_metadata():\n        from . import importlib\n\n        return cast(Backend, importlib)\n    from . import pkg_resources\n\n    return cast(Backend, pkg_resources)\n\n\ndef get_default_environment() -> BaseEnvironment:\n    \"\"\"Get the default representation for the current environment.\n\n    This returns an Environment instance from the chosen backend. The default\n    Environment instance should be built from ``sys.path`` and may use caching\n    to share instance state across calls.\n    \"\"\"\n    return select_backend().Environment.default()\n\n\ndef get_environment(paths: Optional[List[str]]) -> BaseEnvironment:\n    \"\"\"Get a representation of the environment specified by ``paths``.\n\n    This returns an Environment instance from the chosen backend based on the\n    given import paths. The backend must build a fresh instance representing\n    the state of installed distributions when this function is called.\n    \"\"\"\n    return select_backend().Environment.from_paths(paths)\n\n\ndef get_directory_distribution(directory: str) -> BaseDistribution:\n    \"\"\"Get the distribution metadata representation in the specified directory.\n\n    This returns a Distribution instance from the chosen backend based on\n    the given on-disk ``.dist-info`` directory.\n    \"\"\"\n    return select_backend().Distribution.from_directory(directory)\n\n\ndef get_wheel_distribution(wheel: Wheel, canonical_name: str) -> BaseDistribution:\n    \"\"\"Get the representation of the specified wheel's distribution metadata.\n\n    This returns a Distribution instance from the chosen backend based on\n    the given wheel's ``.dist-info`` directory.\n\n    :param canonical_name: Normalized project name of the given wheel.\n    \"\"\"\n    return select_backend().Distribution.from_wheel(wheel, canonical_name)\n\n\ndef get_metadata_distribution(\n    metadata_contents: bytes,\n    filename: str,\n    canonical_name: str,\n) -> BaseDistribution:\n    \"\"\"Get the dist representation of the specified METADATA file contents.\n\n    This returns a Distribution instance from the chosen backend sourced from the data\n    in `metadata_contents`.\n\n    :param metadata_contents: Contents of a METADATA file within a dist, or one served\n                              via PEP 658.\n    :param filename: Filename for the dist this metadata represents.\n    :param canonical_name: Normalized project name of the given dist.\n    \"\"\"\n    return select_backend().Distribution.from_metadata_file_contents(\n        metadata_contents,\n        filename,\n        canonical_name,\n    )\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/metadata/_json.py","size":2707,"sha1":"8f93b64e79fdabc7009e54f0d34d57403ea8e44b","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"# Extracted from https://github.com/pfmoore/pkg_metadata\n\nfrom email.header import Header, decode_header, make_header\nfrom email.message import Message\nfrom typing import Any, Dict, List, Union, cast\n\nMETADATA_FIELDS = [\n    # Name, Multiple-Use\n    (\"Metadata-Version\", False),\n    (\"Name\", False),\n    (\"Version\", False),\n    (\"Dynamic\", True),\n    (\"Platform\", True),\n    (\"Supported-Platform\", True),\n    (\"Summary\", False),\n    (\"Description\", False),\n    (\"Description-Content-Type\", False),\n    (\"Keywords\", False),\n    (\"Home-page\", False),\n    (\"Download-URL\", False),\n    (\"Author\", False),\n    (\"Author-email\", False),\n    (\"Maintainer\", False),\n    (\"Maintainer-email\", False),\n    (\"License\", False),\n    (\"License-Expression\", False),\n    (\"License-File\", True),\n    (\"Classifier\", True),\n    (\"Requires-Dist\", True),\n    (\"Requires-Python\", False),\n    (\"Requires-External\", True),\n    (\"Project-URL\", True),\n    (\"Provides-Extra\", True),\n    (\"Provides-Dist\", True),\n    (\"Obsoletes-Dist\", True),\n]\n\n\ndef json_name(field: str) -> str:\n    return field.lower().replace(\"-\", \"_\")\n\n\ndef msg_to_json(msg: Message) -> Dict[str, Any]:\n    \"\"\"Convert a Message object into a JSON-compatible dictionary.\"\"\"\n\n    def sanitise_header(h: Union[Header, str]) -> str:\n        if isinstance(h, Header):\n            chunks = []\n            for bytes, encoding in decode_header(h):\n                if encoding == \"unknown-8bit\":\n                    try:\n                        # See if UTF-8 works\n                        bytes.decode(\"utf-8\")\n                        encoding = \"utf-8\"\n                    except UnicodeDecodeError:\n                        # If not, latin1 at least won't fail\n                        encoding = \"latin1\"\n                chunks.append((bytes, encoding))\n            return str(make_header(chunks))\n        return str(h)\n\n    result = {}\n    for field, multi in METADATA_FIELDS:\n        if field not in msg:\n            continue\n        key = json_name(field)\n        if multi:\n            value: Union[str, List[str]] = [\n                sanitise_header(v) for v in msg.get_all(field)  # type: ignore\n            ]\n        else:\n            value = sanitise_header(msg.get(field))  # type: ignore\n            if key == \"keywords\":\n                # Accept both comma-separated and space-separated\n                # forms, for better compatibility with old data.\n                if \",\" in value:\n                    value = [v.strip() for v in value.split(\",\")]\n                else:\n                    value = value.split()\n        result[key] = value\n\n    payload = cast(str, msg.get_payload())\n    if payload:\n        result[\"description\"] = payload\n\n    return result\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/metadata/base.py","size":25298,"sha1":"9318db72d31402eef1a48c4d343254f8f8f97202","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import csv\nimport email.message\nimport functools\nimport json\nimport logging\nimport pathlib\nimport re\nimport zipfile\nfrom typing import (\n    IO,\n    Any,\n    Collection,\n    Container,\n    Dict,\n    Iterable,\n    Iterator,\n    List,\n    NamedTuple,\n    Optional,\n    Protocol,\n    Tuple,\n    Union,\n)\n\nfrom pip._vendor.packaging.requirements import Requirement\nfrom pip._vendor.packaging.specifiers import InvalidSpecifier, SpecifierSet\nfrom pip._vendor.packaging.utils import NormalizedName, canonicalize_name\nfrom pip._vendor.packaging.version import Version\n\nfrom pip._internal.exceptions import NoneMetadataError\nfrom pip._internal.locations import site_packages, user_site\nfrom pip._internal.models.direct_url import (\n    DIRECT_URL_METADATA_NAME,\n    DirectUrl,\n    DirectUrlValidationError,\n)\nfrom pip._internal.utils.compat import stdlib_pkgs  # TODO: Move definition here.\nfrom pip._internal.utils.egg_link import egg_link_path_from_sys_path\nfrom pip._internal.utils.misc import is_local, normalize_path\nfrom pip._internal.utils.urls import url_to_path\n\nfrom ._json import msg_to_json\n\nInfoPath = Union[str, pathlib.PurePath]\n\nlogger = logging.getLogger(__name__)\n\n\nclass BaseEntryPoint(Protocol):\n    @property\n    def name(self) -> str:\n        raise NotImplementedError()\n\n    @property\n    def value(self) -> str:\n        raise NotImplementedError()\n\n    @property\n    def group(self) -> str:\n        raise NotImplementedError()\n\n\ndef _convert_installed_files_path(\n    entry: Tuple[str, ...],\n    info: Tuple[str, ...],\n) -> str:\n    \"\"\"Convert a legacy installed-files.txt path into modern RECORD path.\n\n    The legacy format stores paths relative to the info directory, while the\n    modern format stores paths relative to the package root, e.g. the\n    site-packages directory.\n\n    :param entry: Path parts of the installed-files.txt entry.\n    :param info: Path parts of the egg-info directory relative to package root.\n    :returns: The converted entry.\n\n    For best compatibility with symlinks, this does not use ``abspath()`` or\n    ``Path.resolve()``, but tries to work with path parts:\n\n    1. While ``entry`` starts with ``..``, remove the equal amounts of parts\n       from ``info``; if ``info`` is empty, start appending ``..`` instead.\n    2. Join the two directly.\n    \"\"\"\n    while entry and entry[0] == \"..\":\n        if not info or info[-1] == \"..\":\n            info += (\"..\",)\n        else:\n            info = info[:-1]\n        entry = entry[1:]\n    return str(pathlib.Path(*info, *entry))\n\n\nclass RequiresEntry(NamedTuple):\n    requirement: str\n    extra: str\n    marker: str\n\n\nclass BaseDistribution(Protocol):\n    @classmethod\n    def from_directory(cls, directory: str) -> \"BaseDistribution\":\n        \"\"\"Load the distribution from a metadata directory.\n\n        :param directory: Path to a metadata directory, e.g. ``.dist-info``.\n        \"\"\"\n        raise NotImplementedError()\n\n    @classmethod\n    def from_metadata_file_contents(\n        cls,\n        metadata_contents: bytes,\n        filename: str,\n        project_name: str,\n    ) -> \"BaseDistribution\":\n        \"\"\"Load the distribution from the contents of a METADATA file.\n\n        This is used to implement PEP 658 by generating a \"shallow\" dist object that can\n        be used for resolution without downloading or building the actual dist yet.\n\n        :param metadata_contents: The contents of a METADATA file.\n        :param filename: File name for the dist with this metadata.\n        :param project_name: Name of the project this dist represents.\n        \"\"\"\n        raise NotImplementedError()\n\n    @classmethod\n    def from_wheel(cls, wheel: \"Wheel\", name: str) -> \"BaseDistribution\":\n        \"\"\"Load the distribution from a given wheel.\n\n        :param wheel: A concrete wheel definition.\n        :param name: File name of the wheel.\n\n        :raises InvalidWheel: Whenever loading of the wheel causes a\n            :py:exc:`zipfile.BadZipFile` exception to be thrown.\n        :raises UnsupportedWheel: If the wheel is a valid zip, but malformed\n            internally.\n        \"\"\"\n        raise NotImplementedError()\n\n    def __repr__(self) -> str:\n        return f\"{self.raw_name} {self.raw_version} ({self.location})\"\n\n    def __str__(self) -> str:\n        return f\"{self.raw_name} {self.raw_version}\"\n\n    @property\n    def location(self) -> Optional[str]:\n        \"\"\"Where the distribution is loaded from.\n\n        A string value is not necessarily a filesystem path, since distributions\n        can be loaded from other sources, e.g. arbitrary zip archives. ``None``\n        means the distribution is created in-memory.\n\n        Do not canonicalize this value with e.g. ``pathlib.Path.resolve()``. If\n        this is a symbolic link, we want to preserve the relative path between\n        it and files in the distribution.\n        \"\"\"\n        raise NotImplementedError()\n\n    @property\n    def editable_project_location(self) -> Optional[str]:\n        \"\"\"The project location for editable distributions.\n\n        This is the directory where pyproject.toml or setup.py is located.\n        None if the distribution is not installed in editable mode.\n        \"\"\"\n        # TODO: this property is relatively costly to compute, memoize it ?\n        direct_url = self.direct_url\n        if direct_url:\n            if direct_url.is_local_editable():\n                return url_to_path(direct_url.url)\n        else:\n            # Search for an .egg-link file by walking sys.path, as it was\n            # done before by dist_is_editable().\n            egg_link_path = egg_link_path_from_sys_path(self.raw_name)\n            if egg_link_path:\n                # TODO: get project location from second line of egg_link file\n                #       (https://github.com/pypa/pip/issues/10243)\n                return self.location\n        return None\n\n    @property\n    def installed_location(self) -> Optional[str]:\n        \"\"\"The distribution's \"installed\" location.\n\n        This should generally be a ``site-packages`` directory. This is\n        usually ``dist.location``, except for legacy develop-installed packages,\n        where ``dist.location`` is the source code location, and this is where\n        the ``.egg-link`` file is.\n\n        The returned location is normalized (in particular, with symlinks removed).\n        \"\"\"\n        raise NotImplementedError()\n\n    @property\n    def info_location(self) -> Optional[str]:\n        \"\"\"Location of the .[egg|dist]-info directory or file.\n\n        Similarly to ``location``, a string value is not necessarily a\n        filesystem path. ``None`` means the distribution is created in-memory.\n\n        For a modern .dist-info installation on disk, this should be something\n        like ``{location}/{raw_name}-{version}.dist-info``.\n\n        Do not canonicalize this value with e.g. ``pathlib.Path.resolve()``. If\n        this is a symbolic link, we want to preserve the relative path between\n        it and other files in the distribution.\n        \"\"\"\n        raise NotImplementedError()\n\n    @property\n    def installed_by_distutils(self) -> bool:\n        \"\"\"Whether this distribution is installed with legacy distutils format.\n\n        A distribution installed with \"raw\" distutils not patched by setuptools\n        uses one single file at ``info_location`` to store metadata. We need to\n        treat this specially on uninstallation.\n        \"\"\"\n        info_location = self.info_location\n        if not info_location:\n            return False\n        return pathlib.Path(info_location).is_file()\n\n    @property\n    def installed_as_egg(self) -> bool:\n        \"\"\"Whether this distribution is installed as an egg.\n\n        This usually indicates the distribution was installed by (older versions\n        of) easy_install.\n        \"\"\"\n        location = self.location\n        if not location:\n            return False\n        return location.endswith(\".egg\")\n\n    @property\n    def installed_with_setuptools_egg_info(self) -> bool:\n        \"\"\"Whether this distribution is installed with the ``.egg-info`` format.\n\n        This usually indicates the distribution was installed with setuptools\n        with an old pip version or with ``single-version-externally-managed``.\n\n        Note that this ensure the metadata store is a directory. distutils can\n        also installs an ``.egg-info``, but as a file, not a directory. This\n        property is *False* for that case. Also see ``installed_by_distutils``.\n        \"\"\"\n        info_location = self.info_location\n        if not info_location:\n            return False\n        if not info_location.endswith(\".egg-info\"):\n            return False\n        return pathlib.Path(info_location).is_dir()\n\n    @property\n    def installed_with_dist_info(self) -> bool:\n        \"\"\"Whether this distribution is installed with the \"modern format\".\n\n        This indicates a \"modern\" installation, e.g. storing metadata in the\n        ``.dist-info`` directory. This applies to installations made by\n        setuptools (but through pip, not directly), or anything using the\n        standardized build backend interface (PEP 517).\n        \"\"\"\n        info_location = self.info_location\n        if not info_location:\n            return False\n        if not info_location.endswith(\".dist-info\"):\n            return False\n        return pathlib.Path(info_location).is_dir()\n\n    @property\n    def canonical_name(self) -> NormalizedName:\n        raise NotImplementedError()\n\n    @property\n    def version(self) -> Version:\n        raise NotImplementedError()\n\n    @property\n    def raw_version(self) -> str:\n        raise NotImplementedError()\n\n    @property\n    def setuptools_filename(self) -> str:\n        \"\"\"Convert a project name to its setuptools-compatible filename.\n\n        This is a copy of ``pkg_resources.to_filename()`` for compatibility.\n        \"\"\"\n        return self.raw_name.replace(\"-\", \"_\")\n\n    @property\n    def direct_url(self) -> Optional[DirectUrl]:\n        \"\"\"Obtain a DirectUrl from this distribution.\n\n        Returns None if the distribution has no `direct_url.json` metadata,\n        or if `direct_url.json` is invalid.\n        \"\"\"\n        try:\n            content = self.read_text(DIRECT_URL_METADATA_NAME)\n        except FileNotFoundError:\n            return None\n        try:\n            return DirectUrl.from_json(content)\n        except (\n            UnicodeDecodeError,\n            json.JSONDecodeError,\n            DirectUrlValidationError,\n        ) as e:\n            logger.warning(\n                \"Error parsing %s for %s: %s\",\n                DIRECT_URL_METADATA_NAME,\n                self.canonical_name,\n                e,\n            )\n            return None\n\n    @property\n    def installer(self) -> str:\n        try:\n            installer_text = self.read_text(\"INSTALLER\")\n        except (OSError, ValueError, NoneMetadataError):\n            return \"\"  # Fail silently if the installer file cannot be read.\n        for line in installer_text.splitlines():\n            cleaned_line = line.strip()\n            if cleaned_line:\n                return cleaned_line\n        return \"\"\n\n    @property\n    def requested(self) -> bool:\n        return self.is_file(\"REQUESTED\")\n\n    @property\n    def editable(self) -> bool:\n        return bool(self.editable_project_location)\n\n    @property\n    def local(self) -> bool:\n        \"\"\"If distribution is installed in the current virtual environment.\n\n        Always True if we're not in a virtualenv.\n        \"\"\"\n        if self.installed_location is None:\n            return False\n        return is_local(self.installed_location)\n\n    @property\n    def in_usersite(self) -> bool:\n        if self.installed_location is None or user_site is None:\n            return False\n        return self.installed_location.startswith(normalize_path(user_site))\n\n    @property\n    def in_site_packages(self) -> bool:\n        if self.installed_location is None or site_packages is None:\n            return False\n        return self.installed_location.startswith(normalize_path(site_packages))\n\n    def is_file(self, path: InfoPath) -> bool:\n        \"\"\"Check whether an entry in the info directory is a file.\"\"\"\n        raise NotImplementedError()\n\n    def iter_distutils_script_names(self) -> Iterator[str]:\n        \"\"\"Find distutils 'scripts' entries metadata.\n\n        If 'scripts' is supplied in ``setup.py``, distutils records those in the\n        installed distribution's ``scripts`` directory, a file for each script.\n        \"\"\"\n        raise NotImplementedError()\n\n    def read_text(self, path: InfoPath) -> str:\n        \"\"\"Read a file in the info directory.\n\n        :raise FileNotFoundError: If ``path`` does not exist in the directory.\n        :raise NoneMetadataError: If ``path`` exists in the info directory, but\n            cannot be read.\n        \"\"\"\n        raise NotImplementedError()\n\n    def iter_entry_points(self) -> Iterable[BaseEntryPoint]:\n        raise NotImplementedError()\n\n    def _metadata_impl(self) -> email.message.Message:\n        raise NotImplementedError()\n\n    @functools.cached_property\n    def metadata(self) -> email.message.Message:\n        \"\"\"Metadata of distribution parsed from e.g. METADATA or PKG-INFO.\n\n        This should return an empty message if the metadata file is unavailable.\n\n        :raises NoneMetadataError: If the metadata file is available, but does\n            not contain valid metadata.\n        \"\"\"\n        metadata = self._metadata_impl()\n        self._add_egg_info_requires(metadata)\n        return metadata\n\n    @property\n    def metadata_dict(self) -> Dict[str, Any]:\n        \"\"\"PEP 566 compliant JSON-serializable representation of METADATA or PKG-INFO.\n\n        This should return an empty dict if the metadata file is unavailable.\n\n        :raises NoneMetadataError: If the metadata file is available, but does\n            not contain valid metadata.\n        \"\"\"\n        return msg_to_json(self.metadata)\n\n    @property\n    def metadata_version(self) -> Optional[str]:\n        \"\"\"Value of \"Metadata-Version:\" in distribution metadata, if available.\"\"\"\n        return self.metadata.get(\"Metadata-Version\")\n\n    @property\n    def raw_name(self) -> str:\n        \"\"\"Value of \"Name:\" in distribution metadata.\"\"\"\n        # The metadata should NEVER be missing the Name: key, but if it somehow\n        # does, fall back to the known canonical name.\n        return self.metadata.get(\"Name\", self.canonical_name)\n\n    @property\n    def requires_python(self) -> SpecifierSet:\n        \"\"\"Value of \"Requires-Python:\" in distribution metadata.\n\n        If the key does not exist or contains an invalid value, an empty\n        SpecifierSet should be returned.\n        \"\"\"\n        value = self.metadata.get(\"Requires-Python\")\n        if value is None:\n            return SpecifierSet()\n        try:\n            # Convert to str to satisfy the type checker; this can be a Header object.\n            spec = SpecifierSet(str(value))\n        except InvalidSpecifier as e:\n            message = \"Package %r has an invalid Requires-Python: %s\"\n            logger.warning(message, self.raw_name, e)\n            return SpecifierSet()\n        return spec\n\n    def iter_dependencies(self, extras: Collection[str] = ()) -> Iterable[Requirement]:\n        \"\"\"Dependencies of this distribution.\n\n        For modern .dist-info distributions, this is the collection of\n        \"Requires-Dist:\" entries in distribution metadata.\n        \"\"\"\n        raise NotImplementedError()\n\n    def iter_raw_dependencies(self) -> Iterable[str]:\n        \"\"\"Raw Requires-Dist metadata.\"\"\"\n        return self.metadata.get_all(\"Requires-Dist\", [])\n\n    def iter_provided_extras(self) -> Iterable[NormalizedName]:\n        \"\"\"Extras provided by this distribution.\n\n        For modern .dist-info distributions, this is the collection of\n        \"Provides-Extra:\" entries in distribution metadata.\n\n        The return value of this function is expected to be normalised names,\n        per PEP 685, with the returned value being handled appropriately by\n        `iter_dependencies`.\n        \"\"\"\n        raise NotImplementedError()\n\n    def _iter_declared_entries_from_record(self) -> Optional[Iterator[str]]:\n        try:\n            text = self.read_text(\"RECORD\")\n        except FileNotFoundError:\n            return None\n        # This extra Path-str cast normalizes entries.\n        return (str(pathlib.Path(row[0])) for row in csv.reader(text.splitlines()))\n\n    def _iter_declared_entries_from_legacy(self) -> Optional[Iterator[str]]:\n        try:\n            text = self.read_text(\"installed-files.txt\")\n        except FileNotFoundError:\n            return None\n        paths = (p for p in text.splitlines(keepends=False) if p)\n        root = self.location\n        info = self.info_location\n        if root is None or info is None:\n            return paths\n        try:\n            info_rel = pathlib.Path(info).relative_to(root)\n        except ValueError:  # info is not relative to root.\n            return paths\n        if not info_rel.parts:  # info *is* root.\n            return paths\n        return (\n            _convert_installed_files_path(pathlib.Path(p).parts, info_rel.parts)\n            for p in paths\n        )\n\n    def iter_declared_entries(self) -> Optional[Iterator[str]]:\n        \"\"\"Iterate through file entries declared in this distribution.\n\n        For modern .dist-info distributions, this is the files listed in the\n        ``RECORD`` metadata file. For legacy setuptools distributions, this\n        comes from ``installed-files.txt``, with entries normalized to be\n        compatible with the format used by ``RECORD``.\n\n        :return: An iterator for listed entries, or None if the distribution\n            contains neither ``RECORD`` nor ``installed-files.txt``.\n        \"\"\"\n        return (\n            self._iter_declared_entries_from_record()\n            or self._iter_declared_entries_from_legacy()\n        )\n\n    def _iter_requires_txt_entries(self) -> Iterator[RequiresEntry]:\n        \"\"\"Parse a ``requires.txt`` in an egg-info directory.\n\n        This is an INI-ish format where an egg-info stores dependencies. A\n        section name describes extra other environment markers, while each entry\n        is an arbitrary string (not a key-value pair) representing a dependency\n        as a requirement string (no markers).\n\n        There is a construct in ``importlib.metadata`` called ``Sectioned`` that\n        does mostly the same, but the format is currently considered private.\n        \"\"\"\n        try:\n            content = self.read_text(\"requires.txt\")\n        except FileNotFoundError:\n            return\n        extra = marker = \"\"  # Section-less entries don't have markers.\n        for line in content.splitlines():\n            line = line.strip()\n            if not line or line.startswith(\"#\"):  # Comment; ignored.\n                continue\n            if line.startswith(\"[\") and line.endswith(\"]\"):  # A section header.\n                extra, _, marker = line.strip(\"[]\").partition(\":\")\n                continue\n            yield RequiresEntry(requirement=line, extra=extra, marker=marker)\n\n    def _iter_egg_info_extras(self) -> Iterable[str]:\n        \"\"\"Get extras from the egg-info directory.\"\"\"\n        known_extras = {\"\"}\n        for entry in self._iter_requires_txt_entries():\n            extra = canonicalize_name(entry.extra)\n            if extra in known_extras:\n                continue\n            known_extras.add(extra)\n            yield extra\n\n    def _iter_egg_info_dependencies(self) -> Iterable[str]:\n        \"\"\"Get distribution dependencies from the egg-info directory.\n\n        To ease parsing, this converts a legacy dependency entry into a PEP 508\n        requirement string. Like ``_iter_requires_txt_entries()``, there is code\n        in ``importlib.metadata`` that does mostly the same, but not do exactly\n        what we need.\n\n        Namely, ``importlib.metadata`` does not normalize the extra name before\n        putting it into the requirement string, which causes marker comparison\n        to fail because the dist-info format do normalize. This is consistent in\n        all currently available PEP 517 backends, although not standardized.\n        \"\"\"\n        for entry in self._iter_requires_txt_entries():\n            extra = canonicalize_name(entry.extra)\n            if extra and entry.marker:\n                marker = f'({entry.marker}) and extra == \"{extra}\"'\n            elif extra:\n                marker = f'extra == \"{extra}\"'\n            elif entry.marker:\n                marker = entry.marker\n            else:\n                marker = \"\"\n            if marker:\n                yield f\"{entry.requirement} ; {marker}\"\n            else:\n                yield entry.requirement\n\n    def _add_egg_info_requires(self, metadata: email.message.Message) -> None:\n        \"\"\"Add egg-info requires.txt information to the metadata.\"\"\"\n        if not metadata.get_all(\"Requires-Dist\"):\n            for dep in self._iter_egg_info_dependencies():\n                metadata[\"Requires-Dist\"] = dep\n        if not metadata.get_all(\"Provides-Extra\"):\n            for extra in self._iter_egg_info_extras():\n                metadata[\"Provides-Extra\"] = extra\n\n\nclass BaseEnvironment:\n    \"\"\"An environment containing distributions to introspect.\"\"\"\n\n    @classmethod\n    def default(cls) -> \"BaseEnvironment\":\n        raise NotImplementedError()\n\n    @classmethod\n    def from_paths(cls, paths: Optional[List[str]]) -> \"BaseEnvironment\":\n        raise NotImplementedError()\n\n    def get_distribution(self, name: str) -> Optional[\"BaseDistribution\"]:\n        \"\"\"Given a requirement name, return the installed distributions.\n\n        The name may not be normalized. The implementation must canonicalize\n        it for lookup.\n        \"\"\"\n        raise NotImplementedError()\n\n    def _iter_distributions(self) -> Iterator[\"BaseDistribution\"]:\n        \"\"\"Iterate through installed distributions.\n\n        This function should be implemented by subclass, but never called\n        directly. Use the public ``iter_distribution()`` instead, which\n        implements additional logic to make sure the distributions are valid.\n        \"\"\"\n        raise NotImplementedError()\n\n    def iter_all_distributions(self) -> Iterator[BaseDistribution]:\n        \"\"\"Iterate through all installed distributions without any filtering.\"\"\"\n        for dist in self._iter_distributions():\n            # Make sure the distribution actually comes from a valid Python\n            # packaging distribution. Pip's AdjacentTempDirectory leaves folders\n            # e.g. ``~atplotlib.dist-info`` if cleanup was interrupted. The\n            # valid project name pattern is taken from PEP 508.\n            project_name_valid = re.match(\n                r\"^([A-Z0-9]|[A-Z0-9][A-Z0-9._-]*[A-Z0-9])$\",\n                dist.canonical_name,\n                flags=re.IGNORECASE,\n            )\n            if not project_name_valid:\n                logger.warning(\n                    \"Ignoring invalid distribution %s (%s)\",\n                    dist.canonical_name,\n                    dist.location,\n                )\n                continue\n            yield dist\n\n    def iter_installed_distributions(\n        self,\n        local_only: bool = True,\n        skip: Container[str] = stdlib_pkgs,\n        include_editables: bool = True,\n        editables_only: bool = False,\n        user_only: bool = False,\n    ) -> Iterator[BaseDistribution]:\n        \"\"\"Return a list of installed distributions.\n\n        This is based on ``iter_all_distributions()`` with additional filtering\n        options. Note that ``iter_installed_distributions()`` without arguments\n        is *not* equal to ``iter_all_distributions()``, since some of the\n        configurations exclude packages by default.\n\n        :param local_only: If True (default), only return installations\n        local to the current virtualenv, if in a virtualenv.\n        :param skip: An iterable of canonicalized project names to ignore;\n            defaults to ``stdlib_pkgs``.\n        :param include_editables: If False, don't report editables.\n        :param editables_only: If True, only report editables.\n        :param user_only: If True, only report installations in the user\n        site directory.\n        \"\"\"\n        it = self.iter_all_distributions()\n        if local_only:\n            it = (d for d in it if d.local)\n        if not include_editables:\n            it = (d for d in it if not d.editable)\n        if editables_only:\n            it = (d for d in it if d.editable)\n        if user_only:\n            it = (d for d in it if d.in_usersite)\n        return (d for d in it if d.canonical_name not in skip)\n\n\nclass Wheel(Protocol):\n    location: str\n\n    def as_zipfile(self) -> zipfile.ZipFile:\n        raise NotImplementedError()\n\n\nclass FilesystemWheel(Wheel):\n    def __init__(self, location: str) -> None:\n        self.location = location\n\n    def as_zipfile(self) -> zipfile.ZipFile:\n        return zipfile.ZipFile(self.location, allowZip64=True)\n\n\nclass MemoryWheel(Wheel):\n    def __init__(self, location: str, stream: IO[bytes]) -> None:\n        self.location = location\n        self.stream = stream\n\n    def as_zipfile(self) -> zipfile.ZipFile:\n        return zipfile.ZipFile(self.stream, allowZip64=True)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/metadata/importlib/__init__.py","size":135,"sha1":"e7447ed9c17db5df5a9200da03c4d0b8812cc185","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"from ._dists import Distribution\nfrom ._envs import Environment\n\n__all__ = [\"NAME\", \"Distribution\", \"Environment\"]\n\nNAME = \"importlib\"\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/metadata/importlib/_compat.py","size":2796,"sha1":"d440a9db1801ded11768516745c50f4ce997b530","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import importlib.metadata\nimport os\nfrom typing import Any, Optional, Protocol, Tuple, cast\n\nfrom pip._vendor.packaging.utils import NormalizedName, canonicalize_name\n\n\nclass BadMetadata(ValueError):\n    def __init__(self, dist: importlib.metadata.Distribution, *, reason: str) -> None:\n        self.dist = dist\n        self.reason = reason\n\n    def __str__(self) -> str:\n        return f\"Bad metadata in {self.dist} ({self.reason})\"\n\n\nclass BasePath(Protocol):\n    \"\"\"A protocol that various path objects conform.\n\n    This exists because importlib.metadata uses both ``pathlib.Path`` and\n    ``zipfile.Path``, and we need a common base for type hints (Union does not\n    work well since ``zipfile.Path`` is too new for our linter setup).\n\n    This does not mean to be exhaustive, but only contains things that present\n    in both classes *that we need*.\n    \"\"\"\n\n    @property\n    def name(self) -> str:\n        raise NotImplementedError()\n\n    @property\n    def parent(self) -> \"BasePath\":\n        raise NotImplementedError()\n\n\ndef get_info_location(d: importlib.metadata.Distribution) -> Optional[BasePath]:\n    \"\"\"Find the path to the distribution's metadata directory.\n\n    HACK: This relies on importlib.metadata's private ``_path`` attribute. Not\n    all distributions exist on disk, so importlib.metadata is correct to not\n    expose the attribute as public. But pip's code base is old and not as clean,\n    so we do this to avoid having to rewrite too many things. Hopefully we can\n    eliminate this some day.\n    \"\"\"\n    return getattr(d, \"_path\", None)\n\n\ndef parse_name_and_version_from_info_directory(\n    dist: importlib.metadata.Distribution,\n) -> Tuple[Optional[str], Optional[str]]:\n    \"\"\"Get a name and version from the metadata directory name.\n\n    This is much faster than reading distribution metadata.\n    \"\"\"\n    info_location = get_info_location(dist)\n    if info_location is None:\n        return None, None\n\n    stem, suffix = os.path.splitext(info_location.name)\n    if suffix == \".dist-info\":\n        name, sep, version = stem.partition(\"-\")\n        if sep:\n            return name, version\n\n    if suffix == \".egg-info\":\n        name = stem.split(\"-\", 1)[0]\n        return name, None\n\n    return None, None\n\n\ndef get_dist_canonical_name(dist: importlib.metadata.Distribution) -> NormalizedName:\n    \"\"\"Get the distribution's normalized name.\n\n    The ``name`` attribute is only available in Python 3.10 or later. We are\n    targeting exactly that, but Mypy does not know this.\n    \"\"\"\n    if name := parse_name_and_version_from_info_directory(dist)[0]:\n        return canonicalize_name(name)\n\n    name = cast(Any, dist).name\n    if not isinstance(name, str):\n        raise BadMetadata(dist, reason=\"invalid metadata entry 'name'\")\n    return canonicalize_name(name)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/metadata/importlib/_dists.py","size":8279,"sha1":"b092529645c62436c230bb9456008261c6dfba87","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import email.message\nimport importlib.metadata\nimport pathlib\nimport zipfile\nfrom os import PathLike\nfrom typing import (\n    Collection,\n    Dict,\n    Iterable,\n    Iterator,\n    Mapping,\n    Optional,\n    Sequence,\n    Union,\n    cast,\n)\n\nfrom pip._vendor.packaging.requirements import Requirement\nfrom pip._vendor.packaging.utils import NormalizedName, canonicalize_name\nfrom pip._vendor.packaging.version import Version\nfrom pip._vendor.packaging.version import parse as parse_version\n\nfrom pip._internal.exceptions import InvalidWheel, UnsupportedWheel\nfrom pip._internal.metadata.base import (\n    BaseDistribution,\n    BaseEntryPoint,\n    InfoPath,\n    Wheel,\n)\nfrom pip._internal.utils.misc import normalize_path\nfrom pip._internal.utils.packaging import get_requirement\nfrom pip._internal.utils.temp_dir import TempDirectory\nfrom pip._internal.utils.wheel import parse_wheel, read_wheel_metadata_file\n\nfrom ._compat import (\n    BasePath,\n    get_dist_canonical_name,\n    parse_name_and_version_from_info_directory,\n)\n\n\nclass WheelDistribution(importlib.metadata.Distribution):\n    \"\"\"An ``importlib.metadata.Distribution`` read from a wheel.\n\n    Although ``importlib.metadata.PathDistribution`` accepts ``zipfile.Path``,\n    its implementation is too \"lazy\" for pip's needs (we can't keep the ZipFile\n    handle open for the entire lifetime of the distribution object).\n\n    This implementation eagerly reads the entire metadata directory into the\n    memory instead, and operates from that.\n    \"\"\"\n\n    def __init__(\n        self,\n        files: Mapping[pathlib.PurePosixPath, bytes],\n        info_location: pathlib.PurePosixPath,\n    ) -> None:\n        self._files = files\n        self.info_location = info_location\n\n    @classmethod\n    def from_zipfile(\n        cls,\n        zf: zipfile.ZipFile,\n        name: str,\n        location: str,\n    ) -> \"WheelDistribution\":\n        info_dir, _ = parse_wheel(zf, name)\n        paths = (\n            (name, pathlib.PurePosixPath(name.split(\"/\", 1)[-1]))\n            for name in zf.namelist()\n            if name.startswith(f\"{info_dir}/\")\n        )\n        files = {\n            relpath: read_wheel_metadata_file(zf, fullpath)\n            for fullpath, relpath in paths\n        }\n        info_location = pathlib.PurePosixPath(location, info_dir)\n        return cls(files, info_location)\n\n    def iterdir(self, path: InfoPath) -> Iterator[pathlib.PurePosixPath]:\n        # Only allow iterating through the metadata directory.\n        if pathlib.PurePosixPath(str(path)) in self._files:\n            return iter(self._files)\n        raise FileNotFoundError(path)\n\n    def read_text(self, filename: str) -> Optional[str]:\n        try:\n            data = self._files[pathlib.PurePosixPath(filename)]\n        except KeyError:\n            return None\n        try:\n            text = data.decode(\"utf-8\")\n        except UnicodeDecodeError as e:\n            wheel = self.info_location.parent\n            error = f\"Error decoding metadata for {wheel}: {e} in {filename} file\"\n            raise UnsupportedWheel(error)\n        return text\n\n    def locate_file(self, path: Union[str, \"PathLike[str]\"]) -> pathlib.Path:\n        # This method doesn't make sense for our in-memory wheel, but the API\n        # requires us to define it.\n        raise NotImplementedError\n\n\nclass Distribution(BaseDistribution):\n    def __init__(\n        self,\n        dist: importlib.metadata.Distribution,\n        info_location: Optional[BasePath],\n        installed_location: Optional[BasePath],\n    ) -> None:\n        self._dist = dist\n        self._info_location = info_location\n        self._installed_location = installed_location\n\n    @classmethod\n    def from_directory(cls, directory: str) -> BaseDistribution:\n        info_location = pathlib.Path(directory)\n        dist = importlib.metadata.Distribution.at(info_location)\n        return cls(dist, info_location, info_location.parent)\n\n    @classmethod\n    def from_metadata_file_contents(\n        cls,\n        metadata_contents: bytes,\n        filename: str,\n        project_name: str,\n    ) -> BaseDistribution:\n        # Generate temp dir to contain the metadata file, and write the file contents.\n        temp_dir = pathlib.Path(\n            TempDirectory(kind=\"metadata\", globally_managed=True).path\n        )\n        metadata_path = temp_dir / \"METADATA\"\n        metadata_path.write_bytes(metadata_contents)\n        # Construct dist pointing to the newly created directory.\n        dist = importlib.metadata.Distribution.at(metadata_path.parent)\n        return cls(dist, metadata_path.parent, None)\n\n    @classmethod\n    def from_wheel(cls, wheel: Wheel, name: str) -> BaseDistribution:\n        try:\n            with wheel.as_zipfile() as zf:\n                dist = WheelDistribution.from_zipfile(zf, name, wheel.location)\n        except zipfile.BadZipFile as e:\n            raise InvalidWheel(wheel.location, name) from e\n        return cls(dist, dist.info_location, pathlib.PurePosixPath(wheel.location))\n\n    @property\n    def location(self) -> Optional[str]:\n        if self._info_location is None:\n            return None\n        return str(self._info_location.parent)\n\n    @property\n    def info_location(self) -> Optional[str]:\n        if self._info_location is None:\n            return None\n        return str(self._info_location)\n\n    @property\n    def installed_location(self) -> Optional[str]:\n        if self._installed_location is None:\n            return None\n        return normalize_path(str(self._installed_location))\n\n    @property\n    def canonical_name(self) -> NormalizedName:\n        return get_dist_canonical_name(self._dist)\n\n    @property\n    def version(self) -> Version:\n        if version := parse_name_and_version_from_info_directory(self._dist)[1]:\n            return parse_version(version)\n        return parse_version(self._dist.version)\n\n    @property\n    def raw_version(self) -> str:\n        return self._dist.version\n\n    def is_file(self, path: InfoPath) -> bool:\n        return self._dist.read_text(str(path)) is not None\n\n    def iter_distutils_script_names(self) -> Iterator[str]:\n        # A distutils installation is always \"flat\" (not in e.g. egg form), so\n        # if this distribution's info location is NOT a pathlib.Path (but e.g.\n        # zipfile.Path), it can never contain any distutils scripts.\n        if not isinstance(self._info_location, pathlib.Path):\n            return\n        for child in self._info_location.joinpath(\"scripts\").iterdir():\n            yield child.name\n\n    def read_text(self, path: InfoPath) -> str:\n        content = self._dist.read_text(str(path))\n        if content is None:\n            raise FileNotFoundError(path)\n        return content\n\n    def iter_entry_points(self) -> Iterable[BaseEntryPoint]:\n        # importlib.metadata's EntryPoint structure satisfies BaseEntryPoint.\n        return self._dist.entry_points\n\n    def _metadata_impl(self) -> email.message.Message:\n        # From Python 3.10+, importlib.metadata declares PackageMetadata as the\n        # return type. This protocol is unfortunately a disaster now and misses\n        # a ton of fields that we need, including get() and get_payload(). We\n        # rely on the implementation that the object is actually a Message now,\n        # until upstream can improve the protocol. (python/cpython#94952)\n        return cast(email.message.Message, self._dist.metadata)\n\n    def iter_provided_extras(self) -> Iterable[NormalizedName]:\n        return [\n            canonicalize_name(extra)\n            for extra in self.metadata.get_all(\"Provides-Extra\", [])\n        ]\n\n    def iter_dependencies(self, extras: Collection[str] = ()) -> Iterable[Requirement]:\n        contexts: Sequence[Dict[str, str]] = [{\"extra\": e} for e in extras]\n        for req_string in self.metadata.get_all(\"Requires-Dist\", []):\n            # strip() because email.message.Message.get_all() may return a leading \\n\n            # in case a long header was wrapped.\n            req = get_requirement(req_string.strip())\n            if not req.marker:\n                yield req\n            elif not extras and req.marker.evaluate({\"extra\": \"\"}):\n                yield req\n            elif any(req.marker.evaluate(context) for context in contexts):\n                yield req\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/metadata/importlib/_envs.py","size":7431,"sha1":"70dbe7114d46dfbbd1abe789cb856ef243423780","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import functools\nimport importlib.metadata\nimport logging\nimport os\nimport pathlib\nimport sys\nimport zipfile\nimport zipimport\nfrom typing import Iterator, List, Optional, Sequence, Set, Tuple\n\nfrom pip._vendor.packaging.utils import NormalizedName, canonicalize_name\n\nfrom pip._internal.metadata.base import BaseDistribution, BaseEnvironment\nfrom pip._internal.models.wheel import Wheel\nfrom pip._internal.utils.deprecation import deprecated\nfrom pip._internal.utils.filetypes import WHEEL_EXTENSION\n\nfrom ._compat import BadMetadata, BasePath, get_dist_canonical_name, get_info_location\nfrom ._dists import Distribution\n\nlogger = logging.getLogger(__name__)\n\n\ndef _looks_like_wheel(location: str) -> bool:\n    if not location.endswith(WHEEL_EXTENSION):\n        return False\n    if not os.path.isfile(location):\n        return False\n    if not Wheel.wheel_file_re.match(os.path.basename(location)):\n        return False\n    return zipfile.is_zipfile(location)\n\n\nclass _DistributionFinder:\n    \"\"\"Finder to locate distributions.\n\n    The main purpose of this class is to memoize found distributions' names, so\n    only one distribution is returned for each package name. At lot of pip code\n    assumes this (because it is setuptools's behavior), and not doing the same\n    can potentially cause a distribution in lower precedence path to override a\n    higher precedence one if the caller is not careful.\n\n    Eventually we probably want to make it possible to see lower precedence\n    installations as well. It's useful feature, after all.\n    \"\"\"\n\n    FoundResult = Tuple[importlib.metadata.Distribution, Optional[BasePath]]\n\n    def __init__(self) -> None:\n        self._found_names: Set[NormalizedName] = set()\n\n    def _find_impl(self, location: str) -> Iterator[FoundResult]:\n        \"\"\"Find distributions in a location.\"\"\"\n        # Skip looking inside a wheel. Since a package inside a wheel is not\n        # always valid (due to .data directories etc.), its .dist-info entry\n        # should not be considered an installed distribution.\n        if _looks_like_wheel(location):\n            return\n        # To know exactly where we find a distribution, we have to feed in the\n        # paths one by one, instead of dumping the list to importlib.metadata.\n        for dist in importlib.metadata.distributions(path=[location]):\n            info_location = get_info_location(dist)\n            try:\n                name = get_dist_canonical_name(dist)\n            except BadMetadata as e:\n                logger.warning(\"Skipping %s due to %s\", info_location, e.reason)\n                continue\n            if name in self._found_names:\n                continue\n            self._found_names.add(name)\n            yield dist, info_location\n\n    def find(self, location: str) -> Iterator[BaseDistribution]:\n        \"\"\"Find distributions in a location.\n\n        The path can be either a directory, or a ZIP archive.\n        \"\"\"\n        for dist, info_location in self._find_impl(location):\n            if info_location is None:\n                installed_location: Optional[BasePath] = None\n            else:\n                installed_location = info_location.parent\n            yield Distribution(dist, info_location, installed_location)\n\n    def find_linked(self, location: str) -> Iterator[BaseDistribution]:\n        \"\"\"Read location in egg-link files and return distributions in there.\n\n        The path should be a directory; otherwise this returns nothing. This\n        follows how setuptools does this for compatibility. The first non-empty\n        line in the egg-link is read as a path (resolved against the egg-link's\n        containing directory if relative). Distributions found at that linked\n        location are returned.\n        \"\"\"\n        path = pathlib.Path(location)\n        if not path.is_dir():\n            return\n        for child in path.iterdir():\n            if child.suffix != \".egg-link\":\n                continue\n            with child.open() as f:\n                lines = (line.strip() for line in f)\n                target_rel = next((line for line in lines if line), \"\")\n            if not target_rel:\n                continue\n            target_location = str(path.joinpath(target_rel))\n            for dist, info_location in self._find_impl(target_location):\n                yield Distribution(dist, info_location, path)\n\n    def _find_eggs_in_dir(self, location: str) -> Iterator[BaseDistribution]:\n        from pip._vendor.pkg_resources import find_distributions\n\n        from pip._internal.metadata import pkg_resources as legacy\n\n        with os.scandir(location) as it:\n            for entry in it:\n                if not entry.name.endswith(\".egg\"):\n                    continue\n                for dist in find_distributions(entry.path):\n                    yield legacy.Distribution(dist)\n\n    def _find_eggs_in_zip(self, location: str) -> Iterator[BaseDistribution]:\n        from pip._vendor.pkg_resources import find_eggs_in_zip\n\n        from pip._internal.metadata import pkg_resources as legacy\n\n        try:\n            importer = zipimport.zipimporter(location)\n        except zipimport.ZipImportError:\n            return\n        for dist in find_eggs_in_zip(importer, location):\n            yield legacy.Distribution(dist)\n\n    def find_eggs(self, location: str) -> Iterator[BaseDistribution]:\n        \"\"\"Find eggs in a location.\n\n        This actually uses the old *pkg_resources* backend. We likely want to\n        deprecate this so we can eventually remove the *pkg_resources*\n        dependency entirely. Before that, this should first emit a deprecation\n        warning for some versions when using the fallback since importing\n        *pkg_resources* is slow for those who don't need it.\n        \"\"\"\n        if os.path.isdir(location):\n            yield from self._find_eggs_in_dir(location)\n        if zipfile.is_zipfile(location):\n            yield from self._find_eggs_in_zip(location)\n\n\n@functools.lru_cache(maxsize=None)  # Warn a distribution exactly once.\ndef _emit_egg_deprecation(location: Optional[str]) -> None:\n    deprecated(\n        reason=f\"Loading egg at {location} is deprecated.\",\n        replacement=\"to use pip for package installation\",\n        gone_in=\"25.1\",\n        issue=12330,\n    )\n\n\nclass Environment(BaseEnvironment):\n    def __init__(self, paths: Sequence[str]) -> None:\n        self._paths = paths\n\n    @classmethod\n    def default(cls) -> BaseEnvironment:\n        return cls(sys.path)\n\n    @classmethod\n    def from_paths(cls, paths: Optional[List[str]]) -> BaseEnvironment:\n        if paths is None:\n            return cls(sys.path)\n        return cls(paths)\n\n    def _iter_distributions(self) -> Iterator[BaseDistribution]:\n        finder = _DistributionFinder()\n        for location in self._paths:\n            yield from finder.find(location)\n            for dist in finder.find_eggs(location):\n                _emit_egg_deprecation(dist.location)\n                yield dist\n            # This must go last because that's how pkg_resources tie-breaks.\n            yield from finder.find_linked(location)\n\n    def get_distribution(self, name: str) -> Optional[BaseDistribution]:\n        canonical_name = canonicalize_name(name)\n        matches = (\n            distribution\n            for distribution in self.iter_all_distributions()\n            if distribution.canonical_name == canonical_name\n        )\n        return next(matches, None)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/metadata/pkg_resources.py","size":10542,"sha1":"4d42374ffb06a7de3f293752fd79370a57cecb74","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import email.message\nimport email.parser\nimport logging\nimport os\nimport zipfile\nfrom typing import (\n    Collection,\n    Iterable,\n    Iterator,\n    List,\n    Mapping,\n    NamedTuple,\n    Optional,\n)\n\nfrom pip._vendor import pkg_resources\nfrom pip._vendor.packaging.requirements import Requirement\nfrom pip._vendor.packaging.utils import NormalizedName, canonicalize_name\nfrom pip._vendor.packaging.version import Version\nfrom pip._vendor.packaging.version import parse as parse_version\n\nfrom pip._internal.exceptions import InvalidWheel, NoneMetadataError, UnsupportedWheel\nfrom pip._internal.utils.egg_link import egg_link_path_from_location\nfrom pip._internal.utils.misc import display_path, normalize_path\nfrom pip._internal.utils.wheel import parse_wheel, read_wheel_metadata_file\n\nfrom .base import (\n    BaseDistribution,\n    BaseEntryPoint,\n    BaseEnvironment,\n    InfoPath,\n    Wheel,\n)\n\n__all__ = [\"NAME\", \"Distribution\", \"Environment\"]\n\nlogger = logging.getLogger(__name__)\n\nNAME = \"pkg_resources\"\n\n\nclass EntryPoint(NamedTuple):\n    name: str\n    value: str\n    group: str\n\n\nclass InMemoryMetadata:\n    \"\"\"IMetadataProvider that reads metadata files from a dictionary.\n\n    This also maps metadata decoding exceptions to our internal exception type.\n    \"\"\"\n\n    def __init__(self, metadata: Mapping[str, bytes], wheel_name: str) -> None:\n        self._metadata = metadata\n        self._wheel_name = wheel_name\n\n    def has_metadata(self, name: str) -> bool:\n        return name in self._metadata\n\n    def get_metadata(self, name: str) -> str:\n        try:\n            return self._metadata[name].decode()\n        except UnicodeDecodeError as e:\n            # Augment the default error with the origin of the file.\n            raise UnsupportedWheel(\n                f\"Error decoding metadata for {self._wheel_name}: {e} in {name} file\"\n            )\n\n    def get_metadata_lines(self, name: str) -> Iterable[str]:\n        return pkg_resources.yield_lines(self.get_metadata(name))\n\n    def metadata_isdir(self, name: str) -> bool:\n        return False\n\n    def metadata_listdir(self, name: str) -> List[str]:\n        return []\n\n    def run_script(self, script_name: str, namespace: str) -> None:\n        pass\n\n\nclass Distribution(BaseDistribution):\n    def __init__(self, dist: pkg_resources.Distribution) -> None:\n        self._dist = dist\n        # This is populated lazily, to avoid loading metadata for all possible\n        # distributions eagerly.\n        self.__extra_mapping: Optional[Mapping[NormalizedName, str]] = None\n\n    @property\n    def _extra_mapping(self) -> Mapping[NormalizedName, str]:\n        if self.__extra_mapping is None:\n            self.__extra_mapping = {\n                canonicalize_name(extra): extra for extra in self._dist.extras\n            }\n\n        return self.__extra_mapping\n\n    @classmethod\n    def from_directory(cls, directory: str) -> BaseDistribution:\n        dist_dir = directory.rstrip(os.sep)\n\n        # Build a PathMetadata object, from path to metadata. :wink:\n        base_dir, dist_dir_name = os.path.split(dist_dir)\n        metadata = pkg_resources.PathMetadata(base_dir, dist_dir)\n\n        # Determine the correct Distribution object type.\n        if dist_dir.endswith(\".egg-info\"):\n            dist_cls = pkg_resources.Distribution\n            dist_name = os.path.splitext(dist_dir_name)[0]\n        else:\n            assert dist_dir.endswith(\".dist-info\")\n            dist_cls = pkg_resources.DistInfoDistribution\n            dist_name = os.path.splitext(dist_dir_name)[0].split(\"-\")[0]\n\n        dist = dist_cls(base_dir, project_name=dist_name, metadata=metadata)\n        return cls(dist)\n\n    @classmethod\n    def from_metadata_file_contents(\n        cls,\n        metadata_contents: bytes,\n        filename: str,\n        project_name: str,\n    ) -> BaseDistribution:\n        metadata_dict = {\n            \"METADATA\": metadata_contents,\n        }\n        dist = pkg_resources.DistInfoDistribution(\n            location=filename,\n            metadata=InMemoryMetadata(metadata_dict, filename),\n            project_name=project_name,\n        )\n        return cls(dist)\n\n    @classmethod\n    def from_wheel(cls, wheel: Wheel, name: str) -> BaseDistribution:\n        try:\n            with wheel.as_zipfile() as zf:\n                info_dir, _ = parse_wheel(zf, name)\n                metadata_dict = {\n                    path.split(\"/\", 1)[-1]: read_wheel_metadata_file(zf, path)\n                    for path in zf.namelist()\n                    if path.startswith(f\"{info_dir}/\")\n                }\n        except zipfile.BadZipFile as e:\n            raise InvalidWheel(wheel.location, name) from e\n        except UnsupportedWheel as e:\n            raise UnsupportedWheel(f\"{name} has an invalid wheel, {e}\")\n        dist = pkg_resources.DistInfoDistribution(\n            location=wheel.location,\n            metadata=InMemoryMetadata(metadata_dict, wheel.location),\n            project_name=name,\n        )\n        return cls(dist)\n\n    @property\n    def location(self) -> Optional[str]:\n        return self._dist.location\n\n    @property\n    def installed_location(self) -> Optional[str]:\n        egg_link = egg_link_path_from_location(self.raw_name)\n        if egg_link:\n            location = egg_link\n        elif self.location:\n            location = self.location\n        else:\n            return None\n        return normalize_path(location)\n\n    @property\n    def info_location(self) -> Optional[str]:\n        return self._dist.egg_info\n\n    @property\n    def installed_by_distutils(self) -> bool:\n        # A distutils-installed distribution is provided by FileMetadata. This\n        # provider has a \"path\" attribute not present anywhere else. Not the\n        # best introspection logic, but pip has been doing this for a long time.\n        try:\n            return bool(self._dist._provider.path)\n        except AttributeError:\n            return False\n\n    @property\n    def canonical_name(self) -> NormalizedName:\n        return canonicalize_name(self._dist.project_name)\n\n    @property\n    def version(self) -> Version:\n        return parse_version(self._dist.version)\n\n    @property\n    def raw_version(self) -> str:\n        return self._dist.version\n\n    def is_file(self, path: InfoPath) -> bool:\n        return self._dist.has_metadata(str(path))\n\n    def iter_distutils_script_names(self) -> Iterator[str]:\n        yield from self._dist.metadata_listdir(\"scripts\")\n\n    def read_text(self, path: InfoPath) -> str:\n        name = str(path)\n        if not self._dist.has_metadata(name):\n            raise FileNotFoundError(name)\n        content = self._dist.get_metadata(name)\n        if content is None:\n            raise NoneMetadataError(self, name)\n        return content\n\n    def iter_entry_points(self) -> Iterable[BaseEntryPoint]:\n        for group, entries in self._dist.get_entry_map().items():\n            for name, entry_point in entries.items():\n                name, _, value = str(entry_point).partition(\"=\")\n                yield EntryPoint(name=name.strip(), value=value.strip(), group=group)\n\n    def _metadata_impl(self) -> email.message.Message:\n        \"\"\"\n        :raises NoneMetadataError: if the distribution reports `has_metadata()`\n            True but `get_metadata()` returns None.\n        \"\"\"\n        if isinstance(self._dist, pkg_resources.DistInfoDistribution):\n            metadata_name = \"METADATA\"\n        else:\n            metadata_name = \"PKG-INFO\"\n        try:\n            metadata = self.read_text(metadata_name)\n        except FileNotFoundError:\n            if self.location:\n                displaying_path = display_path(self.location)\n            else:\n                displaying_path = repr(self.location)\n            logger.warning(\"No metadata found in %s\", displaying_path)\n            metadata = \"\"\n        feed_parser = email.parser.FeedParser()\n        feed_parser.feed(metadata)\n        return feed_parser.close()\n\n    def iter_dependencies(self, extras: Collection[str] = ()) -> Iterable[Requirement]:\n        if extras:\n            relevant_extras = set(self._extra_mapping) & set(\n                map(canonicalize_name, extras)\n            )\n            extras = [self._extra_mapping[extra] for extra in relevant_extras]\n        return self._dist.requires(extras)\n\n    def iter_provided_extras(self) -> Iterable[NormalizedName]:\n        return self._extra_mapping.keys()\n\n\nclass Environment(BaseEnvironment):\n    def __init__(self, ws: pkg_resources.WorkingSet) -> None:\n        self._ws = ws\n\n    @classmethod\n    def default(cls) -> BaseEnvironment:\n        return cls(pkg_resources.working_set)\n\n    @classmethod\n    def from_paths(cls, paths: Optional[List[str]]) -> BaseEnvironment:\n        return cls(pkg_resources.WorkingSet(paths))\n\n    def _iter_distributions(self) -> Iterator[BaseDistribution]:\n        for dist in self._ws:\n            yield Distribution(dist)\n\n    def _search_distribution(self, name: str) -> Optional[BaseDistribution]:\n        \"\"\"Find a distribution matching the ``name`` in the environment.\n\n        This searches from *all* distributions available in the environment, to\n        match the behavior of ``pkg_resources.get_distribution()``.\n        \"\"\"\n        canonical_name = canonicalize_name(name)\n        for dist in self.iter_all_distributions():\n            if dist.canonical_name == canonical_name:\n                return dist\n        return None\n\n    def get_distribution(self, name: str) -> Optional[BaseDistribution]:\n        # Search the distribution by looking through the working set.\n        dist = self._search_distribution(name)\n        if dist:\n            return dist\n\n        # If distribution could not be found, call working_set.require to\n        # update the working set, and try to find the distribution again.\n        # This might happen for e.g. when you install a package twice, once\n        # using setup.py develop and again using setup.py install. Now when\n        # running pip uninstall twice, the package gets removed from the\n        # working set in the first uninstall, so we have to populate the\n        # working set again so that pip knows about it and the packages gets\n        # picked up and is successfully uninstalled the second time too.\n        try:\n            # We didn't pass in any version specifiers, so this can never\n            # raise pkg_resources.VersionConflict.\n            self._ws.require(name)\n        except pkg_resources.DistributionNotFound:\n            return None\n        return self._search_distribution(name)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/models/__init__.py","size":63,"sha1":"af1b34a8655a6a39832635a34dcbc060412ed6cb","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"A package that contains models that represent entities.\n\"\"\"\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/models/candidate.py","size":753,"sha1":"afbf60b94e62d1e84c51222da4151d1deda70f95","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"from dataclasses import dataclass\n\nfrom pip._vendor.packaging.version import Version\nfrom pip._vendor.packaging.version import parse as parse_version\n\nfrom pip._internal.models.link import Link\n\n\n@dataclass(frozen=True)\nclass InstallationCandidate:\n    \"\"\"Represents a potential \"candidate\" for installation.\"\"\"\n\n    __slots__ = [\"name\", \"version\", \"link\"]\n\n    name: str\n    version: Version\n    link: Link\n\n    def __init__(self, name: str, version: str, link: Link) -> None:\n        object.__setattr__(self, \"name\", name)\n        object.__setattr__(self, \"version\", parse_version(version))\n        object.__setattr__(self, \"link\", link)\n\n    def __str__(self) -> str:\n        return f\"{self.name!r} candidate (version {self.version} at {self.link})\"\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/models/direct_url.py","size":6578,"sha1":"8f451427685c83371522248923245147566ea4d4","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\" PEP 610 \"\"\"\n\nimport json\nimport re\nimport urllib.parse\nfrom dataclasses import dataclass\nfrom typing import Any, ClassVar, Dict, Iterable, Optional, Type, TypeVar, Union\n\n__all__ = [\n    \"DirectUrl\",\n    \"DirectUrlValidationError\",\n    \"DirInfo\",\n    \"ArchiveInfo\",\n    \"VcsInfo\",\n]\n\nT = TypeVar(\"T\")\n\nDIRECT_URL_METADATA_NAME = \"direct_url.json\"\nENV_VAR_RE = re.compile(r\"^\\$\\{[A-Za-z0-9-_]+\\}(:\\$\\{[A-Za-z0-9-_]+\\})?$\")\n\n\nclass DirectUrlValidationError(Exception):\n    pass\n\n\ndef _get(\n    d: Dict[str, Any], expected_type: Type[T], key: str, default: Optional[T] = None\n) -> Optional[T]:\n    \"\"\"Get value from dictionary and verify expected type.\"\"\"\n    if key not in d:\n        return default\n    value = d[key]\n    if not isinstance(value, expected_type):\n        raise DirectUrlValidationError(\n            f\"{value!r} has unexpected type for {key} (expected {expected_type})\"\n        )\n    return value\n\n\ndef _get_required(\n    d: Dict[str, Any], expected_type: Type[T], key: str, default: Optional[T] = None\n) -> T:\n    value = _get(d, expected_type, key, default)\n    if value is None:\n        raise DirectUrlValidationError(f\"{key} must have a value\")\n    return value\n\n\ndef _exactly_one_of(infos: Iterable[Optional[\"InfoType\"]]) -> \"InfoType\":\n    infos = [info for info in infos if info is not None]\n    if not infos:\n        raise DirectUrlValidationError(\n            \"missing one of archive_info, dir_info, vcs_info\"\n        )\n    if len(infos) > 1:\n        raise DirectUrlValidationError(\n            \"more than one of archive_info, dir_info, vcs_info\"\n        )\n    assert infos[0] is not None\n    return infos[0]\n\n\ndef _filter_none(**kwargs: Any) -> Dict[str, Any]:\n    \"\"\"Make dict excluding None values.\"\"\"\n    return {k: v for k, v in kwargs.items() if v is not None}\n\n\n@dataclass\nclass VcsInfo:\n    name: ClassVar = \"vcs_info\"\n\n    vcs: str\n    commit_id: str\n    requested_revision: Optional[str] = None\n\n    @classmethod\n    def _from_dict(cls, d: Optional[Dict[str, Any]]) -> Optional[\"VcsInfo\"]:\n        if d is None:\n            return None\n        return cls(\n            vcs=_get_required(d, str, \"vcs\"),\n            commit_id=_get_required(d, str, \"commit_id\"),\n            requested_revision=_get(d, str, \"requested_revision\"),\n        )\n\n    def _to_dict(self) -> Dict[str, Any]:\n        return _filter_none(\n            vcs=self.vcs,\n            requested_revision=self.requested_revision,\n            commit_id=self.commit_id,\n        )\n\n\nclass ArchiveInfo:\n    name = \"archive_info\"\n\n    def __init__(\n        self,\n        hash: Optional[str] = None,\n        hashes: Optional[Dict[str, str]] = None,\n    ) -> None:\n        # set hashes before hash, since the hash setter will further populate hashes\n        self.hashes = hashes\n        self.hash = hash\n\n    @property\n    def hash(self) -> Optional[str]:\n        return self._hash\n\n    @hash.setter\n    def hash(self, value: Optional[str]) -> None:\n        if value is not None:\n            # Auto-populate the hashes key to upgrade to the new format automatically.\n            # We don't back-populate the legacy hash key from hashes.\n            try:\n                hash_name, hash_value = value.split(\"=\", 1)\n            except ValueError:\n                raise DirectUrlValidationError(\n                    f\"invalid archive_info.hash format: {value!r}\"\n                )\n            if self.hashes is None:\n                self.hashes = {hash_name: hash_value}\n            elif hash_name not in self.hashes:\n                self.hashes = self.hashes.copy()\n                self.hashes[hash_name] = hash_value\n        self._hash = value\n\n    @classmethod\n    def _from_dict(cls, d: Optional[Dict[str, Any]]) -> Optional[\"ArchiveInfo\"]:\n        if d is None:\n            return None\n        return cls(hash=_get(d, str, \"hash\"), hashes=_get(d, dict, \"hashes\"))\n\n    def _to_dict(self) -> Dict[str, Any]:\n        return _filter_none(hash=self.hash, hashes=self.hashes)\n\n\n@dataclass\nclass DirInfo:\n    name: ClassVar = \"dir_info\"\n\n    editable: bool = False\n\n    @classmethod\n    def _from_dict(cls, d: Optional[Dict[str, Any]]) -> Optional[\"DirInfo\"]:\n        if d is None:\n            return None\n        return cls(editable=_get_required(d, bool, \"editable\", default=False))\n\n    def _to_dict(self) -> Dict[str, Any]:\n        return _filter_none(editable=self.editable or None)\n\n\nInfoType = Union[ArchiveInfo, DirInfo, VcsInfo]\n\n\n@dataclass\nclass DirectUrl:\n    url: str\n    info: InfoType\n    subdirectory: Optional[str] = None\n\n    def _remove_auth_from_netloc(self, netloc: str) -> str:\n        if \"@\" not in netloc:\n            return netloc\n        user_pass, netloc_no_user_pass = netloc.split(\"@\", 1)\n        if (\n            isinstance(self.info, VcsInfo)\n            and self.info.vcs == \"git\"\n            and user_pass == \"git\"\n        ):\n            return netloc\n        if ENV_VAR_RE.match(user_pass):\n            return netloc\n        return netloc_no_user_pass\n\n    @property\n    def redacted_url(self) -> str:\n        \"\"\"url with user:password part removed unless it is formed with\n        environment variables as specified in PEP 610, or it is ``git``\n        in the case of a git URL.\n        \"\"\"\n        purl = urllib.parse.urlsplit(self.url)\n        netloc = self._remove_auth_from_netloc(purl.netloc)\n        surl = urllib.parse.urlunsplit(\n            (purl.scheme, netloc, purl.path, purl.query, purl.fragment)\n        )\n        return surl\n\n    def validate(self) -> None:\n        self.from_dict(self.to_dict())\n\n    @classmethod\n    def from_dict(cls, d: Dict[str, Any]) -> \"DirectUrl\":\n        return DirectUrl(\n            url=_get_required(d, str, \"url\"),\n            subdirectory=_get(d, str, \"subdirectory\"),\n            info=_exactly_one_of(\n                [\n                    ArchiveInfo._from_dict(_get(d, dict, \"archive_info\")),\n                    DirInfo._from_dict(_get(d, dict, \"dir_info\")),\n                    VcsInfo._from_dict(_get(d, dict, \"vcs_info\")),\n                ]\n            ),\n        )\n\n    def to_dict(self) -> Dict[str, Any]:\n        res = _filter_none(\n            url=self.redacted_url,\n            subdirectory=self.subdirectory,\n        )\n        res[self.info.name] = self.info._to_dict()\n        return res\n\n    @classmethod\n    def from_json(cls, s: str) -> \"DirectUrl\":\n        return cls.from_dict(json.loads(s))\n\n    def to_json(self) -> str:\n        return json.dumps(self.to_dict(), sort_keys=True)\n\n    def is_local_editable(self) -> bool:\n        return isinstance(self.info, DirInfo) and self.info.editable\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/models/format_control.py","size":2486,"sha1":"22cb3e5d1d2d4921c56bee8b25322405d75660e6","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"from typing import FrozenSet, Optional, Set\n\nfrom pip._vendor.packaging.utils import canonicalize_name\n\nfrom pip._internal.exceptions import CommandError\n\n\nclass FormatControl:\n    \"\"\"Helper for managing formats from which a package can be installed.\"\"\"\n\n    __slots__ = [\"no_binary\", \"only_binary\"]\n\n    def __init__(\n        self,\n        no_binary: Optional[Set[str]] = None,\n        only_binary: Optional[Set[str]] = None,\n    ) -> None:\n        if no_binary is None:\n            no_binary = set()\n        if only_binary is None:\n            only_binary = set()\n\n        self.no_binary = no_binary\n        self.only_binary = only_binary\n\n    def __eq__(self, other: object) -> bool:\n        if not isinstance(other, self.__class__):\n            return NotImplemented\n\n        if self.__slots__ != other.__slots__:\n            return False\n\n        return all(getattr(self, k) == getattr(other, k) for k in self.__slots__)\n\n    def __repr__(self) -> str:\n        return f\"{self.__class__.__name__}({self.no_binary}, {self.only_binary})\"\n\n    @staticmethod\n    def handle_mutual_excludes(value: str, target: Set[str], other: Set[str]) -> None:\n        if value.startswith(\"-\"):\n            raise CommandError(\n                \"--no-binary / --only-binary option requires 1 argument.\"\n            )\n        new = value.split(\",\")\n        while \":all:\" in new:\n            other.clear()\n            target.clear()\n            target.add(\":all:\")\n            del new[: new.index(\":all:\") + 1]\n            # Without a none, we want to discard everything as :all: covers it\n            if \":none:\" not in new:\n                return\n        for name in new:\n            if name == \":none:\":\n                target.clear()\n                continue\n            name = canonicalize_name(name)\n            other.discard(name)\n            target.add(name)\n\n    def get_allowed_formats(self, canonical_name: str) -> FrozenSet[str]:\n        result = {\"binary\", \"source\"}\n        if canonical_name in self.only_binary:\n            result.discard(\"source\")\n        elif canonical_name in self.no_binary:\n            result.discard(\"binary\")\n        elif \":all:\" in self.only_binary:\n            result.discard(\"source\")\n        elif \":all:\" in self.no_binary:\n            result.discard(\"binary\")\n        return frozenset(result)\n\n    def disallow_binaries(self) -> None:\n        self.handle_mutual_excludes(\n            \":all:\",\n            self.no_binary,\n            self.only_binary,\n        )\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/models/index.py","size":1030,"sha1":"26707b880bf178100e5a233e43832c57a4916895","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import urllib.parse\n\n\nclass PackageIndex:\n    \"\"\"Represents a Package Index and provides easier access to endpoints\"\"\"\n\n    __slots__ = [\"url\", \"netloc\", \"simple_url\", \"pypi_url\", \"file_storage_domain\"]\n\n    def __init__(self, url: str, file_storage_domain: str) -> None:\n        super().__init__()\n        self.url = url\n        self.netloc = urllib.parse.urlsplit(url).netloc\n        self.simple_url = self._url_for_path(\"simple\")\n        self.pypi_url = self._url_for_path(\"pypi\")\n\n        # This is part of a temporary hack used to block installs of PyPI\n        # packages which depend on external urls only necessary until PyPI can\n        # block such packages themselves\n        self.file_storage_domain = file_storage_domain\n\n    def _url_for_path(self, path: str) -> str:\n        return urllib.parse.urljoin(self.url, path)\n\n\nPyPI = PackageIndex(\"https://pypi.org/\", file_storage_domain=\"files.pythonhosted.org\")\nTestPyPI = PackageIndex(\n    \"https://test.pypi.org/\", file_storage_domain=\"test-files.pythonhosted.org\"\n)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/models/installation_report.py","size":2818,"sha1":"8e0e2f7c9ae3d859a2f11d6dbbc5f7aea26cc1e5","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"from typing import Any, Dict, Sequence\n\nfrom pip._vendor.packaging.markers import default_environment\n\nfrom pip import __version__\nfrom pip._internal.req.req_install import InstallRequirement\n\n\nclass InstallationReport:\n    def __init__(self, install_requirements: Sequence[InstallRequirement]):\n        self._install_requirements = install_requirements\n\n    @classmethod\n    def _install_req_to_dict(cls, ireq: InstallRequirement) -> Dict[str, Any]:\n        assert ireq.download_info, f\"No download_info for {ireq}\"\n        res = {\n            # PEP 610 json for the download URL. download_info.archive_info.hashes may\n            # be absent when the requirement was installed from the wheel cache\n            # and the cache entry was populated by an older pip version that did not\n            # record origin.json.\n            \"download_info\": ireq.download_info.to_dict(),\n            # is_direct is true if the requirement was a direct URL reference (which\n            # includes editable requirements), and false if the requirement was\n            # downloaded from a PEP 503 index or --find-links.\n            \"is_direct\": ireq.is_direct,\n            # is_yanked is true if the requirement was yanked from the index, but\n            # was still selected by pip to conform to PEP 592.\n            \"is_yanked\": ireq.link.is_yanked if ireq.link else False,\n            # requested is true if the requirement was specified by the user (aka\n            # top level requirement), and false if it was installed as a dependency of a\n            # requirement. https://peps.python.org/pep-0376/#requested\n            \"requested\": ireq.user_supplied,\n            # PEP 566 json encoding for metadata\n            # https://www.python.org/dev/peps/pep-0566/#json-compatible-metadata\n            \"metadata\": ireq.get_dist().metadata_dict,\n        }\n        if ireq.user_supplied and ireq.extras:\n            # For top level requirements, the list of requested extras, if any.\n            res[\"requested_extras\"] = sorted(ireq.extras)\n        return res\n\n    def to_dict(self) -> Dict[str, Any]:\n        return {\n            \"version\": \"1\",\n            \"pip_version\": __version__,\n            \"install\": [\n                self._install_req_to_dict(ireq) for ireq in self._install_requirements\n            ],\n            # https://peps.python.org/pep-0508/#environment-markers\n            # TODO: currently, the resolver uses the default environment to evaluate\n            # environment markers, so that is what we report here. In the future, it\n            # should also take into account options such as --python-version or\n            # --platform, perhaps under the form of an environment_override field?\n            # https://github.com/pypa/pip/issues/11198\n            \"environment\": default_environment(),\n        }\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/models/link.py","size":21448,"sha1":"9ce2dda69621a831193c67dd02f75b4a380fd95c","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import functools\nimport itertools\nimport logging\nimport os\nimport posixpath\nimport re\nimport urllib.parse\nfrom dataclasses import dataclass\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Dict,\n    List,\n    Mapping,\n    NamedTuple,\n    Optional,\n    Tuple,\n    Union,\n)\n\nfrom pip._internal.utils.deprecation import deprecated\nfrom pip._internal.utils.filetypes import WHEEL_EXTENSION\nfrom pip._internal.utils.hashes import Hashes\nfrom pip._internal.utils.misc import (\n    pairwise,\n    redact_auth_from_url,\n    split_auth_from_netloc,\n    splitext,\n)\nfrom pip._internal.utils.urls import path_to_url, url_to_path\n\nif TYPE_CHECKING:\n    from pip._internal.index.collector import IndexContent\n\nlogger = logging.getLogger(__name__)\n\n\n# Order matters, earlier hashes have a precedence over later hashes for what\n# we will pick to use.\n_SUPPORTED_HASHES = (\"sha512\", \"sha384\", \"sha256\", \"sha224\", \"sha1\", \"md5\")\n\n\n@dataclass(frozen=True)\nclass LinkHash:\n    \"\"\"Links to content may have embedded hash values. This class parses those.\n\n    `name` must be any member of `_SUPPORTED_HASHES`.\n\n    This class can be converted to and from `ArchiveInfo`. While ArchiveInfo intends to\n    be JSON-serializable to conform to PEP 610, this class contains the logic for\n    parsing a hash name and value for correctness, and then checking whether that hash\n    conforms to a schema with `.is_hash_allowed()`.\"\"\"\n\n    name: str\n    value: str\n\n    _hash_url_fragment_re = re.compile(\n        # NB: we do not validate that the second group (.*) is a valid hex\n        # digest. Instead, we simply keep that string in this class, and then check it\n        # against Hashes when hash-checking is needed. This is easier to debug than\n        # proactively discarding an invalid hex digest, as we handle incorrect hashes\n        # and malformed hashes in the same place.\n        r\"[#&]({choices})=([^&]*)\".format(\n            choices=\"|\".join(re.escape(hash_name) for hash_name in _SUPPORTED_HASHES)\n        ),\n    )\n\n    def __post_init__(self) -> None:\n        assert self.name in _SUPPORTED_HASHES\n\n    @classmethod\n    @functools.lru_cache(maxsize=None)\n    def find_hash_url_fragment(cls, url: str) -> Optional[\"LinkHash\"]:\n        \"\"\"Search a string for a checksum algorithm name and encoded output value.\"\"\"\n        match = cls._hash_url_fragment_re.search(url)\n        if match is None:\n            return None\n        name, value = match.groups()\n        return cls(name=name, value=value)\n\n    def as_dict(self) -> Dict[str, str]:\n        return {self.name: self.value}\n\n    def as_hashes(self) -> Hashes:\n        \"\"\"Return a Hashes instance which checks only for the current hash.\"\"\"\n        return Hashes({self.name: [self.value]})\n\n    def is_hash_allowed(self, hashes: Optional[Hashes]) -> bool:\n        \"\"\"\n        Return True if the current hash is allowed by `hashes`.\n        \"\"\"\n        if hashes is None:\n            return False\n        return hashes.is_hash_allowed(self.name, hex_digest=self.value)\n\n\n@dataclass(frozen=True)\nclass MetadataFile:\n    \"\"\"Information about a core metadata file associated with a distribution.\"\"\"\n\n    hashes: Optional[Dict[str, str]]\n\n    def __post_init__(self) -> None:\n        if self.hashes is not None:\n            assert all(name in _SUPPORTED_HASHES for name in self.hashes)\n\n\ndef supported_hashes(hashes: Optional[Dict[str, str]]) -> Optional[Dict[str, str]]:\n    # Remove any unsupported hash types from the mapping. If this leaves no\n    # supported hashes, return None\n    if hashes is None:\n        return None\n    hashes = {n: v for n, v in hashes.items() if n in _SUPPORTED_HASHES}\n    if not hashes:\n        return None\n    return hashes\n\n\ndef _clean_url_path_part(part: str) -> str:\n    \"\"\"\n    Clean a \"part\" of a URL path (i.e. after splitting on \"@\" characters).\n    \"\"\"\n    # We unquote prior to quoting to make sure nothing is double quoted.\n    return urllib.parse.quote(urllib.parse.unquote(part))\n\n\ndef _clean_file_url_path(part: str) -> str:\n    \"\"\"\n    Clean the first part of a URL path that corresponds to a local\n    filesystem path (i.e. the first part after splitting on \"@\" characters).\n    \"\"\"\n    # We unquote prior to quoting to make sure nothing is double quoted.\n    # Also, on Windows the path part might contain a drive letter which\n    # should not be quoted. On Linux where drive letters do not\n    # exist, the colon should be quoted. We rely on urllib.request\n    # to do the right thing here.\n    return urllib.request.pathname2url(urllib.request.url2pathname(part))\n\n\n# percent-encoded:                   /\n_reserved_chars_re = re.compile(\"(@|%2F)\", re.IGNORECASE)\n\n\ndef _clean_url_path(path: str, is_local_path: bool) -> str:\n    \"\"\"\n    Clean the path portion of a URL.\n    \"\"\"\n    if is_local_path:\n        clean_func = _clean_file_url_path\n    else:\n        clean_func = _clean_url_path_part\n\n    # Split on the reserved characters prior to cleaning so that\n    # revision strings in VCS URLs are properly preserved.\n    parts = _reserved_chars_re.split(path)\n\n    cleaned_parts = []\n    for to_clean, reserved in pairwise(itertools.chain(parts, [\"\"])):\n        cleaned_parts.append(clean_func(to_clean))\n        # Normalize %xx escapes (e.g. %2f -> %2F)\n        cleaned_parts.append(reserved.upper())\n\n    return \"\".join(cleaned_parts)\n\n\ndef _ensure_quoted_url(url: str) -> str:\n    \"\"\"\n    Make sure a link is fully quoted.\n    For example, if ' ' occurs in the URL, it will be replaced with \"%20\",\n    and without double-quoting other characters.\n    \"\"\"\n    # Split the URL into parts according to the general structure\n    # `scheme://netloc/path?query#fragment`.\n    result = urllib.parse.urlsplit(url)\n    # If the netloc is empty, then the URL refers to a local filesystem path.\n    is_local_path = not result.netloc\n    path = _clean_url_path(result.path, is_local_path=is_local_path)\n    return urllib.parse.urlunsplit(result._replace(path=path))\n\n\ndef _absolute_link_url(base_url: str, url: str) -> str:\n    \"\"\"\n    A faster implementation of urllib.parse.urljoin with a shortcut\n    for absolute http/https URLs.\n    \"\"\"\n    if url.startswith((\"https://\", \"http://\")):\n        return url\n    else:\n        return urllib.parse.urljoin(base_url, url)\n\n\n@functools.total_ordering\nclass Link:\n    \"\"\"Represents a parsed link from a Package Index's simple URL\"\"\"\n\n    __slots__ = [\n        \"_parsed_url\",\n        \"_url\",\n        \"_path\",\n        \"_hashes\",\n        \"comes_from\",\n        \"requires_python\",\n        \"yanked_reason\",\n        \"metadata_file_data\",\n        \"cache_link_parsing\",\n        \"egg_fragment\",\n    ]\n\n    def __init__(\n        self,\n        url: str,\n        comes_from: Optional[Union[str, \"IndexContent\"]] = None,\n        requires_python: Optional[str] = None,\n        yanked_reason: Optional[str] = None,\n        metadata_file_data: Optional[MetadataFile] = None,\n        cache_link_parsing: bool = True,\n        hashes: Optional[Mapping[str, str]] = None,\n    ) -> None:\n        \"\"\"\n        :param url: url of the resource pointed to (href of the link)\n        :param comes_from: instance of IndexContent where the link was found,\n            or string.\n        :param requires_python: String containing the `Requires-Python`\n            metadata field, specified in PEP 345. This may be specified by\n            a data-requires-python attribute in the HTML link tag, as\n            described in PEP 503.\n        :param yanked_reason: the reason the file has been yanked, if the\n            file has been yanked, or None if the file hasn't been yanked.\n            This is the value of the \"data-yanked\" attribute, if present, in\n            a simple repository HTML link. If the file has been yanked but\n            no reason was provided, this should be the empty string. See\n            PEP 592 for more information and the specification.\n        :param metadata_file_data: the metadata attached to the file, or None if\n            no such metadata is provided. This argument, if not None, indicates\n            that a separate metadata file exists, and also optionally supplies\n            hashes for that file.\n        :param cache_link_parsing: A flag that is used elsewhere to determine\n            whether resources retrieved from this link should be cached. PyPI\n            URLs should generally have this set to False, for example.\n        :param hashes: A mapping of hash names to digests to allow us to\n            determine the validity of a download.\n        \"\"\"\n\n        # The comes_from, requires_python, and metadata_file_data arguments are\n        # only used by classmethods of this class, and are not used in client\n        # code directly.\n\n        # url can be a UNC windows share\n        if url.startswith(\"\\\\\\\\\"):\n            url = path_to_url(url)\n\n        self._parsed_url = urllib.parse.urlsplit(url)\n        # Store the url as a private attribute to prevent accidentally\n        # trying to set a new value.\n        self._url = url\n        # The .path property is hot, so calculate its value ahead of time.\n        self._path = urllib.parse.unquote(self._parsed_url.path)\n\n        link_hash = LinkHash.find_hash_url_fragment(url)\n        hashes_from_link = {} if link_hash is None else link_hash.as_dict()\n        if hashes is None:\n            self._hashes = hashes_from_link\n        else:\n            self._hashes = {**hashes, **hashes_from_link}\n\n        self.comes_from = comes_from\n        self.requires_python = requires_python if requires_python else None\n        self.yanked_reason = yanked_reason\n        self.metadata_file_data = metadata_file_data\n\n        self.cache_link_parsing = cache_link_parsing\n        self.egg_fragment = self._egg_fragment()\n\n    @classmethod\n    def from_json(\n        cls,\n        file_data: Dict[str, Any],\n        page_url: str,\n    ) -> Optional[\"Link\"]:\n        \"\"\"\n        Convert an pypi json document from a simple repository page into a Link.\n        \"\"\"\n        file_url = file_data.get(\"url\")\n        if file_url is None:\n            return None\n\n        url = _ensure_quoted_url(_absolute_link_url(page_url, file_url))\n        pyrequire = file_data.get(\"requires-python\")\n        yanked_reason = file_data.get(\"yanked\")\n        hashes = file_data.get(\"hashes\", {})\n\n        # PEP 714: Indexes must use the name core-metadata, but\n        # clients should support the old name as a fallback for compatibility.\n        metadata_info = file_data.get(\"core-metadata\")\n        if metadata_info is None:\n            metadata_info = file_data.get(\"dist-info-metadata\")\n\n        # The metadata info value may be a boolean, or a dict of hashes.\n        if isinstance(metadata_info, dict):\n            # The file exists, and hashes have been supplied\n            metadata_file_data = MetadataFile(supported_hashes(metadata_info))\n        elif metadata_info:\n            # The file exists, but there are no hashes\n            metadata_file_data = MetadataFile(None)\n        else:\n            # False or not present: the file does not exist\n            metadata_file_data = None\n\n        # The Link.yanked_reason expects an empty string instead of a boolean.\n        if yanked_reason and not isinstance(yanked_reason, str):\n            yanked_reason = \"\"\n        # The Link.yanked_reason expects None instead of False.\n        elif not yanked_reason:\n            yanked_reason = None\n\n        return cls(\n            url,\n            comes_from=page_url,\n            requires_python=pyrequire,\n            yanked_reason=yanked_reason,\n            hashes=hashes,\n            metadata_file_data=metadata_file_data,\n        )\n\n    @classmethod\n    def from_element(\n        cls,\n        anchor_attribs: Dict[str, Optional[str]],\n        page_url: str,\n        base_url: str,\n    ) -> Optional[\"Link\"]:\n        \"\"\"\n        Convert an anchor element's attributes in a simple repository page to a Link.\n        \"\"\"\n        href = anchor_attribs.get(\"href\")\n        if not href:\n            return None\n\n        url = _ensure_quoted_url(_absolute_link_url(base_url, href))\n        pyrequire = anchor_attribs.get(\"data-requires-python\")\n        yanked_reason = anchor_attribs.get(\"data-yanked\")\n\n        # PEP 714: Indexes must use the name data-core-metadata, but\n        # clients should support the old name as a fallback for compatibility.\n        metadata_info = anchor_attribs.get(\"data-core-metadata\")\n        if metadata_info is None:\n            metadata_info = anchor_attribs.get(\"data-dist-info-metadata\")\n        # The metadata info value may be the string \"true\", or a string of\n        # the form \"hashname=hashval\"\n        if metadata_info == \"true\":\n            # The file exists, but there are no hashes\n            metadata_file_data = MetadataFile(None)\n        elif metadata_info is None:\n            # The file does not exist\n            metadata_file_data = None\n        else:\n            # The file exists, and hashes have been supplied\n            hashname, sep, hashval = metadata_info.partition(\"=\")\n            if sep == \"=\":\n                metadata_file_data = MetadataFile(supported_hashes({hashname: hashval}))\n            else:\n                # Error - data is wrong. Treat as no hashes supplied.\n                logger.debug(\n                    \"Index returned invalid data-dist-info-metadata value: %s\",\n                    metadata_info,\n                )\n                metadata_file_data = MetadataFile(None)\n\n        return cls(\n            url,\n            comes_from=page_url,\n            requires_python=pyrequire,\n            yanked_reason=yanked_reason,\n            metadata_file_data=metadata_file_data,\n        )\n\n    def __str__(self) -> str:\n        if self.requires_python:\n            rp = f\" (requires-python:{self.requires_python})\"\n        else:\n            rp = \"\"\n        if self.comes_from:\n            return f\"{redact_auth_from_url(self._url)} (from {self.comes_from}){rp}\"\n        else:\n            return redact_auth_from_url(str(self._url))\n\n    def __repr__(self) -> str:\n        return f\"<Link {self}>\"\n\n    def __hash__(self) -> int:\n        return hash(self.url)\n\n    def __eq__(self, other: Any) -> bool:\n        if not isinstance(other, Link):\n            return NotImplemented\n        return self.url == other.url\n\n    def __lt__(self, other: Any) -> bool:\n        if not isinstance(other, Link):\n            return NotImplemented\n        return self.url < other.url\n\n    @property\n    def url(self) -> str:\n        return self._url\n\n    @property\n    def filename(self) -> str:\n        path = self.path.rstrip(\"/\")\n        name = posixpath.basename(path)\n        if not name:\n            # Make sure we don't leak auth information if the netloc\n            # includes a username and password.\n            netloc, user_pass = split_auth_from_netloc(self.netloc)\n            return netloc\n\n        name = urllib.parse.unquote(name)\n        assert name, f\"URL {self._url!r} produced no filename\"\n        return name\n\n    @property\n    def file_path(self) -> str:\n        return url_to_path(self.url)\n\n    @property\n    def scheme(self) -> str:\n        return self._parsed_url.scheme\n\n    @property\n    def netloc(self) -> str:\n        \"\"\"\n        This can contain auth information.\n        \"\"\"\n        return self._parsed_url.netloc\n\n    @property\n    def path(self) -> str:\n        return self._path\n\n    def splitext(self) -> Tuple[str, str]:\n        return splitext(posixpath.basename(self.path.rstrip(\"/\")))\n\n    @property\n    def ext(self) -> str:\n        return self.splitext()[1]\n\n    @property\n    def url_without_fragment(self) -> str:\n        scheme, netloc, path, query, fragment = self._parsed_url\n        return urllib.parse.urlunsplit((scheme, netloc, path, query, \"\"))\n\n    _egg_fragment_re = re.compile(r\"[#&]egg=([^&]*)\")\n\n    # Per PEP 508.\n    _project_name_re = re.compile(\n        r\"^([A-Z0-9]|[A-Z0-9][A-Z0-9._-]*[A-Z0-9])$\", re.IGNORECASE\n    )\n\n    def _egg_fragment(self) -> Optional[str]:\n        match = self._egg_fragment_re.search(self._url)\n        if not match:\n            return None\n\n        # An egg fragment looks like a PEP 508 project name, along with\n        # an optional extras specifier. Anything else is invalid.\n        project_name = match.group(1)\n        if not self._project_name_re.match(project_name):\n            deprecated(\n                reason=f\"{self} contains an egg fragment with a non-PEP 508 name.\",\n                replacement=\"to use the req @ url syntax, and remove the egg fragment\",\n                gone_in=\"25.1\",\n                issue=13157,\n            )\n\n        return project_name\n\n    _subdirectory_fragment_re = re.compile(r\"[#&]subdirectory=([^&]*)\")\n\n    @property\n    def subdirectory_fragment(self) -> Optional[str]:\n        match = self._subdirectory_fragment_re.search(self._url)\n        if not match:\n            return None\n        return match.group(1)\n\n    def metadata_link(self) -> Optional[\"Link\"]:\n        \"\"\"Return a link to the associated core metadata file (if any).\"\"\"\n        if self.metadata_file_data is None:\n            return None\n        metadata_url = f\"{self.url_without_fragment}.metadata\"\n        if self.metadata_file_data.hashes is None:\n            return Link(metadata_url)\n        return Link(metadata_url, hashes=self.metadata_file_data.hashes)\n\n    def as_hashes(self) -> Hashes:\n        return Hashes({k: [v] for k, v in self._hashes.items()})\n\n    @property\n    def hash(self) -> Optional[str]:\n        return next(iter(self._hashes.values()), None)\n\n    @property\n    def hash_name(self) -> Optional[str]:\n        return next(iter(self._hashes), None)\n\n    @property\n    def show_url(self) -> str:\n        return posixpath.basename(self._url.split(\"#\", 1)[0].split(\"?\", 1)[0])\n\n    @property\n    def is_file(self) -> bool:\n        return self.scheme == \"file\"\n\n    def is_existing_dir(self) -> bool:\n        return self.is_file and os.path.isdir(self.file_path)\n\n    @property\n    def is_wheel(self) -> bool:\n        return self.ext == WHEEL_EXTENSION\n\n    @property\n    def is_vcs(self) -> bool:\n        from pip._internal.vcs import vcs\n\n        return self.scheme in vcs.all_schemes\n\n    @property\n    def is_yanked(self) -> bool:\n        return self.yanked_reason is not None\n\n    @property\n    def has_hash(self) -> bool:\n        return bool(self._hashes)\n\n    def is_hash_allowed(self, hashes: Optional[Hashes]) -> bool:\n        \"\"\"\n        Return True if the link has a hash and it is allowed by `hashes`.\n        \"\"\"\n        if hashes is None:\n            return False\n        return any(hashes.is_hash_allowed(k, v) for k, v in self._hashes.items())\n\n\nclass _CleanResult(NamedTuple):\n    \"\"\"Convert link for equivalency check.\n\n    This is used in the resolver to check whether two URL-specified requirements\n    likely point to the same distribution and can be considered equivalent. This\n    equivalency logic avoids comparing URLs literally, which can be too strict\n    (e.g. \"a=1&b=2\" vs \"b=2&a=1\") and produce conflicts unexpecting to users.\n\n    Currently this does three things:\n\n    1. Drop the basic auth part. This is technically wrong since a server can\n       serve different content based on auth, but if it does that, it is even\n       impossible to guarantee two URLs without auth are equivalent, since\n       the user can input different auth information when prompted. So the\n       practical solution is to assume the auth doesn't affect the response.\n    2. Parse the query to avoid the ordering issue. Note that ordering under the\n       same key in the query are NOT cleaned; i.e. \"a=1&a=2\" and \"a=2&a=1\" are\n       still considered different.\n    3. Explicitly drop most of the fragment part, except ``subdirectory=`` and\n       hash values, since it should have no impact the downloaded content. Note\n       that this drops the \"egg=\" part historically used to denote the requested\n       project (and extras), which is wrong in the strictest sense, but too many\n       people are supplying it inconsistently to cause superfluous resolution\n       conflicts, so we choose to also ignore them.\n    \"\"\"\n\n    parsed: urllib.parse.SplitResult\n    query: Dict[str, List[str]]\n    subdirectory: str\n    hashes: Dict[str, str]\n\n\ndef _clean_link(link: Link) -> _CleanResult:\n    parsed = link._parsed_url\n    netloc = parsed.netloc.rsplit(\"@\", 1)[-1]\n    # According to RFC 8089, an empty host in file: means localhost.\n    if parsed.scheme == \"file\" and not netloc:\n        netloc = \"localhost\"\n    fragment = urllib.parse.parse_qs(parsed.fragment)\n    if \"egg\" in fragment:\n        logger.debug(\"Ignoring egg= fragment in %s\", link)\n    try:\n        # If there are multiple subdirectory values, use the first one.\n        # This matches the behavior of Link.subdirectory_fragment.\n        subdirectory = fragment[\"subdirectory\"][0]\n    except (IndexError, KeyError):\n        subdirectory = \"\"\n    # If there are multiple hash values under the same algorithm, use the\n    # first one. This matches the behavior of Link.hash_value.\n    hashes = {k: fragment[k][0] for k in _SUPPORTED_HASHES if k in fragment}\n    return _CleanResult(\n        parsed=parsed._replace(netloc=netloc, query=\"\", fragment=\"\"),\n        query=urllib.parse.parse_qs(parsed.query),\n        subdirectory=subdirectory,\n        hashes=hashes,\n    )\n\n\n@functools.lru_cache(maxsize=None)\ndef links_equivalent(link1: Link, link2: Link) -> bool:\n    return _clean_link(link1) == _clean_link(link2)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/models/scheme.py","size":575,"sha1":"0dfcfe35e05728122f7eb4f279d135358343702f","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"\nFor types associated with installation schemes.\n\nFor a general overview of available schemes and their context, see\nhttps://docs.python.org/3/install/index.html#alternate-installation.\n\"\"\"\n\nfrom dataclasses import dataclass\n\nSCHEME_KEYS = [\"platlib\", \"purelib\", \"headers\", \"scripts\", \"data\"]\n\n\n@dataclass(frozen=True)\nclass Scheme:\n    \"\"\"A Scheme holds paths which are used as the base directories for\n    artifacts associated with a Python package.\n    \"\"\"\n\n    __slots__ = SCHEME_KEYS\n\n    platlib: str\n    purelib: str\n    headers: str\n    scripts: str\n    data: str\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/models/search_scope.py","size":4531,"sha1":"0f72e06bd7b63b9616d87d561d8bba6997f82775","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import itertools\nimport logging\nimport os\nimport posixpath\nimport urllib.parse\nfrom dataclasses import dataclass\nfrom typing import List\n\nfrom pip._vendor.packaging.utils import canonicalize_name\n\nfrom pip._internal.models.index import PyPI\nfrom pip._internal.utils.compat import has_tls\nfrom pip._internal.utils.misc import normalize_path, redact_auth_from_url\n\nlogger = logging.getLogger(__name__)\n\n\n@dataclass(frozen=True)\nclass SearchScope:\n    \"\"\"\n    Encapsulates the locations that pip is configured to search.\n    \"\"\"\n\n    __slots__ = [\"find_links\", \"index_urls\", \"no_index\"]\n\n    find_links: List[str]\n    index_urls: List[str]\n    no_index: bool\n\n    @classmethod\n    def create(\n        cls,\n        find_links: List[str],\n        index_urls: List[str],\n        no_index: bool,\n    ) -> \"SearchScope\":\n        \"\"\"\n        Create a SearchScope object after normalizing the `find_links`.\n        \"\"\"\n        # Build find_links. If an argument starts with ~, it may be\n        # a local file relative to a home directory. So try normalizing\n        # it and if it exists, use the normalized version.\n        # This is deliberately conservative - it might be fine just to\n        # blindly normalize anything starting with a ~...\n        built_find_links: List[str] = []\n        for link in find_links:\n            if link.startswith(\"~\"):\n                new_link = normalize_path(link)\n                if os.path.exists(new_link):\n                    link = new_link\n            built_find_links.append(link)\n\n        # If we don't have TLS enabled, then WARN if anyplace we're looking\n        # relies on TLS.\n        if not has_tls():\n            for link in itertools.chain(index_urls, built_find_links):\n                parsed = urllib.parse.urlparse(link)\n                if parsed.scheme == \"https\":\n                    logger.warning(\n                        \"pip is configured with locations that require \"\n                        \"TLS/SSL, however the ssl module in Python is not \"\n                        \"available.\"\n                    )\n                    break\n\n        return cls(\n            find_links=built_find_links,\n            index_urls=index_urls,\n            no_index=no_index,\n        )\n\n    def get_formatted_locations(self) -> str:\n        lines = []\n        redacted_index_urls = []\n        if self.index_urls and self.index_urls != [PyPI.simple_url]:\n            for url in self.index_urls:\n                redacted_index_url = redact_auth_from_url(url)\n\n                # Parse the URL\n                purl = urllib.parse.urlsplit(redacted_index_url)\n\n                # URL is generally invalid if scheme and netloc is missing\n                # there are issues with Python and URL parsing, so this test\n                # is a bit crude. See bpo-20271, bpo-23505. Python doesn't\n                # always parse invalid URLs correctly - it should raise\n                # exceptions for malformed URLs\n                if not purl.scheme and not purl.netloc:\n                    logger.warning(\n                        'The index url \"%s\" seems invalid, please provide a scheme.',\n                        redacted_index_url,\n                    )\n\n                redacted_index_urls.append(redacted_index_url)\n\n            lines.append(\n                \"Looking in indexes: {}\".format(\", \".join(redacted_index_urls))\n            )\n\n        if self.find_links:\n            lines.append(\n                \"Looking in links: {}\".format(\n                    \", \".join(redact_auth_from_url(url) for url in self.find_links)\n                )\n            )\n        return \"\\n\".join(lines)\n\n    def get_index_urls_locations(self, project_name: str) -> List[str]:\n        \"\"\"Returns the locations found via self.index_urls\n\n        Checks the url_name on the main (first in the list) index and\n        use this url_name to produce all locations\n        \"\"\"\n\n        def mkurl_pypi_url(url: str) -> str:\n            loc = posixpath.join(\n                url, urllib.parse.quote(canonicalize_name(project_name))\n            )\n            # For maximum compatibility with easy_install, ensure the path\n            # ends in a trailing slash.  Although this isn't in the spec\n            # (and PyPI can handle it without the slash) some other index\n            # implementations might break if they relied on easy_install's\n            # behavior.\n            if not loc.endswith(\"/\"):\n                loc = loc + \"/\"\n            return loc\n\n        return [mkurl_pypi_url(url) for url in self.index_urls]\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/models/selection_prefs.py","size":2015,"sha1":"df4ee02f80ae25323daaf963aa49e64a4dd61931","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"from typing import Optional\n\nfrom pip._internal.models.format_control import FormatControl\n\n\n# TODO: This needs Python 3.10's improved slots support for dataclasses\n# to be converted into a dataclass.\nclass SelectionPreferences:\n    \"\"\"\n    Encapsulates the candidate selection preferences for downloading\n    and installing files.\n    \"\"\"\n\n    __slots__ = [\n        \"allow_yanked\",\n        \"allow_all_prereleases\",\n        \"format_control\",\n        \"prefer_binary\",\n        \"ignore_requires_python\",\n    ]\n\n    # Don't include an allow_yanked default value to make sure each call\n    # site considers whether yanked releases are allowed. This also causes\n    # that decision to be made explicit in the calling code, which helps\n    # people when reading the code.\n    def __init__(\n        self,\n        allow_yanked: bool,\n        allow_all_prereleases: bool = False,\n        format_control: Optional[FormatControl] = None,\n        prefer_binary: bool = False,\n        ignore_requires_python: Optional[bool] = None,\n    ) -> None:\n        \"\"\"Create a SelectionPreferences object.\n\n        :param allow_yanked: Whether files marked as yanked (in the sense\n            of PEP 592) are permitted to be candidates for install.\n        :param format_control: A FormatControl object or None. Used to control\n            the selection of source packages / binary packages when consulting\n            the index and links.\n        :param prefer_binary: Whether to prefer an old, but valid, binary\n            dist over a new source dist.\n        :param ignore_requires_python: Whether to ignore incompatible\n            \"Requires-Python\" values in links. Defaults to False.\n        \"\"\"\n        if ignore_requires_python is None:\n            ignore_requires_python = False\n\n        self.allow_yanked = allow_yanked\n        self.allow_all_prereleases = allow_all_prereleases\n        self.format_control = format_control\n        self.prefer_binary = prefer_binary\n        self.ignore_requires_python = ignore_requires_python\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/models/target_python.py","size":4271,"sha1":"66180881c5761052140add108acedea805abb6e8","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import sys\nfrom typing import List, Optional, Set, Tuple\n\nfrom pip._vendor.packaging.tags import Tag\n\nfrom pip._internal.utils.compatibility_tags import get_supported, version_info_to_nodot\nfrom pip._internal.utils.misc import normalize_version_info\n\n\nclass TargetPython:\n    \"\"\"\n    Encapsulates the properties of a Python interpreter one is targeting\n    for a package install, download, etc.\n    \"\"\"\n\n    __slots__ = [\n        \"_given_py_version_info\",\n        \"abis\",\n        \"implementation\",\n        \"platforms\",\n        \"py_version\",\n        \"py_version_info\",\n        \"_valid_tags\",\n        \"_valid_tags_set\",\n    ]\n\n    def __init__(\n        self,\n        platforms: Optional[List[str]] = None,\n        py_version_info: Optional[Tuple[int, ...]] = None,\n        abis: Optional[List[str]] = None,\n        implementation: Optional[str] = None,\n    ) -> None:\n        \"\"\"\n        :param platforms: A list of strings or None. If None, searches for\n            packages that are supported by the current system. Otherwise, will\n            find packages that can be built on the platforms passed in. These\n            packages will only be downloaded for distribution: they will\n            not be built locally.\n        :param py_version_info: An optional tuple of ints representing the\n            Python version information to use (e.g. `sys.version_info[:3]`).\n            This can have length 1, 2, or 3 when provided.\n        :param abis: A list of strings or None. This is passed to\n            compatibility_tags.py's get_supported() function as is.\n        :param implementation: A string or None. This is passed to\n            compatibility_tags.py's get_supported() function as is.\n        \"\"\"\n        # Store the given py_version_info for when we call get_supported().\n        self._given_py_version_info = py_version_info\n\n        if py_version_info is None:\n            py_version_info = sys.version_info[:3]\n        else:\n            py_version_info = normalize_version_info(py_version_info)\n\n        py_version = \".\".join(map(str, py_version_info[:2]))\n\n        self.abis = abis\n        self.implementation = implementation\n        self.platforms = platforms\n        self.py_version = py_version\n        self.py_version_info = py_version_info\n\n        # This is used to cache the return value of get_(un)sorted_tags.\n        self._valid_tags: Optional[List[Tag]] = None\n        self._valid_tags_set: Optional[Set[Tag]] = None\n\n    def format_given(self) -> str:\n        \"\"\"\n        Format the given, non-None attributes for display.\n        \"\"\"\n        display_version = None\n        if self._given_py_version_info is not None:\n            display_version = \".\".join(\n                str(part) for part in self._given_py_version_info\n            )\n\n        key_values = [\n            (\"platforms\", self.platforms),\n            (\"version_info\", display_version),\n            (\"abis\", self.abis),\n            (\"implementation\", self.implementation),\n        ]\n        return \" \".join(\n            f\"{key}={value!r}\" for key, value in key_values if value is not None\n        )\n\n    def get_sorted_tags(self) -> List[Tag]:\n        \"\"\"\n        Return the supported PEP 425 tags to check wheel candidates against.\n\n        The tags are returned in order of preference (most preferred first).\n        \"\"\"\n        if self._valid_tags is None:\n            # Pass versions=None if no py_version_info was given since\n            # versions=None uses special default logic.\n            py_version_info = self._given_py_version_info\n            if py_version_info is None:\n                version = None\n            else:\n                version = version_info_to_nodot(py_version_info)\n\n            tags = get_supported(\n                version=version,\n                platforms=self.platforms,\n                abis=self.abis,\n                impl=self.implementation,\n            )\n            self._valid_tags = tags\n\n        return self._valid_tags\n\n    def get_unsorted_tags(self) -> Set[Tag]:\n        \"\"\"Exactly the same as get_sorted_tags, but returns a set.\n\n        This is important for performance.\n        \"\"\"\n        if self._valid_tags_set is None:\n            self._valid_tags_set = set(self.get_sorted_tags())\n\n        return self._valid_tags_set\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/models/wheel.py","size":4539,"sha1":"aac563f1f3cdd360750ef07f1c07eba1d679e78d","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"Represents a wheel file and provides access to the various parts of the\nname that have meaning.\n\"\"\"\n\nimport re\nfrom typing import Dict, Iterable, List\n\nfrom pip._vendor.packaging.tags import Tag\nfrom pip._vendor.packaging.utils import (\n    InvalidWheelFilename as PackagingInvalidWheelName,\n)\nfrom pip._vendor.packaging.utils import parse_wheel_filename\n\nfrom pip._internal.exceptions import InvalidWheelFilename\nfrom pip._internal.utils.deprecation import deprecated\n\n\nclass Wheel:\n    \"\"\"A wheel file\"\"\"\n\n    wheel_file_re = re.compile(\n        r\"\"\"^(?P<namever>(?P<name>[^\\s-]+?)-(?P<ver>[^\\s-]*?))\n        ((-(?P<build>\\d[^-]*?))?-(?P<pyver>[^\\s-]+?)-(?P<abi>[^\\s-]+?)-(?P<plat>[^\\s-]+?)\n        \\.whl|\\.dist-info)$\"\"\",\n        re.VERBOSE,\n    )\n\n    def __init__(self, filename: str) -> None:\n        \"\"\"\n        :raises InvalidWheelFilename: when the filename is invalid for a wheel\n        \"\"\"\n        wheel_info = self.wheel_file_re.match(filename)\n        if not wheel_info:\n            raise InvalidWheelFilename(f\"{filename} is not a valid wheel filename.\")\n        self.filename = filename\n        self.name = wheel_info.group(\"name\").replace(\"_\", \"-\")\n        _version = wheel_info.group(\"ver\")\n        if \"_\" in _version:\n            try:\n                parse_wheel_filename(filename)\n            except PackagingInvalidWheelName as e:\n                deprecated(\n                    reason=(\n                        f\"Wheel filename {filename!r} is not correctly normalised. \"\n                        \"Future versions of pip will raise the following error:\\n\"\n                        f\"{e.args[0]}\\n\\n\"\n                    ),\n                    replacement=(\n                        \"to rename the wheel to use a correctly normalised \"\n                        \"name (this may require updating the version in \"\n                        \"the project metadata)\"\n                    ),\n                    gone_in=\"25.1\",\n                    issue=12938,\n                )\n\n            _version = _version.replace(\"_\", \"-\")\n\n        self.version = _version\n        self.build_tag = wheel_info.group(\"build\")\n        self.pyversions = wheel_info.group(\"pyver\").split(\".\")\n        self.abis = wheel_info.group(\"abi\").split(\".\")\n        self.plats = wheel_info.group(\"plat\").split(\".\")\n\n        # All the tag combinations from this file\n        self.file_tags = {\n            Tag(x, y, z) for x in self.pyversions for y in self.abis for z in self.plats\n        }\n\n    def get_formatted_file_tags(self) -> List[str]:\n        \"\"\"Return the wheel's tags as a sorted list of strings.\"\"\"\n        return sorted(str(tag) for tag in self.file_tags)\n\n    def support_index_min(self, tags: List[Tag]) -> int:\n        \"\"\"Return the lowest index that one of the wheel's file_tag combinations\n        achieves in the given list of supported tags.\n\n        For example, if there are 8 supported tags and one of the file tags\n        is first in the list, then return 0.\n\n        :param tags: the PEP 425 tags to check the wheel against, in order\n            with most preferred first.\n\n        :raises ValueError: If none of the wheel's file tags match one of\n            the supported tags.\n        \"\"\"\n        try:\n            return next(i for i, t in enumerate(tags) if t in self.file_tags)\n        except StopIteration:\n            raise ValueError()\n\n    def find_most_preferred_tag(\n        self, tags: List[Tag], tag_to_priority: Dict[Tag, int]\n    ) -> int:\n        \"\"\"Return the priority of the most preferred tag that one of the wheel's file\n        tag combinations achieves in the given list of supported tags using the given\n        tag_to_priority mapping, where lower priorities are more-preferred.\n\n        This is used in place of support_index_min in some cases in order to avoid\n        an expensive linear scan of a large list of tags.\n\n        :param tags: the PEP 425 tags to check the wheel against.\n        :param tag_to_priority: a mapping from tag to priority of that tag, where\n            lower is more preferred.\n\n        :raises ValueError: If none of the wheel's file tags match one of\n            the supported tags.\n        \"\"\"\n        return min(\n            tag_to_priority[tag] for tag in self.file_tags if tag in tag_to_priority\n        )\n\n    def supported(self, tags: Iterable[Tag]) -> bool:\n        \"\"\"Return whether the wheel is compatible with one of the given tags.\n\n        :param tags: the PEP 425 tags to check the wheel against.\n        \"\"\"\n        return not self.file_tags.isdisjoint(tags)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/network/__init__.py","size":50,"sha1":"cc7b633895c11040d0b99e7d0575b1d031652035","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"Contains purely network-related utilities.\n\"\"\"\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/network/auth.py","size":20809,"sha1":"f9652bbef1a3212922e0cbc6787299212f11bd53","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"Network Authentication Helpers\n\nContains interface (MultiDomainBasicAuth) and associated glue code for\nproviding credentials in the context of network requests.\n\"\"\"\n\nimport logging\nimport os\nimport shutil\nimport subprocess\nimport sysconfig\nimport typing\nimport urllib.parse\nfrom abc import ABC, abstractmethod\nfrom functools import lru_cache\nfrom os.path import commonprefix\nfrom pathlib import Path\nfrom typing import Any, Dict, List, NamedTuple, Optional, Tuple\n\nfrom pip._vendor.requests.auth import AuthBase, HTTPBasicAuth\nfrom pip._vendor.requests.models import Request, Response\nfrom pip._vendor.requests.utils import get_netrc_auth\n\nfrom pip._internal.utils.logging import getLogger\nfrom pip._internal.utils.misc import (\n    ask,\n    ask_input,\n    ask_password,\n    remove_auth_from_url,\n    split_auth_netloc_from_url,\n)\nfrom pip._internal.vcs.versioncontrol import AuthInfo\n\nlogger = getLogger(__name__)\n\nKEYRING_DISABLED = False\n\n\nclass Credentials(NamedTuple):\n    url: str\n    username: str\n    password: str\n\n\nclass KeyRingBaseProvider(ABC):\n    \"\"\"Keyring base provider interface\"\"\"\n\n    has_keyring: bool\n\n    @abstractmethod\n    def get_auth_info(\n        self, url: str, username: Optional[str]\n    ) -> Optional[AuthInfo]: ...\n\n    @abstractmethod\n    def save_auth_info(self, url: str, username: str, password: str) -> None: ...\n\n\nclass KeyRingNullProvider(KeyRingBaseProvider):\n    \"\"\"Keyring null provider\"\"\"\n\n    has_keyring = False\n\n    def get_auth_info(self, url: str, username: Optional[str]) -> Optional[AuthInfo]:\n        return None\n\n    def save_auth_info(self, url: str, username: str, password: str) -> None:\n        return None\n\n\nclass KeyRingPythonProvider(KeyRingBaseProvider):\n    \"\"\"Keyring interface which uses locally imported `keyring`\"\"\"\n\n    has_keyring = True\n\n    def __init__(self) -> None:\n        import keyring\n\n        self.keyring = keyring\n\n    def get_auth_info(self, url: str, username: Optional[str]) -> Optional[AuthInfo]:\n        # Support keyring's get_credential interface which supports getting\n        # credentials without a username. This is only available for\n        # keyring>=15.2.0.\n        if hasattr(self.keyring, \"get_credential\"):\n            logger.debug(\"Getting credentials from keyring for %s\", url)\n            cred = self.keyring.get_credential(url, username)\n            if cred is not None:\n                return cred.username, cred.password\n            return None\n\n        if username is not None:\n            logger.debug(\"Getting password from keyring for %s\", url)\n            password = self.keyring.get_password(url, username)\n            if password:\n                return username, password\n        return None\n\n    def save_auth_info(self, url: str, username: str, password: str) -> None:\n        self.keyring.set_password(url, username, password)\n\n\nclass KeyRingCliProvider(KeyRingBaseProvider):\n    \"\"\"Provider which uses `keyring` cli\n\n    Instead of calling the keyring package installed alongside pip\n    we call keyring on the command line which will enable pip to\n    use which ever installation of keyring is available first in\n    PATH.\n    \"\"\"\n\n    has_keyring = True\n\n    def __init__(self, cmd: str) -> None:\n        self.keyring = cmd\n\n    def get_auth_info(self, url: str, username: Optional[str]) -> Optional[AuthInfo]:\n        # This is the default implementation of keyring.get_credential\n        # https://github.com/jaraco/keyring/blob/97689324abcf01bd1793d49063e7ca01e03d7d07/keyring/backend.py#L134-L139\n        if username is not None:\n            password = self._get_password(url, username)\n            if password is not None:\n                return username, password\n        return None\n\n    def save_auth_info(self, url: str, username: str, password: str) -> None:\n        return self._set_password(url, username, password)\n\n    def _get_password(self, service_name: str, username: str) -> Optional[str]:\n        \"\"\"Mirror the implementation of keyring.get_password using cli\"\"\"\n        if self.keyring is None:\n            return None\n\n        cmd = [self.keyring, \"get\", service_name, username]\n        env = os.environ.copy()\n        env[\"PYTHONIOENCODING\"] = \"utf-8\"\n        res = subprocess.run(\n            cmd,\n            stdin=subprocess.DEVNULL,\n            stdout=subprocess.PIPE,\n            env=env,\n        )\n        if res.returncode:\n            return None\n        return res.stdout.decode(\"utf-8\").strip(os.linesep)\n\n    def _set_password(self, service_name: str, username: str, password: str) -> None:\n        \"\"\"Mirror the implementation of keyring.set_password using cli\"\"\"\n        if self.keyring is None:\n            return None\n        env = os.environ.copy()\n        env[\"PYTHONIOENCODING\"] = \"utf-8\"\n        subprocess.run(\n            [self.keyring, \"set\", service_name, username],\n            input=f\"{password}{os.linesep}\".encode(),\n            env=env,\n            check=True,\n        )\n        return None\n\n\n@lru_cache(maxsize=None)\ndef get_keyring_provider(provider: str) -> KeyRingBaseProvider:\n    logger.verbose(\"Keyring provider requested: %s\", provider)\n\n    # keyring has previously failed and been disabled\n    if KEYRING_DISABLED:\n        provider = \"disabled\"\n    if provider in [\"import\", \"auto\"]:\n        try:\n            impl = KeyRingPythonProvider()\n            logger.verbose(\"Keyring provider set: import\")\n            return impl\n        except ImportError:\n            pass\n        except Exception as exc:\n            # In the event of an unexpected exception\n            # we should warn the user\n            msg = \"Installed copy of keyring fails with exception %s\"\n            if provider == \"auto\":\n                msg = msg + \", trying to find a keyring executable as a fallback\"\n            logger.warning(msg, exc, exc_info=logger.isEnabledFor(logging.DEBUG))\n    if provider in [\"subprocess\", \"auto\"]:\n        cli = shutil.which(\"keyring\")\n        if cli and cli.startswith(sysconfig.get_path(\"scripts\")):\n            # all code within this function is stolen from shutil.which implementation\n            @typing.no_type_check\n            def PATH_as_shutil_which_determines_it() -> str:\n                path = os.environ.get(\"PATH\", None)\n                if path is None:\n                    try:\n                        path = os.confstr(\"CS_PATH\")\n                    except (AttributeError, ValueError):\n                        # os.confstr() or CS_PATH is not available\n                        path = os.defpath\n                # bpo-35755: Don't use os.defpath if the PATH environment variable is\n                # set to an empty string\n\n                return path\n\n            scripts = Path(sysconfig.get_path(\"scripts\"))\n\n            paths = []\n            for path in PATH_as_shutil_which_determines_it().split(os.pathsep):\n                p = Path(path)\n                try:\n                    if not p.samefile(scripts):\n                        paths.append(path)\n                except FileNotFoundError:\n                    pass\n\n            path = os.pathsep.join(paths)\n\n            cli = shutil.which(\"keyring\", path=path)\n\n        if cli:\n            logger.verbose(\"Keyring provider set: subprocess with executable %s\", cli)\n            return KeyRingCliProvider(cli)\n\n    logger.verbose(\"Keyring provider set: disabled\")\n    return KeyRingNullProvider()\n\n\nclass MultiDomainBasicAuth(AuthBase):\n    def __init__(\n        self,\n        prompting: bool = True,\n        index_urls: Optional[List[str]] = None,\n        keyring_provider: str = \"auto\",\n    ) -> None:\n        self.prompting = prompting\n        self.index_urls = index_urls\n        self.keyring_provider = keyring_provider  # type: ignore[assignment]\n        self.passwords: Dict[str, AuthInfo] = {}\n        # When the user is prompted to enter credentials and keyring is\n        # available, we will offer to save them. If the user accepts,\n        # this value is set to the credentials they entered. After the\n        # request authenticates, the caller should call\n        # ``save_credentials`` to save these.\n        self._credentials_to_save: Optional[Credentials] = None\n\n    @property\n    def keyring_provider(self) -> KeyRingBaseProvider:\n        return get_keyring_provider(self._keyring_provider)\n\n    @keyring_provider.setter\n    def keyring_provider(self, provider: str) -> None:\n        # The free function get_keyring_provider has been decorated with\n        # functools.cache. If an exception occurs in get_keyring_auth that\n        # cache will be cleared and keyring disabled, take that into account\n        # if you want to remove this indirection.\n        self._keyring_provider = provider\n\n    @property\n    def use_keyring(self) -> bool:\n        # We won't use keyring when --no-input is passed unless\n        # a specific provider is requested because it might require\n        # user interaction\n        return self.prompting or self._keyring_provider not in [\"auto\", \"disabled\"]\n\n    def _get_keyring_auth(\n        self,\n        url: Optional[str],\n        username: Optional[str],\n    ) -> Optional[AuthInfo]:\n        \"\"\"Return the tuple auth for a given url from keyring.\"\"\"\n        # Do nothing if no url was provided\n        if not url:\n            return None\n\n        try:\n            return self.keyring_provider.get_auth_info(url, username)\n        except Exception as exc:\n            # Log the full exception (with stacktrace) at debug, so it'll only\n            # show up when running in verbose mode.\n            logger.debug(\"Keyring is skipped due to an exception\", exc_info=True)\n            # Always log a shortened version of the exception.\n            logger.warning(\n                \"Keyring is skipped due to an exception: %s\",\n                str(exc),\n            )\n            global KEYRING_DISABLED\n            KEYRING_DISABLED = True\n            get_keyring_provider.cache_clear()\n            return None\n\n    def _get_index_url(self, url: str) -> Optional[str]:\n        \"\"\"Return the original index URL matching the requested URL.\n\n        Cached or dynamically generated credentials may work against\n        the original index URL rather than just the netloc.\n\n        The provided url should have had its username and password\n        removed already. If the original index url had credentials then\n        they will be included in the return value.\n\n        Returns None if no matching index was found, or if --no-index\n        was specified by the user.\n        \"\"\"\n        if not url or not self.index_urls:\n            return None\n\n        url = remove_auth_from_url(url).rstrip(\"/\") + \"/\"\n        parsed_url = urllib.parse.urlsplit(url)\n\n        candidates = []\n\n        for index in self.index_urls:\n            index = index.rstrip(\"/\") + \"/\"\n            parsed_index = urllib.parse.urlsplit(remove_auth_from_url(index))\n            if parsed_url == parsed_index:\n                return index\n\n            if parsed_url.netloc != parsed_index.netloc:\n                continue\n\n            candidate = urllib.parse.urlsplit(index)\n            candidates.append(candidate)\n\n        if not candidates:\n            return None\n\n        candidates.sort(\n            reverse=True,\n            key=lambda candidate: commonprefix(\n                [\n                    parsed_url.path,\n                    candidate.path,\n                ]\n            ).rfind(\"/\"),\n        )\n\n        return urllib.parse.urlunsplit(candidates[0])\n\n    def _get_new_credentials(\n        self,\n        original_url: str,\n        *,\n        allow_netrc: bool = True,\n        allow_keyring: bool = False,\n    ) -> AuthInfo:\n        \"\"\"Find and return credentials for the specified URL.\"\"\"\n        # Split the credentials and netloc from the url.\n        url, netloc, url_user_password = split_auth_netloc_from_url(\n            original_url,\n        )\n\n        # Start with the credentials embedded in the url\n        username, password = url_user_password\n        if username is not None and password is not None:\n            logger.debug(\"Found credentials in url for %s\", netloc)\n            return url_user_password\n\n        # Find a matching index url for this request\n        index_url = self._get_index_url(url)\n        if index_url:\n            # Split the credentials from the url.\n            index_info = split_auth_netloc_from_url(index_url)\n            if index_info:\n                index_url, _, index_url_user_password = index_info\n                logger.debug(\"Found index url %s\", index_url)\n\n        # If an index URL was found, try its embedded credentials\n        if index_url and index_url_user_password[0] is not None:\n            username, password = index_url_user_password\n            if username is not None and password is not None:\n                logger.debug(\"Found credentials in index url for %s\", netloc)\n                return index_url_user_password\n\n        # Get creds from netrc if we still don't have them\n        if allow_netrc:\n            netrc_auth = get_netrc_auth(original_url)\n            if netrc_auth:\n                logger.debug(\"Found credentials in netrc for %s\", netloc)\n                return netrc_auth\n\n        # If we don't have a password and keyring is available, use it.\n        if allow_keyring:\n            # The index url is more specific than the netloc, so try it first\n            # fmt: off\n            kr_auth = (\n                self._get_keyring_auth(index_url, username) or\n                self._get_keyring_auth(netloc, username)\n            )\n            # fmt: on\n            if kr_auth:\n                logger.debug(\"Found credentials in keyring for %s\", netloc)\n                return kr_auth\n\n        return username, password\n\n    def _get_url_and_credentials(\n        self, original_url: str\n    ) -> Tuple[str, Optional[str], Optional[str]]:\n        \"\"\"Return the credentials to use for the provided URL.\n\n        If allowed, netrc and keyring may be used to obtain the\n        correct credentials.\n\n        Returns (url_without_credentials, username, password). Note\n        that even if the original URL contains credentials, this\n        function may return a different username and password.\n        \"\"\"\n        url, netloc, _ = split_auth_netloc_from_url(original_url)\n\n        # Try to get credentials from original url\n        username, password = self._get_new_credentials(original_url)\n\n        # If credentials not found, use any stored credentials for this netloc.\n        # Do this if either the username or the password is missing.\n        # This accounts for the situation in which the user has specified\n        # the username in the index url, but the password comes from keyring.\n        if (username is None or password is None) and netloc in self.passwords:\n            un, pw = self.passwords[netloc]\n            # It is possible that the cached credentials are for a different username,\n            # in which case the cache should be ignored.\n            if username is None or username == un:\n                username, password = un, pw\n\n        if username is not None or password is not None:\n            # Convert the username and password if they're None, so that\n            # this netloc will show up as \"cached\" in the conditional above.\n            # Further, HTTPBasicAuth doesn't accept None, so it makes sense to\n            # cache the value that is going to be used.\n            username = username or \"\"\n            password = password or \"\"\n\n            # Store any acquired credentials.\n            self.passwords[netloc] = (username, password)\n\n        assert (\n            # Credentials were found\n            (username is not None and password is not None)\n            # Credentials were not found\n            or (username is None and password is None)\n        ), f\"Could not load credentials from url: {original_url}\"\n\n        return url, username, password\n\n    def __call__(self, req: Request) -> Request:\n        # Get credentials for this request\n        url, username, password = self._get_url_and_credentials(req.url)\n\n        # Set the url of the request to the url without any credentials\n        req.url = url\n\n        if username is not None and password is not None:\n            # Send the basic auth with this request\n            req = HTTPBasicAuth(username, password)(req)\n\n        # Attach a hook to handle 401 responses\n        req.register_hook(\"response\", self.handle_401)\n\n        return req\n\n    # Factored out to allow for easy patching in tests\n    def _prompt_for_password(\n        self, netloc: str\n    ) -> Tuple[Optional[str], Optional[str], bool]:\n        username = ask_input(f\"User for {netloc}: \") if self.prompting else None\n        if not username:\n            return None, None, False\n        if self.use_keyring:\n            auth = self._get_keyring_auth(netloc, username)\n            if auth and auth[0] is not None and auth[1] is not None:\n                return auth[0], auth[1], False\n        password = ask_password(\"Password: \")\n        return username, password, True\n\n    # Factored out to allow for easy patching in tests\n    def _should_save_password_to_keyring(self) -> bool:\n        if (\n            not self.prompting\n            or not self.use_keyring\n            or not self.keyring_provider.has_keyring\n        ):\n            return False\n        return ask(\"Save credentials to keyring [y/N]: \", [\"y\", \"n\"]) == \"y\"\n\n    def handle_401(self, resp: Response, **kwargs: Any) -> Response:\n        # We only care about 401 responses, anything else we want to just\n        #   pass through the actual response\n        if resp.status_code != 401:\n            return resp\n\n        username, password = None, None\n\n        # Query the keyring for credentials:\n        if self.use_keyring:\n            username, password = self._get_new_credentials(\n                resp.url,\n                allow_netrc=False,\n                allow_keyring=True,\n            )\n\n        # We are not able to prompt the user so simply return the response\n        if not self.prompting and not username and not password:\n            return resp\n\n        parsed = urllib.parse.urlparse(resp.url)\n\n        # Prompt the user for a new username and password\n        save = False\n        if not username and not password:\n            username, password, save = self._prompt_for_password(parsed.netloc)\n\n        # Store the new username and password to use for future requests\n        self._credentials_to_save = None\n        if username is not None and password is not None:\n            self.passwords[parsed.netloc] = (username, password)\n\n            # Prompt to save the password to keyring\n            if save and self._should_save_password_to_keyring():\n                self._credentials_to_save = Credentials(\n                    url=parsed.netloc,\n                    username=username,\n                    password=password,\n                )\n\n        # Consume content and release the original connection to allow our new\n        #   request to reuse the same one.\n        # The result of the assignment isn't used, it's just needed to consume\n        # the content.\n        _ = resp.content\n        resp.raw.release_conn()\n\n        # Add our new username and password to the request\n        req = HTTPBasicAuth(username or \"\", password or \"\")(resp.request)\n        req.register_hook(\"response\", self.warn_on_401)\n\n        # On successful request, save the credentials that were used to\n        # keyring. (Note that if the user responded \"no\" above, this member\n        # is not set and nothing will be saved.)\n        if self._credentials_to_save:\n            req.register_hook(\"response\", self.save_credentials)\n\n        # Send our new request\n        new_resp = resp.connection.send(req, **kwargs)\n        new_resp.history.append(resp)\n\n        return new_resp\n\n    def warn_on_401(self, resp: Response, **kwargs: Any) -> None:\n        \"\"\"Response callback to warn about incorrect credentials.\"\"\"\n        if resp.status_code == 401:\n            logger.warning(\n                \"401 Error, Credentials not correct for %s\",\n                resp.request.url,\n            )\n\n    def save_credentials(self, resp: Response, **kwargs: Any) -> None:\n        \"\"\"Response callback to save credentials on success.\"\"\"\n        assert (\n            self.keyring_provider.has_keyring\n        ), \"should never reach here without keyring\"\n\n        creds = self._credentials_to_save\n        self._credentials_to_save = None\n        if creds and resp.status_code < 400:\n            try:\n                logger.info(\"Saving credentials to keyring\")\n                self.keyring_provider.save_auth_info(\n                    creds.url, creds.username, creds.password\n                )\n            except Exception:\n                logger.exception(\"Failed to save credentials\")\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/network/cache.py","size":4614,"sha1":"317d311cc243e8768dcb11dc3002b1ed67f41856","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"HTTP cache implementation.\n\"\"\"\n\nimport os\nfrom contextlib import contextmanager\nfrom datetime import datetime\nfrom typing import BinaryIO, Generator, Optional, Union\n\nfrom pip._vendor.cachecontrol.cache import SeparateBodyBaseCache\nfrom pip._vendor.cachecontrol.caches import SeparateBodyFileCache\nfrom pip._vendor.requests.models import Response\n\nfrom pip._internal.utils.filesystem import adjacent_tmp_file, replace\nfrom pip._internal.utils.misc import ensure_dir\n\n\ndef is_from_cache(response: Response) -> bool:\n    return getattr(response, \"from_cache\", False)\n\n\n@contextmanager\ndef suppressed_cache_errors() -> Generator[None, None, None]:\n    \"\"\"If we can't access the cache then we can just skip caching and process\n    requests as if caching wasn't enabled.\n    \"\"\"\n    try:\n        yield\n    except OSError:\n        pass\n\n\nclass SafeFileCache(SeparateBodyBaseCache):\n    \"\"\"\n    A file based cache which is safe to use even when the target directory may\n    not be accessible or writable.\n\n    There is a race condition when two processes try to write and/or read the\n    same entry at the same time, since each entry consists of two separate\n    files (https://github.com/psf/cachecontrol/issues/324).  We therefore have\n    additional logic that makes sure that both files to be present before\n    returning an entry; this fixes the read side of the race condition.\n\n    For the write side, we assume that the server will only ever return the\n    same data for the same URL, which ought to be the case for files pip is\n    downloading.  PyPI does not have a mechanism to swap out a wheel for\n    another wheel, for example.  If this assumption is not true, the\n    CacheControl issue will need to be fixed.\n    \"\"\"\n\n    def __init__(self, directory: str) -> None:\n        assert directory is not None, \"Cache directory must not be None.\"\n        super().__init__()\n        self.directory = directory\n\n    def _get_cache_path(self, name: str) -> str:\n        # From cachecontrol.caches.file_cache.FileCache._fn, brought into our\n        # class for backwards-compatibility and to avoid using a non-public\n        # method.\n        hashed = SeparateBodyFileCache.encode(name)\n        parts = list(hashed[:5]) + [hashed]\n        return os.path.join(self.directory, *parts)\n\n    def get(self, key: str) -> Optional[bytes]:\n        # The cache entry is only valid if both metadata and body exist.\n        metadata_path = self._get_cache_path(key)\n        body_path = metadata_path + \".body\"\n        if not (os.path.exists(metadata_path) and os.path.exists(body_path)):\n            return None\n        with suppressed_cache_errors():\n            with open(metadata_path, \"rb\") as f:\n                return f.read()\n\n    def _write(self, path: str, data: bytes) -> None:\n        with suppressed_cache_errors():\n            ensure_dir(os.path.dirname(path))\n\n            with adjacent_tmp_file(path) as f:\n                f.write(data)\n                # Inherit the read/write permissions of the cache directory\n                # to enable multi-user cache use-cases.\n                mode = (\n                    os.stat(self.directory).st_mode\n                    & 0o666  # select read/write permissions of cache directory\n                    | 0o600  # set owner read/write permissions\n                )\n                # Change permissions only if there is no risk of following a symlink.\n                if os.chmod in os.supports_fd:\n                    os.chmod(f.fileno(), mode)\n                elif os.chmod in os.supports_follow_symlinks:\n                    os.chmod(f.name, mode, follow_symlinks=False)\n\n            replace(f.name, path)\n\n    def set(\n        self, key: str, value: bytes, expires: Union[int, datetime, None] = None\n    ) -> None:\n        path = self._get_cache_path(key)\n        self._write(path, value)\n\n    def delete(self, key: str) -> None:\n        path = self._get_cache_path(key)\n        with suppressed_cache_errors():\n            os.remove(path)\n        with suppressed_cache_errors():\n            os.remove(path + \".body\")\n\n    def get_body(self, key: str) -> Optional[BinaryIO]:\n        # The cache entry is only valid if both metadata and body exist.\n        metadata_path = self._get_cache_path(key)\n        body_path = metadata_path + \".body\"\n        if not (os.path.exists(metadata_path) and os.path.exists(body_path)):\n            return None\n        with suppressed_cache_errors():\n            return open(body_path, \"rb\")\n\n    def set_body(self, key: str, body: bytes) -> None:\n        path = self._get_cache_path(key) + \".body\"\n        self._write(path, body)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/network/download.py","size":6048,"sha1":"d4f48cdcf4be9675f939bb59f0f5efc55a31bd4d","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"Download files with progress indicators.\n\"\"\"\n\nimport email.message\nimport logging\nimport mimetypes\nimport os\nfrom typing import Iterable, Optional, Tuple\n\nfrom pip._vendor.requests.models import Response\n\nfrom pip._internal.cli.progress_bars import get_download_progress_renderer\nfrom pip._internal.exceptions import NetworkConnectionError\nfrom pip._internal.models.index import PyPI\nfrom pip._internal.models.link import Link\nfrom pip._internal.network.cache import is_from_cache\nfrom pip._internal.network.session import PipSession\nfrom pip._internal.network.utils import HEADERS, raise_for_status, response_chunks\nfrom pip._internal.utils.misc import format_size, redact_auth_from_url, splitext\n\nlogger = logging.getLogger(__name__)\n\n\ndef _get_http_response_size(resp: Response) -> Optional[int]:\n    try:\n        return int(resp.headers[\"content-length\"])\n    except (ValueError, KeyError, TypeError):\n        return None\n\n\ndef _prepare_download(\n    resp: Response,\n    link: Link,\n    progress_bar: str,\n) -> Iterable[bytes]:\n    total_length = _get_http_response_size(resp)\n\n    if link.netloc == PyPI.file_storage_domain:\n        url = link.show_url\n    else:\n        url = link.url_without_fragment\n\n    logged_url = redact_auth_from_url(url)\n\n    if total_length:\n        logged_url = f\"{logged_url} ({format_size(total_length)})\"\n\n    if is_from_cache(resp):\n        logger.info(\"Using cached %s\", logged_url)\n    else:\n        logger.info(\"Downloading %s\", logged_url)\n\n    if logger.getEffectiveLevel() > logging.INFO:\n        show_progress = False\n    elif is_from_cache(resp):\n        show_progress = False\n    elif not total_length:\n        show_progress = True\n    elif total_length > (512 * 1024):\n        show_progress = True\n    else:\n        show_progress = False\n\n    chunks = response_chunks(resp)\n\n    if not show_progress:\n        return chunks\n\n    renderer = get_download_progress_renderer(bar_type=progress_bar, size=total_length)\n    return renderer(chunks)\n\n\ndef sanitize_content_filename(filename: str) -> str:\n    \"\"\"\n    Sanitize the \"filename\" value from a Content-Disposition header.\n    \"\"\"\n    return os.path.basename(filename)\n\n\ndef parse_content_disposition(content_disposition: str, default_filename: str) -> str:\n    \"\"\"\n    Parse the \"filename\" value from a Content-Disposition header, and\n    return the default filename if the result is empty.\n    \"\"\"\n    m = email.message.Message()\n    m[\"content-type\"] = content_disposition\n    filename = m.get_param(\"filename\")\n    if filename:\n        # We need to sanitize the filename to prevent directory traversal\n        # in case the filename contains \"..\" path parts.\n        filename = sanitize_content_filename(str(filename))\n    return filename or default_filename\n\n\ndef _get_http_response_filename(resp: Response, link: Link) -> str:\n    \"\"\"Get an ideal filename from the given HTTP response, falling back to\n    the link filename if not provided.\n    \"\"\"\n    filename = link.filename  # fallback\n    # Have a look at the Content-Disposition header for a better guess\n    content_disposition = resp.headers.get(\"content-disposition\")\n    if content_disposition:\n        filename = parse_content_disposition(content_disposition, filename)\n    ext: Optional[str] = splitext(filename)[1]\n    if not ext:\n        ext = mimetypes.guess_extension(resp.headers.get(\"content-type\", \"\"))\n        if ext:\n            filename += ext\n    if not ext and link.url != resp.url:\n        ext = os.path.splitext(resp.url)[1]\n        if ext:\n            filename += ext\n    return filename\n\n\ndef _http_get_download(session: PipSession, link: Link) -> Response:\n    target_url = link.url.split(\"#\", 1)[0]\n    resp = session.get(target_url, headers=HEADERS, stream=True)\n    raise_for_status(resp)\n    return resp\n\n\nclass Downloader:\n    def __init__(\n        self,\n        session: PipSession,\n        progress_bar: str,\n    ) -> None:\n        self._session = session\n        self._progress_bar = progress_bar\n\n    def __call__(self, link: Link, location: str) -> Tuple[str, str]:\n        \"\"\"Download the file given by link into location.\"\"\"\n        try:\n            resp = _http_get_download(self._session, link)\n        except NetworkConnectionError as e:\n            assert e.response is not None\n            logger.critical(\n                \"HTTP error %s while getting %s\", e.response.status_code, link\n            )\n            raise\n\n        filename = _get_http_response_filename(resp, link)\n        filepath = os.path.join(location, filename)\n\n        chunks = _prepare_download(resp, link, self._progress_bar)\n        with open(filepath, \"wb\") as content_file:\n            for chunk in chunks:\n                content_file.write(chunk)\n        content_type = resp.headers.get(\"Content-Type\", \"\")\n        return filepath, content_type\n\n\nclass BatchDownloader:\n    def __init__(\n        self,\n        session: PipSession,\n        progress_bar: str,\n    ) -> None:\n        self._session = session\n        self._progress_bar = progress_bar\n\n    def __call__(\n        self, links: Iterable[Link], location: str\n    ) -> Iterable[Tuple[Link, Tuple[str, str]]]:\n        \"\"\"Download the files given by links into location.\"\"\"\n        for link in links:\n            try:\n                resp = _http_get_download(self._session, link)\n            except NetworkConnectionError as e:\n                assert e.response is not None\n                logger.critical(\n                    \"HTTP error %s while getting %s\",\n                    e.response.status_code,\n                    link,\n                )\n                raise\n\n            filename = _get_http_response_filename(resp, link)\n            filepath = os.path.join(location, filename)\n\n            chunks = _prepare_download(resp, link, self._progress_bar)\n            with open(filepath, \"wb\") as content_file:\n                for chunk in chunks:\n                    content_file.write(chunk)\n            content_type = resp.headers.get(\"Content-Type\", \"\")\n            yield link, (filepath, content_type)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/network/lazy_wheel.py","size":7622,"sha1":"7cf373f9a337b6e504303020e49e5fb17f212498","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"Lazy ZIP over HTTP\"\"\"\n\n__all__ = [\"HTTPRangeRequestUnsupported\", \"dist_from_wheel_url\"]\n\nfrom bisect import bisect_left, bisect_right\nfrom contextlib import contextmanager\nfrom tempfile import NamedTemporaryFile\nfrom typing import Any, Dict, Generator, List, Optional, Tuple\nfrom zipfile import BadZipFile, ZipFile\n\nfrom pip._vendor.packaging.utils import canonicalize_name\nfrom pip._vendor.requests.models import CONTENT_CHUNK_SIZE, Response\n\nfrom pip._internal.metadata import BaseDistribution, MemoryWheel, get_wheel_distribution\nfrom pip._internal.network.session import PipSession\nfrom pip._internal.network.utils import HEADERS, raise_for_status, response_chunks\n\n\nclass HTTPRangeRequestUnsupported(Exception):\n    pass\n\n\ndef dist_from_wheel_url(name: str, url: str, session: PipSession) -> BaseDistribution:\n    \"\"\"Return a distribution object from the given wheel URL.\n\n    This uses HTTP range requests to only fetch the portion of the wheel\n    containing metadata, just enough for the object to be constructed.\n    If such requests are not supported, HTTPRangeRequestUnsupported\n    is raised.\n    \"\"\"\n    with LazyZipOverHTTP(url, session) as zf:\n        # For read-only ZIP files, ZipFile only needs methods read,\n        # seek, seekable and tell, not the whole IO protocol.\n        wheel = MemoryWheel(zf.name, zf)  # type: ignore\n        # After context manager exit, wheel.name\n        # is an invalid file by intention.\n        return get_wheel_distribution(wheel, canonicalize_name(name))\n\n\nclass LazyZipOverHTTP:\n    \"\"\"File-like object mapped to a ZIP file over HTTP.\n\n    This uses HTTP range requests to lazily fetch the file's content,\n    which is supposed to be fed to ZipFile.  If such requests are not\n    supported by the server, raise HTTPRangeRequestUnsupported\n    during initialization.\n    \"\"\"\n\n    def __init__(\n        self, url: str, session: PipSession, chunk_size: int = CONTENT_CHUNK_SIZE\n    ) -> None:\n        head = session.head(url, headers=HEADERS)\n        raise_for_status(head)\n        assert head.status_code == 200\n        self._session, self._url, self._chunk_size = session, url, chunk_size\n        self._length = int(head.headers[\"Content-Length\"])\n        self._file = NamedTemporaryFile()\n        self.truncate(self._length)\n        self._left: List[int] = []\n        self._right: List[int] = []\n        if \"bytes\" not in head.headers.get(\"Accept-Ranges\", \"none\"):\n            raise HTTPRangeRequestUnsupported(\"range request is not supported\")\n        self._check_zip()\n\n    @property\n    def mode(self) -> str:\n        \"\"\"Opening mode, which is always rb.\"\"\"\n        return \"rb\"\n\n    @property\n    def name(self) -> str:\n        \"\"\"Path to the underlying file.\"\"\"\n        return self._file.name\n\n    def seekable(self) -> bool:\n        \"\"\"Return whether random access is supported, which is True.\"\"\"\n        return True\n\n    def close(self) -> None:\n        \"\"\"Close the file.\"\"\"\n        self._file.close()\n\n    @property\n    def closed(self) -> bool:\n        \"\"\"Whether the file is closed.\"\"\"\n        return self._file.closed\n\n    def read(self, size: int = -1) -> bytes:\n        \"\"\"Read up to size bytes from the object and return them.\n\n        As a convenience, if size is unspecified or -1,\n        all bytes until EOF are returned.  Fewer than\n        size bytes may be returned if EOF is reached.\n        \"\"\"\n        download_size = max(size, self._chunk_size)\n        start, length = self.tell(), self._length\n        stop = length if size < 0 else min(start + download_size, length)\n        start = max(0, stop - download_size)\n        self._download(start, stop - 1)\n        return self._file.read(size)\n\n    def readable(self) -> bool:\n        \"\"\"Return whether the file is readable, which is True.\"\"\"\n        return True\n\n    def seek(self, offset: int, whence: int = 0) -> int:\n        \"\"\"Change stream position and return the new absolute position.\n\n        Seek to offset relative position indicated by whence:\n        * 0: Start of stream (the default).  pos should be >= 0;\n        * 1: Current position - pos may be negative;\n        * 2: End of stream - pos usually negative.\n        \"\"\"\n        return self._file.seek(offset, whence)\n\n    def tell(self) -> int:\n        \"\"\"Return the current position.\"\"\"\n        return self._file.tell()\n\n    def truncate(self, size: Optional[int] = None) -> int:\n        \"\"\"Resize the stream to the given size in bytes.\n\n        If size is unspecified resize to the current position.\n        The current stream position isn't changed.\n\n        Return the new file size.\n        \"\"\"\n        return self._file.truncate(size)\n\n    def writable(self) -> bool:\n        \"\"\"Return False.\"\"\"\n        return False\n\n    def __enter__(self) -> \"LazyZipOverHTTP\":\n        self._file.__enter__()\n        return self\n\n    def __exit__(self, *exc: Any) -> None:\n        self._file.__exit__(*exc)\n\n    @contextmanager\n    def _stay(self) -> Generator[None, None, None]:\n        \"\"\"Return a context manager keeping the position.\n\n        At the end of the block, seek back to original position.\n        \"\"\"\n        pos = self.tell()\n        try:\n            yield\n        finally:\n            self.seek(pos)\n\n    def _check_zip(self) -> None:\n        \"\"\"Check and download until the file is a valid ZIP.\"\"\"\n        end = self._length - 1\n        for start in reversed(range(0, end, self._chunk_size)):\n            self._download(start, end)\n            with self._stay():\n                try:\n                    # For read-only ZIP files, ZipFile only needs\n                    # methods read, seek, seekable and tell.\n                    ZipFile(self)\n                except BadZipFile:\n                    pass\n                else:\n                    break\n\n    def _stream_response(\n        self, start: int, end: int, base_headers: Dict[str, str] = HEADERS\n    ) -> Response:\n        \"\"\"Return HTTP response to a range request from start to end.\"\"\"\n        headers = base_headers.copy()\n        headers[\"Range\"] = f\"bytes={start}-{end}\"\n        # TODO: Get range requests to be correctly cached\n        headers[\"Cache-Control\"] = \"no-cache\"\n        return self._session.get(self._url, headers=headers, stream=True)\n\n    def _merge(\n        self, start: int, end: int, left: int, right: int\n    ) -> Generator[Tuple[int, int], None, None]:\n        \"\"\"Return a generator of intervals to be fetched.\n\n        Args:\n            start (int): Start of needed interval\n            end (int): End of needed interval\n            left (int): Index of first overlapping downloaded data\n            right (int): Index after last overlapping downloaded data\n        \"\"\"\n        lslice, rslice = self._left[left:right], self._right[left:right]\n        i = start = min([start] + lslice[:1])\n        end = max([end] + rslice[-1:])\n        for j, k in zip(lslice, rslice):\n            if j > i:\n                yield i, j - 1\n            i = k + 1\n        if i <= end:\n            yield i, end\n        self._left[left:right], self._right[left:right] = [start], [end]\n\n    def _download(self, start: int, end: int) -> None:\n        \"\"\"Download bytes from start to end inclusively.\"\"\"\n        with self._stay():\n            left = bisect_left(self._right, start)\n            right = bisect_right(self._left, end)\n            for start, end in self._merge(start, end, left, right):\n                response = self._stream_response(start, end)\n                response.raise_for_status()\n                self.seek(start)\n                for chunk in response_chunks(response, self._chunk_size):\n                    self._file.write(chunk)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/network/session.py","size":18771,"sha1":"6a35b172cd79c59831857255bd5bc2a46553186b","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"PipSession and supporting code, containing all pip-specific\nnetwork request configuration and behavior.\n\"\"\"\n\nimport email.utils\nimport functools\nimport io\nimport ipaddress\nimport json\nimport logging\nimport mimetypes\nimport os\nimport platform\nimport shutil\nimport subprocess\nimport sys\nimport urllib.parse\nimport warnings\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Dict,\n    Generator,\n    List,\n    Mapping,\n    Optional,\n    Sequence,\n    Tuple,\n    Union,\n)\n\nfrom pip._vendor import requests, urllib3\nfrom pip._vendor.cachecontrol import CacheControlAdapter as _BaseCacheControlAdapter\nfrom pip._vendor.requests.adapters import DEFAULT_POOLBLOCK, BaseAdapter\nfrom pip._vendor.requests.adapters import HTTPAdapter as _BaseHTTPAdapter\nfrom pip._vendor.requests.models import PreparedRequest, Response\nfrom pip._vendor.requests.structures import CaseInsensitiveDict\nfrom pip._vendor.urllib3.connectionpool import ConnectionPool\nfrom pip._vendor.urllib3.exceptions import InsecureRequestWarning\n\nfrom pip import __version__\nfrom pip._internal.metadata import get_default_environment\nfrom pip._internal.models.link import Link\nfrom pip._internal.network.auth import MultiDomainBasicAuth\nfrom pip._internal.network.cache import SafeFileCache\n\n# Import ssl from compat so the initial import occurs in only one place.\nfrom pip._internal.utils.compat import has_tls\nfrom pip._internal.utils.glibc import libc_ver\nfrom pip._internal.utils.misc import build_url_from_netloc, parse_netloc\nfrom pip._internal.utils.urls import url_to_path\n\nif TYPE_CHECKING:\n    from ssl import SSLContext\n\n    from pip._vendor.urllib3.poolmanager import PoolManager\n\n\nlogger = logging.getLogger(__name__)\n\nSecureOrigin = Tuple[str, str, Optional[Union[int, str]]]\n\n\n# Ignore warning raised when using --trusted-host.\nwarnings.filterwarnings(\"ignore\", category=InsecureRequestWarning)\n\n\nSECURE_ORIGINS: List[SecureOrigin] = [\n    # protocol, hostname, port\n    # Taken from Chrome's list of secure origins (See: http://bit.ly/1qrySKC)\n    (\"https\", \"*\", \"*\"),\n    (\"*\", \"localhost\", \"*\"),\n    (\"*\", \"127.0.0.0/8\", \"*\"),\n    (\"*\", \"::1/128\", \"*\"),\n    (\"file\", \"*\", None),\n    # ssh is always secure.\n    (\"ssh\", \"*\", \"*\"),\n]\n\n\n# These are environment variables present when running under various\n# CI systems.  For each variable, some CI systems that use the variable\n# are indicated.  The collection was chosen so that for each of a number\n# of popular systems, at least one of the environment variables is used.\n# This list is used to provide some indication of and lower bound for\n# CI traffic to PyPI.  Thus, it is okay if the list is not comprehensive.\n# For more background, see: https://github.com/pypa/pip/issues/5499\nCI_ENVIRONMENT_VARIABLES = (\n    # Azure Pipelines\n    \"BUILD_BUILDID\",\n    # Jenkins\n    \"BUILD_ID\",\n    # AppVeyor, CircleCI, Codeship, Gitlab CI, Shippable, Travis CI\n    \"CI\",\n    # Explicit environment variable.\n    \"PIP_IS_CI\",\n)\n\n\ndef looks_like_ci() -> bool:\n    \"\"\"\n    Return whether it looks like pip is running under CI.\n    \"\"\"\n    # We don't use the method of checking for a tty (e.g. using isatty())\n    # because some CI systems mimic a tty (e.g. Travis CI).  Thus that\n    # method doesn't provide definitive information in either direction.\n    return any(name in os.environ for name in CI_ENVIRONMENT_VARIABLES)\n\n\n@functools.lru_cache(maxsize=1)\ndef user_agent() -> str:\n    \"\"\"\n    Return a string representing the user agent.\n    \"\"\"\n    data: Dict[str, Any] = {\n        \"installer\": {\"name\": \"pip\", \"version\": __version__},\n        \"python\": platform.python_version(),\n        \"implementation\": {\n            \"name\": platform.python_implementation(),\n        },\n    }\n\n    if data[\"implementation\"][\"name\"] == \"CPython\":\n        data[\"implementation\"][\"version\"] = platform.python_version()\n    elif data[\"implementation\"][\"name\"] == \"PyPy\":\n        pypy_version_info = sys.pypy_version_info  # type: ignore\n        if pypy_version_info.releaselevel == \"final\":\n            pypy_version_info = pypy_version_info[:3]\n        data[\"implementation\"][\"version\"] = \".\".join(\n            [str(x) for x in pypy_version_info]\n        )\n    elif data[\"implementation\"][\"name\"] == \"Jython\":\n        # Complete Guess\n        data[\"implementation\"][\"version\"] = platform.python_version()\n    elif data[\"implementation\"][\"name\"] == \"IronPython\":\n        # Complete Guess\n        data[\"implementation\"][\"version\"] = platform.python_version()\n\n    if sys.platform.startswith(\"linux\"):\n        from pip._vendor import distro\n\n        linux_distribution = distro.name(), distro.version(), distro.codename()\n        distro_infos: Dict[str, Any] = dict(\n            filter(\n                lambda x: x[1],\n                zip([\"name\", \"version\", \"id\"], linux_distribution),\n            )\n        )\n        libc = dict(\n            filter(\n                lambda x: x[1],\n                zip([\"lib\", \"version\"], libc_ver()),\n            )\n        )\n        if libc:\n            distro_infos[\"libc\"] = libc\n        if distro_infos:\n            data[\"distro\"] = distro_infos\n\n    if sys.platform.startswith(\"darwin\") and platform.mac_ver()[0]:\n        data[\"distro\"] = {\"name\": \"macOS\", \"version\": platform.mac_ver()[0]}\n\n    if platform.system():\n        data.setdefault(\"system\", {})[\"name\"] = platform.system()\n\n    if platform.release():\n        data.setdefault(\"system\", {})[\"release\"] = platform.release()\n\n    if platform.machine():\n        data[\"cpu\"] = platform.machine()\n\n    if has_tls():\n        import _ssl as ssl\n\n        data[\"openssl_version\"] = ssl.OPENSSL_VERSION\n\n    setuptools_dist = get_default_environment().get_distribution(\"setuptools\")\n    if setuptools_dist is not None:\n        data[\"setuptools_version\"] = str(setuptools_dist.version)\n\n    if shutil.which(\"rustc\") is not None:\n        # If for any reason `rustc --version` fails, silently ignore it\n        try:\n            rustc_output = subprocess.check_output(\n                [\"rustc\", \"--version\"], stderr=subprocess.STDOUT, timeout=0.5\n            )\n        except Exception:\n            pass\n        else:\n            if rustc_output.startswith(b\"rustc \"):\n                # The format of `rustc --version` is:\n                # `b'rustc 1.52.1 (9bc8c42bb 2021-05-09)\\n'`\n                # We extract just the middle (1.52.1) part\n                data[\"rustc_version\"] = rustc_output.split(b\" \")[1].decode()\n\n    # Use None rather than False so as not to give the impression that\n    # pip knows it is not being run under CI.  Rather, it is a null or\n    # inconclusive result.  Also, we include some value rather than no\n    # value to make it easier to know that the check has been run.\n    data[\"ci\"] = True if looks_like_ci() else None\n\n    user_data = os.environ.get(\"PIP_USER_AGENT_USER_DATA\")\n    if user_data is not None:\n        data[\"user_data\"] = user_data\n\n    return \"{data[installer][name]}/{data[installer][version]} {json}\".format(\n        data=data,\n        json=json.dumps(data, separators=(\",\", \":\"), sort_keys=True),\n    )\n\n\nclass LocalFSAdapter(BaseAdapter):\n    def send(\n        self,\n        request: PreparedRequest,\n        stream: bool = False,\n        timeout: Optional[Union[float, Tuple[float, float]]] = None,\n        verify: Union[bool, str] = True,\n        cert: Optional[Union[str, Tuple[str, str]]] = None,\n        proxies: Optional[Mapping[str, str]] = None,\n    ) -> Response:\n        pathname = url_to_path(request.url)\n\n        resp = Response()\n        resp.status_code = 200\n        resp.url = request.url\n\n        try:\n            stats = os.stat(pathname)\n        except OSError as exc:\n            # format the exception raised as a io.BytesIO object,\n            # to return a better error message:\n            resp.status_code = 404\n            resp.reason = type(exc).__name__\n            resp.raw = io.BytesIO(f\"{resp.reason}: {exc}\".encode())\n        else:\n            modified = email.utils.formatdate(stats.st_mtime, usegmt=True)\n            content_type = mimetypes.guess_type(pathname)[0] or \"text/plain\"\n            resp.headers = CaseInsensitiveDict(\n                {\n                    \"Content-Type\": content_type,\n                    \"Content-Length\": stats.st_size,\n                    \"Last-Modified\": modified,\n                }\n            )\n\n            resp.raw = open(pathname, \"rb\")\n            resp.close = resp.raw.close\n\n        return resp\n\n    def close(self) -> None:\n        pass\n\n\nclass _SSLContextAdapterMixin:\n    \"\"\"Mixin to add the ``ssl_context`` constructor argument to HTTP adapters.\n\n    The additional argument is forwarded directly to the pool manager. This allows us\n    to dynamically decide what SSL store to use at runtime, which is used to implement\n    the optional ``truststore`` backend.\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        ssl_context: Optional[\"SSLContext\"] = None,\n        **kwargs: Any,\n    ) -> None:\n        self._ssl_context = ssl_context\n        super().__init__(**kwargs)\n\n    def init_poolmanager(\n        self,\n        connections: int,\n        maxsize: int,\n        block: bool = DEFAULT_POOLBLOCK,\n        **pool_kwargs: Any,\n    ) -> \"PoolManager\":\n        if self._ssl_context is not None:\n            pool_kwargs.setdefault(\"ssl_context\", self._ssl_context)\n        return super().init_poolmanager(  # type: ignore[misc]\n            connections=connections,\n            maxsize=maxsize,\n            block=block,\n            **pool_kwargs,\n        )\n\n\nclass HTTPAdapter(_SSLContextAdapterMixin, _BaseHTTPAdapter):\n    pass\n\n\nclass CacheControlAdapter(_SSLContextAdapterMixin, _BaseCacheControlAdapter):\n    pass\n\n\nclass InsecureHTTPAdapter(HTTPAdapter):\n    def cert_verify(\n        self,\n        conn: ConnectionPool,\n        url: str,\n        verify: Union[bool, str],\n        cert: Optional[Union[str, Tuple[str, str]]],\n    ) -> None:\n        super().cert_verify(conn=conn, url=url, verify=False, cert=cert)\n\n\nclass InsecureCacheControlAdapter(CacheControlAdapter):\n    def cert_verify(\n        self,\n        conn: ConnectionPool,\n        url: str,\n        verify: Union[bool, str],\n        cert: Optional[Union[str, Tuple[str, str]]],\n    ) -> None:\n        super().cert_verify(conn=conn, url=url, verify=False, cert=cert)\n\n\nclass PipSession(requests.Session):\n    timeout: Optional[int] = None\n\n    def __init__(\n        self,\n        *args: Any,\n        retries: int = 0,\n        cache: Optional[str] = None,\n        trusted_hosts: Sequence[str] = (),\n        index_urls: Optional[List[str]] = None,\n        ssl_context: Optional[\"SSLContext\"] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"\n        :param trusted_hosts: Domains not to emit warnings for when not using\n            HTTPS.\n        \"\"\"\n        super().__init__(*args, **kwargs)\n\n        # Namespace the attribute with \"pip_\" just in case to prevent\n        # possible conflicts with the base class.\n        self.pip_trusted_origins: List[Tuple[str, Optional[int]]] = []\n        self.pip_proxy = None\n\n        # Attach our User Agent to the request\n        self.headers[\"User-Agent\"] = user_agent()\n\n        # Attach our Authentication handler to the session\n        self.auth = MultiDomainBasicAuth(index_urls=index_urls)\n\n        # Create our urllib3.Retry instance which will allow us to customize\n        # how we handle retries.\n        retries = urllib3.Retry(\n            # Set the total number of retries that a particular request can\n            # have.\n            total=retries,\n            # A 503 error from PyPI typically means that the Fastly -> Origin\n            # connection got interrupted in some way. A 503 error in general\n            # is typically considered a transient error so we'll go ahead and\n            # retry it.\n            # A 500 may indicate transient error in Amazon S3\n            # A 502 may be a transient error from a CDN like CloudFlare or CloudFront\n            # A 520 or 527 - may indicate transient error in CloudFlare\n            status_forcelist=[500, 502, 503, 520, 527],\n            # Add a small amount of back off between failed requests in\n            # order to prevent hammering the service.\n            backoff_factor=0.25,\n        )  # type: ignore\n\n        # Our Insecure HTTPAdapter disables HTTPS validation. It does not\n        # support caching so we'll use it for all http:// URLs.\n        # If caching is disabled, we will also use it for\n        # https:// hosts that we've marked as ignoring\n        # TLS errors for (trusted-hosts).\n        insecure_adapter = InsecureHTTPAdapter(max_retries=retries)\n\n        # We want to _only_ cache responses on securely fetched origins or when\n        # the host is specified as trusted. We do this because\n        # we can't validate the response of an insecurely/untrusted fetched\n        # origin, and we don't want someone to be able to poison the cache and\n        # require manual eviction from the cache to fix it.\n        if cache:\n            secure_adapter = CacheControlAdapter(\n                cache=SafeFileCache(cache),\n                max_retries=retries,\n                ssl_context=ssl_context,\n            )\n            self._trusted_host_adapter = InsecureCacheControlAdapter(\n                cache=SafeFileCache(cache),\n                max_retries=retries,\n            )\n        else:\n            secure_adapter = HTTPAdapter(max_retries=retries, ssl_context=ssl_context)\n            self._trusted_host_adapter = insecure_adapter\n\n        self.mount(\"https://\", secure_adapter)\n        self.mount(\"http://\", insecure_adapter)\n\n        # Enable file:// urls\n        self.mount(\"file://\", LocalFSAdapter())\n\n        for host in trusted_hosts:\n            self.add_trusted_host(host, suppress_logging=True)\n\n    def update_index_urls(self, new_index_urls: List[str]) -> None:\n        \"\"\"\n        :param new_index_urls: New index urls to update the authentication\n            handler with.\n        \"\"\"\n        self.auth.index_urls = new_index_urls\n\n    def add_trusted_host(\n        self, host: str, source: Optional[str] = None, suppress_logging: bool = False\n    ) -> None:\n        \"\"\"\n        :param host: It is okay to provide a host that has previously been\n            added.\n        :param source: An optional source string, for logging where the host\n            string came from.\n        \"\"\"\n        if not suppress_logging:\n            msg = f\"adding trusted host: {host!r}\"\n            if source is not None:\n                msg += f\" (from {source})\"\n            logger.info(msg)\n\n        parsed_host, parsed_port = parse_netloc(host)\n        if parsed_host is None:\n            raise ValueError(f\"Trusted host URL must include a host part: {host!r}\")\n        if (parsed_host, parsed_port) not in self.pip_trusted_origins:\n            self.pip_trusted_origins.append((parsed_host, parsed_port))\n\n        self.mount(\n            build_url_from_netloc(host, scheme=\"http\") + \"/\", self._trusted_host_adapter\n        )\n        self.mount(build_url_from_netloc(host) + \"/\", self._trusted_host_adapter)\n        if not parsed_port:\n            self.mount(\n                build_url_from_netloc(host, scheme=\"http\") + \":\",\n                self._trusted_host_adapter,\n            )\n            # Mount wildcard ports for the same host.\n            self.mount(build_url_from_netloc(host) + \":\", self._trusted_host_adapter)\n\n    def iter_secure_origins(self) -> Generator[SecureOrigin, None, None]:\n        yield from SECURE_ORIGINS\n        for host, port in self.pip_trusted_origins:\n            yield (\"*\", host, \"*\" if port is None else port)\n\n    def is_secure_origin(self, location: Link) -> bool:\n        # Determine if this url used a secure transport mechanism\n        parsed = urllib.parse.urlparse(str(location))\n        origin_protocol, origin_host, origin_port = (\n            parsed.scheme,\n            parsed.hostname,\n            parsed.port,\n        )\n\n        # The protocol to use to see if the protocol matches.\n        # Don't count the repository type as part of the protocol: in\n        # cases such as \"git+ssh\", only use \"ssh\". (I.e., Only verify against\n        # the last scheme.)\n        origin_protocol = origin_protocol.rsplit(\"+\", 1)[-1]\n\n        # Determine if our origin is a secure origin by looking through our\n        # hardcoded list of secure origins, as well as any additional ones\n        # configured on this PackageFinder instance.\n        for secure_origin in self.iter_secure_origins():\n            secure_protocol, secure_host, secure_port = secure_origin\n            if origin_protocol != secure_protocol and secure_protocol != \"*\":\n                continue\n\n            try:\n                addr = ipaddress.ip_address(origin_host or \"\")\n                network = ipaddress.ip_network(secure_host)\n            except ValueError:\n                # We don't have both a valid address or a valid network, so\n                # we'll check this origin against hostnames.\n                if (\n                    origin_host\n                    and origin_host.lower() != secure_host.lower()\n                    and secure_host != \"*\"\n                ):\n                    continue\n            else:\n                # We have a valid address and network, so see if the address\n                # is contained within the network.\n                if addr not in network:\n                    continue\n\n            # Check to see if the port matches.\n            if (\n                origin_port != secure_port\n                and secure_port != \"*\"\n                and secure_port is not None\n            ):\n                continue\n\n            # If we've gotten here, then this origin matches the current\n            # secure origin and we should return True\n            return True\n\n        # If we've gotten to this point, then the origin isn't secure and we\n        # will not accept it as a valid location to search. We will however\n        # log a warning that we are ignoring it.\n        logger.warning(\n            \"The repository located at %s is not a trusted or secure host and \"\n            \"is being ignored. If this repository is available via HTTPS we \"\n            \"recommend you use HTTPS instead, otherwise you may silence \"\n            \"this warning and allow it anyway with '--trusted-host %s'.\",\n            origin_host,\n            origin_host,\n        )\n\n        return False\n\n    def request(self, method: str, url: str, *args: Any, **kwargs: Any) -> Response:\n        # Allow setting a default timeout on a session\n        kwargs.setdefault(\"timeout\", self.timeout)\n        # Allow setting a default proxies on a session\n        kwargs.setdefault(\"proxies\", self.proxies)\n\n        # Dispatch the actual request\n        return super().request(method, url, *args, **kwargs)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/network/utils.py","size":4088,"sha1":"bb62c8293bb1248d7515a2735dfccbf97ef0298a","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"from typing import Dict, Generator\n\nfrom pip._vendor.requests.models import Response\n\nfrom pip._internal.exceptions import NetworkConnectionError\n\n# The following comments and HTTP headers were originally added by\n# Donald Stufft in git commit 22c562429a61bb77172039e480873fb239dd8c03.\n#\n# We use Accept-Encoding: identity here because requests defaults to\n# accepting compressed responses. This breaks in a variety of ways\n# depending on how the server is configured.\n# - Some servers will notice that the file isn't a compressible file\n#   and will leave the file alone and with an empty Content-Encoding\n# - Some servers will notice that the file is already compressed and\n#   will leave the file alone, adding a Content-Encoding: gzip header\n# - Some servers won't notice anything at all and will take a file\n#   that's already been compressed and compress it again, and set\n#   the Content-Encoding: gzip header\n# By setting this to request only the identity encoding we're hoping\n# to eliminate the third case.  Hopefully there does not exist a server\n# which when given a file will notice it is already compressed and that\n# you're not asking for a compressed file and will then decompress it\n# before sending because if that's the case I don't think it'll ever be\n# possible to make this work.\nHEADERS: Dict[str, str] = {\"Accept-Encoding\": \"identity\"}\n\nDOWNLOAD_CHUNK_SIZE = 256 * 1024\n\n\ndef raise_for_status(resp: Response) -> None:\n    http_error_msg = \"\"\n    if isinstance(resp.reason, bytes):\n        # We attempt to decode utf-8 first because some servers\n        # choose to localize their reason strings. If the string\n        # isn't utf-8, we fall back to iso-8859-1 for all other\n        # encodings.\n        try:\n            reason = resp.reason.decode(\"utf-8\")\n        except UnicodeDecodeError:\n            reason = resp.reason.decode(\"iso-8859-1\")\n    else:\n        reason = resp.reason\n\n    if 400 <= resp.status_code < 500:\n        http_error_msg = (\n            f\"{resp.status_code} Client Error: {reason} for url: {resp.url}\"\n        )\n\n    elif 500 <= resp.status_code < 600:\n        http_error_msg = (\n            f\"{resp.status_code} Server Error: {reason} for url: {resp.url}\"\n        )\n\n    if http_error_msg:\n        raise NetworkConnectionError(http_error_msg, response=resp)\n\n\ndef response_chunks(\n    response: Response, chunk_size: int = DOWNLOAD_CHUNK_SIZE\n) -> Generator[bytes, None, None]:\n    \"\"\"Given a requests Response, provide the data chunks.\"\"\"\n    try:\n        # Special case for urllib3.\n        for chunk in response.raw.stream(\n            chunk_size,\n            # We use decode_content=False here because we don't\n            # want urllib3 to mess with the raw bytes we get\n            # from the server. If we decompress inside of\n            # urllib3 then we cannot verify the checksum\n            # because the checksum will be of the compressed\n            # file. This breakage will only occur if the\n            # server adds a Content-Encoding header, which\n            # depends on how the server was configured:\n            # - Some servers will notice that the file isn't a\n            #   compressible file and will leave the file alone\n            #   and with an empty Content-Encoding\n            # - Some servers will notice that the file is\n            #   already compressed and will leave the file\n            #   alone and will add a Content-Encoding: gzip\n            #   header\n            # - Some servers won't notice anything at all and\n            #   will take a file that's already been compressed\n            #   and compress it again and set the\n            #   Content-Encoding: gzip header\n            #\n            # By setting this not to decode automatically we\n            # hope to eliminate problems with the second case.\n            decode_content=False,\n        ):\n            yield chunk\n    except AttributeError:\n        # Standard file-like object.\n        while True:\n            chunk = response.raw.read(chunk_size)\n            if not chunk:\n                break\n            yield chunk\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/network/xmlrpc.py","size":1838,"sha1":"1b9d05d0166567a0f7b6d0295e5450ce8627cb64","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"xmlrpclib.Transport implementation\n\"\"\"\n\nimport logging\nimport urllib.parse\nimport xmlrpc.client\nfrom typing import TYPE_CHECKING, Tuple\n\nfrom pip._internal.exceptions import NetworkConnectionError\nfrom pip._internal.network.session import PipSession\nfrom pip._internal.network.utils import raise_for_status\n\nif TYPE_CHECKING:\n    from xmlrpc.client import _HostType, _Marshallable\n\n    from _typeshed import SizedBuffer\n\nlogger = logging.getLogger(__name__)\n\n\nclass PipXmlrpcTransport(xmlrpc.client.Transport):\n    \"\"\"Provide a `xmlrpclib.Transport` implementation via a `PipSession`\n    object.\n    \"\"\"\n\n    def __init__(\n        self, index_url: str, session: PipSession, use_datetime: bool = False\n    ) -> None:\n        super().__init__(use_datetime)\n        index_parts = urllib.parse.urlparse(index_url)\n        self._scheme = index_parts.scheme\n        self._session = session\n\n    def request(\n        self,\n        host: \"_HostType\",\n        handler: str,\n        request_body: \"SizedBuffer\",\n        verbose: bool = False,\n    ) -> Tuple[\"_Marshallable\", ...]:\n        assert isinstance(host, str)\n        parts = (self._scheme, host, handler, None, None, None)\n        url = urllib.parse.urlunparse(parts)\n        try:\n            headers = {\"Content-Type\": \"text/xml\"}\n            response = self._session.post(\n                url,\n                data=request_body,\n                headers=headers,\n                stream=True,\n            )\n            raise_for_status(response)\n            self.verbose = verbose\n            return self.parse_response(response.raw)\n        except NetworkConnectionError as exc:\n            assert exc.response\n            logger.critical(\n                \"HTTP error %s while getting %s\",\n                exc.response.status_code,\n                url,\n            )\n            raise\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/operations/__init__.py","size":0,"sha1":"da39a3ee5e6b4b0d3255bfef95601890afd80709","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":""},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/operations/build/__init__.py","size":0,"sha1":"da39a3ee5e6b4b0d3255bfef95601890afd80709","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":""},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/operations/build/build_tracker.py","size":4774,"sha1":"3ce919ff6b3538a4ce4f3d360378760933510885","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import contextlib\nimport hashlib\nimport logging\nimport os\nfrom types import TracebackType\nfrom typing import Dict, Generator, Optional, Type, Union\n\nfrom pip._internal.req.req_install import InstallRequirement\nfrom pip._internal.utils.temp_dir import TempDirectory\n\nlogger = logging.getLogger(__name__)\n\n\n@contextlib.contextmanager\ndef update_env_context_manager(**changes: str) -> Generator[None, None, None]:\n    target = os.environ\n\n    # Save values from the target and change them.\n    non_existent_marker = object()\n    saved_values: Dict[str, Union[object, str]] = {}\n    for name, new_value in changes.items():\n        try:\n            saved_values[name] = target[name]\n        except KeyError:\n            saved_values[name] = non_existent_marker\n        target[name] = new_value\n\n    try:\n        yield\n    finally:\n        # Restore original values in the target.\n        for name, original_value in saved_values.items():\n            if original_value is non_existent_marker:\n                del target[name]\n            else:\n                assert isinstance(original_value, str)  # for mypy\n                target[name] = original_value\n\n\n@contextlib.contextmanager\ndef get_build_tracker() -> Generator[\"BuildTracker\", None, None]:\n    root = os.environ.get(\"PIP_BUILD_TRACKER\")\n    with contextlib.ExitStack() as ctx:\n        if root is None:\n            root = ctx.enter_context(TempDirectory(kind=\"build-tracker\")).path\n            ctx.enter_context(update_env_context_manager(PIP_BUILD_TRACKER=root))\n            logger.debug(\"Initialized build tracking at %s\", root)\n\n        with BuildTracker(root) as tracker:\n            yield tracker\n\n\nclass TrackerId(str):\n    \"\"\"Uniquely identifying string provided to the build tracker.\"\"\"\n\n\nclass BuildTracker:\n    \"\"\"Ensure that an sdist cannot request itself as a setup requirement.\n\n    When an sdist is prepared, it identifies its setup requirements in the\n    context of ``BuildTracker.track()``. If a requirement shows up recursively, this\n    raises an exception.\n\n    This stops fork bombs embedded in malicious packages.\"\"\"\n\n    def __init__(self, root: str) -> None:\n        self._root = root\n        self._entries: Dict[TrackerId, InstallRequirement] = {}\n        logger.debug(\"Created build tracker: %s\", self._root)\n\n    def __enter__(self) -> \"BuildTracker\":\n        logger.debug(\"Entered build tracker: %s\", self._root)\n        return self\n\n    def __exit__(\n        self,\n        exc_type: Optional[Type[BaseException]],\n        exc_val: Optional[BaseException],\n        exc_tb: Optional[TracebackType],\n    ) -> None:\n        self.cleanup()\n\n    def _entry_path(self, key: TrackerId) -> str:\n        hashed = hashlib.sha224(key.encode()).hexdigest()\n        return os.path.join(self._root, hashed)\n\n    def add(self, req: InstallRequirement, key: TrackerId) -> None:\n        \"\"\"Add an InstallRequirement to build tracking.\"\"\"\n\n        # Get the file to write information about this requirement.\n        entry_path = self._entry_path(key)\n\n        # Try reading from the file. If it exists and can be read from, a build\n        # is already in progress, so a LookupError is raised.\n        try:\n            with open(entry_path) as fp:\n                contents = fp.read()\n        except FileNotFoundError:\n            pass\n        else:\n            message = f\"{req.link} is already being built: {contents}\"\n            raise LookupError(message)\n\n        # If we're here, req should really not be building already.\n        assert key not in self._entries\n\n        # Start tracking this requirement.\n        with open(entry_path, \"w\", encoding=\"utf-8\") as fp:\n            fp.write(str(req))\n        self._entries[key] = req\n\n        logger.debug(\"Added %s to build tracker %r\", req, self._root)\n\n    def remove(self, req: InstallRequirement, key: TrackerId) -> None:\n        \"\"\"Remove an InstallRequirement from build tracking.\"\"\"\n\n        # Delete the created file and the corresponding entry.\n        os.unlink(self._entry_path(key))\n        del self._entries[key]\n\n        logger.debug(\"Removed %s from build tracker %r\", req, self._root)\n\n    def cleanup(self) -> None:\n        for key, req in list(self._entries.items()):\n            self.remove(req, key)\n\n        logger.debug(\"Removed build tracker: %r\", self._root)\n\n    @contextlib.contextmanager\n    def track(self, req: InstallRequirement, key: str) -> Generator[None, None, None]:\n        \"\"\"Ensure that `key` cannot install itself as a setup requirement.\n\n        :raises LookupError: If `key` was already provided in a parent invocation of\n                             the context introduced by this method.\"\"\"\n        tracker_id = TrackerId(key)\n        self.add(req, tracker_id)\n        yield\n        self.remove(req, tracker_id)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/operations/build/metadata.py","size":1422,"sha1":"c816fd8f874f799a9620d92db505598d21c82ba8","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"Metadata generation logic for source distributions.\n\"\"\"\n\nimport os\n\nfrom pip._vendor.pyproject_hooks import BuildBackendHookCaller\n\nfrom pip._internal.build_env import BuildEnvironment\nfrom pip._internal.exceptions import (\n    InstallationSubprocessError,\n    MetadataGenerationFailed,\n)\nfrom pip._internal.utils.subprocess import runner_with_spinner_message\nfrom pip._internal.utils.temp_dir import TempDirectory\n\n\ndef generate_metadata(\n    build_env: BuildEnvironment, backend: BuildBackendHookCaller, details: str\n) -> str:\n    \"\"\"Generate metadata using mechanisms described in PEP 517.\n\n    Returns the generated metadata directory.\n    \"\"\"\n    metadata_tmpdir = TempDirectory(kind=\"modern-metadata\", globally_managed=True)\n\n    metadata_dir = metadata_tmpdir.path\n\n    with build_env:\n        # Note that BuildBackendHookCaller implements a fallback for\n        # prepare_metadata_for_build_wheel, so we don't have to\n        # consider the possibility that this hook doesn't exist.\n        runner = runner_with_spinner_message(\"Preparing metadata (pyproject.toml)\")\n        with backend.subprocess_runner(runner):\n            try:\n                distinfo_dir = backend.prepare_metadata_for_build_wheel(metadata_dir)\n            except InstallationSubprocessError as error:\n                raise MetadataGenerationFailed(package_details=details) from error\n\n    return os.path.join(metadata_dir, distinfo_dir)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/operations/build/metadata_editable.py","size":1510,"sha1":"319cde6d1c14dc9d6b0ddf9eb917b3656f28e8e7","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"Metadata generation logic for source distributions.\n\"\"\"\n\nimport os\n\nfrom pip._vendor.pyproject_hooks import BuildBackendHookCaller\n\nfrom pip._internal.build_env import BuildEnvironment\nfrom pip._internal.exceptions import (\n    InstallationSubprocessError,\n    MetadataGenerationFailed,\n)\nfrom pip._internal.utils.subprocess import runner_with_spinner_message\nfrom pip._internal.utils.temp_dir import TempDirectory\n\n\ndef generate_editable_metadata(\n    build_env: BuildEnvironment, backend: BuildBackendHookCaller, details: str\n) -> str:\n    \"\"\"Generate metadata using mechanisms described in PEP 660.\n\n    Returns the generated metadata directory.\n    \"\"\"\n    metadata_tmpdir = TempDirectory(kind=\"modern-metadata\", globally_managed=True)\n\n    metadata_dir = metadata_tmpdir.path\n\n    with build_env:\n        # Note that BuildBackendHookCaller implements a fallback for\n        # prepare_metadata_for_build_wheel/editable, so we don't have to\n        # consider the possibility that this hook doesn't exist.\n        runner = runner_with_spinner_message(\n            \"Preparing editable metadata (pyproject.toml)\"\n        )\n        with backend.subprocess_runner(runner):\n            try:\n                distinfo_dir = backend.prepare_metadata_for_build_editable(metadata_dir)\n            except InstallationSubprocessError as error:\n                raise MetadataGenerationFailed(package_details=details) from error\n\n    assert distinfo_dir is not None\n    return os.path.join(metadata_dir, distinfo_dir)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/operations/build/metadata_legacy.py","size":2190,"sha1":"e6d7d90cacce22677e8ba340b0bb31bb77dc90da","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"Metadata generation logic for legacy source distributions.\n\"\"\"\n\nimport logging\nimport os\n\nfrom pip._internal.build_env import BuildEnvironment\nfrom pip._internal.cli.spinners import open_spinner\nfrom pip._internal.exceptions import (\n    InstallationError,\n    InstallationSubprocessError,\n    MetadataGenerationFailed,\n)\nfrom pip._internal.utils.setuptools_build import make_setuptools_egg_info_args\nfrom pip._internal.utils.subprocess import call_subprocess\nfrom pip._internal.utils.temp_dir import TempDirectory\n\nlogger = logging.getLogger(__name__)\n\n\ndef _find_egg_info(directory: str) -> str:\n    \"\"\"Find an .egg-info subdirectory in `directory`.\"\"\"\n    filenames = [f for f in os.listdir(directory) if f.endswith(\".egg-info\")]\n\n    if not filenames:\n        raise InstallationError(f\"No .egg-info directory found in {directory}\")\n\n    if len(filenames) > 1:\n        raise InstallationError(\n            f\"More than one .egg-info directory found in {directory}\"\n        )\n\n    return os.path.join(directory, filenames[0])\n\n\ndef generate_metadata(\n    build_env: BuildEnvironment,\n    setup_py_path: str,\n    source_dir: str,\n    isolated: bool,\n    details: str,\n) -> str:\n    \"\"\"Generate metadata using setup.py-based defacto mechanisms.\n\n    Returns the generated metadata directory.\n    \"\"\"\n    logger.debug(\n        \"Running setup.py (path:%s) egg_info for package %s\",\n        setup_py_path,\n        details,\n    )\n\n    egg_info_dir = TempDirectory(kind=\"pip-egg-info\", globally_managed=True).path\n\n    args = make_setuptools_egg_info_args(\n        setup_py_path,\n        egg_info_dir=egg_info_dir,\n        no_user_config=isolated,\n    )\n\n    with build_env:\n        with open_spinner(\"Preparing metadata (setup.py)\") as spinner:\n            try:\n                call_subprocess(\n                    args,\n                    cwd=source_dir,\n                    command_desc=\"python setup.py egg_info\",\n                    spinner=spinner,\n                )\n            except InstallationSubprocessError as error:\n                raise MetadataGenerationFailed(package_details=details) from error\n\n    # Return the .egg-info directory.\n    return _find_egg_info(egg_info_dir)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/operations/build/wheel.py","size":1075,"sha1":"dcd764c358f280cc9fdb2e90ab06a9686d3f21ba","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import logging\nimport os\nfrom typing import Optional\n\nfrom pip._vendor.pyproject_hooks import BuildBackendHookCaller\n\nfrom pip._internal.utils.subprocess import runner_with_spinner_message\n\nlogger = logging.getLogger(__name__)\n\n\ndef build_wheel_pep517(\n    name: str,\n    backend: BuildBackendHookCaller,\n    metadata_directory: str,\n    tempd: str,\n) -> Optional[str]:\n    \"\"\"Build one InstallRequirement using the PEP 517 build process.\n\n    Returns path to wheel if successfully built. Otherwise, returns None.\n    \"\"\"\n    assert metadata_directory is not None\n    try:\n        logger.debug(\"Destination directory: %s\", tempd)\n\n        runner = runner_with_spinner_message(\n            f\"Building wheel for {name} (pyproject.toml)\"\n        )\n        with backend.subprocess_runner(runner):\n            wheel_name = backend.build_wheel(\n                tempd,\n                metadata_directory=metadata_directory,\n            )\n    except Exception:\n        logger.error(\"Failed building wheel for %s\", name)\n        return None\n    return os.path.join(tempd, wheel_name)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/operations/build/wheel_editable.py","size":1417,"sha1":"1d8e256134a57f9c5fa78bb388b31b61d2d0c3ce","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import logging\nimport os\nfrom typing import Optional\n\nfrom pip._vendor.pyproject_hooks import BuildBackendHookCaller, HookMissing\n\nfrom pip._internal.utils.subprocess import runner_with_spinner_message\n\nlogger = logging.getLogger(__name__)\n\n\ndef build_wheel_editable(\n    name: str,\n    backend: BuildBackendHookCaller,\n    metadata_directory: str,\n    tempd: str,\n) -> Optional[str]:\n    \"\"\"Build one InstallRequirement using the PEP 660 build process.\n\n    Returns path to wheel if successfully built. Otherwise, returns None.\n    \"\"\"\n    assert metadata_directory is not None\n    try:\n        logger.debug(\"Destination directory: %s\", tempd)\n\n        runner = runner_with_spinner_message(\n            f\"Building editable for {name} (pyproject.toml)\"\n        )\n        with backend.subprocess_runner(runner):\n            try:\n                wheel_name = backend.build_editable(\n                    tempd,\n                    metadata_directory=metadata_directory,\n                )\n            except HookMissing as e:\n                logger.error(\n                    \"Cannot build editable %s because the build \"\n                    \"backend does not have the %s hook\",\n                    name,\n                    e,\n                )\n                return None\n    except Exception:\n        logger.error(\"Failed building editable for %s\", name)\n        return None\n    return os.path.join(tempd, wheel_name)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/operations/build/wheel_legacy.py","size":3045,"sha1":"c11da3688040faca17b3b89417f5f8dd6d8d7c2d","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import logging\nimport os.path\nfrom typing import List, Optional\n\nfrom pip._internal.cli.spinners import open_spinner\nfrom pip._internal.utils.setuptools_build import make_setuptools_bdist_wheel_args\nfrom pip._internal.utils.subprocess import call_subprocess, format_command_args\n\nlogger = logging.getLogger(__name__)\n\n\ndef format_command_result(\n    command_args: List[str],\n    command_output: str,\n) -> str:\n    \"\"\"Format command information for logging.\"\"\"\n    command_desc = format_command_args(command_args)\n    text = f\"Command arguments: {command_desc}\\n\"\n\n    if not command_output:\n        text += \"Command output: None\"\n    elif logger.getEffectiveLevel() > logging.DEBUG:\n        text += \"Command output: [use --verbose to show]\"\n    else:\n        if not command_output.endswith(\"\\n\"):\n            command_output += \"\\n\"\n        text += f\"Command output:\\n{command_output}\"\n\n    return text\n\n\ndef get_legacy_build_wheel_path(\n    names: List[str],\n    temp_dir: str,\n    name: str,\n    command_args: List[str],\n    command_output: str,\n) -> Optional[str]:\n    \"\"\"Return the path to the wheel in the temporary build directory.\"\"\"\n    # Sort for determinism.\n    names = sorted(names)\n    if not names:\n        msg = f\"Legacy build of wheel for {name!r} created no files.\\n\"\n        msg += format_command_result(command_args, command_output)\n        logger.warning(msg)\n        return None\n\n    if len(names) > 1:\n        msg = (\n            f\"Legacy build of wheel for {name!r} created more than one file.\\n\"\n            f\"Filenames (choosing first): {names}\\n\"\n        )\n        msg += format_command_result(command_args, command_output)\n        logger.warning(msg)\n\n    return os.path.join(temp_dir, names[0])\n\n\ndef build_wheel_legacy(\n    name: str,\n    setup_py_path: str,\n    source_dir: str,\n    global_options: List[str],\n    build_options: List[str],\n    tempd: str,\n) -> Optional[str]:\n    \"\"\"Build one unpacked package using the \"legacy\" build process.\n\n    Returns path to wheel if successfully built. Otherwise, returns None.\n    \"\"\"\n    wheel_args = make_setuptools_bdist_wheel_args(\n        setup_py_path,\n        global_options=global_options,\n        build_options=build_options,\n        destination_dir=tempd,\n    )\n\n    spin_message = f\"Building wheel for {name} (setup.py)\"\n    with open_spinner(spin_message) as spinner:\n        logger.debug(\"Destination directory: %s\", tempd)\n\n        try:\n            output = call_subprocess(\n                wheel_args,\n                command_desc=\"python setup.py bdist_wheel\",\n                cwd=source_dir,\n                spinner=spinner,\n            )\n        except Exception:\n            spinner.finish(\"error\")\n            logger.error(\"Failed building wheel for %s\", name)\n            return None\n\n        names = os.listdir(tempd)\n        wheel_path = get_legacy_build_wheel_path(\n            names=names,\n            temp_dir=tempd,\n            name=name,\n            command_args=wheel_args,\n            command_output=output,\n        )\n        return wheel_path\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/operations/check.py","size":5912,"sha1":"85e2d739a4f7c67a948944d3c8c8d34bf609f6a2","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"Validation of dependencies of packages\n\"\"\"\n\nimport logging\nfrom contextlib import suppress\nfrom email.parser import Parser\nfrom functools import reduce\nfrom typing import (\n    Callable,\n    Dict,\n    FrozenSet,\n    Generator,\n    Iterable,\n    List,\n    NamedTuple,\n    Optional,\n    Set,\n    Tuple,\n)\n\nfrom pip._vendor.packaging.requirements import Requirement\nfrom pip._vendor.packaging.tags import Tag, parse_tag\nfrom pip._vendor.packaging.utils import NormalizedName, canonicalize_name\nfrom pip._vendor.packaging.version import Version\n\nfrom pip._internal.distributions import make_distribution_for_install_requirement\nfrom pip._internal.metadata import get_default_environment\nfrom pip._internal.metadata.base import BaseDistribution\nfrom pip._internal.req.req_install import InstallRequirement\n\nlogger = logging.getLogger(__name__)\n\n\nclass PackageDetails(NamedTuple):\n    version: Version\n    dependencies: List[Requirement]\n\n\n# Shorthands\nPackageSet = Dict[NormalizedName, PackageDetails]\nMissing = Tuple[NormalizedName, Requirement]\nConflicting = Tuple[NormalizedName, Version, Requirement]\n\nMissingDict = Dict[NormalizedName, List[Missing]]\nConflictingDict = Dict[NormalizedName, List[Conflicting]]\nCheckResult = Tuple[MissingDict, ConflictingDict]\nConflictDetails = Tuple[PackageSet, CheckResult]\n\n\ndef create_package_set_from_installed() -> Tuple[PackageSet, bool]:\n    \"\"\"Converts a list of distributions into a PackageSet.\"\"\"\n    package_set = {}\n    problems = False\n    env = get_default_environment()\n    for dist in env.iter_installed_distributions(local_only=False, skip=()):\n        name = dist.canonical_name\n        try:\n            dependencies = list(dist.iter_dependencies())\n            package_set[name] = PackageDetails(dist.version, dependencies)\n        except (OSError, ValueError) as e:\n            # Don't crash on unreadable or broken metadata.\n            logger.warning(\"Error parsing dependencies of %s: %s\", name, e)\n            problems = True\n    return package_set, problems\n\n\ndef check_package_set(\n    package_set: PackageSet, should_ignore: Optional[Callable[[str], bool]] = None\n) -> CheckResult:\n    \"\"\"Check if a package set is consistent\n\n    If should_ignore is passed, it should be a callable that takes a\n    package name and returns a boolean.\n    \"\"\"\n\n    missing = {}\n    conflicting = {}\n\n    for package_name, package_detail in package_set.items():\n        # Info about dependencies of package_name\n        missing_deps: Set[Missing] = set()\n        conflicting_deps: Set[Conflicting] = set()\n\n        if should_ignore and should_ignore(package_name):\n            continue\n\n        for req in package_detail.dependencies:\n            name = canonicalize_name(req.name)\n\n            # Check if it's missing\n            if name not in package_set:\n                missed = True\n                if req.marker is not None:\n                    missed = req.marker.evaluate({\"extra\": \"\"})\n                if missed:\n                    missing_deps.add((name, req))\n                continue\n\n            # Check if there's a conflict\n            version = package_set[name].version\n            if not req.specifier.contains(version, prereleases=True):\n                conflicting_deps.add((name, version, req))\n\n        if missing_deps:\n            missing[package_name] = sorted(missing_deps, key=str)\n        if conflicting_deps:\n            conflicting[package_name] = sorted(conflicting_deps, key=str)\n\n    return missing, conflicting\n\n\ndef check_install_conflicts(to_install: List[InstallRequirement]) -> ConflictDetails:\n    \"\"\"For checking if the dependency graph would be consistent after \\\n    installing given requirements\n    \"\"\"\n    # Start from the current state\n    package_set, _ = create_package_set_from_installed()\n    # Install packages\n    would_be_installed = _simulate_installation_of(to_install, package_set)\n\n    # Only warn about directly-dependent packages; create a whitelist of them\n    whitelist = _create_whitelist(would_be_installed, package_set)\n\n    return (\n        package_set,\n        check_package_set(\n            package_set, should_ignore=lambda name: name not in whitelist\n        ),\n    )\n\n\ndef check_unsupported(\n    packages: Iterable[BaseDistribution],\n    supported_tags: Iterable[Tag],\n) -> Generator[BaseDistribution, None, None]:\n    for p in packages:\n        with suppress(FileNotFoundError):\n            wheel_file = p.read_text(\"WHEEL\")\n            wheel_tags: FrozenSet[Tag] = reduce(\n                frozenset.union,\n                map(parse_tag, Parser().parsestr(wheel_file).get_all(\"Tag\", [])),\n                frozenset(),\n            )\n            if wheel_tags.isdisjoint(supported_tags):\n                yield p\n\n\ndef _simulate_installation_of(\n    to_install: List[InstallRequirement], package_set: PackageSet\n) -> Set[NormalizedName]:\n    \"\"\"Computes the version of packages after installing to_install.\"\"\"\n    # Keep track of packages that were installed\n    installed = set()\n\n    # Modify it as installing requirement_set would (assuming no errors)\n    for inst_req in to_install:\n        abstract_dist = make_distribution_for_install_requirement(inst_req)\n        dist = abstract_dist.get_metadata_distribution()\n        name = dist.canonical_name\n        package_set[name] = PackageDetails(dist.version, list(dist.iter_dependencies()))\n\n        installed.add(name)\n\n    return installed\n\n\ndef _create_whitelist(\n    would_be_installed: Set[NormalizedName], package_set: PackageSet\n) -> Set[NormalizedName]:\n    packages_affected = set(would_be_installed)\n\n    for package_name in package_set:\n        if package_name in packages_affected:\n            continue\n\n        for req in package_set[package_name].dependencies:\n            if canonicalize_name(req.name) in packages_affected:\n                packages_affected.add(package_name)\n                break\n\n    return packages_affected\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/operations/freeze.py","size":9843,"sha1":"47df3e86b3164387ca1b88911ed9a983ba9cde94","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import collections\nimport logging\nimport os\nfrom dataclasses import dataclass, field\nfrom typing import Container, Dict, Generator, Iterable, List, NamedTuple, Optional, Set\n\nfrom pip._vendor.packaging.utils import NormalizedName, canonicalize_name\nfrom pip._vendor.packaging.version import InvalidVersion\n\nfrom pip._internal.exceptions import BadCommand, InstallationError\nfrom pip._internal.metadata import BaseDistribution, get_environment\nfrom pip._internal.req.constructors import (\n    install_req_from_editable,\n    install_req_from_line,\n)\nfrom pip._internal.req.req_file import COMMENT_RE\nfrom pip._internal.utils.direct_url_helpers import direct_url_as_pep440_direct_reference\n\nlogger = logging.getLogger(__name__)\n\n\nclass _EditableInfo(NamedTuple):\n    requirement: str\n    comments: List[str]\n\n\ndef freeze(\n    requirement: Optional[List[str]] = None,\n    local_only: bool = False,\n    user_only: bool = False,\n    paths: Optional[List[str]] = None,\n    isolated: bool = False,\n    exclude_editable: bool = False,\n    skip: Container[str] = (),\n) -> Generator[str, None, None]:\n    installations: Dict[str, FrozenRequirement] = {}\n\n    dists = get_environment(paths).iter_installed_distributions(\n        local_only=local_only,\n        skip=(),\n        user_only=user_only,\n    )\n    for dist in dists:\n        req = FrozenRequirement.from_dist(dist)\n        if exclude_editable and req.editable:\n            continue\n        installations[req.canonical_name] = req\n\n    if requirement:\n        # the options that don't get turned into an InstallRequirement\n        # should only be emitted once, even if the same option is in multiple\n        # requirements files, so we need to keep track of what has been emitted\n        # so that we don't emit it again if it's seen again\n        emitted_options: Set[str] = set()\n        # keep track of which files a requirement is in so that we can\n        # give an accurate warning if a requirement appears multiple times.\n        req_files: Dict[str, List[str]] = collections.defaultdict(list)\n        for req_file_path in requirement:\n            with open(req_file_path) as req_file:\n                for line in req_file:\n                    if (\n                        not line.strip()\n                        or line.strip().startswith(\"#\")\n                        or line.startswith(\n                            (\n                                \"-r\",\n                                \"--requirement\",\n                                \"-f\",\n                                \"--find-links\",\n                                \"-i\",\n                                \"--index-url\",\n                                \"--pre\",\n                                \"--trusted-host\",\n                                \"--process-dependency-links\",\n                                \"--extra-index-url\",\n                                \"--use-feature\",\n                            )\n                        )\n                    ):\n                        line = line.rstrip()\n                        if line not in emitted_options:\n                            emitted_options.add(line)\n                            yield line\n                        continue\n\n                    if line.startswith(\"-e\") or line.startswith(\"--editable\"):\n                        if line.startswith(\"-e\"):\n                            line = line[2:].strip()\n                        else:\n                            line = line[len(\"--editable\") :].strip().lstrip(\"=\")\n                        line_req = install_req_from_editable(\n                            line,\n                            isolated=isolated,\n                        )\n                    else:\n                        line_req = install_req_from_line(\n                            COMMENT_RE.sub(\"\", line).strip(),\n                            isolated=isolated,\n                        )\n\n                    if not line_req.name:\n                        logger.info(\n                            \"Skipping line in requirement file [%s] because \"\n                            \"it's not clear what it would install: %s\",\n                            req_file_path,\n                            line.strip(),\n                        )\n                        logger.info(\n                            \"  (add #egg=PackageName to the URL to avoid\"\n                            \" this warning)\"\n                        )\n                    else:\n                        line_req_canonical_name = canonicalize_name(line_req.name)\n                        if line_req_canonical_name not in installations:\n                            # either it's not installed, or it is installed\n                            # but has been processed already\n                            if not req_files[line_req.name]:\n                                logger.warning(\n                                    \"Requirement file [%s] contains %s, but \"\n                                    \"package %r is not installed\",\n                                    req_file_path,\n                                    COMMENT_RE.sub(\"\", line).strip(),\n                                    line_req.name,\n                                )\n                            else:\n                                req_files[line_req.name].append(req_file_path)\n                        else:\n                            yield str(installations[line_req_canonical_name]).rstrip()\n                            del installations[line_req_canonical_name]\n                            req_files[line_req.name].append(req_file_path)\n\n        # Warn about requirements that were included multiple times (in a\n        # single requirements file or in different requirements files).\n        for name, files in req_files.items():\n            if len(files) > 1:\n                logger.warning(\n                    \"Requirement %s included multiple times [%s]\",\n                    name,\n                    \", \".join(sorted(set(files))),\n                )\n\n        yield (\"## The following requirements were added by pip freeze:\")\n    for installation in sorted(installations.values(), key=lambda x: x.name.lower()):\n        if installation.canonical_name not in skip:\n            yield str(installation).rstrip()\n\n\ndef _format_as_name_version(dist: BaseDistribution) -> str:\n    try:\n        dist_version = dist.version\n    except InvalidVersion:\n        # legacy version\n        return f\"{dist.raw_name}==={dist.raw_version}\"\n    else:\n        return f\"{dist.raw_name}=={dist_version}\"\n\n\ndef _get_editable_info(dist: BaseDistribution) -> _EditableInfo:\n    \"\"\"\n    Compute and return values (req, comments) for use in\n    FrozenRequirement.from_dist().\n    \"\"\"\n    editable_project_location = dist.editable_project_location\n    assert editable_project_location\n    location = os.path.normcase(os.path.abspath(editable_project_location))\n\n    from pip._internal.vcs import RemoteNotFoundError, RemoteNotValidError, vcs\n\n    vcs_backend = vcs.get_backend_for_dir(location)\n\n    if vcs_backend is None:\n        display = _format_as_name_version(dist)\n        logger.debug(\n            'No VCS found for editable requirement \"%s\" in: %r',\n            display,\n            location,\n        )\n        return _EditableInfo(\n            requirement=location,\n            comments=[f\"# Editable install with no version control ({display})\"],\n        )\n\n    vcs_name = type(vcs_backend).__name__\n\n    try:\n        req = vcs_backend.get_src_requirement(location, dist.raw_name)\n    except RemoteNotFoundError:\n        display = _format_as_name_version(dist)\n        return _EditableInfo(\n            requirement=location,\n            comments=[f\"# Editable {vcs_name} install with no remote ({display})\"],\n        )\n    except RemoteNotValidError as ex:\n        display = _format_as_name_version(dist)\n        return _EditableInfo(\n            requirement=location,\n            comments=[\n                f\"# Editable {vcs_name} install ({display}) with either a deleted \"\n                f\"local remote or invalid URI:\",\n                f\"# '{ex.url}'\",\n            ],\n        )\n    except BadCommand:\n        logger.warning(\n            \"cannot determine version of editable source in %s \"\n            \"(%s command not found in path)\",\n            location,\n            vcs_backend.name,\n        )\n        return _EditableInfo(requirement=location, comments=[])\n    except InstallationError as exc:\n        logger.warning(\"Error when trying to get requirement for VCS system %s\", exc)\n    else:\n        return _EditableInfo(requirement=req, comments=[])\n\n    logger.warning(\"Could not determine repository location of %s\", location)\n\n    return _EditableInfo(\n        requirement=location,\n        comments=[\"## !! Could not determine repository location\"],\n    )\n\n\n@dataclass(frozen=True)\nclass FrozenRequirement:\n    name: str\n    req: str\n    editable: bool\n    comments: Iterable[str] = field(default_factory=tuple)\n\n    @property\n    def canonical_name(self) -> NormalizedName:\n        return canonicalize_name(self.name)\n\n    @classmethod\n    def from_dist(cls, dist: BaseDistribution) -> \"FrozenRequirement\":\n        editable = dist.editable\n        if editable:\n            req, comments = _get_editable_info(dist)\n        else:\n            comments = []\n            direct_url = dist.direct_url\n            if direct_url:\n                # if PEP 610 metadata is present, use it\n                req = direct_url_as_pep440_direct_reference(direct_url, dist.raw_name)\n            else:\n                # name==version requirement\n                req = _format_as_name_version(dist)\n\n        return cls(dist.raw_name, req, editable, comments=comments)\n\n    def __str__(self) -> str:\n        req = self.req\n        if self.editable:\n            req = f\"-e {req}\"\n        return \"\\n\".join(list(self.comments) + [str(req)]) + \"\\n\"\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/operations/install/__init__.py","size":51,"sha1":"c72c58e6cd7763f27ac8041d54f6390149afc48e","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"For modules related to installing packages.\n\"\"\"\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/operations/install/editable_legacy.py","size":1283,"sha1":"ec9a71056232bf166dd8887676789766ae2e4e17","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"Legacy editable installation process, i.e. `setup.py develop`.\n\"\"\"\n\nimport logging\nfrom typing import Optional, Sequence\n\nfrom pip._internal.build_env import BuildEnvironment\nfrom pip._internal.utils.logging import indent_log\nfrom pip._internal.utils.setuptools_build import make_setuptools_develop_args\nfrom pip._internal.utils.subprocess import call_subprocess\n\nlogger = logging.getLogger(__name__)\n\n\ndef install_editable(\n    *,\n    global_options: Sequence[str],\n    prefix: Optional[str],\n    home: Optional[str],\n    use_user_site: bool,\n    name: str,\n    setup_py_path: str,\n    isolated: bool,\n    build_env: BuildEnvironment,\n    unpacked_source_directory: str,\n) -> None:\n    \"\"\"Install a package in editable mode. Most arguments are pass-through\n    to setuptools.\n    \"\"\"\n    logger.info(\"Running setup.py develop for %s\", name)\n\n    args = make_setuptools_develop_args(\n        setup_py_path,\n        global_options=global_options,\n        no_user_config=isolated,\n        prefix=prefix,\n        home=home,\n        use_user_site=use_user_site,\n    )\n\n    with indent_log():\n        with build_env:\n            call_subprocess(\n                args,\n                command_desc=\"python setup.py develop\",\n                cwd=unpacked_source_directory,\n            )\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/operations/install/wheel.py","size":27615,"sha1":"10a43344cea2cc40fbe56023d8d7dad24d64c436","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"Support for installing and building the \"wheel\" binary package format.\n\"\"\"\n\nimport collections\nimport compileall\nimport contextlib\nimport csv\nimport importlib\nimport logging\nimport os.path\nimport re\nimport shutil\nimport sys\nimport warnings\nfrom base64 import urlsafe_b64encode\nfrom email.message import Message\nfrom itertools import chain, filterfalse, starmap\nfrom typing import (\n    IO,\n    TYPE_CHECKING,\n    Any,\n    BinaryIO,\n    Callable,\n    Dict,\n    Generator,\n    Iterable,\n    Iterator,\n    List,\n    NewType,\n    Optional,\n    Protocol,\n    Sequence,\n    Set,\n    Tuple,\n    Union,\n    cast,\n)\nfrom zipfile import ZipFile, ZipInfo\n\nfrom pip._vendor.distlib.scripts import ScriptMaker\nfrom pip._vendor.distlib.util import get_export_entry\nfrom pip._vendor.packaging.utils import canonicalize_name\n\nfrom pip._internal.exceptions import InstallationError\nfrom pip._internal.locations import get_major_minor_version\nfrom pip._internal.metadata import (\n    BaseDistribution,\n    FilesystemWheel,\n    get_wheel_distribution,\n)\nfrom pip._internal.models.direct_url import DIRECT_URL_METADATA_NAME, DirectUrl\nfrom pip._internal.models.scheme import SCHEME_KEYS, Scheme\nfrom pip._internal.utils.filesystem import adjacent_tmp_file, replace\nfrom pip._internal.utils.misc import StreamWrapper, ensure_dir, hash_file, partition\nfrom pip._internal.utils.unpacking import (\n    current_umask,\n    is_within_directory,\n    set_extracted_file_to_default_mode_plus_executable,\n    zip_item_is_executable,\n)\nfrom pip._internal.utils.wheel import parse_wheel\n\nif TYPE_CHECKING:\n\n    class File(Protocol):\n        src_record_path: \"RecordPath\"\n        dest_path: str\n        changed: bool\n\n        def save(self) -> None:\n            pass\n\n\nlogger = logging.getLogger(__name__)\n\nRecordPath = NewType(\"RecordPath\", str)\nInstalledCSVRow = Tuple[RecordPath, str, Union[int, str]]\n\n\ndef rehash(path: str, blocksize: int = 1 << 20) -> Tuple[str, str]:\n    \"\"\"Return (encoded_digest, length) for path using hashlib.sha256()\"\"\"\n    h, length = hash_file(path, blocksize)\n    digest = \"sha256=\" + urlsafe_b64encode(h.digest()).decode(\"latin1\").rstrip(\"=\")\n    return (digest, str(length))\n\n\ndef csv_io_kwargs(mode: str) -> Dict[str, Any]:\n    \"\"\"Return keyword arguments to properly open a CSV file\n    in the given mode.\n    \"\"\"\n    return {\"mode\": mode, \"newline\": \"\", \"encoding\": \"utf-8\"}\n\n\ndef fix_script(path: str) -> bool:\n    \"\"\"Replace #!python with #!/path/to/python\n    Return True if file was changed.\n    \"\"\"\n    # XXX RECORD hashes will need to be updated\n    assert os.path.isfile(path)\n\n    with open(path, \"rb\") as script:\n        firstline = script.readline()\n        if not firstline.startswith(b\"#!python\"):\n            return False\n        exename = sys.executable.encode(sys.getfilesystemencoding())\n        firstline = b\"#!\" + exename + os.linesep.encode(\"ascii\")\n        rest = script.read()\n    with open(path, \"wb\") as script:\n        script.write(firstline)\n        script.write(rest)\n    return True\n\n\ndef wheel_root_is_purelib(metadata: Message) -> bool:\n    return metadata.get(\"Root-Is-Purelib\", \"\").lower() == \"true\"\n\n\ndef get_entrypoints(dist: BaseDistribution) -> Tuple[Dict[str, str], Dict[str, str]]:\n    console_scripts = {}\n    gui_scripts = {}\n    for entry_point in dist.iter_entry_points():\n        if entry_point.group == \"console_scripts\":\n            console_scripts[entry_point.name] = entry_point.value\n        elif entry_point.group == \"gui_scripts\":\n            gui_scripts[entry_point.name] = entry_point.value\n    return console_scripts, gui_scripts\n\n\ndef message_about_scripts_not_on_PATH(scripts: Sequence[str]) -> Optional[str]:\n    \"\"\"Determine if any scripts are not on PATH and format a warning.\n    Returns a warning message if one or more scripts are not on PATH,\n    otherwise None.\n    \"\"\"\n    if not scripts:\n        return None\n\n    # Group scripts by the path they were installed in\n    grouped_by_dir: Dict[str, Set[str]] = collections.defaultdict(set)\n    for destfile in scripts:\n        parent_dir = os.path.dirname(destfile)\n        script_name = os.path.basename(destfile)\n        grouped_by_dir[parent_dir].add(script_name)\n\n    # We don't want to warn for directories that are on PATH.\n    not_warn_dirs = [\n        os.path.normcase(os.path.normpath(i)).rstrip(os.sep)\n        for i in os.environ.get(\"PATH\", \"\").split(os.pathsep)\n    ]\n    # If an executable sits with sys.executable, we don't warn for it.\n    #     This covers the case of venv invocations without activating the venv.\n    not_warn_dirs.append(\n        os.path.normcase(os.path.normpath(os.path.dirname(sys.executable)))\n    )\n    warn_for: Dict[str, Set[str]] = {\n        parent_dir: scripts\n        for parent_dir, scripts in grouped_by_dir.items()\n        if os.path.normcase(os.path.normpath(parent_dir)) not in not_warn_dirs\n    }\n    if not warn_for:\n        return None\n\n    # Format a message\n    msg_lines = []\n    for parent_dir, dir_scripts in warn_for.items():\n        sorted_scripts: List[str] = sorted(dir_scripts)\n        if len(sorted_scripts) == 1:\n            start_text = f\"script {sorted_scripts[0]} is\"\n        else:\n            start_text = \"scripts {} are\".format(\n                \", \".join(sorted_scripts[:-1]) + \" and \" + sorted_scripts[-1]\n            )\n\n        msg_lines.append(\n            f\"The {start_text} installed in '{parent_dir}' which is not on PATH.\"\n        )\n\n    last_line_fmt = (\n        \"Consider adding {} to PATH or, if you prefer \"\n        \"to suppress this warning, use --no-warn-script-location.\"\n    )\n    if len(msg_lines) == 1:\n        msg_lines.append(last_line_fmt.format(\"this directory\"))\n    else:\n        msg_lines.append(last_line_fmt.format(\"these directories\"))\n\n    # Add a note if any directory starts with ~\n    warn_for_tilde = any(\n        i[0] == \"~\" for i in os.environ.get(\"PATH\", \"\").split(os.pathsep) if i\n    )\n    if warn_for_tilde:\n        tilde_warning_msg = (\n            \"NOTE: The current PATH contains path(s) starting with `~`, \"\n            \"which may not be expanded by all applications.\"\n        )\n        msg_lines.append(tilde_warning_msg)\n\n    # Returns the formatted multiline message\n    return \"\\n\".join(msg_lines)\n\n\ndef _normalized_outrows(\n    outrows: Iterable[InstalledCSVRow],\n) -> List[Tuple[str, str, str]]:\n    \"\"\"Normalize the given rows of a RECORD file.\n\n    Items in each row are converted into str. Rows are then sorted to make\n    the value more predictable for tests.\n\n    Each row is a 3-tuple (path, hash, size) and corresponds to a record of\n    a RECORD file (see PEP 376 and PEP 427 for details).  For the rows\n    passed to this function, the size can be an integer as an int or string,\n    or the empty string.\n    \"\"\"\n    # Normally, there should only be one row per path, in which case the\n    # second and third elements don't come into play when sorting.\n    # However, in cases in the wild where a path might happen to occur twice,\n    # we don't want the sort operation to trigger an error (but still want\n    # determinism).  Since the third element can be an int or string, we\n    # coerce each element to a string to avoid a TypeError in this case.\n    # For additional background, see--\n    # https://github.com/pypa/pip/issues/5868\n    return sorted(\n        (record_path, hash_, str(size)) for record_path, hash_, size in outrows\n    )\n\n\ndef _record_to_fs_path(record_path: RecordPath, lib_dir: str) -> str:\n    return os.path.join(lib_dir, record_path)\n\n\ndef _fs_to_record_path(path: str, lib_dir: str) -> RecordPath:\n    # On Windows, do not handle relative paths if they belong to different\n    # logical disks\n    if os.path.splitdrive(path)[0].lower() == os.path.splitdrive(lib_dir)[0].lower():\n        path = os.path.relpath(path, lib_dir)\n\n    path = path.replace(os.path.sep, \"/\")\n    return cast(\"RecordPath\", path)\n\n\ndef get_csv_rows_for_installed(\n    old_csv_rows: List[List[str]],\n    installed: Dict[RecordPath, RecordPath],\n    changed: Set[RecordPath],\n    generated: List[str],\n    lib_dir: str,\n) -> List[InstalledCSVRow]:\n    \"\"\"\n    :param installed: A map from archive RECORD path to installation RECORD\n        path.\n    \"\"\"\n    installed_rows: List[InstalledCSVRow] = []\n    for row in old_csv_rows:\n        if len(row) > 3:\n            logger.warning(\"RECORD line has more than three elements: %s\", row)\n        old_record_path = cast(\"RecordPath\", row[0])\n        new_record_path = installed.pop(old_record_path, old_record_path)\n        if new_record_path in changed:\n            digest, length = rehash(_record_to_fs_path(new_record_path, lib_dir))\n        else:\n            digest = row[1] if len(row) > 1 else \"\"\n            length = row[2] if len(row) > 2 else \"\"\n        installed_rows.append((new_record_path, digest, length))\n    for f in generated:\n        path = _fs_to_record_path(f, lib_dir)\n        digest, length = rehash(f)\n        installed_rows.append((path, digest, length))\n    return installed_rows + [\n        (installed_record_path, \"\", \"\") for installed_record_path in installed.values()\n    ]\n\n\ndef get_console_script_specs(console: Dict[str, str]) -> List[str]:\n    \"\"\"\n    Given the mapping from entrypoint name to callable, return the relevant\n    console script specs.\n    \"\"\"\n    # Don't mutate caller's version\n    console = console.copy()\n\n    scripts_to_generate = []\n\n    # Special case pip and setuptools to generate versioned wrappers\n    #\n    # The issue is that some projects (specifically, pip and setuptools) use\n    # code in setup.py to create \"versioned\" entry points - pip2.7 on Python\n    # 2.7, pip3.3 on Python 3.3, etc. But these entry points are baked into\n    # the wheel metadata at build time, and so if the wheel is installed with\n    # a *different* version of Python the entry points will be wrong. The\n    # correct fix for this is to enhance the metadata to be able to describe\n    # such versioned entry points.\n    # Currently, projects using versioned entry points will either have\n    # incorrect versioned entry points, or they will not be able to distribute\n    # \"universal\" wheels (i.e., they will need a wheel per Python version).\n    #\n    # Because setuptools and pip are bundled with _ensurepip and virtualenv,\n    # we need to use universal wheels. As a workaround, we\n    # override the versioned entry points in the wheel and generate the\n    # correct ones.\n    #\n    # To add the level of hack in this section of code, in order to support\n    # ensurepip this code will look for an ``ENSUREPIP_OPTIONS`` environment\n    # variable which will control which version scripts get installed.\n    #\n    # ENSUREPIP_OPTIONS=altinstall\n    #   - Only pipX.Y and easy_install-X.Y will be generated and installed\n    # ENSUREPIP_OPTIONS=install\n    #   - pipX.Y, pipX, easy_install-X.Y will be generated and installed. Note\n    #     that this option is technically if ENSUREPIP_OPTIONS is set and is\n    #     not altinstall\n    # DEFAULT\n    #   - The default behavior is to install pip, pipX, pipX.Y, easy_install\n    #     and easy_install-X.Y.\n    pip_script = console.pop(\"pip\", None)\n    if pip_script:\n        if \"ENSUREPIP_OPTIONS\" not in os.environ:\n            scripts_to_generate.append(\"pip = \" + pip_script)\n\n        if os.environ.get(\"ENSUREPIP_OPTIONS\", \"\") != \"altinstall\":\n            scripts_to_generate.append(f\"pip{sys.version_info[0]} = {pip_script}\")\n\n        scripts_to_generate.append(f\"pip{get_major_minor_version()} = {pip_script}\")\n        # Delete any other versioned pip entry points\n        pip_ep = [k for k in console if re.match(r\"pip(\\d+(\\.\\d+)?)?$\", k)]\n        for k in pip_ep:\n            del console[k]\n    easy_install_script = console.pop(\"easy_install\", None)\n    if easy_install_script:\n        if \"ENSUREPIP_OPTIONS\" not in os.environ:\n            scripts_to_generate.append(\"easy_install = \" + easy_install_script)\n\n        scripts_to_generate.append(\n            f\"easy_install-{get_major_minor_version()} = {easy_install_script}\"\n        )\n        # Delete any other versioned easy_install entry points\n        easy_install_ep = [\n            k for k in console if re.match(r\"easy_install(-\\d+\\.\\d+)?$\", k)\n        ]\n        for k in easy_install_ep:\n            del console[k]\n\n    # Generate the console entry points specified in the wheel\n    scripts_to_generate.extend(starmap(\"{} = {}\".format, console.items()))\n\n    return scripts_to_generate\n\n\nclass ZipBackedFile:\n    def __init__(\n        self, src_record_path: RecordPath, dest_path: str, zip_file: ZipFile\n    ) -> None:\n        self.src_record_path = src_record_path\n        self.dest_path = dest_path\n        self._zip_file = zip_file\n        self.changed = False\n\n    def _getinfo(self) -> ZipInfo:\n        return self._zip_file.getinfo(self.src_record_path)\n\n    def save(self) -> None:\n        # When we open the output file below, any existing file is truncated\n        # before we start writing the new contents. This is fine in most\n        # cases, but can cause a segfault if pip has loaded a shared\n        # object (e.g. from pyopenssl through its vendored urllib3)\n        # Since the shared object is mmap'd an attempt to call a\n        # symbol in it will then cause a segfault. Unlinking the file\n        # allows writing of new contents while allowing the process to\n        # continue to use the old copy.\n        if os.path.exists(self.dest_path):\n            os.unlink(self.dest_path)\n\n        zipinfo = self._getinfo()\n\n        # optimization: the file is created by open(),\n        # skip the decompression when there is 0 bytes to decompress.\n        with open(self.dest_path, \"wb\") as dest:\n            if zipinfo.file_size > 0:\n                with self._zip_file.open(zipinfo) as f:\n                    blocksize = min(zipinfo.file_size, 1024 * 1024)\n                    shutil.copyfileobj(f, dest, blocksize)\n\n        if zip_item_is_executable(zipinfo):\n            set_extracted_file_to_default_mode_plus_executable(self.dest_path)\n\n\nclass ScriptFile:\n    def __init__(self, file: \"File\") -> None:\n        self._file = file\n        self.src_record_path = self._file.src_record_path\n        self.dest_path = self._file.dest_path\n        self.changed = False\n\n    def save(self) -> None:\n        self._file.save()\n        self.changed = fix_script(self.dest_path)\n\n\nclass MissingCallableSuffix(InstallationError):\n    def __init__(self, entry_point: str) -> None:\n        super().__init__(\n            f\"Invalid script entry point: {entry_point} - A callable \"\n            \"suffix is required. Cf https://packaging.python.org/\"\n            \"specifications/entry-points/#use-for-scripts for more \"\n            \"information.\"\n        )\n\n\ndef _raise_for_invalid_entrypoint(specification: str) -> None:\n    entry = get_export_entry(specification)\n    if entry is not None and entry.suffix is None:\n        raise MissingCallableSuffix(str(entry))\n\n\nclass PipScriptMaker(ScriptMaker):\n    def make(\n        self, specification: str, options: Optional[Dict[str, Any]] = None\n    ) -> List[str]:\n        _raise_for_invalid_entrypoint(specification)\n        return super().make(specification, options)\n\n\ndef _install_wheel(  # noqa: C901, PLR0915 function is too long\n    name: str,\n    wheel_zip: ZipFile,\n    wheel_path: str,\n    scheme: Scheme,\n    pycompile: bool = True,\n    warn_script_location: bool = True,\n    direct_url: Optional[DirectUrl] = None,\n    requested: bool = False,\n) -> None:\n    \"\"\"Install a wheel.\n\n    :param name: Name of the project to install\n    :param wheel_zip: open ZipFile for wheel being installed\n    :param scheme: Distutils scheme dictating the install directories\n    :param req_description: String used in place of the requirement, for\n        logging\n    :param pycompile: Whether to byte-compile installed Python files\n    :param warn_script_location: Whether to check that scripts are installed\n        into a directory on PATH\n    :raises UnsupportedWheel:\n        * when the directory holds an unpacked wheel with incompatible\n          Wheel-Version\n        * when the .dist-info dir does not match the wheel\n    \"\"\"\n    info_dir, metadata = parse_wheel(wheel_zip, name)\n\n    if wheel_root_is_purelib(metadata):\n        lib_dir = scheme.purelib\n    else:\n        lib_dir = scheme.platlib\n\n    # Record details of the files moved\n    #   installed = files copied from the wheel to the destination\n    #   changed = files changed while installing (scripts #! line typically)\n    #   generated = files newly generated during the install (script wrappers)\n    installed: Dict[RecordPath, RecordPath] = {}\n    changed: Set[RecordPath] = set()\n    generated: List[str] = []\n\n    def record_installed(\n        srcfile: RecordPath, destfile: str, modified: bool = False\n    ) -> None:\n        \"\"\"Map archive RECORD paths to installation RECORD paths.\"\"\"\n        newpath = _fs_to_record_path(destfile, lib_dir)\n        installed[srcfile] = newpath\n        if modified:\n            changed.add(newpath)\n\n    def is_dir_path(path: RecordPath) -> bool:\n        return path.endswith(\"/\")\n\n    def assert_no_path_traversal(dest_dir_path: str, target_path: str) -> None:\n        if not is_within_directory(dest_dir_path, target_path):\n            message = (\n                \"The wheel {!r} has a file {!r} trying to install\"\n                \" outside the target directory {!r}\"\n            )\n            raise InstallationError(\n                message.format(wheel_path, target_path, dest_dir_path)\n            )\n\n    def root_scheme_file_maker(\n        zip_file: ZipFile, dest: str\n    ) -> Callable[[RecordPath], \"File\"]:\n        def make_root_scheme_file(record_path: RecordPath) -> \"File\":\n            normed_path = os.path.normpath(record_path)\n            dest_path = os.path.join(dest, normed_path)\n            assert_no_path_traversal(dest, dest_path)\n            return ZipBackedFile(record_path, dest_path, zip_file)\n\n        return make_root_scheme_file\n\n    def data_scheme_file_maker(\n        zip_file: ZipFile, scheme: Scheme\n    ) -> Callable[[RecordPath], \"File\"]:\n        scheme_paths = {key: getattr(scheme, key) for key in SCHEME_KEYS}\n\n        def make_data_scheme_file(record_path: RecordPath) -> \"File\":\n            normed_path = os.path.normpath(record_path)\n            try:\n                _, scheme_key, dest_subpath = normed_path.split(os.path.sep, 2)\n            except ValueError:\n                message = (\n                    f\"Unexpected file in {wheel_path}: {record_path!r}. .data directory\"\n                    \" contents should be named like: '<scheme key>/<path>'.\"\n                )\n                raise InstallationError(message)\n\n            try:\n                scheme_path = scheme_paths[scheme_key]\n            except KeyError:\n                valid_scheme_keys = \", \".join(sorted(scheme_paths))\n                message = (\n                    f\"Unknown scheme key used in {wheel_path}: {scheme_key} \"\n                    f\"(for file {record_path!r}). .data directory contents \"\n                    f\"should be in subdirectories named with a valid scheme \"\n                    f\"key ({valid_scheme_keys})\"\n                )\n                raise InstallationError(message)\n\n            dest_path = os.path.join(scheme_path, dest_subpath)\n            assert_no_path_traversal(scheme_path, dest_path)\n            return ZipBackedFile(record_path, dest_path, zip_file)\n\n        return make_data_scheme_file\n\n    def is_data_scheme_path(path: RecordPath) -> bool:\n        return path.split(\"/\", 1)[0].endswith(\".data\")\n\n    paths = cast(List[RecordPath], wheel_zip.namelist())\n    file_paths = filterfalse(is_dir_path, paths)\n    root_scheme_paths, data_scheme_paths = partition(is_data_scheme_path, file_paths)\n\n    make_root_scheme_file = root_scheme_file_maker(wheel_zip, lib_dir)\n    files: Iterator[File] = map(make_root_scheme_file, root_scheme_paths)\n\n    def is_script_scheme_path(path: RecordPath) -> bool:\n        parts = path.split(\"/\", 2)\n        return len(parts) > 2 and parts[0].endswith(\".data\") and parts[1] == \"scripts\"\n\n    other_scheme_paths, script_scheme_paths = partition(\n        is_script_scheme_path, data_scheme_paths\n    )\n\n    make_data_scheme_file = data_scheme_file_maker(wheel_zip, scheme)\n    other_scheme_files = map(make_data_scheme_file, other_scheme_paths)\n    files = chain(files, other_scheme_files)\n\n    # Get the defined entry points\n    distribution = get_wheel_distribution(\n        FilesystemWheel(wheel_path),\n        canonicalize_name(name),\n    )\n    console, gui = get_entrypoints(distribution)\n\n    def is_entrypoint_wrapper(file: \"File\") -> bool:\n        # EP, EP.exe and EP-script.py are scripts generated for\n        # entry point EP by setuptools\n        path = file.dest_path\n        name = os.path.basename(path)\n        if name.lower().endswith(\".exe\"):\n            matchname = name[:-4]\n        elif name.lower().endswith(\"-script.py\"):\n            matchname = name[:-10]\n        elif name.lower().endswith(\".pya\"):\n            matchname = name[:-4]\n        else:\n            matchname = name\n        # Ignore setuptools-generated scripts\n        return matchname in console or matchname in gui\n\n    script_scheme_files: Iterator[File] = map(\n        make_data_scheme_file, script_scheme_paths\n    )\n    script_scheme_files = filterfalse(is_entrypoint_wrapper, script_scheme_files)\n    script_scheme_files = map(ScriptFile, script_scheme_files)\n    files = chain(files, script_scheme_files)\n\n    existing_parents = set()\n    for file in files:\n        # directory creation is lazy and after file filtering\n        # to ensure we don't install empty dirs; empty dirs can't be\n        # uninstalled.\n        parent_dir = os.path.dirname(file.dest_path)\n        if parent_dir not in existing_parents:\n            ensure_dir(parent_dir)\n            existing_parents.add(parent_dir)\n        file.save()\n        record_installed(file.src_record_path, file.dest_path, file.changed)\n\n    def pyc_source_file_paths() -> Generator[str, None, None]:\n        # We de-duplicate installation paths, since there can be overlap (e.g.\n        # file in .data maps to same location as file in wheel root).\n        # Sorting installation paths makes it easier to reproduce and debug\n        # issues related to permissions on existing files.\n        for installed_path in sorted(set(installed.values())):\n            full_installed_path = os.path.join(lib_dir, installed_path)\n            if not os.path.isfile(full_installed_path):\n                continue\n            if not full_installed_path.endswith(\".py\"):\n                continue\n            yield full_installed_path\n\n    def pyc_output_path(path: str) -> str:\n        \"\"\"Return the path the pyc file would have been written to.\"\"\"\n        return importlib.util.cache_from_source(path)\n\n    # Compile all of the pyc files for the installed files\n    if pycompile:\n        with contextlib.redirect_stdout(\n            StreamWrapper.from_stream(sys.stdout)\n        ) as stdout:\n            with warnings.catch_warnings():\n                warnings.filterwarnings(\"ignore\")\n                for path in pyc_source_file_paths():\n                    success = compileall.compile_file(path, force=True, quiet=True)\n                    if success:\n                        pyc_path = pyc_output_path(path)\n                        assert os.path.exists(pyc_path)\n                        pyc_record_path = cast(\n                            \"RecordPath\", pyc_path.replace(os.path.sep, \"/\")\n                        )\n                        record_installed(pyc_record_path, pyc_path)\n        logger.debug(stdout.getvalue())\n\n    maker = PipScriptMaker(None, scheme.scripts)\n\n    # Ensure old scripts are overwritten.\n    # See https://github.com/pypa/pip/issues/1800\n    maker.clobber = True\n\n    # Ensure we don't generate any variants for scripts because this is almost\n    # never what somebody wants.\n    # See https://bitbucket.org/pypa/distlib/issue/35/\n    maker.variants = {\"\"}\n\n    # This is required because otherwise distlib creates scripts that are not\n    # executable.\n    # See https://bitbucket.org/pypa/distlib/issue/32/\n    maker.set_mode = True\n\n    # Generate the console and GUI entry points specified in the wheel\n    scripts_to_generate = get_console_script_specs(console)\n\n    gui_scripts_to_generate = list(starmap(\"{} = {}\".format, gui.items()))\n\n    generated_console_scripts = maker.make_multiple(scripts_to_generate)\n    generated.extend(generated_console_scripts)\n\n    generated.extend(maker.make_multiple(gui_scripts_to_generate, {\"gui\": True}))\n\n    if warn_script_location:\n        msg = message_about_scripts_not_on_PATH(generated_console_scripts)\n        if msg is not None:\n            logger.warning(msg)\n\n    generated_file_mode = 0o666 & ~current_umask()\n\n    @contextlib.contextmanager\n    def _generate_file(path: str, **kwargs: Any) -> Generator[BinaryIO, None, None]:\n        with adjacent_tmp_file(path, **kwargs) as f:\n            yield f\n        os.chmod(f.name, generated_file_mode)\n        replace(f.name, path)\n\n    dest_info_dir = os.path.join(lib_dir, info_dir)\n\n    # Record pip as the installer\n    installer_path = os.path.join(dest_info_dir, \"INSTALLER\")\n    with _generate_file(installer_path) as installer_file:\n        installer_file.write(b\"pip\\n\")\n    generated.append(installer_path)\n\n    # Record the PEP 610 direct URL reference\n    if direct_url is not None:\n        direct_url_path = os.path.join(dest_info_dir, DIRECT_URL_METADATA_NAME)\n        with _generate_file(direct_url_path) as direct_url_file:\n            direct_url_file.write(direct_url.to_json().encode(\"utf-8\"))\n        generated.append(direct_url_path)\n\n    # Record the REQUESTED file\n    if requested:\n        requested_path = os.path.join(dest_info_dir, \"REQUESTED\")\n        with open(requested_path, \"wb\"):\n            pass\n        generated.append(requested_path)\n\n    record_text = distribution.read_text(\"RECORD\")\n    record_rows = list(csv.reader(record_text.splitlines()))\n\n    rows = get_csv_rows_for_installed(\n        record_rows,\n        installed=installed,\n        changed=changed,\n        generated=generated,\n        lib_dir=lib_dir,\n    )\n\n    # Record details of all files installed\n    record_path = os.path.join(dest_info_dir, \"RECORD\")\n\n    with _generate_file(record_path, **csv_io_kwargs(\"w\")) as record_file:\n        # Explicitly cast to typing.IO[str] as a workaround for the mypy error:\n        # \"writer\" has incompatible type \"BinaryIO\"; expected \"_Writer\"\n        writer = csv.writer(cast(\"IO[str]\", record_file))\n        writer.writerows(_normalized_outrows(rows))\n\n\n@contextlib.contextmanager\ndef req_error_context(req_description: str) -> Generator[None, None, None]:\n    try:\n        yield\n    except InstallationError as e:\n        message = f\"For req: {req_description}. {e.args[0]}\"\n        raise InstallationError(message) from e\n\n\ndef install_wheel(\n    name: str,\n    wheel_path: str,\n    scheme: Scheme,\n    req_description: str,\n    pycompile: bool = True,\n    warn_script_location: bool = True,\n    direct_url: Optional[DirectUrl] = None,\n    requested: bool = False,\n) -> None:\n    with ZipFile(wheel_path, allowZip64=True) as z:\n        with req_error_context(req_description):\n            _install_wheel(\n                name=name,\n                wheel_zip=z,\n                wheel_path=wheel_path,\n                scheme=scheme,\n                pycompile=pycompile,\n                warn_script_location=warn_script_location,\n                direct_url=direct_url,\n                requested=requested,\n            )\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/operations/prepare.py","size":28118,"sha1":"c47189dc3bb27d53f27f4304c84aee03a997fdcd","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"Prepares a distribution for installation\n\"\"\"\n\n# The following comment should be removed at some point in the future.\n# mypy: strict-optional=False\n\nimport mimetypes\nimport os\nimport shutil\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Dict, Iterable, List, Optional\n\nfrom pip._vendor.packaging.utils import canonicalize_name\n\nfrom pip._internal.distributions import make_distribution_for_install_requirement\nfrom pip._internal.distributions.installed import InstalledDistribution\nfrom pip._internal.exceptions import (\n    DirectoryUrlHashUnsupported,\n    HashMismatch,\n    HashUnpinned,\n    InstallationError,\n    MetadataInconsistent,\n    NetworkConnectionError,\n    VcsHashUnsupported,\n)\nfrom pip._internal.index.package_finder import PackageFinder\nfrom pip._internal.metadata import BaseDistribution, get_metadata_distribution\nfrom pip._internal.models.direct_url import ArchiveInfo\nfrom pip._internal.models.link import Link\nfrom pip._internal.models.wheel import Wheel\nfrom pip._internal.network.download import BatchDownloader, Downloader\nfrom pip._internal.network.lazy_wheel import (\n    HTTPRangeRequestUnsupported,\n    dist_from_wheel_url,\n)\nfrom pip._internal.network.session import PipSession\nfrom pip._internal.operations.build.build_tracker import BuildTracker\nfrom pip._internal.req.req_install import InstallRequirement\nfrom pip._internal.utils._log import getLogger\nfrom pip._internal.utils.direct_url_helpers import (\n    direct_url_for_editable,\n    direct_url_from_link,\n)\nfrom pip._internal.utils.hashes import Hashes, MissingHashes\nfrom pip._internal.utils.logging import indent_log\nfrom pip._internal.utils.misc import (\n    display_path,\n    hash_file,\n    hide_url,\n    redact_auth_from_requirement,\n)\nfrom pip._internal.utils.temp_dir import TempDirectory\nfrom pip._internal.utils.unpacking import unpack_file\nfrom pip._internal.vcs import vcs\n\nlogger = getLogger(__name__)\n\n\ndef _get_prepared_distribution(\n    req: InstallRequirement,\n    build_tracker: BuildTracker,\n    finder: PackageFinder,\n    build_isolation: bool,\n    check_build_deps: bool,\n) -> BaseDistribution:\n    \"\"\"Prepare a distribution for installation.\"\"\"\n    abstract_dist = make_distribution_for_install_requirement(req)\n    tracker_id = abstract_dist.build_tracker_id\n    if tracker_id is not None:\n        with build_tracker.track(req, tracker_id):\n            abstract_dist.prepare_distribution_metadata(\n                finder, build_isolation, check_build_deps\n            )\n    return abstract_dist.get_metadata_distribution()\n\n\ndef unpack_vcs_link(link: Link, location: str, verbosity: int) -> None:\n    vcs_backend = vcs.get_backend_for_scheme(link.scheme)\n    assert vcs_backend is not None\n    vcs_backend.unpack(location, url=hide_url(link.url), verbosity=verbosity)\n\n\n@dataclass\nclass File:\n    path: str\n    content_type: Optional[str] = None\n\n    def __post_init__(self) -> None:\n        if self.content_type is None:\n            self.content_type = mimetypes.guess_type(self.path)[0]\n\n\ndef get_http_url(\n    link: Link,\n    download: Downloader,\n    download_dir: Optional[str] = None,\n    hashes: Optional[Hashes] = None,\n) -> File:\n    temp_dir = TempDirectory(kind=\"unpack\", globally_managed=True)\n    # If a download dir is specified, is the file already downloaded there?\n    already_downloaded_path = None\n    if download_dir:\n        already_downloaded_path = _check_download_dir(link, download_dir, hashes)\n\n    if already_downloaded_path:\n        from_path = already_downloaded_path\n        content_type = None\n    else:\n        # let's download to a tmp dir\n        from_path, content_type = download(link, temp_dir.path)\n        if hashes:\n            hashes.check_against_path(from_path)\n\n    return File(from_path, content_type)\n\n\ndef get_file_url(\n    link: Link, download_dir: Optional[str] = None, hashes: Optional[Hashes] = None\n) -> File:\n    \"\"\"Get file and optionally check its hash.\"\"\"\n    # If a download dir is specified, is the file already there and valid?\n    already_downloaded_path = None\n    if download_dir:\n        already_downloaded_path = _check_download_dir(link, download_dir, hashes)\n\n    if already_downloaded_path:\n        from_path = already_downloaded_path\n    else:\n        from_path = link.file_path\n\n    # If --require-hashes is off, `hashes` is either empty, the\n    # link's embedded hash, or MissingHashes; it is required to\n    # match. If --require-hashes is on, we are satisfied by any\n    # hash in `hashes` matching: a URL-based or an option-based\n    # one; no internet-sourced hash will be in `hashes`.\n    if hashes:\n        hashes.check_against_path(from_path)\n    return File(from_path, None)\n\n\ndef unpack_url(\n    link: Link,\n    location: str,\n    download: Downloader,\n    verbosity: int,\n    download_dir: Optional[str] = None,\n    hashes: Optional[Hashes] = None,\n) -> Optional[File]:\n    \"\"\"Unpack link into location, downloading if required.\n\n    :param hashes: A Hashes object, one of whose embedded hashes must match,\n        or HashMismatch will be raised. If the Hashes is empty, no matches are\n        required, and unhashable types of requirements (like VCS ones, which\n        would ordinarily raise HashUnsupported) are allowed.\n    \"\"\"\n    # non-editable vcs urls\n    if link.is_vcs:\n        unpack_vcs_link(link, location, verbosity=verbosity)\n        return None\n\n    assert not link.is_existing_dir()\n\n    # file urls\n    if link.is_file:\n        file = get_file_url(link, download_dir, hashes=hashes)\n\n    # http urls\n    else:\n        file = get_http_url(\n            link,\n            download,\n            download_dir,\n            hashes=hashes,\n        )\n\n    # unpack the archive to the build dir location. even when only downloading\n    # archives, they have to be unpacked to parse dependencies, except wheels\n    if not link.is_wheel:\n        unpack_file(file.path, location, file.content_type)\n\n    return file\n\n\ndef _check_download_dir(\n    link: Link,\n    download_dir: str,\n    hashes: Optional[Hashes],\n    warn_on_hash_mismatch: bool = True,\n) -> Optional[str]:\n    \"\"\"Check download_dir for previously downloaded file with correct hash\n    If a correct file is found return its path else None\n    \"\"\"\n    download_path = os.path.join(download_dir, link.filename)\n\n    if not os.path.exists(download_path):\n        return None\n\n    # If already downloaded, does its hash match?\n    logger.info(\"File was already downloaded %s\", download_path)\n    if hashes:\n        try:\n            hashes.check_against_path(download_path)\n        except HashMismatch:\n            if warn_on_hash_mismatch:\n                logger.warning(\n                    \"Previously-downloaded file %s has bad hash. Re-downloading.\",\n                    download_path,\n                )\n            os.unlink(download_path)\n            return None\n    return download_path\n\n\nclass RequirementPreparer:\n    \"\"\"Prepares a Requirement\"\"\"\n\n    def __init__(\n        self,\n        build_dir: str,\n        download_dir: Optional[str],\n        src_dir: str,\n        build_isolation: bool,\n        check_build_deps: bool,\n        build_tracker: BuildTracker,\n        session: PipSession,\n        progress_bar: str,\n        finder: PackageFinder,\n        require_hashes: bool,\n        use_user_site: bool,\n        lazy_wheel: bool,\n        verbosity: int,\n        legacy_resolver: bool,\n    ) -> None:\n        super().__init__()\n\n        self.src_dir = src_dir\n        self.build_dir = build_dir\n        self.build_tracker = build_tracker\n        self._session = session\n        self._download = Downloader(session, progress_bar)\n        self._batch_download = BatchDownloader(session, progress_bar)\n        self.finder = finder\n\n        # Where still-packed archives should be written to. If None, they are\n        # not saved, and are deleted immediately after unpacking.\n        self.download_dir = download_dir\n\n        # Is build isolation allowed?\n        self.build_isolation = build_isolation\n\n        # Should check build dependencies?\n        self.check_build_deps = check_build_deps\n\n        # Should hash-checking be required?\n        self.require_hashes = require_hashes\n\n        # Should install in user site-packages?\n        self.use_user_site = use_user_site\n\n        # Should wheels be downloaded lazily?\n        self.use_lazy_wheel = lazy_wheel\n\n        # How verbose should underlying tooling be?\n        self.verbosity = verbosity\n\n        # Are we using the legacy resolver?\n        self.legacy_resolver = legacy_resolver\n\n        # Memoized downloaded files, as mapping of url: path.\n        self._downloaded: Dict[str, str] = {}\n\n        # Previous \"header\" printed for a link-based InstallRequirement\n        self._previous_requirement_header = (\"\", \"\")\n\n    def _log_preparing_link(self, req: InstallRequirement) -> None:\n        \"\"\"Provide context for the requirement being prepared.\"\"\"\n        if req.link.is_file and not req.is_wheel_from_cache:\n            message = \"Processing %s\"\n            information = str(display_path(req.link.file_path))\n        else:\n            message = \"Collecting %s\"\n            information = redact_auth_from_requirement(req.req) if req.req else str(req)\n\n        # If we used req.req, inject requirement source if available (this\n        # would already be included if we used req directly)\n        if req.req and req.comes_from:\n            if isinstance(req.comes_from, str):\n                comes_from: Optional[str] = req.comes_from\n            else:\n                comes_from = req.comes_from.from_path()\n            if comes_from:\n                information += f\" (from {comes_from})\"\n\n        if (message, information) != self._previous_requirement_header:\n            self._previous_requirement_header = (message, information)\n            logger.info(message, information)\n\n        if req.is_wheel_from_cache:\n            with indent_log():\n                logger.info(\"Using cached %s\", req.link.filename)\n\n    def _ensure_link_req_src_dir(\n        self, req: InstallRequirement, parallel_builds: bool\n    ) -> None:\n        \"\"\"Ensure source_dir of a linked InstallRequirement.\"\"\"\n        # Since source_dir is only set for editable requirements.\n        if req.link.is_wheel:\n            # We don't need to unpack wheels, so no need for a source\n            # directory.\n            return\n        assert req.source_dir is None\n        if req.link.is_existing_dir():\n            # build local directories in-tree\n            req.source_dir = req.link.file_path\n            return\n\n        # We always delete unpacked sdists after pip runs.\n        req.ensure_has_source_dir(\n            self.build_dir,\n            autodelete=True,\n            parallel_builds=parallel_builds,\n        )\n        req.ensure_pristine_source_checkout()\n\n    def _get_linked_req_hashes(self, req: InstallRequirement) -> Hashes:\n        # By the time this is called, the requirement's link should have\n        # been checked so we can tell what kind of requirements req is\n        # and raise some more informative errors than otherwise.\n        # (For example, we can raise VcsHashUnsupported for a VCS URL\n        # rather than HashMissing.)\n        if not self.require_hashes:\n            return req.hashes(trust_internet=True)\n\n        # We could check these first 2 conditions inside unpack_url\n        # and save repetition of conditions, but then we would\n        # report less-useful error messages for unhashable\n        # requirements, complaining that there's no hash provided.\n        if req.link.is_vcs:\n            raise VcsHashUnsupported()\n        if req.link.is_existing_dir():\n            raise DirectoryUrlHashUnsupported()\n\n        # Unpinned packages are asking for trouble when a new version\n        # is uploaded.  This isn't a security check, but it saves users\n        # a surprising hash mismatch in the future.\n        # file:/// URLs aren't pinnable, so don't complain about them\n        # not being pinned.\n        if not req.is_direct and not req.is_pinned:\n            raise HashUnpinned()\n\n        # If known-good hashes are missing for this requirement,\n        # shim it with a facade object that will provoke hash\n        # computation and then raise a HashMissing exception\n        # showing the user what the hash should be.\n        return req.hashes(trust_internet=False) or MissingHashes()\n\n    def _fetch_metadata_only(\n        self,\n        req: InstallRequirement,\n    ) -> Optional[BaseDistribution]:\n        if self.legacy_resolver:\n            logger.debug(\n                \"Metadata-only fetching is not used in the legacy resolver\",\n            )\n            return None\n        if self.require_hashes:\n            logger.debug(\n                \"Metadata-only fetching is not used as hash checking is required\",\n            )\n            return None\n        # Try PEP 658 metadata first, then fall back to lazy wheel if unavailable.\n        return self._fetch_metadata_using_link_data_attr(\n            req\n        ) or self._fetch_metadata_using_lazy_wheel(req.link)\n\n    def _fetch_metadata_using_link_data_attr(\n        self,\n        req: InstallRequirement,\n    ) -> Optional[BaseDistribution]:\n        \"\"\"Fetch metadata from the data-dist-info-metadata attribute, if possible.\"\"\"\n        # (1) Get the link to the metadata file, if provided by the backend.\n        metadata_link = req.link.metadata_link()\n        if metadata_link is None:\n            return None\n        assert req.req is not None\n        logger.verbose(\n            \"Obtaining dependency information for %s from %s\",\n            req.req,\n            metadata_link,\n        )\n        # (2) Download the contents of the METADATA file, separate from the dist itself.\n        metadata_file = get_http_url(\n            metadata_link,\n            self._download,\n            hashes=metadata_link.as_hashes(),\n        )\n        with open(metadata_file.path, \"rb\") as f:\n            metadata_contents = f.read()\n        # (3) Generate a dist just from those file contents.\n        metadata_dist = get_metadata_distribution(\n            metadata_contents,\n            req.link.filename,\n            req.req.name,\n        )\n        # (4) Ensure the Name: field from the METADATA file matches the name from the\n        #     install requirement.\n        #\n        #     NB: raw_name will fall back to the name from the install requirement if\n        #     the Name: field is not present, but it's noted in the raw_name docstring\n        #     that that should NEVER happen anyway.\n        if canonicalize_name(metadata_dist.raw_name) != canonicalize_name(req.req.name):\n            raise MetadataInconsistent(\n                req, \"Name\", req.req.name, metadata_dist.raw_name\n            )\n        return metadata_dist\n\n    def _fetch_metadata_using_lazy_wheel(\n        self,\n        link: Link,\n    ) -> Optional[BaseDistribution]:\n        \"\"\"Fetch metadata using lazy wheel, if possible.\"\"\"\n        # --use-feature=fast-deps must be provided.\n        if not self.use_lazy_wheel:\n            return None\n        if link.is_file or not link.is_wheel:\n            logger.debug(\n                \"Lazy wheel is not used as %r does not point to a remote wheel\",\n                link,\n            )\n            return None\n\n        wheel = Wheel(link.filename)\n        name = canonicalize_name(wheel.name)\n        logger.info(\n            \"Obtaining dependency information from %s %s\",\n            name,\n            wheel.version,\n        )\n        url = link.url.split(\"#\", 1)[0]\n        try:\n            return dist_from_wheel_url(name, url, self._session)\n        except HTTPRangeRequestUnsupported:\n            logger.debug(\"%s does not support range requests\", url)\n            return None\n\n    def _complete_partial_requirements(\n        self,\n        partially_downloaded_reqs: Iterable[InstallRequirement],\n        parallel_builds: bool = False,\n    ) -> None:\n        \"\"\"Download any requirements which were only fetched by metadata.\"\"\"\n        # Download to a temporary directory. These will be copied over as\n        # needed for downstream 'download', 'wheel', and 'install' commands.\n        temp_dir = TempDirectory(kind=\"unpack\", globally_managed=True).path\n\n        # Map each link to the requirement that owns it. This allows us to set\n        # `req.local_file_path` on the appropriate requirement after passing\n        # all the links at once into BatchDownloader.\n        links_to_fully_download: Dict[Link, InstallRequirement] = {}\n        for req in partially_downloaded_reqs:\n            assert req.link\n            links_to_fully_download[req.link] = req\n\n        batch_download = self._batch_download(\n            links_to_fully_download.keys(),\n            temp_dir,\n        )\n        for link, (filepath, _) in batch_download:\n            logger.debug(\"Downloading link %s to %s\", link, filepath)\n            req = links_to_fully_download[link]\n            # Record the downloaded file path so wheel reqs can extract a Distribution\n            # in .get_dist().\n            req.local_file_path = filepath\n            # Record that the file is downloaded so we don't do it again in\n            # _prepare_linked_requirement().\n            self._downloaded[req.link.url] = filepath\n\n            # If this is an sdist, we need to unpack it after downloading, but the\n            # .source_dir won't be set up until we are in _prepare_linked_requirement().\n            # Add the downloaded archive to the install requirement to unpack after\n            # preparing the source dir.\n            if not req.is_wheel:\n                req.needs_unpacked_archive(Path(filepath))\n\n        # This step is necessary to ensure all lazy wheels are processed\n        # successfully by the 'download', 'wheel', and 'install' commands.\n        for req in partially_downloaded_reqs:\n            self._prepare_linked_requirement(req, parallel_builds)\n\n    def prepare_linked_requirement(\n        self, req: InstallRequirement, parallel_builds: bool = False\n    ) -> BaseDistribution:\n        \"\"\"Prepare a requirement to be obtained from req.link.\"\"\"\n        assert req.link\n        self._log_preparing_link(req)\n        with indent_log():\n            # Check if the relevant file is already available\n            # in the download directory\n            file_path = None\n            if self.download_dir is not None and req.link.is_wheel:\n                hashes = self._get_linked_req_hashes(req)\n                file_path = _check_download_dir(\n                    req.link,\n                    self.download_dir,\n                    hashes,\n                    # When a locally built wheel has been found in cache, we don't warn\n                    # about re-downloading when the already downloaded wheel hash does\n                    # not match. This is because the hash must be checked against the\n                    # original link, not the cached link. It that case the already\n                    # downloaded file will be removed and re-fetched from cache (which\n                    # implies a hash check against the cache entry's origin.json).\n                    warn_on_hash_mismatch=not req.is_wheel_from_cache,\n                )\n\n            if file_path is not None:\n                # The file is already available, so mark it as downloaded\n                self._downloaded[req.link.url] = file_path\n            else:\n                # The file is not available, attempt to fetch only metadata\n                metadata_dist = self._fetch_metadata_only(req)\n                if metadata_dist is not None:\n                    req.needs_more_preparation = True\n                    return metadata_dist\n\n            # None of the optimizations worked, fully prepare the requirement\n            return self._prepare_linked_requirement(req, parallel_builds)\n\n    def prepare_linked_requirements_more(\n        self, reqs: Iterable[InstallRequirement], parallel_builds: bool = False\n    ) -> None:\n        \"\"\"Prepare linked requirements more, if needed.\"\"\"\n        reqs = [req for req in reqs if req.needs_more_preparation]\n        for req in reqs:\n            # Determine if any of these requirements were already downloaded.\n            if self.download_dir is not None and req.link.is_wheel:\n                hashes = self._get_linked_req_hashes(req)\n                file_path = _check_download_dir(req.link, self.download_dir, hashes)\n                if file_path is not None:\n                    self._downloaded[req.link.url] = file_path\n                    req.needs_more_preparation = False\n\n        # Prepare requirements we found were already downloaded for some\n        # reason. The other downloads will be completed separately.\n        partially_downloaded_reqs: List[InstallRequirement] = []\n        for req in reqs:\n            if req.needs_more_preparation:\n                partially_downloaded_reqs.append(req)\n            else:\n                self._prepare_linked_requirement(req, parallel_builds)\n\n        # TODO: separate this part out from RequirementPreparer when the v1\n        # resolver can be removed!\n        self._complete_partial_requirements(\n            partially_downloaded_reqs,\n            parallel_builds=parallel_builds,\n        )\n\n    def _prepare_linked_requirement(\n        self, req: InstallRequirement, parallel_builds: bool\n    ) -> BaseDistribution:\n        assert req.link\n        link = req.link\n\n        hashes = self._get_linked_req_hashes(req)\n\n        if hashes and req.is_wheel_from_cache:\n            assert req.download_info is not None\n            assert link.is_wheel\n            assert link.is_file\n            # We need to verify hashes, and we have found the requirement in the cache\n            # of locally built wheels.\n            if (\n                isinstance(req.download_info.info, ArchiveInfo)\n                and req.download_info.info.hashes\n                and hashes.has_one_of(req.download_info.info.hashes)\n            ):\n                # At this point we know the requirement was built from a hashable source\n                # artifact, and we verified that the cache entry's hash of the original\n                # artifact matches one of the hashes we expect. We don't verify hashes\n                # against the cached wheel, because the wheel is not the original.\n                hashes = None\n            else:\n                logger.warning(\n                    \"The hashes of the source archive found in cache entry \"\n                    \"don't match, ignoring cached built wheel \"\n                    \"and re-downloading source.\"\n                )\n                req.link = req.cached_wheel_source_link\n                link = req.link\n\n        self._ensure_link_req_src_dir(req, parallel_builds)\n\n        if link.is_existing_dir():\n            local_file = None\n        elif link.url not in self._downloaded:\n            try:\n                local_file = unpack_url(\n                    link,\n                    req.source_dir,\n                    self._download,\n                    self.verbosity,\n                    self.download_dir,\n                    hashes,\n                )\n            except NetworkConnectionError as exc:\n                raise InstallationError(\n                    f\"Could not install requirement {req} because of HTTP \"\n                    f\"error {exc} for URL {link}\"\n                )\n        else:\n            file_path = self._downloaded[link.url]\n            if hashes:\n                hashes.check_against_path(file_path)\n            local_file = File(file_path, content_type=None)\n\n        # If download_info is set, we got it from the wheel cache.\n        if req.download_info is None:\n            # Editables don't go through this function (see\n            # prepare_editable_requirement).\n            assert not req.editable\n            req.download_info = direct_url_from_link(link, req.source_dir)\n            # Make sure we have a hash in download_info. If we got it as part of the\n            # URL, it will have been verified and we can rely on it. Otherwise we\n            # compute it from the downloaded file.\n            # FIXME: https://github.com/pypa/pip/issues/11943\n            if (\n                isinstance(req.download_info.info, ArchiveInfo)\n                and not req.download_info.info.hashes\n                and local_file\n            ):\n                hash = hash_file(local_file.path)[0].hexdigest()\n                # We populate info.hash for backward compatibility.\n                # This will automatically populate info.hashes.\n                req.download_info.info.hash = f\"sha256={hash}\"\n\n        # For use in later processing,\n        # preserve the file path on the requirement.\n        if local_file:\n            req.local_file_path = local_file.path\n\n        dist = _get_prepared_distribution(\n            req,\n            self.build_tracker,\n            self.finder,\n            self.build_isolation,\n            self.check_build_deps,\n        )\n        return dist\n\n    def save_linked_requirement(self, req: InstallRequirement) -> None:\n        assert self.download_dir is not None\n        assert req.link is not None\n        link = req.link\n        if link.is_vcs or (link.is_existing_dir() and req.editable):\n            # Make a .zip of the source_dir we already created.\n            req.archive(self.download_dir)\n            return\n\n        if link.is_existing_dir():\n            logger.debug(\n                \"Not copying link to destination directory \"\n                \"since it is a directory: %s\",\n                link,\n            )\n            return\n        if req.local_file_path is None:\n            # No distribution was downloaded for this requirement.\n            return\n\n        download_location = os.path.join(self.download_dir, link.filename)\n        if not os.path.exists(download_location):\n            shutil.copy(req.local_file_path, download_location)\n            download_path = display_path(download_location)\n            logger.info(\"Saved %s\", download_path)\n\n    def prepare_editable_requirement(\n        self,\n        req: InstallRequirement,\n    ) -> BaseDistribution:\n        \"\"\"Prepare an editable requirement.\"\"\"\n        assert req.editable, \"cannot prepare a non-editable req as editable\"\n\n        logger.info(\"Obtaining %s\", req)\n\n        with indent_log():\n            if self.require_hashes:\n                raise InstallationError(\n                    f\"The editable requirement {req} cannot be installed when \"\n                    \"requiring hashes, because there is no single file to \"\n                    \"hash.\"\n                )\n            req.ensure_has_source_dir(self.src_dir)\n            req.update_editable()\n            assert req.source_dir\n            req.download_info = direct_url_for_editable(req.unpacked_source_directory)\n\n            dist = _get_prepared_distribution(\n                req,\n                self.build_tracker,\n                self.finder,\n                self.build_isolation,\n                self.check_build_deps,\n            )\n\n            req.check_if_exists(self.use_user_site)\n\n        return dist\n\n    def prepare_installed_requirement(\n        self,\n        req: InstallRequirement,\n        skip_reason: str,\n    ) -> BaseDistribution:\n        \"\"\"Prepare an already-installed requirement.\"\"\"\n        assert req.satisfied_by, \"req should have been satisfied but isn't\"\n        assert skip_reason is not None, (\n            \"did not get skip reason skipped but req.satisfied_by \"\n            f\"is set to {req.satisfied_by}\"\n        )\n        logger.info(\n            \"Requirement %s: %s (%s)\", skip_reason, req, req.satisfied_by.version\n        )\n        with indent_log():\n            if self.require_hashes:\n                logger.debug(\n                    \"Since it is already installed, we are trusting this \"\n                    \"package without checking its hash. To ensure a \"\n                    \"completely repeatable environment, install into an \"\n                    \"empty virtualenv.\"\n                )\n            return InstalledDistribution(req).get_metadata_distribution()\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/pyproject.py","size":7286,"sha1":"e4b375c5ef19d15e13f4fe07b3162c6125ccfbb3","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import importlib.util\nimport os\nimport sys\nfrom collections import namedtuple\nfrom typing import Any, List, Optional\n\nif sys.version_info >= (3, 11):\n    import tomllib\nelse:\n    from pip._vendor import tomli as tomllib\n\nfrom pip._vendor.packaging.requirements import InvalidRequirement\n\nfrom pip._internal.exceptions import (\n    InstallationError,\n    InvalidPyProjectBuildRequires,\n    MissingPyProjectBuildRequires,\n)\nfrom pip._internal.utils.packaging import get_requirement\n\n\ndef _is_list_of_str(obj: Any) -> bool:\n    return isinstance(obj, list) and all(isinstance(item, str) for item in obj)\n\n\ndef make_pyproject_path(unpacked_source_directory: str) -> str:\n    return os.path.join(unpacked_source_directory, \"pyproject.toml\")\n\n\nBuildSystemDetails = namedtuple(\n    \"BuildSystemDetails\", [\"requires\", \"backend\", \"check\", \"backend_path\"]\n)\n\n\ndef load_pyproject_toml(\n    use_pep517: Optional[bool], pyproject_toml: str, setup_py: str, req_name: str\n) -> Optional[BuildSystemDetails]:\n    \"\"\"Load the pyproject.toml file.\n\n    Parameters:\n        use_pep517 - Has the user requested PEP 517 processing? None\n                     means the user hasn't explicitly specified.\n        pyproject_toml - Location of the project's pyproject.toml file\n        setup_py - Location of the project's setup.py file\n        req_name - The name of the requirement we're processing (for\n                   error reporting)\n\n    Returns:\n        None if we should use the legacy code path, otherwise a tuple\n        (\n            requirements from pyproject.toml,\n            name of PEP 517 backend,\n            requirements we should check are installed after setting\n                up the build environment\n            directory paths to import the backend from (backend-path),\n                relative to the project root.\n        )\n    \"\"\"\n    has_pyproject = os.path.isfile(pyproject_toml)\n    has_setup = os.path.isfile(setup_py)\n\n    if not has_pyproject and not has_setup:\n        raise InstallationError(\n            f\"{req_name} does not appear to be a Python project: \"\n            f\"neither 'setup.py' nor 'pyproject.toml' found.\"\n        )\n\n    if has_pyproject:\n        with open(pyproject_toml, encoding=\"utf-8\") as f:\n            pp_toml = tomllib.loads(f.read())\n        build_system = pp_toml.get(\"build-system\")\n    else:\n        build_system = None\n\n    # The following cases must use PEP 517\n    # We check for use_pep517 being non-None and falsy because that means\n    # the user explicitly requested --no-use-pep517.  The value 0 as\n    # opposed to False can occur when the value is provided via an\n    # environment variable or config file option (due to the quirk of\n    # strtobool() returning an integer in pip's configuration code).\n    if has_pyproject and not has_setup:\n        if use_pep517 is not None and not use_pep517:\n            raise InstallationError(\n                \"Disabling PEP 517 processing is invalid: \"\n                \"project does not have a setup.py\"\n            )\n        use_pep517 = True\n    elif build_system and \"build-backend\" in build_system:\n        if use_pep517 is not None and not use_pep517:\n            raise InstallationError(\n                \"Disabling PEP 517 processing is invalid: \"\n                \"project specifies a build backend of {} \"\n                \"in pyproject.toml\".format(build_system[\"build-backend\"])\n            )\n        use_pep517 = True\n\n    # If we haven't worked out whether to use PEP 517 yet,\n    # and the user hasn't explicitly stated a preference,\n    # we do so if the project has a pyproject.toml file\n    # or if we cannot import setuptools or wheels.\n\n    # We fallback to PEP 517 when without setuptools or without the wheel package,\n    # so setuptools can be installed as a default build backend.\n    # For more info see:\n    # https://discuss.python.org/t/pip-without-setuptools-could-the-experience-be-improved/11810/9\n    # https://github.com/pypa/pip/issues/8559\n    elif use_pep517 is None:\n        use_pep517 = (\n            has_pyproject\n            or not importlib.util.find_spec(\"setuptools\")\n            or not importlib.util.find_spec(\"wheel\")\n        )\n\n    # At this point, we know whether we're going to use PEP 517.\n    assert use_pep517 is not None\n\n    # If we're using the legacy code path, there is nothing further\n    # for us to do here.\n    if not use_pep517:\n        return None\n\n    if build_system is None:\n        # Either the user has a pyproject.toml with no build-system\n        # section, or the user has no pyproject.toml, but has opted in\n        # explicitly via --use-pep517.\n        # In the absence of any explicit backend specification, we\n        # assume the setuptools backend that most closely emulates the\n        # traditional direct setup.py execution, and require wheel and\n        # a version of setuptools that supports that backend.\n\n        build_system = {\n            \"requires\": [\"setuptools>=40.8.0\"],\n            \"build-backend\": \"setuptools.build_meta:__legacy__\",\n        }\n\n    # If we're using PEP 517, we have build system information (either\n    # from pyproject.toml, or defaulted by the code above).\n    # Note that at this point, we do not know if the user has actually\n    # specified a backend, though.\n    assert build_system is not None\n\n    # Ensure that the build-system section in pyproject.toml conforms\n    # to PEP 518.\n\n    # Specifying the build-system table but not the requires key is invalid\n    if \"requires\" not in build_system:\n        raise MissingPyProjectBuildRequires(package=req_name)\n\n    # Error out if requires is not a list of strings\n    requires = build_system[\"requires\"]\n    if not _is_list_of_str(requires):\n        raise InvalidPyProjectBuildRequires(\n            package=req_name,\n            reason=\"It is not a list of strings.\",\n        )\n\n    # Each requirement must be valid as per PEP 508\n    for requirement in requires:\n        try:\n            get_requirement(requirement)\n        except InvalidRequirement as error:\n            raise InvalidPyProjectBuildRequires(\n                package=req_name,\n                reason=f\"It contains an invalid requirement: {requirement!r}\",\n            ) from error\n\n    backend = build_system.get(\"build-backend\")\n    backend_path = build_system.get(\"backend-path\", [])\n    check: List[str] = []\n    if backend is None:\n        # If the user didn't specify a backend, we assume they want to use\n        # the setuptools backend. But we can't be sure they have included\n        # a version of setuptools which supplies the backend. So we\n        # make a note to check that this requirement is present once\n        # we have set up the environment.\n        # This is quite a lot of work to check for a very specific case. But\n        # the problem is, that case is potentially quite common - projects that\n        # adopted PEP 518 early for the ability to specify requirements to\n        # execute setup.py, but never considered needing to mention the build\n        # tools themselves. The original PEP 518 code had a similar check (but\n        # implemented in a different way).\n        backend = \"setuptools.build_meta:__legacy__\"\n        check = [\"setuptools>=40.8.0\"]\n\n    return BuildSystemDetails(requires, backend, check, backend_path)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/req/__init__.py","size":2653,"sha1":"cec4e482de5c5ad5112d930f61cd15beee34bc2f","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import collections\nimport logging\nfrom dataclasses import dataclass\nfrom typing import Generator, List, Optional, Sequence, Tuple\n\nfrom pip._internal.utils.logging import indent_log\n\nfrom .req_file import parse_requirements\nfrom .req_install import InstallRequirement\nfrom .req_set import RequirementSet\n\n__all__ = [\n    \"RequirementSet\",\n    \"InstallRequirement\",\n    \"parse_requirements\",\n    \"install_given_reqs\",\n]\n\nlogger = logging.getLogger(__name__)\n\n\n@dataclass(frozen=True)\nclass InstallationResult:\n    name: str\n\n\ndef _validate_requirements(\n    requirements: List[InstallRequirement],\n) -> Generator[Tuple[str, InstallRequirement], None, None]:\n    for req in requirements:\n        assert req.name, f\"invalid to-be-installed requirement: {req}\"\n        yield req.name, req\n\n\ndef install_given_reqs(\n    requirements: List[InstallRequirement],\n    global_options: Sequence[str],\n    root: Optional[str],\n    home: Optional[str],\n    prefix: Optional[str],\n    warn_script_location: bool,\n    use_user_site: bool,\n    pycompile: bool,\n) -> List[InstallationResult]:\n    \"\"\"\n    Install everything in the given list.\n\n    (to be called after having downloaded and unpacked the packages)\n    \"\"\"\n    to_install = collections.OrderedDict(_validate_requirements(requirements))\n\n    if to_install:\n        logger.info(\n            \"Installing collected packages: %s\",\n            \", \".join(to_install.keys()),\n        )\n\n    installed = []\n\n    with indent_log():\n        for req_name, requirement in to_install.items():\n            if requirement.should_reinstall:\n                logger.info(\"Attempting uninstall: %s\", req_name)\n                with indent_log():\n                    uninstalled_pathset = requirement.uninstall(auto_confirm=True)\n            else:\n                uninstalled_pathset = None\n\n            try:\n                requirement.install(\n                    global_options,\n                    root=root,\n                    home=home,\n                    prefix=prefix,\n                    warn_script_location=warn_script_location,\n                    use_user_site=use_user_site,\n                    pycompile=pycompile,\n                )\n            except Exception:\n                # if install did not succeed, rollback previous uninstall\n                if uninstalled_pathset and not requirement.install_succeeded:\n                    uninstalled_pathset.rollback()\n                raise\n            else:\n                if uninstalled_pathset and requirement.install_succeeded:\n                    uninstalled_pathset.commit()\n\n            installed.append(InstallationResult(req_name))\n\n    return installed\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/req/constructors.py","size":18430,"sha1":"481b2fed7cbcf321f8ecb9da20388ba4785a4b31","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"Backing implementation for InstallRequirement's various constructors\n\nThe idea here is that these formed a major chunk of InstallRequirement's size\nso, moving them and support code dedicated to them outside of that class\nhelps creates for better understandability for the rest of the code.\n\nThese are meant to be used elsewhere within pip to create instances of\nInstallRequirement.\n\"\"\"\n\nimport copy\nimport logging\nimport os\nimport re\nfrom dataclasses import dataclass\nfrom typing import Collection, Dict, List, Optional, Set, Tuple, Union\n\nfrom pip._vendor.packaging.markers import Marker\nfrom pip._vendor.packaging.requirements import InvalidRequirement, Requirement\nfrom pip._vendor.packaging.specifiers import Specifier\n\nfrom pip._internal.exceptions import InstallationError\nfrom pip._internal.models.index import PyPI, TestPyPI\nfrom pip._internal.models.link import Link\nfrom pip._internal.models.wheel import Wheel\nfrom pip._internal.req.req_file import ParsedRequirement\nfrom pip._internal.req.req_install import InstallRequirement\nfrom pip._internal.utils.filetypes import is_archive_file\nfrom pip._internal.utils.misc import is_installable_dir\nfrom pip._internal.utils.packaging import get_requirement\nfrom pip._internal.utils.urls import path_to_url\nfrom pip._internal.vcs import is_url, vcs\n\n__all__ = [\n    \"install_req_from_editable\",\n    \"install_req_from_line\",\n    \"parse_editable\",\n]\n\nlogger = logging.getLogger(__name__)\noperators = Specifier._operators.keys()\n\n\ndef _strip_extras(path: str) -> Tuple[str, Optional[str]]:\n    m = re.match(r\"^(.+)(\\[[^\\]]+\\])$\", path)\n    extras = None\n    if m:\n        path_no_extras = m.group(1)\n        extras = m.group(2)\n    else:\n        path_no_extras = path\n\n    return path_no_extras, extras\n\n\ndef convert_extras(extras: Optional[str]) -> Set[str]:\n    if not extras:\n        return set()\n    return get_requirement(\"placeholder\" + extras.lower()).extras\n\n\ndef _set_requirement_extras(req: Requirement, new_extras: Set[str]) -> Requirement:\n    \"\"\"\n    Returns a new requirement based on the given one, with the supplied extras. If the\n    given requirement already has extras those are replaced (or dropped if no new extras\n    are given).\n    \"\"\"\n    match: Optional[re.Match[str]] = re.fullmatch(\n        # see https://peps.python.org/pep-0508/#complete-grammar\n        r\"([\\w\\t .-]+)(\\[[^\\]]*\\])?(.*)\",\n        str(req),\n        flags=re.ASCII,\n    )\n    # ireq.req is a valid requirement so the regex should always match\n    assert (\n        match is not None\n    ), f\"regex match on requirement {req} failed, this should never happen\"\n    pre: Optional[str] = match.group(1)\n    post: Optional[str] = match.group(3)\n    assert (\n        pre is not None and post is not None\n    ), f\"regex group selection for requirement {req} failed, this should never happen\"\n    extras: str = \"[{}]\".format(\",\".join(sorted(new_extras)) if new_extras else \"\")\n    return get_requirement(f\"{pre}{extras}{post}\")\n\n\ndef parse_editable(editable_req: str) -> Tuple[Optional[str], str, Set[str]]:\n    \"\"\"Parses an editable requirement into:\n        - a requirement name\n        - an URL\n        - extras\n        - editable options\n    Accepted requirements:\n        svn+http://blahblah@rev#egg=Foobar[baz]&subdirectory=version_subdir\n        .[some_extra]\n    \"\"\"\n\n    url = editable_req\n\n    # If a file path is specified with extras, strip off the extras.\n    url_no_extras, extras = _strip_extras(url)\n\n    if os.path.isdir(url_no_extras):\n        # Treating it as code that has already been checked out\n        url_no_extras = path_to_url(url_no_extras)\n\n    if url_no_extras.lower().startswith(\"file:\"):\n        package_name = Link(url_no_extras).egg_fragment\n        if extras:\n            return (\n                package_name,\n                url_no_extras,\n                get_requirement(\"placeholder\" + extras.lower()).extras,\n            )\n        else:\n            return package_name, url_no_extras, set()\n\n    for version_control in vcs:\n        if url.lower().startswith(f\"{version_control}:\"):\n            url = f\"{version_control}+{url}\"\n            break\n\n    link = Link(url)\n\n    if not link.is_vcs:\n        backends = \", \".join(vcs.all_schemes)\n        raise InstallationError(\n            f\"{editable_req} is not a valid editable requirement. \"\n            f\"It should either be a path to a local project or a VCS URL \"\n            f\"(beginning with {backends}).\"\n        )\n\n    package_name = link.egg_fragment\n    if not package_name:\n        raise InstallationError(\n            f\"Could not detect requirement name for '{editable_req}', \"\n            \"please specify one with #egg=your_package_name\"\n        )\n    return package_name, url, set()\n\n\ndef check_first_requirement_in_file(filename: str) -> None:\n    \"\"\"Check if file is parsable as a requirements file.\n\n    This is heavily based on ``pkg_resources.parse_requirements``, but\n    simplified to just check the first meaningful line.\n\n    :raises InvalidRequirement: If the first meaningful line cannot be parsed\n        as an requirement.\n    \"\"\"\n    with open(filename, encoding=\"utf-8\", errors=\"ignore\") as f:\n        # Create a steppable iterator, so we can handle \\-continuations.\n        lines = (\n            line\n            for line in (line.strip() for line in f)\n            if line and not line.startswith(\"#\")  # Skip blank lines/comments.\n        )\n\n        for line in lines:\n            # Drop comments -- a hash without a space may be in a URL.\n            if \" #\" in line:\n                line = line[: line.find(\" #\")]\n            # If there is a line continuation, drop it, and append the next line.\n            if line.endswith(\"\\\\\"):\n                line = line[:-2].strip() + next(lines, \"\")\n            get_requirement(line)\n            return\n\n\ndef deduce_helpful_msg(req: str) -> str:\n    \"\"\"Returns helpful msg in case requirements file does not exist,\n    or cannot be parsed.\n\n    :params req: Requirements file path\n    \"\"\"\n    if not os.path.exists(req):\n        return f\" File '{req}' does not exist.\"\n    msg = \" The path does exist. \"\n    # Try to parse and check if it is a requirements file.\n    try:\n        check_first_requirement_in_file(req)\n    except InvalidRequirement:\n        logger.debug(\"Cannot parse '%s' as requirements file\", req)\n    else:\n        msg += (\n            f\"The argument you provided \"\n            f\"({req}) appears to be a\"\n            f\" requirements file. If that is the\"\n            f\" case, use the '-r' flag to install\"\n            f\" the packages specified within it.\"\n        )\n    return msg\n\n\n@dataclass(frozen=True)\nclass RequirementParts:\n    requirement: Optional[Requirement]\n    link: Optional[Link]\n    markers: Optional[Marker]\n    extras: Set[str]\n\n\ndef parse_req_from_editable(editable_req: str) -> RequirementParts:\n    name, url, extras_override = parse_editable(editable_req)\n\n    if name is not None:\n        try:\n            req: Optional[Requirement] = get_requirement(name)\n        except InvalidRequirement as exc:\n            raise InstallationError(f\"Invalid requirement: {name!r}: {exc}\")\n    else:\n        req = None\n\n    link = Link(url)\n\n    return RequirementParts(req, link, None, extras_override)\n\n\n# ---- The actual constructors follow ----\n\n\ndef install_req_from_editable(\n    editable_req: str,\n    comes_from: Optional[Union[InstallRequirement, str]] = None,\n    *,\n    use_pep517: Optional[bool] = None,\n    isolated: bool = False,\n    global_options: Optional[List[str]] = None,\n    hash_options: Optional[Dict[str, List[str]]] = None,\n    constraint: bool = False,\n    user_supplied: bool = False,\n    permit_editable_wheels: bool = False,\n    config_settings: Optional[Dict[str, Union[str, List[str]]]] = None,\n) -> InstallRequirement:\n    parts = parse_req_from_editable(editable_req)\n\n    return InstallRequirement(\n        parts.requirement,\n        comes_from=comes_from,\n        user_supplied=user_supplied,\n        editable=True,\n        permit_editable_wheels=permit_editable_wheels,\n        link=parts.link,\n        constraint=constraint,\n        use_pep517=use_pep517,\n        isolated=isolated,\n        global_options=global_options,\n        hash_options=hash_options,\n        config_settings=config_settings,\n        extras=parts.extras,\n    )\n\n\ndef _looks_like_path(name: str) -> bool:\n    \"\"\"Checks whether the string \"looks like\" a path on the filesystem.\n\n    This does not check whether the target actually exists, only judge from the\n    appearance.\n\n    Returns true if any of the following conditions is true:\n    * a path separator is found (either os.path.sep or os.path.altsep);\n    * a dot is found (which represents the current directory).\n    \"\"\"\n    if os.path.sep in name:\n        return True\n    if os.path.altsep is not None and os.path.altsep in name:\n        return True\n    if name.startswith(\".\"):\n        return True\n    return False\n\n\ndef _get_url_from_path(path: str, name: str) -> Optional[str]:\n    \"\"\"\n    First, it checks whether a provided path is an installable directory. If it\n    is, returns the path.\n\n    If false, check if the path is an archive file (such as a .whl).\n    The function checks if the path is a file. If false, if the path has\n    an @, it will treat it as a PEP 440 URL requirement and return the path.\n    \"\"\"\n    if _looks_like_path(name) and os.path.isdir(path):\n        if is_installable_dir(path):\n            return path_to_url(path)\n        # TODO: The is_installable_dir test here might not be necessary\n        #       now that it is done in load_pyproject_toml too.\n        raise InstallationError(\n            f\"Directory {name!r} is not installable. Neither 'setup.py' \"\n            \"nor 'pyproject.toml' found.\"\n        )\n    if not is_archive_file(path):\n        return None\n    if os.path.isfile(path):\n        return path_to_url(path)\n    urlreq_parts = name.split(\"@\", 1)\n    if len(urlreq_parts) >= 2 and not _looks_like_path(urlreq_parts[0]):\n        # If the path contains '@' and the part before it does not look\n        # like a path, try to treat it as a PEP 440 URL req instead.\n        return None\n    logger.warning(\n        \"Requirement %r looks like a filename, but the file does not exist\",\n        name,\n    )\n    return path_to_url(path)\n\n\ndef parse_req_from_line(name: str, line_source: Optional[str]) -> RequirementParts:\n    if is_url(name):\n        marker_sep = \"; \"\n    else:\n        marker_sep = \";\"\n    if marker_sep in name:\n        name, markers_as_string = name.split(marker_sep, 1)\n        markers_as_string = markers_as_string.strip()\n        if not markers_as_string:\n            markers = None\n        else:\n            markers = Marker(markers_as_string)\n    else:\n        markers = None\n    name = name.strip()\n    req_as_string = None\n    path = os.path.normpath(os.path.abspath(name))\n    link = None\n    extras_as_string = None\n\n    if is_url(name):\n        link = Link(name)\n    else:\n        p, extras_as_string = _strip_extras(path)\n        url = _get_url_from_path(p, name)\n        if url is not None:\n            link = Link(url)\n\n    # it's a local file, dir, or url\n    if link:\n        # Handle relative file URLs\n        if link.scheme == \"file\" and re.search(r\"\\.\\./\", link.url):\n            link = Link(path_to_url(os.path.normpath(os.path.abspath(link.path))))\n        # wheel file\n        if link.is_wheel:\n            wheel = Wheel(link.filename)  # can raise InvalidWheelFilename\n            req_as_string = f\"{wheel.name}=={wheel.version}\"\n        else:\n            # set the req to the egg fragment.  when it's not there, this\n            # will become an 'unnamed' requirement\n            req_as_string = link.egg_fragment\n\n    # a requirement specifier\n    else:\n        req_as_string = name\n\n    extras = convert_extras(extras_as_string)\n\n    def with_source(text: str) -> str:\n        if not line_source:\n            return text\n        return f\"{text} (from {line_source})\"\n\n    def _parse_req_string(req_as_string: str) -> Requirement:\n        try:\n            return get_requirement(req_as_string)\n        except InvalidRequirement as exc:\n            if os.path.sep in req_as_string:\n                add_msg = \"It looks like a path.\"\n                add_msg += deduce_helpful_msg(req_as_string)\n            elif \"=\" in req_as_string and not any(\n                op in req_as_string for op in operators\n            ):\n                add_msg = \"= is not a valid operator. Did you mean == ?\"\n            else:\n                add_msg = \"\"\n            msg = with_source(f\"Invalid requirement: {req_as_string!r}: {exc}\")\n            if add_msg:\n                msg += f\"\\nHint: {add_msg}\"\n            raise InstallationError(msg)\n\n    if req_as_string is not None:\n        req: Optional[Requirement] = _parse_req_string(req_as_string)\n    else:\n        req = None\n\n    return RequirementParts(req, link, markers, extras)\n\n\ndef install_req_from_line(\n    name: str,\n    comes_from: Optional[Union[str, InstallRequirement]] = None,\n    *,\n    use_pep517: Optional[bool] = None,\n    isolated: bool = False,\n    global_options: Optional[List[str]] = None,\n    hash_options: Optional[Dict[str, List[str]]] = None,\n    constraint: bool = False,\n    line_source: Optional[str] = None,\n    user_supplied: bool = False,\n    config_settings: Optional[Dict[str, Union[str, List[str]]]] = None,\n) -> InstallRequirement:\n    \"\"\"Creates an InstallRequirement from a name, which might be a\n    requirement, directory containing 'setup.py', filename, or URL.\n\n    :param line_source: An optional string describing where the line is from,\n        for logging purposes in case of an error.\n    \"\"\"\n    parts = parse_req_from_line(name, line_source)\n\n    return InstallRequirement(\n        parts.requirement,\n        comes_from,\n        link=parts.link,\n        markers=parts.markers,\n        use_pep517=use_pep517,\n        isolated=isolated,\n        global_options=global_options,\n        hash_options=hash_options,\n        config_settings=config_settings,\n        constraint=constraint,\n        extras=parts.extras,\n        user_supplied=user_supplied,\n    )\n\n\ndef install_req_from_req_string(\n    req_string: str,\n    comes_from: Optional[InstallRequirement] = None,\n    isolated: bool = False,\n    use_pep517: Optional[bool] = None,\n    user_supplied: bool = False,\n) -> InstallRequirement:\n    try:\n        req = get_requirement(req_string)\n    except InvalidRequirement as exc:\n        raise InstallationError(f\"Invalid requirement: {req_string!r}: {exc}\")\n\n    domains_not_allowed = [\n        PyPI.file_storage_domain,\n        TestPyPI.file_storage_domain,\n    ]\n    if (\n        req.url\n        and comes_from\n        and comes_from.link\n        and comes_from.link.netloc in domains_not_allowed\n    ):\n        # Explicitly disallow pypi packages that depend on external urls\n        raise InstallationError(\n            \"Packages installed from PyPI cannot depend on packages \"\n            \"which are not also hosted on PyPI.\\n\"\n            f\"{comes_from.name} depends on {req} \"\n        )\n\n    return InstallRequirement(\n        req,\n        comes_from,\n        isolated=isolated,\n        use_pep517=use_pep517,\n        user_supplied=user_supplied,\n    )\n\n\ndef install_req_from_parsed_requirement(\n    parsed_req: ParsedRequirement,\n    isolated: bool = False,\n    use_pep517: Optional[bool] = None,\n    user_supplied: bool = False,\n    config_settings: Optional[Dict[str, Union[str, List[str]]]] = None,\n) -> InstallRequirement:\n    if parsed_req.is_editable:\n        req = install_req_from_editable(\n            parsed_req.requirement,\n            comes_from=parsed_req.comes_from,\n            use_pep517=use_pep517,\n            constraint=parsed_req.constraint,\n            isolated=isolated,\n            user_supplied=user_supplied,\n            config_settings=config_settings,\n        )\n\n    else:\n        req = install_req_from_line(\n            parsed_req.requirement,\n            comes_from=parsed_req.comes_from,\n            use_pep517=use_pep517,\n            isolated=isolated,\n            global_options=(\n                parsed_req.options.get(\"global_options\", [])\n                if parsed_req.options\n                else []\n            ),\n            hash_options=(\n                parsed_req.options.get(\"hashes\", {}) if parsed_req.options else {}\n            ),\n            constraint=parsed_req.constraint,\n            line_source=parsed_req.line_source,\n            user_supplied=user_supplied,\n            config_settings=config_settings,\n        )\n    return req\n\n\ndef install_req_from_link_and_ireq(\n    link: Link, ireq: InstallRequirement\n) -> InstallRequirement:\n    return InstallRequirement(\n        req=ireq.req,\n        comes_from=ireq.comes_from,\n        editable=ireq.editable,\n        link=link,\n        markers=ireq.markers,\n        use_pep517=ireq.use_pep517,\n        isolated=ireq.isolated,\n        global_options=ireq.global_options,\n        hash_options=ireq.hash_options,\n        config_settings=ireq.config_settings,\n        user_supplied=ireq.user_supplied,\n    )\n\n\ndef install_req_drop_extras(ireq: InstallRequirement) -> InstallRequirement:\n    \"\"\"\n    Creates a new InstallationRequirement using the given template but without\n    any extras. Sets the original requirement as the new one's parent\n    (comes_from).\n    \"\"\"\n    return InstallRequirement(\n        req=(\n            _set_requirement_extras(ireq.req, set()) if ireq.req is not None else None\n        ),\n        comes_from=ireq,\n        editable=ireq.editable,\n        link=ireq.link,\n        markers=ireq.markers,\n        use_pep517=ireq.use_pep517,\n        isolated=ireq.isolated,\n        global_options=ireq.global_options,\n        hash_options=ireq.hash_options,\n        constraint=ireq.constraint,\n        extras=[],\n        config_settings=ireq.config_settings,\n        user_supplied=ireq.user_supplied,\n        permit_editable_wheels=ireq.permit_editable_wheels,\n    )\n\n\ndef install_req_extend_extras(\n    ireq: InstallRequirement,\n    extras: Collection[str],\n) -> InstallRequirement:\n    \"\"\"\n    Returns a copy of an installation requirement with some additional extras.\n    Makes a shallow copy of the ireq object.\n    \"\"\"\n    result = copy.copy(ireq)\n    result.extras = {*ireq.extras, *extras}\n    result.req = (\n        _set_requirement_extras(ireq.req, result.extras)\n        if ireq.req is not None\n        else None\n    )\n    return result\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/req/req_file.py","size":20234,"sha1":"eec45e3d757d015d101362fafc57da0c97c632aa","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"\nRequirements file parsing\n\"\"\"\n\nimport codecs\nimport locale\nimport logging\nimport optparse\nimport os\nimport re\nimport shlex\nimport sys\nimport urllib.parse\nfrom dataclasses import dataclass\nfrom optparse import Values\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Dict,\n    Generator,\n    Iterable,\n    List,\n    NoReturn,\n    Optional,\n    Tuple,\n)\n\nfrom pip._internal.cli import cmdoptions\nfrom pip._internal.exceptions import InstallationError, RequirementsFileParseError\nfrom pip._internal.models.search_scope import SearchScope\n\nif TYPE_CHECKING:\n    from pip._internal.index.package_finder import PackageFinder\n    from pip._internal.network.session import PipSession\n\n__all__ = [\"parse_requirements\"]\n\nReqFileLines = Iterable[Tuple[int, str]]\n\nLineParser = Callable[[str], Tuple[str, Values]]\n\nSCHEME_RE = re.compile(r\"^(http|https|file):\", re.I)\nCOMMENT_RE = re.compile(r\"(^|\\s+)#.*$\")\n\n# Matches environment variable-style values in '${MY_VARIABLE_1}' with the\n# variable name consisting of only uppercase letters, digits or the '_'\n# (underscore). This follows the POSIX standard defined in IEEE Std 1003.1,\n# 2013 Edition.\nENV_VAR_RE = re.compile(r\"(?P<var>\\$\\{(?P<name>[A-Z0-9_]+)\\})\")\n\nSUPPORTED_OPTIONS: List[Callable[..., optparse.Option]] = [\n    cmdoptions.index_url,\n    cmdoptions.extra_index_url,\n    cmdoptions.no_index,\n    cmdoptions.constraints,\n    cmdoptions.requirements,\n    cmdoptions.editable,\n    cmdoptions.find_links,\n    cmdoptions.no_binary,\n    cmdoptions.only_binary,\n    cmdoptions.prefer_binary,\n    cmdoptions.require_hashes,\n    cmdoptions.pre,\n    cmdoptions.trusted_host,\n    cmdoptions.use_new_feature,\n]\n\n# options to be passed to requirements\nSUPPORTED_OPTIONS_REQ: List[Callable[..., optparse.Option]] = [\n    cmdoptions.global_options,\n    cmdoptions.hash,\n    cmdoptions.config_settings,\n]\n\nSUPPORTED_OPTIONS_EDITABLE_REQ: List[Callable[..., optparse.Option]] = [\n    cmdoptions.config_settings,\n]\n\n\n# the 'dest' string values\nSUPPORTED_OPTIONS_REQ_DEST = [str(o().dest) for o in SUPPORTED_OPTIONS_REQ]\nSUPPORTED_OPTIONS_EDITABLE_REQ_DEST = [\n    str(o().dest) for o in SUPPORTED_OPTIONS_EDITABLE_REQ\n]\n\n# order of BOMS is important: codecs.BOM_UTF16_LE is a prefix of codecs.BOM_UTF32_LE\n# so data.startswith(BOM_UTF16_LE) would be true for UTF32_LE data\nBOMS: List[Tuple[bytes, str]] = [\n    (codecs.BOM_UTF8, \"utf-8\"),\n    (codecs.BOM_UTF32, \"utf-32\"),\n    (codecs.BOM_UTF32_BE, \"utf-32-be\"),\n    (codecs.BOM_UTF32_LE, \"utf-32-le\"),\n    (codecs.BOM_UTF16, \"utf-16\"),\n    (codecs.BOM_UTF16_BE, \"utf-16-be\"),\n    (codecs.BOM_UTF16_LE, \"utf-16-le\"),\n]\n\nPEP263_ENCODING_RE = re.compile(rb\"coding[:=]\\s*([-\\w.]+)\")\nDEFAULT_ENCODING = \"utf-8\"\n\nlogger = logging.getLogger(__name__)\n\n\n@dataclass(frozen=True)\nclass ParsedRequirement:\n    # TODO: replace this with slots=True when dropping Python 3.9 support.\n    __slots__ = (\n        \"requirement\",\n        \"is_editable\",\n        \"comes_from\",\n        \"constraint\",\n        \"options\",\n        \"line_source\",\n    )\n\n    requirement: str\n    is_editable: bool\n    comes_from: str\n    constraint: bool\n    options: Optional[Dict[str, Any]]\n    line_source: Optional[str]\n\n\n@dataclass(frozen=True)\nclass ParsedLine:\n    __slots__ = (\"filename\", \"lineno\", \"args\", \"opts\", \"constraint\")\n\n    filename: str\n    lineno: int\n    args: str\n    opts: Values\n    constraint: bool\n\n    @property\n    def is_editable(self) -> bool:\n        return bool(self.opts.editables)\n\n    @property\n    def requirement(self) -> Optional[str]:\n        if self.args:\n            return self.args\n        elif self.is_editable:\n            # We don't support multiple -e on one line\n            return self.opts.editables[0]\n        return None\n\n\ndef parse_requirements(\n    filename: str,\n    session: \"PipSession\",\n    finder: Optional[\"PackageFinder\"] = None,\n    options: Optional[optparse.Values] = None,\n    constraint: bool = False,\n) -> Generator[ParsedRequirement, None, None]:\n    \"\"\"Parse a requirements file and yield ParsedRequirement instances.\n\n    :param filename:    Path or url of requirements file.\n    :param session:     PipSession instance.\n    :param finder:      Instance of pip.index.PackageFinder.\n    :param options:     cli options.\n    :param constraint:  If true, parsing a constraint file rather than\n        requirements file.\n    \"\"\"\n    line_parser = get_line_parser(finder)\n    parser = RequirementsFileParser(session, line_parser)\n\n    for parsed_line in parser.parse(filename, constraint):\n        parsed_req = handle_line(\n            parsed_line, options=options, finder=finder, session=session\n        )\n        if parsed_req is not None:\n            yield parsed_req\n\n\ndef preprocess(content: str) -> ReqFileLines:\n    \"\"\"Split, filter, and join lines, and return a line iterator\n\n    :param content: the content of the requirements file\n    \"\"\"\n    lines_enum: ReqFileLines = enumerate(content.splitlines(), start=1)\n    lines_enum = join_lines(lines_enum)\n    lines_enum = ignore_comments(lines_enum)\n    lines_enum = expand_env_variables(lines_enum)\n    return lines_enum\n\n\ndef handle_requirement_line(\n    line: ParsedLine,\n    options: Optional[optparse.Values] = None,\n) -> ParsedRequirement:\n    # preserve for the nested code path\n    line_comes_from = \"{} {} (line {})\".format(\n        \"-c\" if line.constraint else \"-r\",\n        line.filename,\n        line.lineno,\n    )\n\n    assert line.requirement is not None\n\n    # get the options that apply to requirements\n    if line.is_editable:\n        supported_dest = SUPPORTED_OPTIONS_EDITABLE_REQ_DEST\n    else:\n        supported_dest = SUPPORTED_OPTIONS_REQ_DEST\n    req_options = {}\n    for dest in supported_dest:\n        if dest in line.opts.__dict__ and line.opts.__dict__[dest]:\n            req_options[dest] = line.opts.__dict__[dest]\n\n    line_source = f\"line {line.lineno} of {line.filename}\"\n    return ParsedRequirement(\n        requirement=line.requirement,\n        is_editable=line.is_editable,\n        comes_from=line_comes_from,\n        constraint=line.constraint,\n        options=req_options,\n        line_source=line_source,\n    )\n\n\ndef handle_option_line(\n    opts: Values,\n    filename: str,\n    lineno: int,\n    finder: Optional[\"PackageFinder\"] = None,\n    options: Optional[optparse.Values] = None,\n    session: Optional[\"PipSession\"] = None,\n) -> None:\n    if opts.hashes:\n        logger.warning(\n            \"%s line %s has --hash but no requirement, and will be ignored.\",\n            filename,\n            lineno,\n        )\n\n    if options:\n        # percolate options upward\n        if opts.require_hashes:\n            options.require_hashes = opts.require_hashes\n        if opts.features_enabled:\n            options.features_enabled.extend(\n                f for f in opts.features_enabled if f not in options.features_enabled\n            )\n\n    # set finder options\n    if finder:\n        find_links = finder.find_links\n        index_urls = finder.index_urls\n        no_index = finder.search_scope.no_index\n        if opts.no_index is True:\n            no_index = True\n            index_urls = []\n        if opts.index_url and not no_index:\n            index_urls = [opts.index_url]\n        if opts.extra_index_urls and not no_index:\n            index_urls.extend(opts.extra_index_urls)\n        if opts.find_links:\n            # FIXME: it would be nice to keep track of the source\n            # of the find_links: support a find-links local path\n            # relative to a requirements file.\n            value = opts.find_links[0]\n            req_dir = os.path.dirname(os.path.abspath(filename))\n            relative_to_reqs_file = os.path.join(req_dir, value)\n            if os.path.exists(relative_to_reqs_file):\n                value = relative_to_reqs_file\n            find_links.append(value)\n\n        if session:\n            # We need to update the auth urls in session\n            session.update_index_urls(index_urls)\n\n        search_scope = SearchScope(\n            find_links=find_links,\n            index_urls=index_urls,\n            no_index=no_index,\n        )\n        finder.search_scope = search_scope\n\n        if opts.pre:\n            finder.set_allow_all_prereleases()\n\n        if opts.prefer_binary:\n            finder.set_prefer_binary()\n\n        if session:\n            for host in opts.trusted_hosts or []:\n                source = f\"line {lineno} of {filename}\"\n                session.add_trusted_host(host, source=source)\n\n\ndef handle_line(\n    line: ParsedLine,\n    options: Optional[optparse.Values] = None,\n    finder: Optional[\"PackageFinder\"] = None,\n    session: Optional[\"PipSession\"] = None,\n) -> Optional[ParsedRequirement]:\n    \"\"\"Handle a single parsed requirements line; This can result in\n    creating/yielding requirements, or updating the finder.\n\n    :param line:        The parsed line to be processed.\n    :param options:     CLI options.\n    :param finder:      The finder - updated by non-requirement lines.\n    :param session:     The session - updated by non-requirement lines.\n\n    Returns a ParsedRequirement object if the line is a requirement line,\n    otherwise returns None.\n\n    For lines that contain requirements, the only options that have an effect\n    are from SUPPORTED_OPTIONS_REQ, and they are scoped to the\n    requirement. Other options from SUPPORTED_OPTIONS may be present, but are\n    ignored.\n\n    For lines that do not contain requirements, the only options that have an\n    effect are from SUPPORTED_OPTIONS. Options from SUPPORTED_OPTIONS_REQ may\n    be present, but are ignored. These lines may contain multiple options\n    (although our docs imply only one is supported), and all our parsed and\n    affect the finder.\n    \"\"\"\n\n    if line.requirement is not None:\n        parsed_req = handle_requirement_line(line, options)\n        return parsed_req\n    else:\n        handle_option_line(\n            line.opts,\n            line.filename,\n            line.lineno,\n            finder,\n            options,\n            session,\n        )\n        return None\n\n\nclass RequirementsFileParser:\n    def __init__(\n        self,\n        session: \"PipSession\",\n        line_parser: LineParser,\n    ) -> None:\n        self._session = session\n        self._line_parser = line_parser\n\n    def parse(\n        self, filename: str, constraint: bool\n    ) -> Generator[ParsedLine, None, None]:\n        \"\"\"Parse a given file, yielding parsed lines.\"\"\"\n        yield from self._parse_and_recurse(\n            filename, constraint, [{os.path.abspath(filename): None}]\n        )\n\n    def _parse_and_recurse(\n        self,\n        filename: str,\n        constraint: bool,\n        parsed_files_stack: List[Dict[str, Optional[str]]],\n    ) -> Generator[ParsedLine, None, None]:\n        for line in self._parse_file(filename, constraint):\n            if line.requirement is None and (\n                line.opts.requirements or line.opts.constraints\n            ):\n                # parse a nested requirements file\n                if line.opts.requirements:\n                    req_path = line.opts.requirements[0]\n                    nested_constraint = False\n                else:\n                    req_path = line.opts.constraints[0]\n                    nested_constraint = True\n\n                # original file is over http\n                if SCHEME_RE.search(filename):\n                    # do a url join so relative paths work\n                    req_path = urllib.parse.urljoin(filename, req_path)\n                # original file and nested file are paths\n                elif not SCHEME_RE.search(req_path):\n                    # do a join so relative paths work\n                    # and then abspath so that we can identify recursive references\n                    req_path = os.path.abspath(\n                        os.path.join(\n                            os.path.dirname(filename),\n                            req_path,\n                        )\n                    )\n                parsed_files = parsed_files_stack[0]\n                if req_path in parsed_files:\n                    initial_file = parsed_files[req_path]\n                    tail = (\n                        f\" and again in {initial_file}\"\n                        if initial_file is not None\n                        else \"\"\n                    )\n                    raise RequirementsFileParseError(\n                        f\"{req_path} recursively references itself in {filename}{tail}\"\n                    )\n                # Keeping a track where was each file first included in\n                new_parsed_files = parsed_files.copy()\n                new_parsed_files[req_path] = filename\n                yield from self._parse_and_recurse(\n                    req_path, nested_constraint, [new_parsed_files, *parsed_files_stack]\n                )\n            else:\n                yield line\n\n    def _parse_file(\n        self, filename: str, constraint: bool\n    ) -> Generator[ParsedLine, None, None]:\n        _, content = get_file_content(filename, self._session)\n\n        lines_enum = preprocess(content)\n\n        for line_number, line in lines_enum:\n            try:\n                args_str, opts = self._line_parser(line)\n            except OptionParsingError as e:\n                # add offending line\n                msg = f\"Invalid requirement: {line}\\n{e.msg}\"\n                raise RequirementsFileParseError(msg)\n\n            yield ParsedLine(\n                filename,\n                line_number,\n                args_str,\n                opts,\n                constraint,\n            )\n\n\ndef get_line_parser(finder: Optional[\"PackageFinder\"]) -> LineParser:\n    def parse_line(line: str) -> Tuple[str, Values]:\n        # Build new parser for each line since it accumulates appendable\n        # options.\n        parser = build_parser()\n        defaults = parser.get_default_values()\n        defaults.index_url = None\n        if finder:\n            defaults.format_control = finder.format_control\n\n        args_str, options_str = break_args_options(line)\n\n        try:\n            options = shlex.split(options_str)\n        except ValueError as e:\n            raise OptionParsingError(f\"Could not split options: {options_str}\") from e\n\n        opts, _ = parser.parse_args(options, defaults)\n\n        return args_str, opts\n\n    return parse_line\n\n\ndef break_args_options(line: str) -> Tuple[str, str]:\n    \"\"\"Break up the line into an args and options string.  We only want to shlex\n    (and then optparse) the options, not the args.  args can contain markers\n    which are corrupted by shlex.\n    \"\"\"\n    tokens = line.split(\" \")\n    args = []\n    options = tokens[:]\n    for token in tokens:\n        if token.startswith(\"-\") or token.startswith(\"--\"):\n            break\n        else:\n            args.append(token)\n            options.pop(0)\n    return \" \".join(args), \" \".join(options)\n\n\nclass OptionParsingError(Exception):\n    def __init__(self, msg: str) -> None:\n        self.msg = msg\n\n\ndef build_parser() -> optparse.OptionParser:\n    \"\"\"\n    Return a parser for parsing requirement lines\n    \"\"\"\n    parser = optparse.OptionParser(add_help_option=False)\n\n    option_factories = SUPPORTED_OPTIONS + SUPPORTED_OPTIONS_REQ\n    for option_factory in option_factories:\n        option = option_factory()\n        parser.add_option(option)\n\n    # By default optparse sys.exits on parsing errors. We want to wrap\n    # that in our own exception.\n    def parser_exit(self: Any, msg: str) -> \"NoReturn\":\n        raise OptionParsingError(msg)\n\n    # NOTE: mypy disallows assigning to a method\n    #       https://github.com/python/mypy/issues/2427\n    parser.exit = parser_exit  # type: ignore\n\n    return parser\n\n\ndef join_lines(lines_enum: ReqFileLines) -> ReqFileLines:\n    \"\"\"Joins a line ending in '\\' with the previous line (except when following\n    comments).  The joined line takes on the index of the first line.\n    \"\"\"\n    primary_line_number = None\n    new_line: List[str] = []\n    for line_number, line in lines_enum:\n        if not line.endswith(\"\\\\\") or COMMENT_RE.match(line):\n            if COMMENT_RE.match(line):\n                # this ensures comments are always matched later\n                line = \" \" + line\n            if new_line:\n                new_line.append(line)\n                assert primary_line_number is not None\n                yield primary_line_number, \"\".join(new_line)\n                new_line = []\n            else:\n                yield line_number, line\n        else:\n            if not new_line:\n                primary_line_number = line_number\n            new_line.append(line.strip(\"\\\\\"))\n\n    # last line contains \\\n    if new_line:\n        assert primary_line_number is not None\n        yield primary_line_number, \"\".join(new_line)\n\n    # TODO: handle space after '\\'.\n\n\ndef ignore_comments(lines_enum: ReqFileLines) -> ReqFileLines:\n    \"\"\"\n    Strips comments and filter empty lines.\n    \"\"\"\n    for line_number, line in lines_enum:\n        line = COMMENT_RE.sub(\"\", line)\n        line = line.strip()\n        if line:\n            yield line_number, line\n\n\ndef expand_env_variables(lines_enum: ReqFileLines) -> ReqFileLines:\n    \"\"\"Replace all environment variables that can be retrieved via `os.getenv`.\n\n    The only allowed format for environment variables defined in the\n    requirement file is `${MY_VARIABLE_1}` to ensure two things:\n\n    1. Strings that contain a `$` aren't accidentally (partially) expanded.\n    2. Ensure consistency across platforms for requirement files.\n\n    These points are the result of a discussion on the `github pull\n    request #3514 <https://github.com/pypa/pip/pull/3514>`_.\n\n    Valid characters in variable names follow the `POSIX standard\n    <http://pubs.opengroup.org/onlinepubs/9699919799/>`_ and are limited\n    to uppercase letter, digits and the `_` (underscore).\n    \"\"\"\n    for line_number, line in lines_enum:\n        for env_var, var_name in ENV_VAR_RE.findall(line):\n            value = os.getenv(var_name)\n            if not value:\n                continue\n\n            line = line.replace(env_var, value)\n\n        yield line_number, line\n\n\ndef get_file_content(url: str, session: \"PipSession\") -> Tuple[str, str]:\n    \"\"\"Gets the content of a file; it may be a filename, file: URL, or\n    http: URL.  Returns (location, content).  Content is unicode.\n    Respects # -*- coding: declarations on the retrieved files.\n\n    :param url:         File path or url.\n    :param session:     PipSession instance.\n    \"\"\"\n    scheme = urllib.parse.urlsplit(url).scheme\n    # Pip has special support for file:// URLs (LocalFSAdapter).\n    if scheme in [\"http\", \"https\", \"file\"]:\n        # Delay importing heavy network modules until absolutely necessary.\n        from pip._internal.network.utils import raise_for_status\n\n        resp = session.get(url)\n        raise_for_status(resp)\n        return resp.url, resp.text\n\n    # Assume this is a bare path.\n    try:\n        with open(url, \"rb\") as f:\n            raw_content = f.read()\n    except OSError as exc:\n        raise InstallationError(f\"Could not open requirements file: {exc}\")\n\n    content = _decode_req_file(raw_content, url)\n\n    return url, content\n\n\ndef _decode_req_file(data: bytes, url: str) -> str:\n    for bom, encoding in BOMS:\n        if data.startswith(bom):\n            return data[len(bom) :].decode(encoding)\n\n    for line in data.split(b\"\\n\")[:2]:\n        if line[0:1] == b\"#\":\n            result = PEP263_ENCODING_RE.search(line)\n            if result is not None:\n                encoding = result.groups()[0].decode(\"ascii\")\n                return data.decode(encoding)\n\n    try:\n        return data.decode(DEFAULT_ENCODING)\n    except UnicodeDecodeError:\n        locale_encoding = locale.getpreferredencoding(False) or sys.getdefaultencoding()\n        logging.warning(\n            \"unable to decode data from %s with default encoding %s, \"\n            \"falling back to encoding from locale: %s. \"\n            \"If this is intentional you should specify the encoding with a \"\n            \"PEP-263 style comment, e.g. '# -*- coding: %s -*-'\",\n            url,\n            DEFAULT_ENCODING,\n            locale_encoding,\n            locale_encoding,\n        )\n        return data.decode(locale_encoding)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/req/req_install.py","size":35786,"sha1":"49886cbf707a27a3ff78d38f1a62fa6ac0789871","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import functools\nimport logging\nimport os\nimport shutil\nimport sys\nimport uuid\nimport zipfile\nfrom optparse import Values\nfrom pathlib import Path\nfrom typing import Any, Collection, Dict, Iterable, List, Optional, Sequence, Union\n\nfrom pip._vendor.packaging.markers import Marker\nfrom pip._vendor.packaging.requirements import Requirement\nfrom pip._vendor.packaging.specifiers import SpecifierSet\nfrom pip._vendor.packaging.utils import canonicalize_name\nfrom pip._vendor.packaging.version import Version\nfrom pip._vendor.packaging.version import parse as parse_version\nfrom pip._vendor.pyproject_hooks import BuildBackendHookCaller\n\nfrom pip._internal.build_env import BuildEnvironment, NoOpBuildEnvironment\nfrom pip._internal.exceptions import InstallationError, PreviousBuildDirError\nfrom pip._internal.locations import get_scheme\nfrom pip._internal.metadata import (\n    BaseDistribution,\n    get_default_environment,\n    get_directory_distribution,\n    get_wheel_distribution,\n)\nfrom pip._internal.metadata.base import FilesystemWheel\nfrom pip._internal.models.direct_url import DirectUrl\nfrom pip._internal.models.link import Link\nfrom pip._internal.operations.build.metadata import generate_metadata\nfrom pip._internal.operations.build.metadata_editable import generate_editable_metadata\nfrom pip._internal.operations.build.metadata_legacy import (\n    generate_metadata as generate_metadata_legacy,\n)\nfrom pip._internal.operations.install.editable_legacy import (\n    install_editable as install_editable_legacy,\n)\nfrom pip._internal.operations.install.wheel import install_wheel\nfrom pip._internal.pyproject import load_pyproject_toml, make_pyproject_path\nfrom pip._internal.req.req_uninstall import UninstallPathSet\nfrom pip._internal.utils.deprecation import deprecated\nfrom pip._internal.utils.hashes import Hashes\nfrom pip._internal.utils.misc import (\n    ConfiguredBuildBackendHookCaller,\n    ask_path_exists,\n    backup_dir,\n    display_path,\n    hide_url,\n    is_installable_dir,\n    redact_auth_from_requirement,\n    redact_auth_from_url,\n)\nfrom pip._internal.utils.packaging import get_requirement\nfrom pip._internal.utils.subprocess import runner_with_spinner_message\nfrom pip._internal.utils.temp_dir import TempDirectory, tempdir_kinds\nfrom pip._internal.utils.unpacking import unpack_file\nfrom pip._internal.utils.virtualenv import running_under_virtualenv\nfrom pip._internal.vcs import vcs\n\nlogger = logging.getLogger(__name__)\n\n\nclass InstallRequirement:\n    \"\"\"\n    Represents something that may be installed later on, may have information\n    about where to fetch the relevant requirement and also contains logic for\n    installing the said requirement.\n    \"\"\"\n\n    def __init__(\n        self,\n        req: Optional[Requirement],\n        comes_from: Optional[Union[str, \"InstallRequirement\"]],\n        editable: bool = False,\n        link: Optional[Link] = None,\n        markers: Optional[Marker] = None,\n        use_pep517: Optional[bool] = None,\n        isolated: bool = False,\n        *,\n        global_options: Optional[List[str]] = None,\n        hash_options: Optional[Dict[str, List[str]]] = None,\n        config_settings: Optional[Dict[str, Union[str, List[str]]]] = None,\n        constraint: bool = False,\n        extras: Collection[str] = (),\n        user_supplied: bool = False,\n        permit_editable_wheels: bool = False,\n    ) -> None:\n        assert req is None or isinstance(req, Requirement), req\n        self.req = req\n        self.comes_from = comes_from\n        self.constraint = constraint\n        self.editable = editable\n        self.permit_editable_wheels = permit_editable_wheels\n\n        # source_dir is the local directory where the linked requirement is\n        # located, or unpacked. In case unpacking is needed, creating and\n        # populating source_dir is done by the RequirementPreparer. Note this\n        # is not necessarily the directory where pyproject.toml or setup.py is\n        # located - that one is obtained via unpacked_source_directory.\n        self.source_dir: Optional[str] = None\n        if self.editable:\n            assert link\n            if link.is_file:\n                self.source_dir = os.path.normpath(os.path.abspath(link.file_path))\n\n        # original_link is the direct URL that was provided by the user for the\n        # requirement, either directly or via a constraints file.\n        if link is None and req and req.url:\n            # PEP 508 URL requirement\n            link = Link(req.url)\n        self.link = self.original_link = link\n\n        # When this InstallRequirement is a wheel obtained from the cache of locally\n        # built wheels, this is the source link corresponding to the cache entry, which\n        # was used to download and build the cached wheel.\n        self.cached_wheel_source_link: Optional[Link] = None\n\n        # Information about the location of the artifact that was downloaded . This\n        # property is guaranteed to be set in resolver results.\n        self.download_info: Optional[DirectUrl] = None\n\n        # Path to any downloaded or already-existing package.\n        self.local_file_path: Optional[str] = None\n        if self.link and self.link.is_file:\n            self.local_file_path = self.link.file_path\n\n        if extras:\n            self.extras = extras\n        elif req:\n            self.extras = req.extras\n        else:\n            self.extras = set()\n        if markers is None and req:\n            markers = req.marker\n        self.markers = markers\n\n        # This holds the Distribution object if this requirement is already installed.\n        self.satisfied_by: Optional[BaseDistribution] = None\n        # Whether the installation process should try to uninstall an existing\n        # distribution before installing this requirement.\n        self.should_reinstall = False\n        # Temporary build location\n        self._temp_build_dir: Optional[TempDirectory] = None\n        # Set to True after successful installation\n        self.install_succeeded: Optional[bool] = None\n        # Supplied options\n        self.global_options = global_options if global_options else []\n        self.hash_options = hash_options if hash_options else {}\n        self.config_settings = config_settings\n        # Set to True after successful preparation of this requirement\n        self.prepared = False\n        # User supplied requirement are explicitly requested for installation\n        # by the user via CLI arguments or requirements files, as opposed to,\n        # e.g. dependencies, extras or constraints.\n        self.user_supplied = user_supplied\n\n        self.isolated = isolated\n        self.build_env: BuildEnvironment = NoOpBuildEnvironment()\n\n        # For PEP 517, the directory where we request the project metadata\n        # gets stored. We need this to pass to build_wheel, so the backend\n        # can ensure that the wheel matches the metadata (see the PEP for\n        # details).\n        self.metadata_directory: Optional[str] = None\n\n        # The static build requirements (from pyproject.toml)\n        self.pyproject_requires: Optional[List[str]] = None\n\n        # Build requirements that we will check are available\n        self.requirements_to_check: List[str] = []\n\n        # The PEP 517 backend we should use to build the project\n        self.pep517_backend: Optional[BuildBackendHookCaller] = None\n\n        # Are we using PEP 517 for this requirement?\n        # After pyproject.toml has been loaded, the only valid values are True\n        # and False. Before loading, None is valid (meaning \"use the default\").\n        # Setting an explicit value before loading pyproject.toml is supported,\n        # but after loading this flag should be treated as read only.\n        self.use_pep517 = use_pep517\n\n        # If config settings are provided, enforce PEP 517.\n        if self.config_settings:\n            if self.use_pep517 is False:\n                logger.warning(\n                    \"--no-use-pep517 ignored for %s \"\n                    \"because --config-settings are specified.\",\n                    self,\n                )\n            self.use_pep517 = True\n\n        # This requirement needs more preparation before it can be built\n        self.needs_more_preparation = False\n\n        # This requirement needs to be unpacked before it can be installed.\n        self._archive_source: Optional[Path] = None\n\n    def __str__(self) -> str:\n        if self.req:\n            s = redact_auth_from_requirement(self.req)\n            if self.link:\n                s += f\" from {redact_auth_from_url(self.link.url)}\"\n        elif self.link:\n            s = redact_auth_from_url(self.link.url)\n        else:\n            s = \"<InstallRequirement>\"\n        if self.satisfied_by is not None:\n            if self.satisfied_by.location is not None:\n                location = display_path(self.satisfied_by.location)\n            else:\n                location = \"<memory>\"\n            s += f\" in {location}\"\n        if self.comes_from:\n            if isinstance(self.comes_from, str):\n                comes_from: Optional[str] = self.comes_from\n            else:\n                comes_from = self.comes_from.from_path()\n            if comes_from:\n                s += f\" (from {comes_from})\"\n        return s\n\n    def __repr__(self) -> str:\n        return (\n            f\"<{self.__class__.__name__} object: \"\n            f\"{str(self)} editable={self.editable!r}>\"\n        )\n\n    def format_debug(self) -> str:\n        \"\"\"An un-tested helper for getting state, for debugging.\"\"\"\n        attributes = vars(self)\n        names = sorted(attributes)\n\n        state = (f\"{attr}={attributes[attr]!r}\" for attr in sorted(names))\n        return \"<{name} object: {{{state}}}>\".format(\n            name=self.__class__.__name__,\n            state=\", \".join(state),\n        )\n\n    # Things that are valid for all kinds of requirements?\n    @property\n    def name(self) -> Optional[str]:\n        if self.req is None:\n            return None\n        return self.req.name\n\n    @functools.cached_property\n    def supports_pyproject_editable(self) -> bool:\n        if not self.use_pep517:\n            return False\n        assert self.pep517_backend\n        with self.build_env:\n            runner = runner_with_spinner_message(\n                \"Checking if build backend supports build_editable\"\n            )\n            with self.pep517_backend.subprocess_runner(runner):\n                return \"build_editable\" in self.pep517_backend._supported_features()\n\n    @property\n    def specifier(self) -> SpecifierSet:\n        assert self.req is not None\n        return self.req.specifier\n\n    @property\n    def is_direct(self) -> bool:\n        \"\"\"Whether this requirement was specified as a direct URL.\"\"\"\n        return self.original_link is not None\n\n    @property\n    def is_pinned(self) -> bool:\n        \"\"\"Return whether I am pinned to an exact version.\n\n        For example, some-package==1.2 is pinned; some-package>1.2 is not.\n        \"\"\"\n        assert self.req is not None\n        specifiers = self.req.specifier\n        return len(specifiers) == 1 and next(iter(specifiers)).operator in {\"==\", \"===\"}\n\n    def match_markers(self, extras_requested: Optional[Iterable[str]] = None) -> bool:\n        if not extras_requested:\n            # Provide an extra to safely evaluate the markers\n            # without matching any extra\n            extras_requested = (\"\",)\n        if self.markers is not None:\n            return any(\n                self.markers.evaluate({\"extra\": extra}) for extra in extras_requested\n            )\n        else:\n            return True\n\n    @property\n    def has_hash_options(self) -> bool:\n        \"\"\"Return whether any known-good hashes are specified as options.\n\n        These activate --require-hashes mode; hashes specified as part of a\n        URL do not.\n\n        \"\"\"\n        return bool(self.hash_options)\n\n    def hashes(self, trust_internet: bool = True) -> Hashes:\n        \"\"\"Return a hash-comparer that considers my option- and URL-based\n        hashes to be known-good.\n\n        Hashes in URLs--ones embedded in the requirements file, not ones\n        downloaded from an index server--are almost peers with ones from\n        flags. They satisfy --require-hashes (whether it was implicitly or\n        explicitly activated) but do not activate it. md5 and sha224 are not\n        allowed in flags, which should nudge people toward good algos. We\n        always OR all hashes together, even ones from URLs.\n\n        :param trust_internet: Whether to trust URL-based (#md5=...) hashes\n            downloaded from the internet, as by populate_link()\n\n        \"\"\"\n        good_hashes = self.hash_options.copy()\n        if trust_internet:\n            link = self.link\n        elif self.is_direct and self.user_supplied:\n            link = self.original_link\n        else:\n            link = None\n        if link and link.hash:\n            assert link.hash_name is not None\n            good_hashes.setdefault(link.hash_name, []).append(link.hash)\n        return Hashes(good_hashes)\n\n    def from_path(self) -> Optional[str]:\n        \"\"\"Format a nice indicator to show where this \"comes from\" \"\"\"\n        if self.req is None:\n            return None\n        s = str(self.req)\n        if self.comes_from:\n            comes_from: Optional[str]\n            if isinstance(self.comes_from, str):\n                comes_from = self.comes_from\n            else:\n                comes_from = self.comes_from.from_path()\n            if comes_from:\n                s += \"->\" + comes_from\n        return s\n\n    def ensure_build_location(\n        self, build_dir: str, autodelete: bool, parallel_builds: bool\n    ) -> str:\n        assert build_dir is not None\n        if self._temp_build_dir is not None:\n            assert self._temp_build_dir.path\n            return self._temp_build_dir.path\n        if self.req is None:\n            # Some systems have /tmp as a symlink which confuses custom\n            # builds (such as numpy). Thus, we ensure that the real path\n            # is returned.\n            self._temp_build_dir = TempDirectory(\n                kind=tempdir_kinds.REQ_BUILD, globally_managed=True\n            )\n\n            return self._temp_build_dir.path\n\n        # This is the only remaining place where we manually determine the path\n        # for the temporary directory. It is only needed for editables where\n        # it is the value of the --src option.\n\n        # When parallel builds are enabled, add a UUID to the build directory\n        # name so multiple builds do not interfere with each other.\n        dir_name: str = canonicalize_name(self.req.name)\n        if parallel_builds:\n            dir_name = f\"{dir_name}_{uuid.uuid4().hex}\"\n\n        # FIXME: Is there a better place to create the build_dir? (hg and bzr\n        # need this)\n        if not os.path.exists(build_dir):\n            logger.debug(\"Creating directory %s\", build_dir)\n            os.makedirs(build_dir)\n        actual_build_dir = os.path.join(build_dir, dir_name)\n        # `None` indicates that we respect the globally-configured deletion\n        # settings, which is what we actually want when auto-deleting.\n        delete_arg = None if autodelete else False\n        return TempDirectory(\n            path=actual_build_dir,\n            delete=delete_arg,\n            kind=tempdir_kinds.REQ_BUILD,\n            globally_managed=True,\n        ).path\n\n    def _set_requirement(self) -> None:\n        \"\"\"Set requirement after generating metadata.\"\"\"\n        assert self.req is None\n        assert self.metadata is not None\n        assert self.source_dir is not None\n\n        # Construct a Requirement object from the generated metadata\n        if isinstance(parse_version(self.metadata[\"Version\"]), Version):\n            op = \"==\"\n        else:\n            op = \"===\"\n\n        self.req = get_requirement(\n            \"\".join(\n                [\n                    self.metadata[\"Name\"],\n                    op,\n                    self.metadata[\"Version\"],\n                ]\n            )\n        )\n\n    def warn_on_mismatching_name(self) -> None:\n        assert self.req is not None\n        metadata_name = canonicalize_name(self.metadata[\"Name\"])\n        if canonicalize_name(self.req.name) == metadata_name:\n            # Everything is fine.\n            return\n\n        # If we're here, there's a mismatch. Log a warning about it.\n        logger.warning(\n            \"Generating metadata for package %s \"\n            \"produced metadata for project name %s. Fix your \"\n            \"#egg=%s fragments.\",\n            self.name,\n            metadata_name,\n            self.name,\n        )\n        self.req = get_requirement(metadata_name)\n\n    def check_if_exists(self, use_user_site: bool) -> None:\n        \"\"\"Find an installed distribution that satisfies or conflicts\n        with this requirement, and set self.satisfied_by or\n        self.should_reinstall appropriately.\n        \"\"\"\n        if self.req is None:\n            return\n        existing_dist = get_default_environment().get_distribution(self.req.name)\n        if not existing_dist:\n            return\n\n        version_compatible = self.req.specifier.contains(\n            existing_dist.version,\n            prereleases=True,\n        )\n        if not version_compatible:\n            self.satisfied_by = None\n            if use_user_site:\n                if existing_dist.in_usersite:\n                    self.should_reinstall = True\n                elif running_under_virtualenv() and existing_dist.in_site_packages:\n                    raise InstallationError(\n                        f\"Will not install to the user site because it will \"\n                        f\"lack sys.path precedence to {existing_dist.raw_name} \"\n                        f\"in {existing_dist.location}\"\n                    )\n            else:\n                self.should_reinstall = True\n        else:\n            if self.editable:\n                self.should_reinstall = True\n                # when installing editables, nothing pre-existing should ever\n                # satisfy\n                self.satisfied_by = None\n            else:\n                self.satisfied_by = existing_dist\n\n    # Things valid for wheels\n    @property\n    def is_wheel(self) -> bool:\n        if not self.link:\n            return False\n        return self.link.is_wheel\n\n    @property\n    def is_wheel_from_cache(self) -> bool:\n        # When True, it means that this InstallRequirement is a local wheel file in the\n        # cache of locally built wheels.\n        return self.cached_wheel_source_link is not None\n\n    # Things valid for sdists\n    @property\n    def unpacked_source_directory(self) -> str:\n        assert self.source_dir, f\"No source dir for {self}\"\n        return os.path.join(\n            self.source_dir, self.link and self.link.subdirectory_fragment or \"\"\n        )\n\n    @property\n    def setup_py_path(self) -> str:\n        assert self.source_dir, f\"No source dir for {self}\"\n        setup_py = os.path.join(self.unpacked_source_directory, \"setup.py\")\n\n        return setup_py\n\n    @property\n    def setup_cfg_path(self) -> str:\n        assert self.source_dir, f\"No source dir for {self}\"\n        setup_cfg = os.path.join(self.unpacked_source_directory, \"setup.cfg\")\n\n        return setup_cfg\n\n    @property\n    def pyproject_toml_path(self) -> str:\n        assert self.source_dir, f\"No source dir for {self}\"\n        return make_pyproject_path(self.unpacked_source_directory)\n\n    def load_pyproject_toml(self) -> None:\n        \"\"\"Load the pyproject.toml file.\n\n        After calling this routine, all of the attributes related to PEP 517\n        processing for this requirement have been set. In particular, the\n        use_pep517 attribute can be used to determine whether we should\n        follow the PEP 517 or legacy (setup.py) code path.\n        \"\"\"\n        pyproject_toml_data = load_pyproject_toml(\n            self.use_pep517, self.pyproject_toml_path, self.setup_py_path, str(self)\n        )\n\n        if pyproject_toml_data is None:\n            assert not self.config_settings\n            self.use_pep517 = False\n            return\n\n        self.use_pep517 = True\n        requires, backend, check, backend_path = pyproject_toml_data\n        self.requirements_to_check = check\n        self.pyproject_requires = requires\n        self.pep517_backend = ConfiguredBuildBackendHookCaller(\n            self,\n            self.unpacked_source_directory,\n            backend,\n            backend_path=backend_path,\n        )\n\n    def isolated_editable_sanity_check(self) -> None:\n        \"\"\"Check that an editable requirement if valid for use with PEP 517/518.\n\n        This verifies that an editable that has a pyproject.toml either supports PEP 660\n        or as a setup.py or a setup.cfg\n        \"\"\"\n        if (\n            self.editable\n            and self.use_pep517\n            and not self.supports_pyproject_editable\n            and not os.path.isfile(self.setup_py_path)\n            and not os.path.isfile(self.setup_cfg_path)\n        ):\n            raise InstallationError(\n                f\"Project {self} has a 'pyproject.toml' and its build \"\n                f\"backend is missing the 'build_editable' hook. Since it does not \"\n                f\"have a 'setup.py' nor a 'setup.cfg', \"\n                f\"it cannot be installed in editable mode. \"\n                f\"Consider using a build backend that supports PEP 660.\"\n            )\n\n    def prepare_metadata(self) -> None:\n        \"\"\"Ensure that project metadata is available.\n\n        Under PEP 517 and PEP 660, call the backend hook to prepare the metadata.\n        Under legacy processing, call setup.py egg-info.\n        \"\"\"\n        assert self.source_dir, f\"No source dir for {self}\"\n        details = self.name or f\"from {self.link}\"\n\n        if self.use_pep517:\n            assert self.pep517_backend is not None\n            if (\n                self.editable\n                and self.permit_editable_wheels\n                and self.supports_pyproject_editable\n            ):\n                self.metadata_directory = generate_editable_metadata(\n                    build_env=self.build_env,\n                    backend=self.pep517_backend,\n                    details=details,\n                )\n            else:\n                self.metadata_directory = generate_metadata(\n                    build_env=self.build_env,\n                    backend=self.pep517_backend,\n                    details=details,\n                )\n        else:\n            self.metadata_directory = generate_metadata_legacy(\n                build_env=self.build_env,\n                setup_py_path=self.setup_py_path,\n                source_dir=self.unpacked_source_directory,\n                isolated=self.isolated,\n                details=details,\n            )\n\n        # Act on the newly generated metadata, based on the name and version.\n        if not self.name:\n            self._set_requirement()\n        else:\n            self.warn_on_mismatching_name()\n\n        self.assert_source_matches_version()\n\n    @property\n    def metadata(self) -> Any:\n        if not hasattr(self, \"_metadata\"):\n            self._metadata = self.get_dist().metadata\n\n        return self._metadata\n\n    def get_dist(self) -> BaseDistribution:\n        if self.metadata_directory:\n            return get_directory_distribution(self.metadata_directory)\n        elif self.local_file_path and self.is_wheel:\n            assert self.req is not None\n            return get_wheel_distribution(\n                FilesystemWheel(self.local_file_path),\n                canonicalize_name(self.req.name),\n            )\n        raise AssertionError(\n            f\"InstallRequirement {self} has no metadata directory and no wheel: \"\n            f\"can't make a distribution.\"\n        )\n\n    def assert_source_matches_version(self) -> None:\n        assert self.source_dir, f\"No source dir for {self}\"\n        version = self.metadata[\"version\"]\n        if self.req and self.req.specifier and version not in self.req.specifier:\n            logger.warning(\n                \"Requested %s, but installing version %s\",\n                self,\n                version,\n            )\n        else:\n            logger.debug(\n                \"Source in %s has version %s, which satisfies requirement %s\",\n                display_path(self.source_dir),\n                version,\n                self,\n            )\n\n    # For both source distributions and editables\n    def ensure_has_source_dir(\n        self,\n        parent_dir: str,\n        autodelete: bool = False,\n        parallel_builds: bool = False,\n    ) -> None:\n        \"\"\"Ensure that a source_dir is set.\n\n        This will create a temporary build dir if the name of the requirement\n        isn't known yet.\n\n        :param parent_dir: The ideal pip parent_dir for the source_dir.\n            Generally src_dir for editables and build_dir for sdists.\n        :return: self.source_dir\n        \"\"\"\n        if self.source_dir is None:\n            self.source_dir = self.ensure_build_location(\n                parent_dir,\n                autodelete=autodelete,\n                parallel_builds=parallel_builds,\n            )\n\n    def needs_unpacked_archive(self, archive_source: Path) -> None:\n        assert self._archive_source is None\n        self._archive_source = archive_source\n\n    def ensure_pristine_source_checkout(self) -> None:\n        \"\"\"Ensure the source directory has not yet been built in.\"\"\"\n        assert self.source_dir is not None\n        if self._archive_source is not None:\n            unpack_file(str(self._archive_source), self.source_dir)\n        elif is_installable_dir(self.source_dir):\n            # If a checkout exists, it's unwise to keep going.\n            # version inconsistencies are logged later, but do not fail\n            # the installation.\n            raise PreviousBuildDirError(\n                f\"pip can't proceed with requirements '{self}' due to a \"\n                f\"pre-existing build directory ({self.source_dir}). This is likely \"\n                \"due to a previous installation that failed . pip is \"\n                \"being responsible and not assuming it can delete this. \"\n                \"Please delete it and try again.\"\n            )\n\n    # For editable installations\n    def update_editable(self) -> None:\n        if not self.link:\n            logger.debug(\n                \"Cannot update repository at %s; repository location is unknown\",\n                self.source_dir,\n            )\n            return\n        assert self.editable\n        assert self.source_dir\n        if self.link.scheme == \"file\":\n            # Static paths don't get updated\n            return\n        vcs_backend = vcs.get_backend_for_scheme(self.link.scheme)\n        # Editable requirements are validated in Requirement constructors.\n        # So here, if it's neither a path nor a valid VCS URL, it's a bug.\n        assert vcs_backend, f\"Unsupported VCS URL {self.link.url}\"\n        hidden_url = hide_url(self.link.url)\n        vcs_backend.obtain(self.source_dir, url=hidden_url, verbosity=0)\n\n    # Top-level Actions\n    def uninstall(\n        self, auto_confirm: bool = False, verbose: bool = False\n    ) -> Optional[UninstallPathSet]:\n        \"\"\"\n        Uninstall the distribution currently satisfying this requirement.\n\n        Prompts before removing or modifying files unless\n        ``auto_confirm`` is True.\n\n        Refuses to delete or modify files outside of ``sys.prefix`` -\n        thus uninstallation within a virtual environment can only\n        modify that virtual environment, even if the virtualenv is\n        linked to global site-packages.\n\n        \"\"\"\n        assert self.req\n        dist = get_default_environment().get_distribution(self.req.name)\n        if not dist:\n            logger.warning(\"Skipping %s as it is not installed.\", self.name)\n            return None\n        logger.info(\"Found existing installation: %s\", dist)\n\n        uninstalled_pathset = UninstallPathSet.from_dist(dist)\n        uninstalled_pathset.remove(auto_confirm, verbose)\n        return uninstalled_pathset\n\n    def _get_archive_name(self, path: str, parentdir: str, rootdir: str) -> str:\n        def _clean_zip_name(name: str, prefix: str) -> str:\n            assert name.startswith(\n                prefix + os.path.sep\n            ), f\"name {name!r} doesn't start with prefix {prefix!r}\"\n            name = name[len(prefix) + 1 :]\n            name = name.replace(os.path.sep, \"/\")\n            return name\n\n        assert self.req is not None\n        path = os.path.join(parentdir, path)\n        name = _clean_zip_name(path, rootdir)\n        return self.req.name + \"/\" + name\n\n    def archive(self, build_dir: Optional[str]) -> None:\n        \"\"\"Saves archive to provided build_dir.\n\n        Used for saving downloaded VCS requirements as part of `pip download`.\n        \"\"\"\n        assert self.source_dir\n        if build_dir is None:\n            return\n\n        create_archive = True\n        archive_name = \"{}-{}.zip\".format(self.name, self.metadata[\"version\"])\n        archive_path = os.path.join(build_dir, archive_name)\n\n        if os.path.exists(archive_path):\n            response = ask_path_exists(\n                f\"The file {display_path(archive_path)} exists. (i)gnore, (w)ipe, \"\n                \"(b)ackup, (a)bort \",\n                (\"i\", \"w\", \"b\", \"a\"),\n            )\n            if response == \"i\":\n                create_archive = False\n            elif response == \"w\":\n                logger.warning(\"Deleting %s\", display_path(archive_path))\n                os.remove(archive_path)\n            elif response == \"b\":\n                dest_file = backup_dir(archive_path)\n                logger.warning(\n                    \"Backing up %s to %s\",\n                    display_path(archive_path),\n                    display_path(dest_file),\n                )\n                shutil.move(archive_path, dest_file)\n            elif response == \"a\":\n                sys.exit(-1)\n\n        if not create_archive:\n            return\n\n        zip_output = zipfile.ZipFile(\n            archive_path,\n            \"w\",\n            zipfile.ZIP_DEFLATED,\n            allowZip64=True,\n        )\n        with zip_output:\n            dir = os.path.normcase(os.path.abspath(self.unpacked_source_directory))\n            for dirpath, dirnames, filenames in os.walk(dir):\n                for dirname in dirnames:\n                    dir_arcname = self._get_archive_name(\n                        dirname,\n                        parentdir=dirpath,\n                        rootdir=dir,\n                    )\n                    zipdir = zipfile.ZipInfo(dir_arcname + \"/\")\n                    zipdir.external_attr = 0x1ED << 16  # 0o755\n                    zip_output.writestr(zipdir, \"\")\n                for filename in filenames:\n                    file_arcname = self._get_archive_name(\n                        filename,\n                        parentdir=dirpath,\n                        rootdir=dir,\n                    )\n                    filename = os.path.join(dirpath, filename)\n                    zip_output.write(filename, file_arcname)\n\n        logger.info(\"Saved %s\", display_path(archive_path))\n\n    def install(\n        self,\n        global_options: Optional[Sequence[str]] = None,\n        root: Optional[str] = None,\n        home: Optional[str] = None,\n        prefix: Optional[str] = None,\n        warn_script_location: bool = True,\n        use_user_site: bool = False,\n        pycompile: bool = True,\n    ) -> None:\n        assert self.req is not None\n        scheme = get_scheme(\n            self.req.name,\n            user=use_user_site,\n            home=home,\n            root=root,\n            isolated=self.isolated,\n            prefix=prefix,\n        )\n\n        if self.editable and not self.is_wheel:\n            deprecated(\n                reason=(\n                    f\"Legacy editable install of {self} (setup.py develop) \"\n                    \"is deprecated.\"\n                ),\n                replacement=(\n                    \"to add a pyproject.toml or enable --use-pep517, \"\n                    \"and use setuptools >= 64. \"\n                    \"If the resulting installation is not behaving as expected, \"\n                    \"try using --config-settings editable_mode=compat. \"\n                    \"Please consult the setuptools documentation for more information\"\n                ),\n                gone_in=\"25.1\",\n                issue=11457,\n            )\n            if self.config_settings:\n                logger.warning(\n                    \"--config-settings ignored for legacy editable install of %s. \"\n                    \"Consider upgrading to a version of setuptools \"\n                    \"that supports PEP 660 (>= 64).\",\n                    self,\n                )\n            install_editable_legacy(\n                global_options=global_options if global_options is not None else [],\n                prefix=prefix,\n                home=home,\n                use_user_site=use_user_site,\n                name=self.req.name,\n                setup_py_path=self.setup_py_path,\n                isolated=self.isolated,\n                build_env=self.build_env,\n                unpacked_source_directory=self.unpacked_source_directory,\n            )\n            self.install_succeeded = True\n            return\n\n        assert self.is_wheel\n        assert self.local_file_path\n\n        install_wheel(\n            self.req.name,\n            self.local_file_path,\n            scheme=scheme,\n            req_description=str(self.req),\n            pycompile=pycompile,\n            warn_script_location=warn_script_location,\n            direct_url=self.download_info if self.is_direct else None,\n            requested=self.user_supplied,\n        )\n        self.install_succeeded = True\n\n\ndef check_invalid_constraint_type(req: InstallRequirement) -> str:\n    # Check for unsupported forms\n    problem = \"\"\n    if not req.name:\n        problem = \"Unnamed requirements are not allowed as constraints\"\n    elif req.editable:\n        problem = \"Editable requirements are not allowed as constraints\"\n    elif req.extras:\n        problem = \"Constraints cannot have extras\"\n\n    if problem:\n        deprecated(\n            reason=(\n                \"Constraints are only allowed to take the form of a package \"\n                \"name and a version specifier. Other forms were originally \"\n                \"permitted as an accident of the implementation, but were \"\n                \"undocumented. The new implementation of the resolver no \"\n                \"longer supports these forms.\"\n            ),\n            replacement=\"replacing the constraint with a requirement\",\n            # No plan yet for when the new resolver becomes default\n            gone_in=None,\n            issue=8210,\n        )\n\n    return problem\n\n\ndef _has_option(options: Values, reqs: List[InstallRequirement], option: str) -> bool:\n    if getattr(options, option, None):\n        return True\n    for req in reqs:\n        if getattr(req, option, None):\n            return True\n    return False\n\n\ndef check_legacy_setup_py_options(\n    options: Values,\n    reqs: List[InstallRequirement],\n) -> None:\n    has_build_options = _has_option(options, reqs, \"build_options\")\n    has_global_options = _has_option(options, reqs, \"global_options\")\n    if has_build_options or has_global_options:\n        deprecated(\n            reason=\"--build-option and --global-option are deprecated.\",\n            issue=11859,\n            replacement=\"to use --config-settings\",\n            gone_in=None,\n        )\n        logger.warning(\n            \"Implying --no-binary=:all: due to the presence of \"\n            \"--build-option / --global-option. \"\n        )\n        options.format_control.disallow_binaries()\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/req/req_set.py","size":2858,"sha1":"f4e5b3a69525d2d7dde180fc39de4188c85ea89e","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import logging\nfrom collections import OrderedDict\nfrom typing import Dict, List\n\nfrom pip._vendor.packaging.utils import canonicalize_name\n\nfrom pip._internal.req.req_install import InstallRequirement\n\nlogger = logging.getLogger(__name__)\n\n\nclass RequirementSet:\n    def __init__(self, check_supported_wheels: bool = True) -> None:\n        \"\"\"Create a RequirementSet.\"\"\"\n\n        self.requirements: Dict[str, InstallRequirement] = OrderedDict()\n        self.check_supported_wheels = check_supported_wheels\n\n        self.unnamed_requirements: List[InstallRequirement] = []\n\n    def __str__(self) -> str:\n        requirements = sorted(\n            (req for req in self.requirements.values() if not req.comes_from),\n            key=lambda req: canonicalize_name(req.name or \"\"),\n        )\n        return \" \".join(str(req.req) for req in requirements)\n\n    def __repr__(self) -> str:\n        requirements = sorted(\n            self.requirements.values(),\n            key=lambda req: canonicalize_name(req.name or \"\"),\n        )\n\n        format_string = \"<{classname} object; {count} requirement(s): {reqs}>\"\n        return format_string.format(\n            classname=self.__class__.__name__,\n            count=len(requirements),\n            reqs=\", \".join(str(req.req) for req in requirements),\n        )\n\n    def add_unnamed_requirement(self, install_req: InstallRequirement) -> None:\n        assert not install_req.name\n        self.unnamed_requirements.append(install_req)\n\n    def add_named_requirement(self, install_req: InstallRequirement) -> None:\n        assert install_req.name\n\n        project_name = canonicalize_name(install_req.name)\n        self.requirements[project_name] = install_req\n\n    def has_requirement(self, name: str) -> bool:\n        project_name = canonicalize_name(name)\n\n        return (\n            project_name in self.requirements\n            and not self.requirements[project_name].constraint\n        )\n\n    def get_requirement(self, name: str) -> InstallRequirement:\n        project_name = canonicalize_name(name)\n\n        if project_name in self.requirements:\n            return self.requirements[project_name]\n\n        raise KeyError(f\"No project with the name {name!r}\")\n\n    @property\n    def all_requirements(self) -> List[InstallRequirement]:\n        return self.unnamed_requirements + list(self.requirements.values())\n\n    @property\n    def requirements_to_install(self) -> List[InstallRequirement]:\n        \"\"\"Return the list of requirements that need to be installed.\n\n        TODO remove this property together with the legacy resolver, since the new\n             resolver only returns requirements that need to be installed.\n        \"\"\"\n        return [\n            install_req\n            for install_req in self.all_requirements\n            if not install_req.constraint and not install_req.satisfied_by\n        ]\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/req/req_uninstall.py","size":23853,"sha1":"564dae23c51ac36d510f629a9f1df838aba8fa14","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import functools\nimport os\nimport sys\nimport sysconfig\nfrom importlib.util import cache_from_source\nfrom typing import Any, Callable, Dict, Generator, Iterable, List, Optional, Set, Tuple\n\nfrom pip._internal.exceptions import LegacyDistutilsInstall, UninstallMissingRecord\nfrom pip._internal.locations import get_bin_prefix, get_bin_user\nfrom pip._internal.metadata import BaseDistribution\nfrom pip._internal.utils.compat import WINDOWS\nfrom pip._internal.utils.egg_link import egg_link_path_from_location\nfrom pip._internal.utils.logging import getLogger, indent_log\nfrom pip._internal.utils.misc import ask, normalize_path, renames, rmtree\nfrom pip._internal.utils.temp_dir import AdjacentTempDirectory, TempDirectory\nfrom pip._internal.utils.virtualenv import running_under_virtualenv\n\nlogger = getLogger(__name__)\n\n\ndef _script_names(\n    bin_dir: str, script_name: str, is_gui: bool\n) -> Generator[str, None, None]:\n    \"\"\"Create the fully qualified name of the files created by\n    {console,gui}_scripts for the given ``dist``.\n    Returns the list of file names\n    \"\"\"\n    exe_name = os.path.join(bin_dir, script_name)\n    yield exe_name\n    if not WINDOWS:\n        return\n    yield f\"{exe_name}.exe\"\n    yield f\"{exe_name}.exe.manifest\"\n    if is_gui:\n        yield f\"{exe_name}-script.pyw\"\n    else:\n        yield f\"{exe_name}-script.py\"\n\n\ndef _unique(\n    fn: Callable[..., Generator[Any, None, None]]\n) -> Callable[..., Generator[Any, None, None]]:\n    @functools.wraps(fn)\n    def unique(*args: Any, **kw: Any) -> Generator[Any, None, None]:\n        seen: Set[Any] = set()\n        for item in fn(*args, **kw):\n            if item not in seen:\n                seen.add(item)\n                yield item\n\n    return unique\n\n\n@_unique\ndef uninstallation_paths(dist: BaseDistribution) -> Generator[str, None, None]:\n    \"\"\"\n    Yield all the uninstallation paths for dist based on RECORD-without-.py[co]\n\n    Yield paths to all the files in RECORD. For each .py file in RECORD, add\n    the .pyc and .pyo in the same directory.\n\n    UninstallPathSet.add() takes care of the __pycache__ .py[co].\n\n    If RECORD is not found, raises an error,\n    with possible information from the INSTALLER file.\n\n    https://packaging.python.org/specifications/recording-installed-packages/\n    \"\"\"\n    location = dist.location\n    assert location is not None, \"not installed\"\n\n    entries = dist.iter_declared_entries()\n    if entries is None:\n        raise UninstallMissingRecord(distribution=dist)\n\n    for entry in entries:\n        path = os.path.join(location, entry)\n        yield path\n        if path.endswith(\".py\"):\n            dn, fn = os.path.split(path)\n            base = fn[:-3]\n            path = os.path.join(dn, base + \".pyc\")\n            yield path\n            path = os.path.join(dn, base + \".pyo\")\n            yield path\n\n\ndef compact(paths: Iterable[str]) -> Set[str]:\n    \"\"\"Compact a path set to contain the minimal number of paths\n    necessary to contain all paths in the set. If /a/path/ and\n    /a/path/to/a/file.txt are both in the set, leave only the\n    shorter path.\"\"\"\n\n    sep = os.path.sep\n    short_paths: Set[str] = set()\n    for path in sorted(paths, key=len):\n        should_skip = any(\n            path.startswith(shortpath.rstrip(\"*\"))\n            and path[len(shortpath.rstrip(\"*\").rstrip(sep))] == sep\n            for shortpath in short_paths\n        )\n        if not should_skip:\n            short_paths.add(path)\n    return short_paths\n\n\ndef compress_for_rename(paths: Iterable[str]) -> Set[str]:\n    \"\"\"Returns a set containing the paths that need to be renamed.\n\n    This set may include directories when the original sequence of paths\n    included every file on disk.\n    \"\"\"\n    case_map = {os.path.normcase(p): p for p in paths}\n    remaining = set(case_map)\n    unchecked = sorted({os.path.split(p)[0] for p in case_map.values()}, key=len)\n    wildcards: Set[str] = set()\n\n    def norm_join(*a: str) -> str:\n        return os.path.normcase(os.path.join(*a))\n\n    for root in unchecked:\n        if any(os.path.normcase(root).startswith(w) for w in wildcards):\n            # This directory has already been handled.\n            continue\n\n        all_files: Set[str] = set()\n        all_subdirs: Set[str] = set()\n        for dirname, subdirs, files in os.walk(root):\n            all_subdirs.update(norm_join(root, dirname, d) for d in subdirs)\n            all_files.update(norm_join(root, dirname, f) for f in files)\n        # If all the files we found are in our remaining set of files to\n        # remove, then remove them from the latter set and add a wildcard\n        # for the directory.\n        if not (all_files - remaining):\n            remaining.difference_update(all_files)\n            wildcards.add(root + os.sep)\n\n    return set(map(case_map.__getitem__, remaining)) | wildcards\n\n\ndef compress_for_output_listing(paths: Iterable[str]) -> Tuple[Set[str], Set[str]]:\n    \"\"\"Returns a tuple of 2 sets of which paths to display to user\n\n    The first set contains paths that would be deleted. Files of a package\n    are not added and the top-level directory of the package has a '*' added\n    at the end - to signify that all it's contents are removed.\n\n    The second set contains files that would have been skipped in the above\n    folders.\n    \"\"\"\n\n    will_remove = set(paths)\n    will_skip = set()\n\n    # Determine folders and files\n    folders = set()\n    files = set()\n    for path in will_remove:\n        if path.endswith(\".pyc\"):\n            continue\n        if path.endswith(\"__init__.py\") or \".dist-info\" in path:\n            folders.add(os.path.dirname(path))\n        files.add(path)\n\n    _normcased_files = set(map(os.path.normcase, files))\n\n    folders = compact(folders)\n\n    # This walks the tree using os.walk to not miss extra folders\n    # that might get added.\n    for folder in folders:\n        for dirpath, _, dirfiles in os.walk(folder):\n            for fname in dirfiles:\n                if fname.endswith(\".pyc\"):\n                    continue\n\n                file_ = os.path.join(dirpath, fname)\n                if (\n                    os.path.isfile(file_)\n                    and os.path.normcase(file_) not in _normcased_files\n                ):\n                    # We are skipping this file. Add it to the set.\n                    will_skip.add(file_)\n\n    will_remove = files | {os.path.join(folder, \"*\") for folder in folders}\n\n    return will_remove, will_skip\n\n\nclass StashedUninstallPathSet:\n    \"\"\"A set of file rename operations to stash files while\n    tentatively uninstalling them.\"\"\"\n\n    def __init__(self) -> None:\n        # Mapping from source file root to [Adjacent]TempDirectory\n        # for files under that directory.\n        self._save_dirs: Dict[str, TempDirectory] = {}\n        # (old path, new path) tuples for each move that may need\n        # to be undone.\n        self._moves: List[Tuple[str, str]] = []\n\n    def _get_directory_stash(self, path: str) -> str:\n        \"\"\"Stashes a directory.\n\n        Directories are stashed adjacent to their original location if\n        possible, or else moved/copied into the user's temp dir.\"\"\"\n\n        try:\n            save_dir: TempDirectory = AdjacentTempDirectory(path)\n        except OSError:\n            save_dir = TempDirectory(kind=\"uninstall\")\n        self._save_dirs[os.path.normcase(path)] = save_dir\n\n        return save_dir.path\n\n    def _get_file_stash(self, path: str) -> str:\n        \"\"\"Stashes a file.\n\n        If no root has been provided, one will be created for the directory\n        in the user's temp directory.\"\"\"\n        path = os.path.normcase(path)\n        head, old_head = os.path.dirname(path), None\n        save_dir = None\n\n        while head != old_head:\n            try:\n                save_dir = self._save_dirs[head]\n                break\n            except KeyError:\n                pass\n            head, old_head = os.path.dirname(head), head\n        else:\n            # Did not find any suitable root\n            head = os.path.dirname(path)\n            save_dir = TempDirectory(kind=\"uninstall\")\n            self._save_dirs[head] = save_dir\n\n        relpath = os.path.relpath(path, head)\n        if relpath and relpath != os.path.curdir:\n            return os.path.join(save_dir.path, relpath)\n        return save_dir.path\n\n    def stash(self, path: str) -> str:\n        \"\"\"Stashes the directory or file and returns its new location.\n        Handle symlinks as files to avoid modifying the symlink targets.\n        \"\"\"\n        path_is_dir = os.path.isdir(path) and not os.path.islink(path)\n        if path_is_dir:\n            new_path = self._get_directory_stash(path)\n        else:\n            new_path = self._get_file_stash(path)\n\n        self._moves.append((path, new_path))\n        if path_is_dir and os.path.isdir(new_path):\n            # If we're moving a directory, we need to\n            # remove the destination first or else it will be\n            # moved to inside the existing directory.\n            # We just created new_path ourselves, so it will\n            # be removable.\n            os.rmdir(new_path)\n        renames(path, new_path)\n        return new_path\n\n    def commit(self) -> None:\n        \"\"\"Commits the uninstall by removing stashed files.\"\"\"\n        for save_dir in self._save_dirs.values():\n            save_dir.cleanup()\n        self._moves = []\n        self._save_dirs = {}\n\n    def rollback(self) -> None:\n        \"\"\"Undoes the uninstall by moving stashed files back.\"\"\"\n        for p in self._moves:\n            logger.info(\"Moving to %s\\n from %s\", *p)\n\n        for new_path, path in self._moves:\n            try:\n                logger.debug(\"Replacing %s from %s\", new_path, path)\n                if os.path.isfile(new_path) or os.path.islink(new_path):\n                    os.unlink(new_path)\n                elif os.path.isdir(new_path):\n                    rmtree(new_path)\n                renames(path, new_path)\n            except OSError as ex:\n                logger.error(\"Failed to restore %s\", new_path)\n                logger.debug(\"Exception: %s\", ex)\n\n        self.commit()\n\n    @property\n    def can_rollback(self) -> bool:\n        return bool(self._moves)\n\n\nclass UninstallPathSet:\n    \"\"\"A set of file paths to be removed in the uninstallation of a\n    requirement.\"\"\"\n\n    def __init__(self, dist: BaseDistribution) -> None:\n        self._paths: Set[str] = set()\n        self._refuse: Set[str] = set()\n        self._pth: Dict[str, UninstallPthEntries] = {}\n        self._dist = dist\n        self._moved_paths = StashedUninstallPathSet()\n        # Create local cache of normalize_path results. Creating an UninstallPathSet\n        # can result in hundreds/thousands of redundant calls to normalize_path with\n        # the same args, which hurts performance.\n        self._normalize_path_cached = functools.lru_cache(normalize_path)\n\n    def _permitted(self, path: str) -> bool:\n        \"\"\"\n        Return True if the given path is one we are permitted to\n        remove/modify, False otherwise.\n\n        \"\"\"\n        # aka is_local, but caching normalized sys.prefix\n        if not running_under_virtualenv():\n            return True\n        return path.startswith(self._normalize_path_cached(sys.prefix))\n\n    def add(self, path: str) -> None:\n        head, tail = os.path.split(path)\n\n        # we normalize the head to resolve parent directory symlinks, but not\n        # the tail, since we only want to uninstall symlinks, not their targets\n        path = os.path.join(self._normalize_path_cached(head), os.path.normcase(tail))\n\n        if not os.path.exists(path):\n            return\n        if self._permitted(path):\n            self._paths.add(path)\n        else:\n            self._refuse.add(path)\n\n        # __pycache__ files can show up after 'installed-files.txt' is created,\n        # due to imports\n        if os.path.splitext(path)[1] == \".py\":\n            self.add(cache_from_source(path))\n\n    def add_pth(self, pth_file: str, entry: str) -> None:\n        pth_file = self._normalize_path_cached(pth_file)\n        if self._permitted(pth_file):\n            if pth_file not in self._pth:\n                self._pth[pth_file] = UninstallPthEntries(pth_file)\n            self._pth[pth_file].add(entry)\n        else:\n            self._refuse.add(pth_file)\n\n    def remove(self, auto_confirm: bool = False, verbose: bool = False) -> None:\n        \"\"\"Remove paths in ``self._paths`` with confirmation (unless\n        ``auto_confirm`` is True).\"\"\"\n\n        if not self._paths:\n            logger.info(\n                \"Can't uninstall '%s'. No files were found to uninstall.\",\n                self._dist.raw_name,\n            )\n            return\n\n        dist_name_version = f\"{self._dist.raw_name}-{self._dist.raw_version}\"\n        logger.info(\"Uninstalling %s:\", dist_name_version)\n\n        with indent_log():\n            if auto_confirm or self._allowed_to_proceed(verbose):\n                moved = self._moved_paths\n\n                for_rename = compress_for_rename(self._paths)\n\n                for path in sorted(compact(for_rename)):\n                    moved.stash(path)\n                    logger.verbose(\"Removing file or directory %s\", path)\n\n                for pth in self._pth.values():\n                    pth.remove()\n\n                logger.info(\"Successfully uninstalled %s\", dist_name_version)\n\n    def _allowed_to_proceed(self, verbose: bool) -> bool:\n        \"\"\"Display which files would be deleted and prompt for confirmation\"\"\"\n\n        def _display(msg: str, paths: Iterable[str]) -> None:\n            if not paths:\n                return\n\n            logger.info(msg)\n            with indent_log():\n                for path in sorted(compact(paths)):\n                    logger.info(path)\n\n        if not verbose:\n            will_remove, will_skip = compress_for_output_listing(self._paths)\n        else:\n            # In verbose mode, display all the files that are going to be\n            # deleted.\n            will_remove = set(self._paths)\n            will_skip = set()\n\n        _display(\"Would remove:\", will_remove)\n        _display(\"Would not remove (might be manually added):\", will_skip)\n        _display(\"Would not remove (outside of prefix):\", self._refuse)\n        if verbose:\n            _display(\"Will actually move:\", compress_for_rename(self._paths))\n\n        return ask(\"Proceed (Y/n)? \", (\"y\", \"n\", \"\")) != \"n\"\n\n    def rollback(self) -> None:\n        \"\"\"Rollback the changes previously made by remove().\"\"\"\n        if not self._moved_paths.can_rollback:\n            logger.error(\n                \"Can't roll back %s; was not uninstalled\",\n                self._dist.raw_name,\n            )\n            return\n        logger.info(\"Rolling back uninstall of %s\", self._dist.raw_name)\n        self._moved_paths.rollback()\n        for pth in self._pth.values():\n            pth.rollback()\n\n    def commit(self) -> None:\n        \"\"\"Remove temporary save dir: rollback will no longer be possible.\"\"\"\n        self._moved_paths.commit()\n\n    @classmethod\n    def from_dist(cls, dist: BaseDistribution) -> \"UninstallPathSet\":\n        dist_location = dist.location\n        info_location = dist.info_location\n        if dist_location is None:\n            logger.info(\n                \"Not uninstalling %s since it is not installed\",\n                dist.canonical_name,\n            )\n            return cls(dist)\n\n        normalized_dist_location = normalize_path(dist_location)\n        if not dist.local:\n            logger.info(\n                \"Not uninstalling %s at %s, outside environment %s\",\n                dist.canonical_name,\n                normalized_dist_location,\n                sys.prefix,\n            )\n            return cls(dist)\n\n        if normalized_dist_location in {\n            p\n            for p in {sysconfig.get_path(\"stdlib\"), sysconfig.get_path(\"platstdlib\")}\n            if p\n        }:\n            logger.info(\n                \"Not uninstalling %s at %s, as it is in the standard library.\",\n                dist.canonical_name,\n                normalized_dist_location,\n            )\n            return cls(dist)\n\n        paths_to_remove = cls(dist)\n        develop_egg_link = egg_link_path_from_location(dist.raw_name)\n\n        # Distribution is installed with metadata in a \"flat\" .egg-info\n        # directory. This means it is not a modern .dist-info installation, an\n        # egg, or legacy editable.\n        setuptools_flat_installation = (\n            dist.installed_with_setuptools_egg_info\n            and info_location is not None\n            and os.path.exists(info_location)\n            # If dist is editable and the location points to a ``.egg-info``,\n            # we are in fact in the legacy editable case.\n            and not info_location.endswith(f\"{dist.setuptools_filename}.egg-info\")\n        )\n\n        # Uninstall cases order do matter as in the case of 2 installs of the\n        # same package, pip needs to uninstall the currently detected version\n        if setuptools_flat_installation:\n            if info_location is not None:\n                paths_to_remove.add(info_location)\n            installed_files = dist.iter_declared_entries()\n            if installed_files is not None:\n                for installed_file in installed_files:\n                    paths_to_remove.add(os.path.join(dist_location, installed_file))\n            # FIXME: need a test for this elif block\n            # occurs with --single-version-externally-managed/--record outside\n            # of pip\n            elif dist.is_file(\"top_level.txt\"):\n                try:\n                    namespace_packages = dist.read_text(\"namespace_packages.txt\")\n                except FileNotFoundError:\n                    namespaces = []\n                else:\n                    namespaces = namespace_packages.splitlines(keepends=False)\n                for top_level_pkg in [\n                    p\n                    for p in dist.read_text(\"top_level.txt\").splitlines()\n                    if p and p not in namespaces\n                ]:\n                    path = os.path.join(dist_location, top_level_pkg)\n                    paths_to_remove.add(path)\n                    paths_to_remove.add(f\"{path}.py\")\n                    paths_to_remove.add(f\"{path}.pyc\")\n                    paths_to_remove.add(f\"{path}.pyo\")\n\n        elif dist.installed_by_distutils:\n            raise LegacyDistutilsInstall(distribution=dist)\n\n        elif dist.installed_as_egg:\n            # package installed by easy_install\n            # We cannot match on dist.egg_name because it can slightly vary\n            # i.e. setuptools-0.6c11-py2.6.egg vs setuptools-0.6rc11-py2.6.egg\n            paths_to_remove.add(dist_location)\n            easy_install_egg = os.path.split(dist_location)[1]\n            easy_install_pth = os.path.join(\n                os.path.dirname(dist_location),\n                \"easy-install.pth\",\n            )\n            paths_to_remove.add_pth(easy_install_pth, \"./\" + easy_install_egg)\n\n        elif dist.installed_with_dist_info:\n            for path in uninstallation_paths(dist):\n                paths_to_remove.add(path)\n\n        elif develop_egg_link:\n            # PEP 660 modern editable is handled in the ``.dist-info`` case\n            # above, so this only covers the setuptools-style editable.\n            with open(develop_egg_link) as fh:\n                link_pointer = os.path.normcase(fh.readline().strip())\n                normalized_link_pointer = paths_to_remove._normalize_path_cached(\n                    link_pointer\n                )\n            assert os.path.samefile(\n                normalized_link_pointer, normalized_dist_location\n            ), (\n                f\"Egg-link {develop_egg_link} (to {link_pointer}) does not match \"\n                f\"installed location of {dist.raw_name} (at {dist_location})\"\n            )\n            paths_to_remove.add(develop_egg_link)\n            easy_install_pth = os.path.join(\n                os.path.dirname(develop_egg_link), \"easy-install.pth\"\n            )\n            paths_to_remove.add_pth(easy_install_pth, dist_location)\n\n        else:\n            logger.debug(\n                \"Not sure how to uninstall: %s - Check: %s\",\n                dist,\n                dist_location,\n            )\n\n        if dist.in_usersite:\n            bin_dir = get_bin_user()\n        else:\n            bin_dir = get_bin_prefix()\n\n        # find distutils scripts= scripts\n        try:\n            for script in dist.iter_distutils_script_names():\n                paths_to_remove.add(os.path.join(bin_dir, script))\n                if WINDOWS:\n                    paths_to_remove.add(os.path.join(bin_dir, f\"{script}.bat\"))\n        except (FileNotFoundError, NotADirectoryError):\n            pass\n\n        # find console_scripts and gui_scripts\n        def iter_scripts_to_remove(\n            dist: BaseDistribution,\n            bin_dir: str,\n        ) -> Generator[str, None, None]:\n            for entry_point in dist.iter_entry_points():\n                if entry_point.group == \"console_scripts\":\n                    yield from _script_names(bin_dir, entry_point.name, False)\n                elif entry_point.group == \"gui_scripts\":\n                    yield from _script_names(bin_dir, entry_point.name, True)\n\n        for s in iter_scripts_to_remove(dist, bin_dir):\n            paths_to_remove.add(s)\n\n        return paths_to_remove\n\n\nclass UninstallPthEntries:\n    def __init__(self, pth_file: str) -> None:\n        self.file = pth_file\n        self.entries: Set[str] = set()\n        self._saved_lines: Optional[List[bytes]] = None\n\n    def add(self, entry: str) -> None:\n        entry = os.path.normcase(entry)\n        # On Windows, os.path.normcase converts the entry to use\n        # backslashes.  This is correct for entries that describe absolute\n        # paths outside of site-packages, but all the others use forward\n        # slashes.\n        # os.path.splitdrive is used instead of os.path.isabs because isabs\n        # treats non-absolute paths with drive letter markings like c:foo\\bar\n        # as absolute paths. It also does not recognize UNC paths if they don't\n        # have more than \"\\\\sever\\share\". Valid examples: \"\\\\server\\share\\\" or\n        # \"\\\\server\\share\\folder\".\n        if WINDOWS and not os.path.splitdrive(entry)[0]:\n            entry = entry.replace(\"\\\\\", \"/\")\n        self.entries.add(entry)\n\n    def remove(self) -> None:\n        logger.verbose(\"Removing pth entries from %s:\", self.file)\n\n        # If the file doesn't exist, log a warning and return\n        if not os.path.isfile(self.file):\n            logger.warning(\"Cannot remove entries from nonexistent file %s\", self.file)\n            return\n        with open(self.file, \"rb\") as fh:\n            # windows uses '\\r\\n' with py3k, but uses '\\n' with py2.x\n            lines = fh.readlines()\n            self._saved_lines = lines\n        if any(b\"\\r\\n\" in line for line in lines):\n            endline = \"\\r\\n\"\n        else:\n            endline = \"\\n\"\n        # handle missing trailing newline\n        if lines and not lines[-1].endswith(endline.encode(\"utf-8\")):\n            lines[-1] = lines[-1] + endline.encode(\"utf-8\")\n        for entry in self.entries:\n            try:\n                logger.verbose(\"Removing entry: %s\", entry)\n                lines.remove((entry + endline).encode(\"utf-8\"))\n            except ValueError:\n                pass\n        with open(self.file, \"wb\") as fh:\n            fh.writelines(lines)\n\n    def rollback(self) -> bool:\n        if self._saved_lines is None:\n            logger.error(\"Cannot roll back changes to %s, none were made\", self.file)\n            return False\n        logger.debug(\"Rolling %s back to previous state\", self.file)\n        with open(self.file, \"wb\") as fh:\n            fh.writelines(self._saved_lines)\n        return True\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/resolution/__init__.py","size":0,"sha1":"da39a3ee5e6b4b0d3255bfef95601890afd80709","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":""},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/resolution/base.py","size":583,"sha1":"bb0a50e2866d29bb4c616cf2900fa3eb8eed3051","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"from typing import Callable, List, Optional\n\nfrom pip._internal.req.req_install import InstallRequirement\nfrom pip._internal.req.req_set import RequirementSet\n\nInstallRequirementProvider = Callable[\n    [str, Optional[InstallRequirement]], InstallRequirement\n]\n\n\nclass BaseResolver:\n    def resolve(\n        self, root_reqs: List[InstallRequirement], check_supported_wheels: bool\n    ) -> RequirementSet:\n        raise NotImplementedError()\n\n    def get_installation_order(\n        self, req_set: RequirementSet\n    ) -> List[InstallRequirement]:\n        raise NotImplementedError()\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/resolution/legacy/__init__.py","size":0,"sha1":"da39a3ee5e6b4b0d3255bfef95601890afd80709","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":""},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/resolution/legacy/resolver.py","size":24068,"sha1":"1dcc2a85b3c60b33e061384f23852fe2d5e26b4f","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"Dependency Resolution\n\nThe dependency resolution in pip is performed as follows:\n\nfor top-level requirements:\n    a. only one spec allowed per project, regardless of conflicts or not.\n       otherwise a \"double requirement\" exception is raised\n    b. they override sub-dependency requirements.\nfor sub-dependencies\n    a. \"first found, wins\" (where the order is breadth first)\n\"\"\"\n\nimport logging\nimport sys\nfrom collections import defaultdict\nfrom itertools import chain\nfrom typing import DefaultDict, Iterable, List, Optional, Set, Tuple\n\nfrom pip._vendor.packaging import specifiers\nfrom pip._vendor.packaging.requirements import Requirement\n\nfrom pip._internal.cache import WheelCache\nfrom pip._internal.exceptions import (\n    BestVersionAlreadyInstalled,\n    DistributionNotFound,\n    HashError,\n    HashErrors,\n    InstallationError,\n    NoneMetadataError,\n    UnsupportedPythonVersion,\n)\nfrom pip._internal.index.package_finder import PackageFinder\nfrom pip._internal.metadata import BaseDistribution\nfrom pip._internal.models.link import Link\nfrom pip._internal.models.wheel import Wheel\nfrom pip._internal.operations.prepare import RequirementPreparer\nfrom pip._internal.req.req_install import (\n    InstallRequirement,\n    check_invalid_constraint_type,\n)\nfrom pip._internal.req.req_set import RequirementSet\nfrom pip._internal.resolution.base import BaseResolver, InstallRequirementProvider\nfrom pip._internal.utils import compatibility_tags\nfrom pip._internal.utils.compatibility_tags import get_supported\nfrom pip._internal.utils.direct_url_helpers import direct_url_from_link\nfrom pip._internal.utils.logging import indent_log\nfrom pip._internal.utils.misc import normalize_version_info\nfrom pip._internal.utils.packaging import check_requires_python\n\nlogger = logging.getLogger(__name__)\n\nDiscoveredDependencies = DefaultDict[Optional[str], List[InstallRequirement]]\n\n\ndef _check_dist_requires_python(\n    dist: BaseDistribution,\n    version_info: Tuple[int, int, int],\n    ignore_requires_python: bool = False,\n) -> None:\n    \"\"\"\n    Check whether the given Python version is compatible with a distribution's\n    \"Requires-Python\" value.\n\n    :param version_info: A 3-tuple of ints representing the Python\n        major-minor-micro version to check.\n    :param ignore_requires_python: Whether to ignore the \"Requires-Python\"\n        value if the given Python version isn't compatible.\n\n    :raises UnsupportedPythonVersion: When the given Python version isn't\n        compatible.\n    \"\"\"\n    # This idiosyncratically converts the SpecifierSet to str and let\n    # check_requires_python then parse it again into SpecifierSet. But this\n    # is the legacy resolver so I'm just not going to bother refactoring.\n    try:\n        requires_python = str(dist.requires_python)\n    except FileNotFoundError as e:\n        raise NoneMetadataError(dist, str(e))\n    try:\n        is_compatible = check_requires_python(\n            requires_python,\n            version_info=version_info,\n        )\n    except specifiers.InvalidSpecifier as exc:\n        logger.warning(\n            \"Package %r has an invalid Requires-Python: %s\", dist.raw_name, exc\n        )\n        return\n\n    if is_compatible:\n        return\n\n    version = \".\".join(map(str, version_info))\n    if ignore_requires_python:\n        logger.debug(\n            \"Ignoring failed Requires-Python check for package %r: %s not in %r\",\n            dist.raw_name,\n            version,\n            requires_python,\n        )\n        return\n\n    raise UnsupportedPythonVersion(\n        f\"Package {dist.raw_name!r} requires a different Python: \"\n        f\"{version} not in {requires_python!r}\"\n    )\n\n\nclass Resolver(BaseResolver):\n    \"\"\"Resolves which packages need to be installed/uninstalled to perform \\\n    the requested operation without breaking the requirements of any package.\n    \"\"\"\n\n    _allowed_strategies = {\"eager\", \"only-if-needed\", \"to-satisfy-only\"}\n\n    def __init__(\n        self,\n        preparer: RequirementPreparer,\n        finder: PackageFinder,\n        wheel_cache: Optional[WheelCache],\n        make_install_req: InstallRequirementProvider,\n        use_user_site: bool,\n        ignore_dependencies: bool,\n        ignore_installed: bool,\n        ignore_requires_python: bool,\n        force_reinstall: bool,\n        upgrade_strategy: str,\n        py_version_info: Optional[Tuple[int, ...]] = None,\n    ) -> None:\n        super().__init__()\n        assert upgrade_strategy in self._allowed_strategies\n\n        if py_version_info is None:\n            py_version_info = sys.version_info[:3]\n        else:\n            py_version_info = normalize_version_info(py_version_info)\n\n        self._py_version_info = py_version_info\n\n        self.preparer = preparer\n        self.finder = finder\n        self.wheel_cache = wheel_cache\n\n        self.upgrade_strategy = upgrade_strategy\n        self.force_reinstall = force_reinstall\n        self.ignore_dependencies = ignore_dependencies\n        self.ignore_installed = ignore_installed\n        self.ignore_requires_python = ignore_requires_python\n        self.use_user_site = use_user_site\n        self._make_install_req = make_install_req\n\n        self._discovered_dependencies: DiscoveredDependencies = defaultdict(list)\n\n    def resolve(\n        self, root_reqs: List[InstallRequirement], check_supported_wheels: bool\n    ) -> RequirementSet:\n        \"\"\"Resolve what operations need to be done\n\n        As a side-effect of this method, the packages (and their dependencies)\n        are downloaded, unpacked and prepared for installation. This\n        preparation is done by ``pip.operations.prepare``.\n\n        Once PyPI has static dependency metadata available, it would be\n        possible to move the preparation to become a step separated from\n        dependency resolution.\n        \"\"\"\n        requirement_set = RequirementSet(check_supported_wheels=check_supported_wheels)\n        for req in root_reqs:\n            if req.constraint:\n                check_invalid_constraint_type(req)\n            self._add_requirement_to_set(requirement_set, req)\n\n        # Actually prepare the files, and collect any exceptions. Most hash\n        # exceptions cannot be checked ahead of time, because\n        # _populate_link() needs to be called before we can make decisions\n        # based on link type.\n        discovered_reqs: List[InstallRequirement] = []\n        hash_errors = HashErrors()\n        for req in chain(requirement_set.all_requirements, discovered_reqs):\n            try:\n                discovered_reqs.extend(self._resolve_one(requirement_set, req))\n            except HashError as exc:\n                exc.req = req\n                hash_errors.append(exc)\n\n        if hash_errors:\n            raise hash_errors\n\n        return requirement_set\n\n    def _add_requirement_to_set(\n        self,\n        requirement_set: RequirementSet,\n        install_req: InstallRequirement,\n        parent_req_name: Optional[str] = None,\n        extras_requested: Optional[Iterable[str]] = None,\n    ) -> Tuple[List[InstallRequirement], Optional[InstallRequirement]]:\n        \"\"\"Add install_req as a requirement to install.\n\n        :param parent_req_name: The name of the requirement that needed this\n            added. The name is used because when multiple unnamed requirements\n            resolve to the same name, we could otherwise end up with dependency\n            links that point outside the Requirements set. parent_req must\n            already be added. Note that None implies that this is a user\n            supplied requirement, vs an inferred one.\n        :param extras_requested: an iterable of extras used to evaluate the\n            environment markers.\n        :return: Additional requirements to scan. That is either [] if\n            the requirement is not applicable, or [install_req] if the\n            requirement is applicable and has just been added.\n        \"\"\"\n        # If the markers do not match, ignore this requirement.\n        if not install_req.match_markers(extras_requested):\n            logger.info(\n                \"Ignoring %s: markers '%s' don't match your environment\",\n                install_req.name,\n                install_req.markers,\n            )\n            return [], None\n\n        # If the wheel is not supported, raise an error.\n        # Should check this after filtering out based on environment markers to\n        # allow specifying different wheels based on the environment/OS, in a\n        # single requirements file.\n        if install_req.link and install_req.link.is_wheel:\n            wheel = Wheel(install_req.link.filename)\n            tags = compatibility_tags.get_supported()\n            if requirement_set.check_supported_wheels and not wheel.supported(tags):\n                raise InstallationError(\n                    f\"{wheel.filename} is not a supported wheel on this platform.\"\n                )\n\n        # This next bit is really a sanity check.\n        assert (\n            not install_req.user_supplied or parent_req_name is None\n        ), \"a user supplied req shouldn't have a parent\"\n\n        # Unnamed requirements are scanned again and the requirement won't be\n        # added as a dependency until after scanning.\n        if not install_req.name:\n            requirement_set.add_unnamed_requirement(install_req)\n            return [install_req], None\n\n        try:\n            existing_req: Optional[InstallRequirement] = (\n                requirement_set.get_requirement(install_req.name)\n            )\n        except KeyError:\n            existing_req = None\n\n        has_conflicting_requirement = (\n            parent_req_name is None\n            and existing_req\n            and not existing_req.constraint\n            and existing_req.extras == install_req.extras\n            and existing_req.req\n            and install_req.req\n            and existing_req.req.specifier != install_req.req.specifier\n        )\n        if has_conflicting_requirement:\n            raise InstallationError(\n                f\"Double requirement given: {install_req} \"\n                f\"(already in {existing_req}, name={install_req.name!r})\"\n            )\n\n        # When no existing requirement exists, add the requirement as a\n        # dependency and it will be scanned again after.\n        if not existing_req:\n            requirement_set.add_named_requirement(install_req)\n            # We'd want to rescan this requirement later\n            return [install_req], install_req\n\n        # Assume there's no need to scan, and that we've already\n        # encountered this for scanning.\n        if install_req.constraint or not existing_req.constraint:\n            return [], existing_req\n\n        does_not_satisfy_constraint = install_req.link and not (\n            existing_req.link and install_req.link.path == existing_req.link.path\n        )\n        if does_not_satisfy_constraint:\n            raise InstallationError(\n                f\"Could not satisfy constraints for '{install_req.name}': \"\n                \"installation from path or url cannot be \"\n                \"constrained to a version\"\n            )\n        # If we're now installing a constraint, mark the existing\n        # object for real installation.\n        existing_req.constraint = False\n        # If we're now installing a user supplied requirement,\n        # mark the existing object as such.\n        if install_req.user_supplied:\n            existing_req.user_supplied = True\n        existing_req.extras = tuple(\n            sorted(set(existing_req.extras) | set(install_req.extras))\n        )\n        logger.debug(\n            \"Setting %s extras to: %s\",\n            existing_req,\n            existing_req.extras,\n        )\n        # Return the existing requirement for addition to the parent and\n        # scanning again.\n        return [existing_req], existing_req\n\n    def _is_upgrade_allowed(self, req: InstallRequirement) -> bool:\n        if self.upgrade_strategy == \"to-satisfy-only\":\n            return False\n        elif self.upgrade_strategy == \"eager\":\n            return True\n        else:\n            assert self.upgrade_strategy == \"only-if-needed\"\n            return req.user_supplied or req.constraint\n\n    def _set_req_to_reinstall(self, req: InstallRequirement) -> None:\n        \"\"\"\n        Set a requirement to be installed.\n        \"\"\"\n        # Don't uninstall the conflict if doing a user install and the\n        # conflict is not a user install.\n        assert req.satisfied_by is not None\n        if not self.use_user_site or req.satisfied_by.in_usersite:\n            req.should_reinstall = True\n        req.satisfied_by = None\n\n    def _check_skip_installed(\n        self, req_to_install: InstallRequirement\n    ) -> Optional[str]:\n        \"\"\"Check if req_to_install should be skipped.\n\n        This will check if the req is installed, and whether we should upgrade\n        or reinstall it, taking into account all the relevant user options.\n\n        After calling this req_to_install will only have satisfied_by set to\n        None if the req_to_install is to be upgraded/reinstalled etc. Any\n        other value will be a dist recording the current thing installed that\n        satisfies the requirement.\n\n        Note that for vcs urls and the like we can't assess skipping in this\n        routine - we simply identify that we need to pull the thing down,\n        then later on it is pulled down and introspected to assess upgrade/\n        reinstalls etc.\n\n        :return: A text reason for why it was skipped, or None.\n        \"\"\"\n        if self.ignore_installed:\n            return None\n\n        req_to_install.check_if_exists(self.use_user_site)\n        if not req_to_install.satisfied_by:\n            return None\n\n        if self.force_reinstall:\n            self._set_req_to_reinstall(req_to_install)\n            return None\n\n        if not self._is_upgrade_allowed(req_to_install):\n            if self.upgrade_strategy == \"only-if-needed\":\n                return \"already satisfied, skipping upgrade\"\n            return \"already satisfied\"\n\n        # Check for the possibility of an upgrade.  For link-based\n        # requirements we have to pull the tree down and inspect to assess\n        # the version #, so it's handled way down.\n        if not req_to_install.link:\n            try:\n                self.finder.find_requirement(req_to_install, upgrade=True)\n            except BestVersionAlreadyInstalled:\n                # Then the best version is installed.\n                return \"already up-to-date\"\n            except DistributionNotFound:\n                # No distribution found, so we squash the error.  It will\n                # be raised later when we re-try later to do the install.\n                # Why don't we just raise here?\n                pass\n\n        self._set_req_to_reinstall(req_to_install)\n        return None\n\n    def _find_requirement_link(self, req: InstallRequirement) -> Optional[Link]:\n        upgrade = self._is_upgrade_allowed(req)\n        best_candidate = self.finder.find_requirement(req, upgrade)\n        if not best_candidate:\n            return None\n\n        # Log a warning per PEP 592 if necessary before returning.\n        link = best_candidate.link\n        if link.is_yanked:\n            reason = link.yanked_reason or \"<none given>\"\n            msg = (\n                # Mark this as a unicode string to prevent\n                # \"UnicodeEncodeError: 'ascii' codec can't encode character\"\n                # in Python 2 when the reason contains non-ascii characters.\n                \"The candidate selected for download or install is a \"\n                f\"yanked version: {best_candidate}\\n\"\n                f\"Reason for being yanked: {reason}\"\n            )\n            logger.warning(msg)\n\n        return link\n\n    def _populate_link(self, req: InstallRequirement) -> None:\n        \"\"\"Ensure that if a link can be found for this, that it is found.\n\n        Note that req.link may still be None - if the requirement is already\n        installed and not needed to be upgraded based on the return value of\n        _is_upgrade_allowed().\n\n        If preparer.require_hashes is True, don't use the wheel cache, because\n        cached wheels, always built locally, have different hashes than the\n        files downloaded from the index server and thus throw false hash\n        mismatches. Furthermore, cached wheels at present have undeterministic\n        contents due to file modification times.\n        \"\"\"\n        if req.link is None:\n            req.link = self._find_requirement_link(req)\n\n        if self.wheel_cache is None or self.preparer.require_hashes:\n            return\n\n        assert req.link is not None, \"_find_requirement_link unexpectedly returned None\"\n        cache_entry = self.wheel_cache.get_cache_entry(\n            link=req.link,\n            package_name=req.name,\n            supported_tags=get_supported(),\n        )\n        if cache_entry is not None:\n            logger.debug(\"Using cached wheel link: %s\", cache_entry.link)\n            if req.link is req.original_link and cache_entry.persistent:\n                req.cached_wheel_source_link = req.link\n            if cache_entry.origin is not None:\n                req.download_info = cache_entry.origin\n            else:\n                # Legacy cache entry that does not have origin.json.\n                # download_info may miss the archive_info.hashes field.\n                req.download_info = direct_url_from_link(\n                    req.link, link_is_in_wheel_cache=cache_entry.persistent\n                )\n            req.link = cache_entry.link\n\n    def _get_dist_for(self, req: InstallRequirement) -> BaseDistribution:\n        \"\"\"Takes a InstallRequirement and returns a single AbstractDist \\\n        representing a prepared variant of the same.\n        \"\"\"\n        if req.editable:\n            return self.preparer.prepare_editable_requirement(req)\n\n        # satisfied_by is only evaluated by calling _check_skip_installed,\n        # so it must be None here.\n        assert req.satisfied_by is None\n        skip_reason = self._check_skip_installed(req)\n\n        if req.satisfied_by:\n            return self.preparer.prepare_installed_requirement(req, skip_reason)\n\n        # We eagerly populate the link, since that's our \"legacy\" behavior.\n        self._populate_link(req)\n        dist = self.preparer.prepare_linked_requirement(req)\n\n        # NOTE\n        # The following portion is for determining if a certain package is\n        # going to be re-installed/upgraded or not and reporting to the user.\n        # This should probably get cleaned up in a future refactor.\n\n        # req.req is only avail after unpack for URL\n        # pkgs repeat check_if_exists to uninstall-on-upgrade\n        # (#14)\n        if not self.ignore_installed:\n            req.check_if_exists(self.use_user_site)\n\n        if req.satisfied_by:\n            should_modify = (\n                self.upgrade_strategy != \"to-satisfy-only\"\n                or self.force_reinstall\n                or self.ignore_installed\n                or req.link.scheme == \"file\"\n            )\n            if should_modify:\n                self._set_req_to_reinstall(req)\n            else:\n                logger.info(\n                    \"Requirement already satisfied (use --upgrade to upgrade): %s\",\n                    req,\n                )\n        return dist\n\n    def _resolve_one(\n        self,\n        requirement_set: RequirementSet,\n        req_to_install: InstallRequirement,\n    ) -> List[InstallRequirement]:\n        \"\"\"Prepare a single requirements file.\n\n        :return: A list of additional InstallRequirements to also install.\n        \"\"\"\n        # Tell user what we are doing for this requirement:\n        # obtain (editable), skipping, processing (local url), collecting\n        # (remote url or package name)\n        if req_to_install.constraint or req_to_install.prepared:\n            return []\n\n        req_to_install.prepared = True\n\n        # Parse and return dependencies\n        dist = self._get_dist_for(req_to_install)\n        # This will raise UnsupportedPythonVersion if the given Python\n        # version isn't compatible with the distribution's Requires-Python.\n        _check_dist_requires_python(\n            dist,\n            version_info=self._py_version_info,\n            ignore_requires_python=self.ignore_requires_python,\n        )\n\n        more_reqs: List[InstallRequirement] = []\n\n        def add_req(subreq: Requirement, extras_requested: Iterable[str]) -> None:\n            # This idiosyncratically converts the Requirement to str and let\n            # make_install_req then parse it again into Requirement. But this is\n            # the legacy resolver so I'm just not going to bother refactoring.\n            sub_install_req = self._make_install_req(str(subreq), req_to_install)\n            parent_req_name = req_to_install.name\n            to_scan_again, add_to_parent = self._add_requirement_to_set(\n                requirement_set,\n                sub_install_req,\n                parent_req_name=parent_req_name,\n                extras_requested=extras_requested,\n            )\n            if parent_req_name and add_to_parent:\n                self._discovered_dependencies[parent_req_name].append(add_to_parent)\n            more_reqs.extend(to_scan_again)\n\n        with indent_log():\n            # We add req_to_install before its dependencies, so that we\n            # can refer to it when adding dependencies.\n            assert req_to_install.name is not None\n            if not requirement_set.has_requirement(req_to_install.name):\n                # 'unnamed' requirements will get added here\n                # 'unnamed' requirements can only come from being directly\n                # provided by the user.\n                assert req_to_install.user_supplied\n                self._add_requirement_to_set(\n                    requirement_set, req_to_install, parent_req_name=None\n                )\n\n            if not self.ignore_dependencies:\n                if req_to_install.extras:\n                    logger.debug(\n                        \"Installing extra requirements: %r\",\n                        \",\".join(req_to_install.extras),\n                    )\n                missing_requested = sorted(\n                    set(req_to_install.extras) - set(dist.iter_provided_extras())\n                )\n                for missing in missing_requested:\n                    logger.warning(\n                        \"%s %s does not provide the extra '%s'\",\n                        dist.raw_name,\n                        dist.version,\n                        missing,\n                    )\n\n                available_requested = sorted(\n                    set(dist.iter_provided_extras()) & set(req_to_install.extras)\n                )\n                for subreq in dist.iter_dependencies(available_requested):\n                    add_req(subreq, extras_requested=available_requested)\n\n        return more_reqs\n\n    def get_installation_order(\n        self, req_set: RequirementSet\n    ) -> List[InstallRequirement]:\n        \"\"\"Create the installation order.\n\n        The installation order is topological - requirements are installed\n        before the requiring thing. We break cycles at an arbitrary point,\n        and make no other guarantees.\n        \"\"\"\n        # The current implementation, which we may change at any point\n        # installs the user specified things in the order given, except when\n        # dependencies must come earlier to achieve topological order.\n        order = []\n        ordered_reqs: Set[InstallRequirement] = set()\n\n        def schedule(req: InstallRequirement) -> None:\n            if req.satisfied_by or req in ordered_reqs:\n                return\n            if req.constraint:\n                return\n            ordered_reqs.add(req)\n            for dep in self._discovered_dependencies[req.name]:\n                schedule(dep)\n            order.append(req)\n\n        for install_req in req_set.requirements.values():\n            schedule(install_req)\n        return order\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/resolution/resolvelib/__init__.py","size":0,"sha1":"da39a3ee5e6b4b0d3255bfef95601890afd80709","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":""},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/resolution/resolvelib/base.py","size":5023,"sha1":"8f49b0ae40fcbf00e3c170af47a76e86f6f9cd25","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"from dataclasses import dataclass\nfrom typing import FrozenSet, Iterable, Optional, Tuple\n\nfrom pip._vendor.packaging.specifiers import SpecifierSet\nfrom pip._vendor.packaging.utils import NormalizedName\nfrom pip._vendor.packaging.version import Version\n\nfrom pip._internal.models.link import Link, links_equivalent\nfrom pip._internal.req.req_install import InstallRequirement\nfrom pip._internal.utils.hashes import Hashes\n\nCandidateLookup = Tuple[Optional[\"Candidate\"], Optional[InstallRequirement]]\n\n\ndef format_name(project: NormalizedName, extras: FrozenSet[NormalizedName]) -> str:\n    if not extras:\n        return project\n    extras_expr = \",\".join(sorted(extras))\n    return f\"{project}[{extras_expr}]\"\n\n\n@dataclass(frozen=True)\nclass Constraint:\n    specifier: SpecifierSet\n    hashes: Hashes\n    links: FrozenSet[Link]\n\n    @classmethod\n    def empty(cls) -> \"Constraint\":\n        return Constraint(SpecifierSet(), Hashes(), frozenset())\n\n    @classmethod\n    def from_ireq(cls, ireq: InstallRequirement) -> \"Constraint\":\n        links = frozenset([ireq.link]) if ireq.link else frozenset()\n        return Constraint(ireq.specifier, ireq.hashes(trust_internet=False), links)\n\n    def __bool__(self) -> bool:\n        return bool(self.specifier) or bool(self.hashes) or bool(self.links)\n\n    def __and__(self, other: InstallRequirement) -> \"Constraint\":\n        if not isinstance(other, InstallRequirement):\n            return NotImplemented\n        specifier = self.specifier & other.specifier\n        hashes = self.hashes & other.hashes(trust_internet=False)\n        links = self.links\n        if other.link:\n            links = links.union([other.link])\n        return Constraint(specifier, hashes, links)\n\n    def is_satisfied_by(self, candidate: \"Candidate\") -> bool:\n        # Reject if there are any mismatched URL constraints on this package.\n        if self.links and not all(_match_link(link, candidate) for link in self.links):\n            return False\n        # We can safely always allow prereleases here since PackageFinder\n        # already implements the prerelease logic, and would have filtered out\n        # prerelease candidates if the user does not expect them.\n        return self.specifier.contains(candidate.version, prereleases=True)\n\n\nclass Requirement:\n    @property\n    def project_name(self) -> NormalizedName:\n        \"\"\"The \"project name\" of a requirement.\n\n        This is different from ``name`` if this requirement contains extras,\n        in which case ``name`` would contain the ``[...]`` part, while this\n        refers to the name of the project.\n        \"\"\"\n        raise NotImplementedError(\"Subclass should override\")\n\n    @property\n    def name(self) -> str:\n        \"\"\"The name identifying this requirement in the resolver.\n\n        This is different from ``project_name`` if this requirement contains\n        extras, where ``project_name`` would not contain the ``[...]`` part.\n        \"\"\"\n        raise NotImplementedError(\"Subclass should override\")\n\n    def is_satisfied_by(self, candidate: \"Candidate\") -> bool:\n        return False\n\n    def get_candidate_lookup(self) -> CandidateLookup:\n        raise NotImplementedError(\"Subclass should override\")\n\n    def format_for_error(self) -> str:\n        raise NotImplementedError(\"Subclass should override\")\n\n\ndef _match_link(link: Link, candidate: \"Candidate\") -> bool:\n    if candidate.source_link:\n        return links_equivalent(link, candidate.source_link)\n    return False\n\n\nclass Candidate:\n    @property\n    def project_name(self) -> NormalizedName:\n        \"\"\"The \"project name\" of the candidate.\n\n        This is different from ``name`` if this candidate contains extras,\n        in which case ``name`` would contain the ``[...]`` part, while this\n        refers to the name of the project.\n        \"\"\"\n        raise NotImplementedError(\"Override in subclass\")\n\n    @property\n    def name(self) -> str:\n        \"\"\"The name identifying this candidate in the resolver.\n\n        This is different from ``project_name`` if this candidate contains\n        extras, where ``project_name`` would not contain the ``[...]`` part.\n        \"\"\"\n        raise NotImplementedError(\"Override in subclass\")\n\n    @property\n    def version(self) -> Version:\n        raise NotImplementedError(\"Override in subclass\")\n\n    @property\n    def is_installed(self) -> bool:\n        raise NotImplementedError(\"Override in subclass\")\n\n    @property\n    def is_editable(self) -> bool:\n        raise NotImplementedError(\"Override in subclass\")\n\n    @property\n    def source_link(self) -> Optional[Link]:\n        raise NotImplementedError(\"Override in subclass\")\n\n    def iter_dependencies(self, with_requires: bool) -> Iterable[Optional[Requirement]]:\n        raise NotImplementedError(\"Override in subclass\")\n\n    def get_install_requirement(self) -> Optional[InstallRequirement]:\n        raise NotImplementedError(\"Override in subclass\")\n\n    def format_for_error(self) -> str:\n        raise NotImplementedError(\"Subclass should override\")\n"}]}