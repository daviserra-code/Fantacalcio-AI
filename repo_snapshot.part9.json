{"generated_at":"2025-08-17T20:04:18.057896Z","root":"/home/runner/workspace","git":{"head":"7b7135163eb4227ef98e4c2d7b4ab78ea306bd73","branch":"main","status":" M corrections.db\n?? app_changes.json\n?? export_changes.py\n"},"filters":{"git_range":null,"since":null,"include_ext":[".cfg",".css",".env",".htm",".html",".ini",".jinja",".jinja2",".js",".json",".md",".py",".toml",".ts",".yaml",".yml"],"exclude_dirs":[".git",".ipynb_checkpoints",".mypy_cache",".pytest_cache",".pythonlibs",".venv","__pycache__","cache","chroma_db","data/exports","node_modules","venv"],"exclude_globs":["*.bmp","*.db","*.feather","*.gif","*.gz","*.ico","*.jpeg","*.jpg","*.jsonl","*.lock","*.log","*.parquet","*.png","*.sqlite","*.sqlite3","*.tar","*.webp","*.zip"],"max_file_bytes":400000},"summary":{"file_count":876,"total_bytes":9265289},"files":[{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/_vendor/typeguard/_checkers.py","size":31360,"sha1":"00773d1618fc9192128cd890a93c5a9a6358a6b5","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"from __future__ import annotations\n\nimport collections.abc\nimport inspect\nimport sys\nimport types\nimport typing\nimport warnings\nfrom enum import Enum\nfrom inspect import Parameter, isclass, isfunction\nfrom io import BufferedIOBase, IOBase, RawIOBase, TextIOBase\nfrom textwrap import indent\nfrom typing import (\n    IO,\n    AbstractSet,\n    Any,\n    BinaryIO,\n    Callable,\n    Dict,\n    ForwardRef,\n    List,\n    Mapping,\n    MutableMapping,\n    NewType,\n    Optional,\n    Sequence,\n    Set,\n    TextIO,\n    Tuple,\n    Type,\n    TypeVar,\n    Union,\n)\nfrom unittest.mock import Mock\nfrom weakref import WeakKeyDictionary\n\ntry:\n    import typing_extensions\nexcept ImportError:\n    typing_extensions = None  # type: ignore[assignment]\n\n# Must use this because typing.is_typeddict does not recognize\n# TypedDict from typing_extensions, and as of version 4.12.0\n# typing_extensions.TypedDict is different from typing.TypedDict\n# on all versions.\nfrom typing_extensions import is_typeddict\n\nfrom ._config import ForwardRefPolicy\nfrom ._exceptions import TypeCheckError, TypeHintWarning\nfrom ._memo import TypeCheckMemo\nfrom ._utils import evaluate_forwardref, get_stacklevel, get_type_name, qualified_name\n\nif sys.version_info >= (3, 11):\n    from typing import (\n        Annotated,\n        NotRequired,\n        TypeAlias,\n        get_args,\n        get_origin,\n    )\n\n    SubclassableAny = Any\nelse:\n    from typing_extensions import (\n        Annotated,\n        NotRequired,\n        TypeAlias,\n        get_args,\n        get_origin,\n    )\n    from typing_extensions import Any as SubclassableAny\n\nif sys.version_info >= (3, 10):\n    from importlib.metadata import entry_points\n    from typing import ParamSpec\nelse:\n    from importlib_metadata import entry_points\n    from typing_extensions import ParamSpec\n\nTypeCheckerCallable: TypeAlias = Callable[\n    [Any, Any, Tuple[Any, ...], TypeCheckMemo], Any\n]\nTypeCheckLookupCallback: TypeAlias = Callable[\n    [Any, Tuple[Any, ...], Tuple[Any, ...]], Optional[TypeCheckerCallable]\n]\n\nchecker_lookup_functions: list[TypeCheckLookupCallback] = []\ngeneric_alias_types: tuple[type, ...] = (type(List), type(List[Any]))\nif sys.version_info >= (3, 9):\n    generic_alias_types += (types.GenericAlias,)\n\nprotocol_check_cache: WeakKeyDictionary[\n    type[Any], dict[type[Any], TypeCheckError | None]\n] = WeakKeyDictionary()\n\n# Sentinel\n_missing = object()\n\n# Lifted from mypy.sharedparse\nBINARY_MAGIC_METHODS = {\n    \"__add__\",\n    \"__and__\",\n    \"__cmp__\",\n    \"__divmod__\",\n    \"__div__\",\n    \"__eq__\",\n    \"__floordiv__\",\n    \"__ge__\",\n    \"__gt__\",\n    \"__iadd__\",\n    \"__iand__\",\n    \"__idiv__\",\n    \"__ifloordiv__\",\n    \"__ilshift__\",\n    \"__imatmul__\",\n    \"__imod__\",\n    \"__imul__\",\n    \"__ior__\",\n    \"__ipow__\",\n    \"__irshift__\",\n    \"__isub__\",\n    \"__itruediv__\",\n    \"__ixor__\",\n    \"__le__\",\n    \"__lshift__\",\n    \"__lt__\",\n    \"__matmul__\",\n    \"__mod__\",\n    \"__mul__\",\n    \"__ne__\",\n    \"__or__\",\n    \"__pow__\",\n    \"__radd__\",\n    \"__rand__\",\n    \"__rdiv__\",\n    \"__rfloordiv__\",\n    \"__rlshift__\",\n    \"__rmatmul__\",\n    \"__rmod__\",\n    \"__rmul__\",\n    \"__ror__\",\n    \"__rpow__\",\n    \"__rrshift__\",\n    \"__rshift__\",\n    \"__rsub__\",\n    \"__rtruediv__\",\n    \"__rxor__\",\n    \"__sub__\",\n    \"__truediv__\",\n    \"__xor__\",\n}\n\n\ndef check_callable(\n    value: Any,\n    origin_type: Any,\n    args: tuple[Any, ...],\n    memo: TypeCheckMemo,\n) -> None:\n    if not callable(value):\n        raise TypeCheckError(\"is not callable\")\n\n    if args:\n        try:\n            signature = inspect.signature(value)\n        except (TypeError, ValueError):\n            return\n\n        argument_types = args[0]\n        if isinstance(argument_types, list) and not any(\n            type(item) is ParamSpec for item in argument_types\n        ):\n            # The callable must not have keyword-only arguments without defaults\n            unfulfilled_kwonlyargs = [\n                param.name\n                for param in signature.parameters.values()\n                if param.kind == Parameter.KEYWORD_ONLY\n                and param.default == Parameter.empty\n            ]\n            if unfulfilled_kwonlyargs:\n                raise TypeCheckError(\n                    f\"has mandatory keyword-only arguments in its declaration: \"\n                    f'{\", \".join(unfulfilled_kwonlyargs)}'\n                )\n\n            num_positional_args = num_mandatory_pos_args = 0\n            has_varargs = False\n            for param in signature.parameters.values():\n                if param.kind in (\n                    Parameter.POSITIONAL_ONLY,\n                    Parameter.POSITIONAL_OR_KEYWORD,\n                ):\n                    num_positional_args += 1\n                    if param.default is Parameter.empty:\n                        num_mandatory_pos_args += 1\n                elif param.kind == Parameter.VAR_POSITIONAL:\n                    has_varargs = True\n\n            if num_mandatory_pos_args > len(argument_types):\n                raise TypeCheckError(\n                    f\"has too many mandatory positional arguments in its declaration; \"\n                    f\"expected {len(argument_types)} but {num_mandatory_pos_args} \"\n                    f\"mandatory positional argument(s) declared\"\n                )\n            elif not has_varargs and num_positional_args < len(argument_types):\n                raise TypeCheckError(\n                    f\"has too few arguments in its declaration; expected \"\n                    f\"{len(argument_types)} but {num_positional_args} argument(s) \"\n                    f\"declared\"\n                )\n\n\ndef check_mapping(\n    value: Any,\n    origin_type: Any,\n    args: tuple[Any, ...],\n    memo: TypeCheckMemo,\n) -> None:\n    if origin_type is Dict or origin_type is dict:\n        if not isinstance(value, dict):\n            raise TypeCheckError(\"is not a dict\")\n    if origin_type is MutableMapping or origin_type is collections.abc.MutableMapping:\n        if not isinstance(value, collections.abc.MutableMapping):\n            raise TypeCheckError(\"is not a mutable mapping\")\n    elif not isinstance(value, collections.abc.Mapping):\n        raise TypeCheckError(\"is not a mapping\")\n\n    if args:\n        key_type, value_type = args\n        if key_type is not Any or value_type is not Any:\n            samples = memo.config.collection_check_strategy.iterate_samples(\n                value.items()\n            )\n            for k, v in samples:\n                try:\n                    check_type_internal(k, key_type, memo)\n                except TypeCheckError as exc:\n                    exc.append_path_element(f\"key {k!r}\")\n                    raise\n\n                try:\n                    check_type_internal(v, value_type, memo)\n                except TypeCheckError as exc:\n                    exc.append_path_element(f\"value of key {k!r}\")\n                    raise\n\n\ndef check_typed_dict(\n    value: Any,\n    origin_type: Any,\n    args: tuple[Any, ...],\n    memo: TypeCheckMemo,\n) -> None:\n    if not isinstance(value, dict):\n        raise TypeCheckError(\"is not a dict\")\n\n    declared_keys = frozenset(origin_type.__annotations__)\n    if hasattr(origin_type, \"__required_keys__\"):\n        required_keys = set(origin_type.__required_keys__)\n    else:  # py3.8 and lower\n        required_keys = set(declared_keys) if origin_type.__total__ else set()\n\n    existing_keys = set(value)\n    extra_keys = existing_keys - declared_keys\n    if extra_keys:\n        keys_formatted = \", \".join(f'\"{key}\"' for key in sorted(extra_keys, key=repr))\n        raise TypeCheckError(f\"has unexpected extra key(s): {keys_formatted}\")\n\n    # Detect NotRequired fields which are hidden by get_type_hints()\n    type_hints: dict[str, type] = {}\n    for key, annotation in origin_type.__annotations__.items():\n        if isinstance(annotation, ForwardRef):\n            annotation = evaluate_forwardref(annotation, memo)\n            if get_origin(annotation) is NotRequired:\n                required_keys.discard(key)\n                annotation = get_args(annotation)[0]\n\n        type_hints[key] = annotation\n\n    missing_keys = required_keys - existing_keys\n    if missing_keys:\n        keys_formatted = \", \".join(f'\"{key}\"' for key in sorted(missing_keys, key=repr))\n        raise TypeCheckError(f\"is missing required key(s): {keys_formatted}\")\n\n    for key, argtype in type_hints.items():\n        argvalue = value.get(key, _missing)\n        if argvalue is not _missing:\n            try:\n                check_type_internal(argvalue, argtype, memo)\n            except TypeCheckError as exc:\n                exc.append_path_element(f\"value of key {key!r}\")\n                raise\n\n\ndef check_list(\n    value: Any,\n    origin_type: Any,\n    args: tuple[Any, ...],\n    memo: TypeCheckMemo,\n) -> None:\n    if not isinstance(value, list):\n        raise TypeCheckError(\"is not a list\")\n\n    if args and args != (Any,):\n        samples = memo.config.collection_check_strategy.iterate_samples(value)\n        for i, v in enumerate(samples):\n            try:\n                check_type_internal(v, args[0], memo)\n            except TypeCheckError as exc:\n                exc.append_path_element(f\"item {i}\")\n                raise\n\n\ndef check_sequence(\n    value: Any,\n    origin_type: Any,\n    args: tuple[Any, ...],\n    memo: TypeCheckMemo,\n) -> None:\n    if not isinstance(value, collections.abc.Sequence):\n        raise TypeCheckError(\"is not a sequence\")\n\n    if args and args != (Any,):\n        samples = memo.config.collection_check_strategy.iterate_samples(value)\n        for i, v in enumerate(samples):\n            try:\n                check_type_internal(v, args[0], memo)\n            except TypeCheckError as exc:\n                exc.append_path_element(f\"item {i}\")\n                raise\n\n\ndef check_set(\n    value: Any,\n    origin_type: Any,\n    args: tuple[Any, ...],\n    memo: TypeCheckMemo,\n) -> None:\n    if origin_type is frozenset:\n        if not isinstance(value, frozenset):\n            raise TypeCheckError(\"is not a frozenset\")\n    elif not isinstance(value, AbstractSet):\n        raise TypeCheckError(\"is not a set\")\n\n    if args and args != (Any,):\n        samples = memo.config.collection_check_strategy.iterate_samples(value)\n        for v in samples:\n            try:\n                check_type_internal(v, args[0], memo)\n            except TypeCheckError as exc:\n                exc.append_path_element(f\"[{v}]\")\n                raise\n\n\ndef check_tuple(\n    value: Any,\n    origin_type: Any,\n    args: tuple[Any, ...],\n    memo: TypeCheckMemo,\n) -> None:\n    # Specialized check for NamedTuples\n    if field_types := getattr(origin_type, \"__annotations__\", None):\n        if not isinstance(value, origin_type):\n            raise TypeCheckError(\n                f\"is not a named tuple of type {qualified_name(origin_type)}\"\n            )\n\n        for name, field_type in field_types.items():\n            try:\n                check_type_internal(getattr(value, name), field_type, memo)\n            except TypeCheckError as exc:\n                exc.append_path_element(f\"attribute {name!r}\")\n                raise\n\n        return\n    elif not isinstance(value, tuple):\n        raise TypeCheckError(\"is not a tuple\")\n\n    if args:\n        use_ellipsis = args[-1] is Ellipsis\n        tuple_params = args[: -1 if use_ellipsis else None]\n    else:\n        # Unparametrized Tuple or plain tuple\n        return\n\n    if use_ellipsis:\n        element_type = tuple_params[0]\n        samples = memo.config.collection_check_strategy.iterate_samples(value)\n        for i, element in enumerate(samples):\n            try:\n                check_type_internal(element, element_type, memo)\n            except TypeCheckError as exc:\n                exc.append_path_element(f\"item {i}\")\n                raise\n    elif tuple_params == ((),):\n        if value != ():\n            raise TypeCheckError(\"is not an empty tuple\")\n    else:\n        if len(value) != len(tuple_params):\n            raise TypeCheckError(\n                f\"has wrong number of elements (expected {len(tuple_params)}, got \"\n                f\"{len(value)} instead)\"\n            )\n\n        for i, (element, element_type) in enumerate(zip(value, tuple_params)):\n            try:\n                check_type_internal(element, element_type, memo)\n            except TypeCheckError as exc:\n                exc.append_path_element(f\"item {i}\")\n                raise\n\n\ndef check_union(\n    value: Any,\n    origin_type: Any,\n    args: tuple[Any, ...],\n    memo: TypeCheckMemo,\n) -> None:\n    errors: dict[str, TypeCheckError] = {}\n    try:\n        for type_ in args:\n            try:\n                check_type_internal(value, type_, memo)\n                return\n            except TypeCheckError as exc:\n                errors[get_type_name(type_)] = exc\n\n        formatted_errors = indent(\n            \"\\n\".join(f\"{key}: {error}\" for key, error in errors.items()), \"  \"\n        )\n    finally:\n        del errors  # avoid creating ref cycle\n    raise TypeCheckError(f\"did not match any element in the union:\\n{formatted_errors}\")\n\n\ndef check_uniontype(\n    value: Any,\n    origin_type: Any,\n    args: tuple[Any, ...],\n    memo: TypeCheckMemo,\n) -> None:\n    errors: dict[str, TypeCheckError] = {}\n    for type_ in args:\n        try:\n            check_type_internal(value, type_, memo)\n            return\n        except TypeCheckError as exc:\n            errors[get_type_name(type_)] = exc\n\n    formatted_errors = indent(\n        \"\\n\".join(f\"{key}: {error}\" for key, error in errors.items()), \"  \"\n    )\n    raise TypeCheckError(f\"did not match any element in the union:\\n{formatted_errors}\")\n\n\ndef check_class(\n    value: Any,\n    origin_type: Any,\n    args: tuple[Any, ...],\n    memo: TypeCheckMemo,\n) -> None:\n    if not isclass(value) and not isinstance(value, generic_alias_types):\n        raise TypeCheckError(\"is not a class\")\n\n    if not args:\n        return\n\n    if isinstance(args[0], ForwardRef):\n        expected_class = evaluate_forwardref(args[0], memo)\n    else:\n        expected_class = args[0]\n\n    if expected_class is Any:\n        return\n    elif getattr(expected_class, \"_is_protocol\", False):\n        check_protocol(value, expected_class, (), memo)\n    elif isinstance(expected_class, TypeVar):\n        check_typevar(value, expected_class, (), memo, subclass_check=True)\n    elif get_origin(expected_class) is Union:\n        errors: dict[str, TypeCheckError] = {}\n        for arg in get_args(expected_class):\n            if arg is Any:\n                return\n\n            try:\n                check_class(value, type, (arg,), memo)\n                return\n            except TypeCheckError as exc:\n                errors[get_type_name(arg)] = exc\n        else:\n            formatted_errors = indent(\n                \"\\n\".join(f\"{key}: {error}\" for key, error in errors.items()), \"  \"\n            )\n            raise TypeCheckError(\n                f\"did not match any element in the union:\\n{formatted_errors}\"\n            )\n    elif not issubclass(value, expected_class):  # type: ignore[arg-type]\n        raise TypeCheckError(f\"is not a subclass of {qualified_name(expected_class)}\")\n\n\ndef check_newtype(\n    value: Any,\n    origin_type: Any,\n    args: tuple[Any, ...],\n    memo: TypeCheckMemo,\n) -> None:\n    check_type_internal(value, origin_type.__supertype__, memo)\n\n\ndef check_instance(\n    value: Any,\n    origin_type: Any,\n    args: tuple[Any, ...],\n    memo: TypeCheckMemo,\n) -> None:\n    if not isinstance(value, origin_type):\n        raise TypeCheckError(f\"is not an instance of {qualified_name(origin_type)}\")\n\n\ndef check_typevar(\n    value: Any,\n    origin_type: TypeVar,\n    args: tuple[Any, ...],\n    memo: TypeCheckMemo,\n    *,\n    subclass_check: bool = False,\n) -> None:\n    if origin_type.__bound__ is not None:\n        annotation = (\n            Type[origin_type.__bound__] if subclass_check else origin_type.__bound__\n        )\n        check_type_internal(value, annotation, memo)\n    elif origin_type.__constraints__:\n        for constraint in origin_type.__constraints__:\n            annotation = Type[constraint] if subclass_check else constraint\n            try:\n                check_type_internal(value, annotation, memo)\n            except TypeCheckError:\n                pass\n            else:\n                break\n        else:\n            formatted_constraints = \", \".join(\n                get_type_name(constraint) for constraint in origin_type.__constraints__\n            )\n            raise TypeCheckError(\n                f\"does not match any of the constraints \" f\"({formatted_constraints})\"\n            )\n\n\nif typing_extensions is None:\n\n    def _is_literal_type(typ: object) -> bool:\n        return typ is typing.Literal\n\nelse:\n\n    def _is_literal_type(typ: object) -> bool:\n        return typ is typing.Literal or typ is typing_extensions.Literal\n\n\ndef check_literal(\n    value: Any,\n    origin_type: Any,\n    args: tuple[Any, ...],\n    memo: TypeCheckMemo,\n) -> None:\n    def get_literal_args(literal_args: tuple[Any, ...]) -> tuple[Any, ...]:\n        retval: list[Any] = []\n        for arg in literal_args:\n            if _is_literal_type(get_origin(arg)):\n                retval.extend(get_literal_args(arg.__args__))\n            elif arg is None or isinstance(arg, (int, str, bytes, bool, Enum)):\n                retval.append(arg)\n            else:\n                raise TypeError(\n                    f\"Illegal literal value: {arg}\"\n                )  # TypeError here is deliberate\n\n        return tuple(retval)\n\n    final_args = tuple(get_literal_args(args))\n    try:\n        index = final_args.index(value)\n    except ValueError:\n        pass\n    else:\n        if type(final_args[index]) is type(value):\n            return\n\n    formatted_args = \", \".join(repr(arg) for arg in final_args)\n    raise TypeCheckError(f\"is not any of ({formatted_args})\") from None\n\n\ndef check_literal_string(\n    value: Any,\n    origin_type: Any,\n    args: tuple[Any, ...],\n    memo: TypeCheckMemo,\n) -> None:\n    check_type_internal(value, str, memo)\n\n\ndef check_typeguard(\n    value: Any,\n    origin_type: Any,\n    args: tuple[Any, ...],\n    memo: TypeCheckMemo,\n) -> None:\n    check_type_internal(value, bool, memo)\n\n\ndef check_none(\n    value: Any,\n    origin_type: Any,\n    args: tuple[Any, ...],\n    memo: TypeCheckMemo,\n) -> None:\n    if value is not None:\n        raise TypeCheckError(\"is not None\")\n\n\ndef check_number(\n    value: Any,\n    origin_type: Any,\n    args: tuple[Any, ...],\n    memo: TypeCheckMemo,\n) -> None:\n    if origin_type is complex and not isinstance(value, (complex, float, int)):\n        raise TypeCheckError(\"is neither complex, float or int\")\n    elif origin_type is float and not isinstance(value, (float, int)):\n        raise TypeCheckError(\"is neither float or int\")\n\n\ndef check_io(\n    value: Any,\n    origin_type: Any,\n    args: tuple[Any, ...],\n    memo: TypeCheckMemo,\n) -> None:\n    if origin_type is TextIO or (origin_type is IO and args == (str,)):\n        if not isinstance(value, TextIOBase):\n            raise TypeCheckError(\"is not a text based I/O object\")\n    elif origin_type is BinaryIO or (origin_type is IO and args == (bytes,)):\n        if not isinstance(value, (RawIOBase, BufferedIOBase)):\n            raise TypeCheckError(\"is not a binary I/O object\")\n    elif not isinstance(value, IOBase):\n        raise TypeCheckError(\"is not an I/O object\")\n\n\ndef check_protocol(\n    value: Any,\n    origin_type: Any,\n    args: tuple[Any, ...],\n    memo: TypeCheckMemo,\n) -> None:\n    subject: type[Any] = value if isclass(value) else type(value)\n\n    if subject in protocol_check_cache:\n        result_map = protocol_check_cache[subject]\n        if origin_type in result_map:\n            if exc := result_map[origin_type]:\n                raise exc\n            else:\n                return\n\n    # Collect a set of methods and non-method attributes present in the protocol\n    ignored_attrs = set(dir(typing.Protocol)) | {\n        \"__annotations__\",\n        \"__non_callable_proto_members__\",\n    }\n    expected_methods: dict[str, tuple[Any, Any]] = {}\n    expected_noncallable_members: dict[str, Any] = {}\n    for attrname in dir(origin_type):\n        # Skip attributes present in typing.Protocol\n        if attrname in ignored_attrs:\n            continue\n\n        member = getattr(origin_type, attrname)\n        if callable(member):\n            signature = inspect.signature(member)\n            argtypes = [\n                (p.annotation if p.annotation is not Parameter.empty else Any)\n                for p in signature.parameters.values()\n                if p.kind is not Parameter.KEYWORD_ONLY\n            ] or Ellipsis\n            return_annotation = (\n                signature.return_annotation\n                if signature.return_annotation is not Parameter.empty\n                else Any\n            )\n            expected_methods[attrname] = argtypes, return_annotation\n        else:\n            expected_noncallable_members[attrname] = member\n\n    for attrname, annotation in typing.get_type_hints(origin_type).items():\n        expected_noncallable_members[attrname] = annotation\n\n    subject_annotations = typing.get_type_hints(subject)\n\n    # Check that all required methods are present and their signatures are compatible\n    result_map = protocol_check_cache.setdefault(subject, {})\n    try:\n        for attrname, callable_args in expected_methods.items():\n            try:\n                method = getattr(subject, attrname)\n            except AttributeError:\n                if attrname in subject_annotations:\n                    raise TypeCheckError(\n                        f\"is not compatible with the {origin_type.__qualname__} protocol \"\n                        f\"because its {attrname!r} attribute is not a method\"\n                    ) from None\n                else:\n                    raise TypeCheckError(\n                        f\"is not compatible with the {origin_type.__qualname__} protocol \"\n                        f\"because it has no method named {attrname!r}\"\n                    ) from None\n\n            if not callable(method):\n                raise TypeCheckError(\n                    f\"is not compatible with the {origin_type.__qualname__} protocol \"\n                    f\"because its {attrname!r} attribute is not a callable\"\n                )\n\n            # TODO: raise exception on added keyword-only arguments without defaults\n            try:\n                check_callable(method, Callable, callable_args, memo)\n            except TypeCheckError as exc:\n                raise TypeCheckError(\n                    f\"is not compatible with the {origin_type.__qualname__} protocol \"\n                    f\"because its {attrname!r} method {exc}\"\n                ) from None\n\n        # Check that all required non-callable members are present\n        for attrname in expected_noncallable_members:\n            # TODO: implement assignability checks for non-callable members\n            if attrname not in subject_annotations and not hasattr(subject, attrname):\n                raise TypeCheckError(\n                    f\"is not compatible with the {origin_type.__qualname__} protocol \"\n                    f\"because it has no attribute named {attrname!r}\"\n                )\n    except TypeCheckError as exc:\n        result_map[origin_type] = exc\n        raise\n    else:\n        result_map[origin_type] = None\n\n\ndef check_byteslike(\n    value: Any,\n    origin_type: Any,\n    args: tuple[Any, ...],\n    memo: TypeCheckMemo,\n) -> None:\n    if not isinstance(value, (bytearray, bytes, memoryview)):\n        raise TypeCheckError(\"is not bytes-like\")\n\n\ndef check_self(\n    value: Any,\n    origin_type: Any,\n    args: tuple[Any, ...],\n    memo: TypeCheckMemo,\n) -> None:\n    if memo.self_type is None:\n        raise TypeCheckError(\"cannot be checked against Self outside of a method call\")\n\n    if isclass(value):\n        if not issubclass(value, memo.self_type):\n            raise TypeCheckError(\n                f\"is not an instance of the self type \"\n                f\"({qualified_name(memo.self_type)})\"\n            )\n    elif not isinstance(value, memo.self_type):\n        raise TypeCheckError(\n            f\"is not an instance of the self type ({qualified_name(memo.self_type)})\"\n        )\n\n\ndef check_paramspec(\n    value: Any,\n    origin_type: Any,\n    args: tuple[Any, ...],\n    memo: TypeCheckMemo,\n) -> None:\n    pass  # No-op for now\n\n\ndef check_instanceof(\n    value: Any,\n    origin_type: Any,\n    args: tuple[Any, ...],\n    memo: TypeCheckMemo,\n) -> None:\n    if not isinstance(value, origin_type):\n        raise TypeCheckError(f\"is not an instance of {qualified_name(origin_type)}\")\n\n\ndef check_type_internal(\n    value: Any,\n    annotation: Any,\n    memo: TypeCheckMemo,\n) -> None:\n    \"\"\"\n    Check that the given object is compatible with the given type annotation.\n\n    This function should only be used by type checker callables. Applications should use\n    :func:`~.check_type` instead.\n\n    :param value: the value to check\n    :param annotation: the type annotation to check against\n    :param memo: a memo object containing configuration and information necessary for\n        looking up forward references\n    \"\"\"\n\n    if isinstance(annotation, ForwardRef):\n        try:\n            annotation = evaluate_forwardref(annotation, memo)\n        except NameError:\n            if memo.config.forward_ref_policy is ForwardRefPolicy.ERROR:\n                raise\n            elif memo.config.forward_ref_policy is ForwardRefPolicy.WARN:\n                warnings.warn(\n                    f\"Cannot resolve forward reference {annotation.__forward_arg__!r}\",\n                    TypeHintWarning,\n                    stacklevel=get_stacklevel(),\n                )\n\n            return\n\n    if annotation is Any or annotation is SubclassableAny or isinstance(value, Mock):\n        return\n\n    # Skip type checks if value is an instance of a class that inherits from Any\n    if not isclass(value) and SubclassableAny in type(value).__bases__:\n        return\n\n    extras: tuple[Any, ...]\n    origin_type = get_origin(annotation)\n    if origin_type is Annotated:\n        annotation, *extras_ = get_args(annotation)\n        extras = tuple(extras_)\n        origin_type = get_origin(annotation)\n    else:\n        extras = ()\n\n    if origin_type is not None:\n        args = get_args(annotation)\n\n        # Compatibility hack to distinguish between unparametrized and empty tuple\n        # (tuple[()]), necessary due to https://github.com/python/cpython/issues/91137\n        if origin_type in (tuple, Tuple) and annotation is not Tuple and not args:\n            args = ((),)\n    else:\n        origin_type = annotation\n        args = ()\n\n    for lookup_func in checker_lookup_functions:\n        checker = lookup_func(origin_type, args, extras)\n        if checker:\n            checker(value, origin_type, args, memo)\n            return\n\n    if isclass(origin_type):\n        if not isinstance(value, origin_type):\n            raise TypeCheckError(f\"is not an instance of {qualified_name(origin_type)}\")\n    elif type(origin_type) is str:  # noqa: E721\n        warnings.warn(\n            f\"Skipping type check against {origin_type!r}; this looks like a \"\n            f\"string-form forward reference imported from another module\",\n            TypeHintWarning,\n            stacklevel=get_stacklevel(),\n        )\n\n\n# Equality checks are applied to these\norigin_type_checkers = {\n    bytes: check_byteslike,\n    AbstractSet: check_set,\n    BinaryIO: check_io,\n    Callable: check_callable,\n    collections.abc.Callable: check_callable,\n    complex: check_number,\n    dict: check_mapping,\n    Dict: check_mapping,\n    float: check_number,\n    frozenset: check_set,\n    IO: check_io,\n    list: check_list,\n    List: check_list,\n    typing.Literal: check_literal,\n    Mapping: check_mapping,\n    MutableMapping: check_mapping,\n    None: check_none,\n    collections.abc.Mapping: check_mapping,\n    collections.abc.MutableMapping: check_mapping,\n    Sequence: check_sequence,\n    collections.abc.Sequence: check_sequence,\n    collections.abc.Set: check_set,\n    set: check_set,\n    Set: check_set,\n    TextIO: check_io,\n    tuple: check_tuple,\n    Tuple: check_tuple,\n    type: check_class,\n    Type: check_class,\n    Union: check_union,\n}\nif sys.version_info >= (3, 10):\n    origin_type_checkers[types.UnionType] = check_uniontype\n    origin_type_checkers[typing.TypeGuard] = check_typeguard\nif sys.version_info >= (3, 11):\n    origin_type_checkers.update(\n        {typing.LiteralString: check_literal_string, typing.Self: check_self}\n    )\nif typing_extensions is not None:\n    # On some Python versions, these may simply be re-exports from typing,\n    # but exactly which Python versions is subject to change,\n    # so it's best to err on the safe side\n    # and update the dictionary on all Python versions\n    # if typing_extensions is installed\n    origin_type_checkers[typing_extensions.Literal] = check_literal\n    origin_type_checkers[typing_extensions.LiteralString] = check_literal_string\n    origin_type_checkers[typing_extensions.Self] = check_self\n    origin_type_checkers[typing_extensions.TypeGuard] = check_typeguard\n\n\ndef builtin_checker_lookup(\n    origin_type: Any, args: tuple[Any, ...], extras: tuple[Any, ...]\n) -> TypeCheckerCallable | None:\n    checker = origin_type_checkers.get(origin_type)\n    if checker is not None:\n        return checker\n    elif is_typeddict(origin_type):\n        return check_typed_dict\n    elif isclass(origin_type) and issubclass(\n        origin_type,\n        Tuple,  # type: ignore[arg-type]\n    ):\n        # NamedTuple\n        return check_tuple\n    elif getattr(origin_type, \"_is_protocol\", False):\n        return check_protocol\n    elif isinstance(origin_type, ParamSpec):\n        return check_paramspec\n    elif isinstance(origin_type, TypeVar):\n        return check_typevar\n    elif origin_type.__class__ is NewType:\n        # typing.NewType on Python 3.10+\n        return check_newtype\n    elif (\n        isfunction(origin_type)\n        and getattr(origin_type, \"__module__\", None) == \"typing\"\n        and getattr(origin_type, \"__qualname__\", \"\").startswith(\"NewType.\")\n        and hasattr(origin_type, \"__supertype__\")\n    ):\n        # typing.NewType on Python 3.9 and below\n        return check_newtype\n\n    return None\n\n\nchecker_lookup_functions.append(builtin_checker_lookup)\n\n\ndef load_plugins() -> None:\n    \"\"\"\n    Load all type checker lookup functions from entry points.\n\n    All entry points from the ``typeguard.checker_lookup`` group are loaded, and the\n    returned lookup functions are added to :data:`typeguard.checker_lookup_functions`.\n\n    .. note:: This function is called implicitly on import, unless the\n        ``TYPEGUARD_DISABLE_PLUGIN_AUTOLOAD`` environment variable is present.\n    \"\"\"\n\n    for ep in entry_points(group=\"typeguard.checker_lookup\"):\n        try:\n            plugin = ep.load()\n        except Exception as exc:\n            warnings.warn(\n                f\"Failed to load plugin {ep.name!r}: \" f\"{qualified_name(exc)}: {exc}\",\n                stacklevel=2,\n            )\n            continue\n\n        if not callable(plugin):\n            warnings.warn(\n                f\"Plugin {ep} returned a non-callable object: {plugin!r}\", stacklevel=2\n            )\n            continue\n\n        checker_lookup_functions.insert(0, plugin)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/_vendor/typeguard/_config.py","size":2846,"sha1":"bd97ea16d3376cface457f53439bf69baa86279b","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"from __future__ import annotations\n\nfrom collections.abc import Iterable\nfrom dataclasses import dataclass\nfrom enum import Enum, auto\nfrom typing import TYPE_CHECKING, TypeVar\n\nif TYPE_CHECKING:\n    from ._functions import TypeCheckFailCallback\n\nT = TypeVar(\"T\")\n\n\nclass ForwardRefPolicy(Enum):\n    \"\"\"\n    Defines how unresolved forward references are handled.\n\n    Members:\n\n    * ``ERROR``: propagate the :exc:`NameError` when the forward reference lookup fails\n    * ``WARN``: emit a :class:`~.TypeHintWarning` if the forward reference lookup fails\n    * ``IGNORE``: silently skip checks for unresolveable forward references\n    \"\"\"\n\n    ERROR = auto()\n    WARN = auto()\n    IGNORE = auto()\n\n\nclass CollectionCheckStrategy(Enum):\n    \"\"\"\n    Specifies how thoroughly the contents of collections are type checked.\n\n    This has an effect on the following built-in checkers:\n\n    * ``AbstractSet``\n    * ``Dict``\n    * ``List``\n    * ``Mapping``\n    * ``Set``\n    * ``Tuple[<type>, ...]`` (arbitrarily sized tuples)\n\n    Members:\n\n    * ``FIRST_ITEM``: check only the first item\n    * ``ALL_ITEMS``: check all items\n    \"\"\"\n\n    FIRST_ITEM = auto()\n    ALL_ITEMS = auto()\n\n    def iterate_samples(self, collection: Iterable[T]) -> Iterable[T]:\n        if self is CollectionCheckStrategy.FIRST_ITEM:\n            try:\n                return [next(iter(collection))]\n            except StopIteration:\n                return ()\n        else:\n            return collection\n\n\n@dataclass\nclass TypeCheckConfiguration:\n    \"\"\"\n     You can change Typeguard's behavior with these settings.\n\n    .. attribute:: typecheck_fail_callback\n       :type: Callable[[TypeCheckError, TypeCheckMemo], Any]\n\n         Callable that is called when type checking fails.\n\n         Default: ``None`` (the :exc:`~.TypeCheckError` is raised directly)\n\n    .. attribute:: forward_ref_policy\n       :type: ForwardRefPolicy\n\n         Specifies what to do when a forward reference fails to resolve.\n\n         Default: ``WARN``\n\n    .. attribute:: collection_check_strategy\n       :type: CollectionCheckStrategy\n\n         Specifies how thoroughly the contents of collections (list, dict, etc.) are\n         type checked.\n\n         Default: ``FIRST_ITEM``\n\n    .. attribute:: debug_instrumentation\n       :type: bool\n\n         If set to ``True``, the code of modules or functions instrumented by typeguard\n         is printed to ``sys.stderr`` after the instrumentation is done\n\n         Requires Python 3.9 or newer.\n\n         Default: ``False``\n    \"\"\"\n\n    forward_ref_policy: ForwardRefPolicy = ForwardRefPolicy.WARN\n    typecheck_fail_callback: TypeCheckFailCallback | None = None\n    collection_check_strategy: CollectionCheckStrategy = (\n        CollectionCheckStrategy.FIRST_ITEM\n    )\n    debug_instrumentation: bool = False\n\n\nglobal_config = TypeCheckConfiguration()\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/_vendor/typeguard/_decorators.py","size":9033,"sha1":"92f305ed3b0c006e9fc818d12b0fb8a164bc5402","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"from __future__ import annotations\n\nimport ast\nimport inspect\nimport sys\nfrom collections.abc import Sequence\nfrom functools import partial\nfrom inspect import isclass, isfunction\nfrom types import CodeType, FrameType, FunctionType\nfrom typing import TYPE_CHECKING, Any, Callable, ForwardRef, TypeVar, cast, overload\nfrom warnings import warn\n\nfrom ._config import CollectionCheckStrategy, ForwardRefPolicy, global_config\nfrom ._exceptions import InstrumentationWarning\nfrom ._functions import TypeCheckFailCallback\nfrom ._transformer import TypeguardTransformer\nfrom ._utils import Unset, function_name, get_stacklevel, is_method_of, unset\n\nif TYPE_CHECKING:\n    from typeshed.stdlib.types import _Cell\n\n    _F = TypeVar(\"_F\")\n\n    def typeguard_ignore(f: _F) -> _F:\n        \"\"\"This decorator is a noop during static type-checking.\"\"\"\n        return f\n\nelse:\n    from typing import no_type_check as typeguard_ignore  # noqa: F401\n\nT_CallableOrType = TypeVar(\"T_CallableOrType\", bound=Callable[..., Any])\n\n\ndef make_cell(value: object) -> _Cell:\n    return (lambda: value).__closure__[0]  # type: ignore[index]\n\n\ndef find_target_function(\n    new_code: CodeType, target_path: Sequence[str], firstlineno: int\n) -> CodeType | None:\n    target_name = target_path[0]\n    for const in new_code.co_consts:\n        if isinstance(const, CodeType):\n            if const.co_name == target_name:\n                if const.co_firstlineno == firstlineno:\n                    return const\n                elif len(target_path) > 1:\n                    target_code = find_target_function(\n                        const, target_path[1:], firstlineno\n                    )\n                    if target_code:\n                        return target_code\n\n    return None\n\n\ndef instrument(f: T_CallableOrType) -> FunctionType | str:\n    if not getattr(f, \"__code__\", None):\n        return \"no code associated\"\n    elif not getattr(f, \"__module__\", None):\n        return \"__module__ attribute is not set\"\n    elif f.__code__.co_filename == \"<stdin>\":\n        return \"cannot instrument functions defined in a REPL\"\n    elif hasattr(f, \"__wrapped__\"):\n        return (\n            \"@typechecked only supports instrumenting functions wrapped with \"\n            \"@classmethod, @staticmethod or @property\"\n        )\n\n    target_path = [item for item in f.__qualname__.split(\".\") if item != \"<locals>\"]\n    module_source = inspect.getsource(sys.modules[f.__module__])\n    module_ast = ast.parse(module_source)\n    instrumentor = TypeguardTransformer(target_path, f.__code__.co_firstlineno)\n    instrumentor.visit(module_ast)\n\n    if not instrumentor.target_node or instrumentor.target_lineno is None:\n        return \"instrumentor did not find the target function\"\n\n    module_code = compile(module_ast, f.__code__.co_filename, \"exec\", dont_inherit=True)\n    new_code = find_target_function(\n        module_code, target_path, instrumentor.target_lineno\n    )\n    if not new_code:\n        return \"cannot find the target function in the AST\"\n\n    if global_config.debug_instrumentation and sys.version_info >= (3, 9):\n        # Find the matching AST node, then unparse it to source and print to stdout\n        print(\n            f\"Source code of {f.__qualname__}() after instrumentation:\"\n            \"\\n----------------------------------------------\",\n            file=sys.stderr,\n        )\n        print(ast.unparse(instrumentor.target_node), file=sys.stderr)\n        print(\n            \"----------------------------------------------\",\n            file=sys.stderr,\n        )\n\n    closure = f.__closure__\n    if new_code.co_freevars != f.__code__.co_freevars:\n        # Create a new closure and find values for the new free variables\n        frame = cast(FrameType, inspect.currentframe())\n        frame = cast(FrameType, frame.f_back)\n        frame_locals = cast(FrameType, frame.f_back).f_locals\n        cells: list[_Cell] = []\n        for key in new_code.co_freevars:\n            if key in instrumentor.names_used_in_annotations:\n                # Find the value and make a new cell from it\n                value = frame_locals.get(key) or ForwardRef(key)\n                cells.append(make_cell(value))\n            else:\n                # Reuse the cell from the existing closure\n                assert f.__closure__\n                cells.append(f.__closure__[f.__code__.co_freevars.index(key)])\n\n        closure = tuple(cells)\n\n    new_function = FunctionType(new_code, f.__globals__, f.__name__, closure=closure)\n    new_function.__module__ = f.__module__\n    new_function.__name__ = f.__name__\n    new_function.__qualname__ = f.__qualname__\n    new_function.__annotations__ = f.__annotations__\n    new_function.__doc__ = f.__doc__\n    new_function.__defaults__ = f.__defaults__\n    new_function.__kwdefaults__ = f.__kwdefaults__\n    return new_function\n\n\n@overload\ndef typechecked(\n    *,\n    forward_ref_policy: ForwardRefPolicy | Unset = unset,\n    typecheck_fail_callback: TypeCheckFailCallback | Unset = unset,\n    collection_check_strategy: CollectionCheckStrategy | Unset = unset,\n    debug_instrumentation: bool | Unset = unset,\n) -> Callable[[T_CallableOrType], T_CallableOrType]: ...\n\n\n@overload\ndef typechecked(target: T_CallableOrType) -> T_CallableOrType: ...\n\n\ndef typechecked(\n    target: T_CallableOrType | None = None,\n    *,\n    forward_ref_policy: ForwardRefPolicy | Unset = unset,\n    typecheck_fail_callback: TypeCheckFailCallback | Unset = unset,\n    collection_check_strategy: CollectionCheckStrategy | Unset = unset,\n    debug_instrumentation: bool | Unset = unset,\n) -> Any:\n    \"\"\"\n    Instrument the target function to perform run-time type checking.\n\n    This decorator recompiles the target function, injecting code to type check\n    arguments, return values, yield values (excluding ``yield from``) and assignments to\n    annotated local variables.\n\n    This can also be used as a class decorator. This will instrument all type annotated\n    methods, including :func:`@classmethod <classmethod>`,\n    :func:`@staticmethod <staticmethod>`,  and :class:`@property <property>` decorated\n    methods in the class.\n\n    .. note:: When Python is run in optimized mode (``-O`` or ``-OO``, this decorator\n        is a no-op). This is a feature meant for selectively introducing type checking\n        into a code base where the checks aren't meant to be run in production.\n\n    :param target: the function or class to enable type checking for\n    :param forward_ref_policy: override for\n        :attr:`.TypeCheckConfiguration.forward_ref_policy`\n    :param typecheck_fail_callback: override for\n        :attr:`.TypeCheckConfiguration.typecheck_fail_callback`\n    :param collection_check_strategy: override for\n        :attr:`.TypeCheckConfiguration.collection_check_strategy`\n    :param debug_instrumentation: override for\n        :attr:`.TypeCheckConfiguration.debug_instrumentation`\n\n    \"\"\"\n    if target is None:\n        return partial(\n            typechecked,\n            forward_ref_policy=forward_ref_policy,\n            typecheck_fail_callback=typecheck_fail_callback,\n            collection_check_strategy=collection_check_strategy,\n            debug_instrumentation=debug_instrumentation,\n        )\n\n    if not __debug__:\n        return target\n\n    if isclass(target):\n        for key, attr in target.__dict__.items():\n            if is_method_of(attr, target):\n                retval = instrument(attr)\n                if isfunction(retval):\n                    setattr(target, key, retval)\n            elif isinstance(attr, (classmethod, staticmethod)):\n                if is_method_of(attr.__func__, target):\n                    retval = instrument(attr.__func__)\n                    if isfunction(retval):\n                        wrapper = attr.__class__(retval)\n                        setattr(target, key, wrapper)\n            elif isinstance(attr, property):\n                kwargs: dict[str, Any] = dict(doc=attr.__doc__)\n                for name in (\"fset\", \"fget\", \"fdel\"):\n                    property_func = kwargs[name] = getattr(attr, name)\n                    if is_method_of(property_func, target):\n                        retval = instrument(property_func)\n                        if isfunction(retval):\n                            kwargs[name] = retval\n\n                setattr(target, key, attr.__class__(**kwargs))\n\n        return target\n\n    # Find either the first Python wrapper or the actual function\n    wrapper_class: (\n        type[classmethod[Any, Any, Any]] | type[staticmethod[Any, Any]] | None\n    ) = None\n    if isinstance(target, (classmethod, staticmethod)):\n        wrapper_class = target.__class__\n        target = target.__func__\n\n    retval = instrument(target)\n    if isinstance(retval, str):\n        warn(\n            f\"{retval} -- not typechecking {function_name(target)}\",\n            InstrumentationWarning,\n            stacklevel=get_stacklevel(),\n        )\n        return target\n\n    if wrapper_class is None:\n        return retval\n    else:\n        return wrapper_class(retval)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/_vendor/typeguard/_exceptions.py","size":1121,"sha1":"5211ee55a93855b1c842ea7c9d42930c6893463b","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"from collections import deque\nfrom typing import Deque\n\n\nclass TypeHintWarning(UserWarning):\n    \"\"\"\n    A warning that is emitted when a type hint in string form could not be resolved to\n    an actual type.\n    \"\"\"\n\n\nclass TypeCheckWarning(UserWarning):\n    \"\"\"Emitted by typeguard's type checkers when a type mismatch is detected.\"\"\"\n\n    def __init__(self, message: str):\n        super().__init__(message)\n\n\nclass InstrumentationWarning(UserWarning):\n    \"\"\"Emitted when there's a problem with instrumenting a function for type checks.\"\"\"\n\n    def __init__(self, message: str):\n        super().__init__(message)\n\n\nclass TypeCheckError(Exception):\n    \"\"\"\n    Raised by typeguard's type checkers when a type mismatch is detected.\n    \"\"\"\n\n    def __init__(self, message: str):\n        super().__init__(message)\n        self._path: Deque[str] = deque()\n\n    def append_path_element(self, element: str) -> None:\n        self._path.append(element)\n\n    def __str__(self) -> str:\n        if self._path:\n            return \" of \".join(self._path) + \" \" + str(self.args[0])\n        else:\n            return str(self.args[0])\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/_vendor/typeguard/_functions.py","size":10393,"sha1":"ee2d9079ccdd10a9e24ac218c8a3673f268f27f3","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"from __future__ import annotations\n\nimport sys\nimport warnings\nfrom typing import Any, Callable, NoReturn, TypeVar, Union, overload\n\nfrom . import _suppression\nfrom ._checkers import BINARY_MAGIC_METHODS, check_type_internal\nfrom ._config import (\n    CollectionCheckStrategy,\n    ForwardRefPolicy,\n    TypeCheckConfiguration,\n)\nfrom ._exceptions import TypeCheckError, TypeCheckWarning\nfrom ._memo import TypeCheckMemo\nfrom ._utils import get_stacklevel, qualified_name\n\nif sys.version_info >= (3, 11):\n    from typing import Literal, Never, TypeAlias\nelse:\n    from typing_extensions import Literal, Never, TypeAlias\n\nT = TypeVar(\"T\")\nTypeCheckFailCallback: TypeAlias = Callable[[TypeCheckError, TypeCheckMemo], Any]\n\n\n@overload\ndef check_type(\n    value: object,\n    expected_type: type[T],\n    *,\n    forward_ref_policy: ForwardRefPolicy = ...,\n    typecheck_fail_callback: TypeCheckFailCallback | None = ...,\n    collection_check_strategy: CollectionCheckStrategy = ...,\n) -> T: ...\n\n\n@overload\ndef check_type(\n    value: object,\n    expected_type: Any,\n    *,\n    forward_ref_policy: ForwardRefPolicy = ...,\n    typecheck_fail_callback: TypeCheckFailCallback | None = ...,\n    collection_check_strategy: CollectionCheckStrategy = ...,\n) -> Any: ...\n\n\ndef check_type(\n    value: object,\n    expected_type: Any,\n    *,\n    forward_ref_policy: ForwardRefPolicy = TypeCheckConfiguration().forward_ref_policy,\n    typecheck_fail_callback: TypeCheckFailCallback | None = (\n        TypeCheckConfiguration().typecheck_fail_callback\n    ),\n    collection_check_strategy: CollectionCheckStrategy = (\n        TypeCheckConfiguration().collection_check_strategy\n    ),\n) -> Any:\n    \"\"\"\n    Ensure that ``value`` matches ``expected_type``.\n\n    The types from the :mod:`typing` module do not support :func:`isinstance` or\n    :func:`issubclass` so a number of type specific checks are required. This function\n    knows which checker to call for which type.\n\n    This function wraps :func:`~.check_type_internal` in the following ways:\n\n    * Respects type checking suppression (:func:`~.suppress_type_checks`)\n    * Forms a :class:`~.TypeCheckMemo` from the current stack frame\n    * Calls the configured type check fail callback if the check fails\n\n    Note that this function is independent of the globally shared configuration in\n    :data:`typeguard.config`. This means that usage within libraries is safe from being\n    affected configuration changes made by other libraries or by the integrating\n    application. Instead, configuration options have the same default values as their\n    corresponding fields in :class:`TypeCheckConfiguration`.\n\n    :param value: value to be checked against ``expected_type``\n    :param expected_type: a class or generic type instance, or a tuple of such things\n    :param forward_ref_policy: see :attr:`TypeCheckConfiguration.forward_ref_policy`\n    :param typecheck_fail_callback:\n        see :attr`TypeCheckConfiguration.typecheck_fail_callback`\n    :param collection_check_strategy:\n        see :attr:`TypeCheckConfiguration.collection_check_strategy`\n    :return: ``value``, unmodified\n    :raises TypeCheckError: if there is a type mismatch\n\n    \"\"\"\n    if type(expected_type) is tuple:\n        expected_type = Union[expected_type]\n\n    config = TypeCheckConfiguration(\n        forward_ref_policy=forward_ref_policy,\n        typecheck_fail_callback=typecheck_fail_callback,\n        collection_check_strategy=collection_check_strategy,\n    )\n\n    if _suppression.type_checks_suppressed or expected_type is Any:\n        return value\n\n    frame = sys._getframe(1)\n    memo = TypeCheckMemo(frame.f_globals, frame.f_locals, config=config)\n    try:\n        check_type_internal(value, expected_type, memo)\n    except TypeCheckError as exc:\n        exc.append_path_element(qualified_name(value, add_class_prefix=True))\n        if config.typecheck_fail_callback:\n            config.typecheck_fail_callback(exc, memo)\n        else:\n            raise\n\n    return value\n\n\ndef check_argument_types(\n    func_name: str,\n    arguments: dict[str, tuple[Any, Any]],\n    memo: TypeCheckMemo,\n) -> Literal[True]:\n    if _suppression.type_checks_suppressed:\n        return True\n\n    for argname, (value, annotation) in arguments.items():\n        if annotation is NoReturn or annotation is Never:\n            exc = TypeCheckError(\n                f\"{func_name}() was declared never to be called but it was\"\n            )\n            if memo.config.typecheck_fail_callback:\n                memo.config.typecheck_fail_callback(exc, memo)\n            else:\n                raise exc\n\n        try:\n            check_type_internal(value, annotation, memo)\n        except TypeCheckError as exc:\n            qualname = qualified_name(value, add_class_prefix=True)\n            exc.append_path_element(f'argument \"{argname}\" ({qualname})')\n            if memo.config.typecheck_fail_callback:\n                memo.config.typecheck_fail_callback(exc, memo)\n            else:\n                raise\n\n    return True\n\n\ndef check_return_type(\n    func_name: str,\n    retval: T,\n    annotation: Any,\n    memo: TypeCheckMemo,\n) -> T:\n    if _suppression.type_checks_suppressed:\n        return retval\n\n    if annotation is NoReturn or annotation is Never:\n        exc = TypeCheckError(f\"{func_name}() was declared never to return but it did\")\n        if memo.config.typecheck_fail_callback:\n            memo.config.typecheck_fail_callback(exc, memo)\n        else:\n            raise exc\n\n    try:\n        check_type_internal(retval, annotation, memo)\n    except TypeCheckError as exc:\n        # Allow NotImplemented if this is a binary magic method (__eq__() et al)\n        if retval is NotImplemented and annotation is bool:\n            # This does (and cannot) not check if it's actually a method\n            func_name = func_name.rsplit(\".\", 1)[-1]\n            if func_name in BINARY_MAGIC_METHODS:\n                return retval\n\n        qualname = qualified_name(retval, add_class_prefix=True)\n        exc.append_path_element(f\"the return value ({qualname})\")\n        if memo.config.typecheck_fail_callback:\n            memo.config.typecheck_fail_callback(exc, memo)\n        else:\n            raise\n\n    return retval\n\n\ndef check_send_type(\n    func_name: str,\n    sendval: T,\n    annotation: Any,\n    memo: TypeCheckMemo,\n) -> T:\n    if _suppression.type_checks_suppressed:\n        return sendval\n\n    if annotation is NoReturn or annotation is Never:\n        exc = TypeCheckError(\n            f\"{func_name}() was declared never to be sent a value to but it was\"\n        )\n        if memo.config.typecheck_fail_callback:\n            memo.config.typecheck_fail_callback(exc, memo)\n        else:\n            raise exc\n\n    try:\n        check_type_internal(sendval, annotation, memo)\n    except TypeCheckError as exc:\n        qualname = qualified_name(sendval, add_class_prefix=True)\n        exc.append_path_element(f\"the value sent to generator ({qualname})\")\n        if memo.config.typecheck_fail_callback:\n            memo.config.typecheck_fail_callback(exc, memo)\n        else:\n            raise\n\n    return sendval\n\n\ndef check_yield_type(\n    func_name: str,\n    yieldval: T,\n    annotation: Any,\n    memo: TypeCheckMemo,\n) -> T:\n    if _suppression.type_checks_suppressed:\n        return yieldval\n\n    if annotation is NoReturn or annotation is Never:\n        exc = TypeCheckError(f\"{func_name}() was declared never to yield but it did\")\n        if memo.config.typecheck_fail_callback:\n            memo.config.typecheck_fail_callback(exc, memo)\n        else:\n            raise exc\n\n    try:\n        check_type_internal(yieldval, annotation, memo)\n    except TypeCheckError as exc:\n        qualname = qualified_name(yieldval, add_class_prefix=True)\n        exc.append_path_element(f\"the yielded value ({qualname})\")\n        if memo.config.typecheck_fail_callback:\n            memo.config.typecheck_fail_callback(exc, memo)\n        else:\n            raise\n\n    return yieldval\n\n\ndef check_variable_assignment(\n    value: object, varname: str, annotation: Any, memo: TypeCheckMemo\n) -> Any:\n    if _suppression.type_checks_suppressed:\n        return value\n\n    try:\n        check_type_internal(value, annotation, memo)\n    except TypeCheckError as exc:\n        qualname = qualified_name(value, add_class_prefix=True)\n        exc.append_path_element(f\"value assigned to {varname} ({qualname})\")\n        if memo.config.typecheck_fail_callback:\n            memo.config.typecheck_fail_callback(exc, memo)\n        else:\n            raise\n\n    return value\n\n\ndef check_multi_variable_assignment(\n    value: Any, targets: list[dict[str, Any]], memo: TypeCheckMemo\n) -> Any:\n    if max(len(target) for target in targets) == 1:\n        iterated_values = [value]\n    else:\n        iterated_values = list(value)\n\n    if not _suppression.type_checks_suppressed:\n        for expected_types in targets:\n            value_index = 0\n            for ann_index, (varname, expected_type) in enumerate(\n                expected_types.items()\n            ):\n                if varname.startswith(\"*\"):\n                    varname = varname[1:]\n                    keys_left = len(expected_types) - 1 - ann_index\n                    next_value_index = len(iterated_values) - keys_left\n                    obj: object = iterated_values[value_index:next_value_index]\n                    value_index = next_value_index\n                else:\n                    obj = iterated_values[value_index]\n                    value_index += 1\n\n                try:\n                    check_type_internal(obj, expected_type, memo)\n                except TypeCheckError as exc:\n                    qualname = qualified_name(obj, add_class_prefix=True)\n                    exc.append_path_element(f\"value assigned to {varname} ({qualname})\")\n                    if memo.config.typecheck_fail_callback:\n                        memo.config.typecheck_fail_callback(exc, memo)\n                    else:\n                        raise\n\n    return iterated_values[0] if len(iterated_values) == 1 else iterated_values\n\n\ndef warn_on_error(exc: TypeCheckError, memo: TypeCheckMemo) -> None:\n    \"\"\"\n    Emit a warning on a type mismatch.\n\n    This is intended to be used as an error handler in\n    :attr:`TypeCheckConfiguration.typecheck_fail_callback`.\n\n    \"\"\"\n    warnings.warn(TypeCheckWarning(str(exc)), stacklevel=get_stacklevel())\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/_vendor/typeguard/_importhook.py","size":6389,"sha1":"8daa1c626c3647af20809a9f0670b1e51abfa0a2","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"from __future__ import annotations\n\nimport ast\nimport sys\nimport types\nfrom collections.abc import Callable, Iterable\nfrom importlib.abc import MetaPathFinder\nfrom importlib.machinery import ModuleSpec, SourceFileLoader\nfrom importlib.util import cache_from_source, decode_source\nfrom inspect import isclass\nfrom os import PathLike\nfrom types import CodeType, ModuleType, TracebackType\nfrom typing import Sequence, TypeVar\nfrom unittest.mock import patch\n\nfrom ._config import global_config\nfrom ._transformer import TypeguardTransformer\n\nif sys.version_info >= (3, 12):\n    from collections.abc import Buffer\nelse:\n    from typing_extensions import Buffer\n\nif sys.version_info >= (3, 11):\n    from typing import ParamSpec\nelse:\n    from typing_extensions import ParamSpec\n\nif sys.version_info >= (3, 10):\n    from importlib.metadata import PackageNotFoundError, version\nelse:\n    from importlib_metadata import PackageNotFoundError, version\n\ntry:\n    OPTIMIZATION = \"typeguard\" + \"\".join(version(\"typeguard\").split(\".\")[:3])\nexcept PackageNotFoundError:\n    OPTIMIZATION = \"typeguard\"\n\nP = ParamSpec(\"P\")\nT = TypeVar(\"T\")\n\n\n# The name of this function is magical\ndef _call_with_frames_removed(\n    f: Callable[P, T], *args: P.args, **kwargs: P.kwargs\n) -> T:\n    return f(*args, **kwargs)\n\n\ndef optimized_cache_from_source(path: str, debug_override: bool | None = None) -> str:\n    return cache_from_source(path, debug_override, optimization=OPTIMIZATION)\n\n\nclass TypeguardLoader(SourceFileLoader):\n    @staticmethod\n    def source_to_code(\n        data: Buffer | str | ast.Module | ast.Expression | ast.Interactive,\n        path: Buffer | str | PathLike[str] = \"<string>\",\n    ) -> CodeType:\n        if isinstance(data, (ast.Module, ast.Expression, ast.Interactive)):\n            tree = data\n        else:\n            if isinstance(data, str):\n                source = data\n            else:\n                source = decode_source(data)\n\n            tree = _call_with_frames_removed(\n                ast.parse,\n                source,\n                path,\n                \"exec\",\n            )\n\n        tree = TypeguardTransformer().visit(tree)\n        ast.fix_missing_locations(tree)\n\n        if global_config.debug_instrumentation and sys.version_info >= (3, 9):\n            print(\n                f\"Source code of {path!r} after instrumentation:\\n\"\n                \"----------------------------------------------\",\n                file=sys.stderr,\n            )\n            print(ast.unparse(tree), file=sys.stderr)\n            print(\"----------------------------------------------\", file=sys.stderr)\n\n        return _call_with_frames_removed(\n            compile, tree, path, \"exec\", 0, dont_inherit=True\n        )\n\n    def exec_module(self, module: ModuleType) -> None:\n        # Use a custom optimization marker  the import lock should make this monkey\n        # patch safe\n        with patch(\n            \"importlib._bootstrap_external.cache_from_source\",\n            optimized_cache_from_source,\n        ):\n            super().exec_module(module)\n\n\nclass TypeguardFinder(MetaPathFinder):\n    \"\"\"\n    Wraps another path finder and instruments the module with\n    :func:`@typechecked <typeguard.typechecked>` if :meth:`should_instrument` returns\n    ``True``.\n\n    Should not be used directly, but rather via :func:`~.install_import_hook`.\n\n    .. versionadded:: 2.6\n    \"\"\"\n\n    def __init__(self, packages: list[str] | None, original_pathfinder: MetaPathFinder):\n        self.packages = packages\n        self._original_pathfinder = original_pathfinder\n\n    def find_spec(\n        self,\n        fullname: str,\n        path: Sequence[str] | None,\n        target: types.ModuleType | None = None,\n    ) -> ModuleSpec | None:\n        if self.should_instrument(fullname):\n            spec = self._original_pathfinder.find_spec(fullname, path, target)\n            if spec is not None and isinstance(spec.loader, SourceFileLoader):\n                spec.loader = TypeguardLoader(spec.loader.name, spec.loader.path)\n                return spec\n\n        return None\n\n    def should_instrument(self, module_name: str) -> bool:\n        \"\"\"\n        Determine whether the module with the given name should be instrumented.\n\n        :param module_name: full name of the module that is about to be imported (e.g.\n            ``xyz.abc``)\n\n        \"\"\"\n        if self.packages is None:\n            return True\n\n        for package in self.packages:\n            if module_name == package or module_name.startswith(package + \".\"):\n                return True\n\n        return False\n\n\nclass ImportHookManager:\n    \"\"\"\n    A handle that can be used to uninstall the Typeguard import hook.\n    \"\"\"\n\n    def __init__(self, hook: MetaPathFinder):\n        self.hook = hook\n\n    def __enter__(self) -> None:\n        pass\n\n    def __exit__(\n        self,\n        exc_type: type[BaseException],\n        exc_val: BaseException,\n        exc_tb: TracebackType,\n    ) -> None:\n        self.uninstall()\n\n    def uninstall(self) -> None:\n        \"\"\"Uninstall the import hook.\"\"\"\n        try:\n            sys.meta_path.remove(self.hook)\n        except ValueError:\n            pass  # already removed\n\n\ndef install_import_hook(\n    packages: Iterable[str] | None = None,\n    *,\n    cls: type[TypeguardFinder] = TypeguardFinder,\n) -> ImportHookManager:\n    \"\"\"\n    Install an import hook that instruments functions for automatic type checking.\n\n    This only affects modules loaded **after** this hook has been installed.\n\n    :param packages: an iterable of package names to instrument, or ``None`` to\n        instrument all packages\n    :param cls: a custom meta path finder class\n    :return: a context manager that uninstalls the hook on exit (or when you call\n        ``.uninstall()``)\n\n    .. versionadded:: 2.6\n\n    \"\"\"\n    if packages is None:\n        target_packages: list[str] | None = None\n    elif isinstance(packages, str):\n        target_packages = [packages]\n    else:\n        target_packages = list(packages)\n\n    for finder in sys.meta_path:\n        if (\n            isclass(finder)\n            and finder.__name__ == \"PathFinder\"\n            and hasattr(finder, \"find_spec\")\n        ):\n            break\n    else:\n        raise RuntimeError(\"Cannot find a PathFinder in sys.meta_path\")\n\n    hook = cls(target_packages, finder)\n    sys.meta_path.insert(0, hook)\n    return ImportHookManager(hook)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/_vendor/typeguard/_memo.py","size":1303,"sha1":"d2dc32ea29346e08d544f7f3cfc3c20794863fd7","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"from __future__ import annotations\n\nfrom typing import Any\n\nfrom typeguard._config import TypeCheckConfiguration, global_config\n\n\nclass TypeCheckMemo:\n    \"\"\"\n    Contains information necessary for type checkers to do their work.\n\n    .. attribute:: globals\n       :type: dict[str, Any]\n\n        Dictionary of global variables to use for resolving forward references.\n\n    .. attribute:: locals\n       :type: dict[str, Any]\n\n        Dictionary of local variables to use for resolving forward references.\n\n    .. attribute:: self_type\n       :type: type | None\n\n        When running type checks within an instance method or class method, this is the\n        class object that the first argument (usually named ``self`` or ``cls``) refers\n        to.\n\n    .. attribute:: config\n       :type: TypeCheckConfiguration\n\n         Contains the configuration for a particular set of type checking operations.\n    \"\"\"\n\n    __slots__ = \"globals\", \"locals\", \"self_type\", \"config\"\n\n    def __init__(\n        self,\n        globals: dict[str, Any],\n        locals: dict[str, Any],\n        *,\n        self_type: type | None = None,\n        config: TypeCheckConfiguration = global_config,\n    ):\n        self.globals = globals\n        self.locals = locals\n        self.self_type = self_type\n        self.config = config\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/_vendor/typeguard/_pytest_plugin.py","size":4416,"sha1":"e5e350f13869348adf8ccf3af28be9fcb1f2f6f8","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"from __future__ import annotations\n\nimport sys\nimport warnings\nfrom typing import TYPE_CHECKING, Any, Literal\n\nfrom typeguard._config import CollectionCheckStrategy, ForwardRefPolicy, global_config\nfrom typeguard._exceptions import InstrumentationWarning\nfrom typeguard._importhook import install_import_hook\nfrom typeguard._utils import qualified_name, resolve_reference\n\nif TYPE_CHECKING:\n    from pytest import Config, Parser\n\n\ndef pytest_addoption(parser: Parser) -> None:\n    def add_ini_option(\n        opt_type: (\n            Literal[\"string\", \"paths\", \"pathlist\", \"args\", \"linelist\", \"bool\"] | None\n        ),\n    ) -> None:\n        parser.addini(\n            group.options[-1].names()[0][2:],\n            group.options[-1].attrs()[\"help\"],\n            opt_type,\n        )\n\n    group = parser.getgroup(\"typeguard\")\n    group.addoption(\n        \"--typeguard-packages\",\n        action=\"store\",\n        help=\"comma separated name list of packages and modules to instrument for \"\n        \"type checking, or :all: to instrument all modules loaded after typeguard\",\n    )\n    add_ini_option(\"linelist\")\n\n    group.addoption(\n        \"--typeguard-debug-instrumentation\",\n        action=\"store_true\",\n        help=\"print all instrumented code to stderr\",\n    )\n    add_ini_option(\"bool\")\n\n    group.addoption(\n        \"--typeguard-typecheck-fail-callback\",\n        action=\"store\",\n        help=(\n            \"a module:varname (e.g. typeguard:warn_on_error) reference to a function \"\n            \"that is called (with the exception, and memo object as arguments) to \"\n            \"handle a TypeCheckError\"\n        ),\n    )\n    add_ini_option(\"string\")\n\n    group.addoption(\n        \"--typeguard-forward-ref-policy\",\n        action=\"store\",\n        choices=list(ForwardRefPolicy.__members__),\n        help=(\n            \"determines how to deal with unresolveable forward references in type \"\n            \"annotations\"\n        ),\n    )\n    add_ini_option(\"string\")\n\n    group.addoption(\n        \"--typeguard-collection-check-strategy\",\n        action=\"store\",\n        choices=list(CollectionCheckStrategy.__members__),\n        help=\"determines how thoroughly to check collections (list, dict, etc)\",\n    )\n    add_ini_option(\"string\")\n\n\ndef pytest_configure(config: Config) -> None:\n    def getoption(name: str) -> Any:\n        return config.getoption(name.replace(\"-\", \"_\")) or config.getini(name)\n\n    packages: list[str] | None = []\n    if packages_option := config.getoption(\"typeguard_packages\"):\n        packages = [pkg.strip() for pkg in packages_option.split(\",\")]\n    elif packages_ini := config.getini(\"typeguard-packages\"):\n        packages = packages_ini\n\n    if packages:\n        if packages == [\":all:\"]:\n            packages = None\n        else:\n            already_imported_packages = sorted(\n                package for package in packages if package in sys.modules\n            )\n            if already_imported_packages:\n                warnings.warn(\n                    f\"typeguard cannot check these packages because they are already \"\n                    f\"imported: {', '.join(already_imported_packages)}\",\n                    InstrumentationWarning,\n                    stacklevel=1,\n                )\n\n        install_import_hook(packages=packages)\n\n    debug_option = getoption(\"typeguard-debug-instrumentation\")\n    if debug_option:\n        global_config.debug_instrumentation = True\n\n    fail_callback_option = getoption(\"typeguard-typecheck-fail-callback\")\n    if fail_callback_option:\n        callback = resolve_reference(fail_callback_option)\n        if not callable(callback):\n            raise TypeError(\n                f\"{fail_callback_option} ({qualified_name(callback.__class__)}) is not \"\n                f\"a callable\"\n            )\n\n        global_config.typecheck_fail_callback = callback\n\n    forward_ref_policy_option = getoption(\"typeguard-forward-ref-policy\")\n    if forward_ref_policy_option:\n        forward_ref_policy = ForwardRefPolicy.__members__[forward_ref_policy_option]\n        global_config.forward_ref_policy = forward_ref_policy\n\n    collection_check_strategy_option = getoption(\"typeguard-collection-check-strategy\")\n    if collection_check_strategy_option:\n        collection_check_strategy = CollectionCheckStrategy.__members__[\n            collection_check_strategy_option\n        ]\n        global_config.collection_check_strategy = collection_check_strategy\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/_vendor/typeguard/_suppression.py","size":2266,"sha1":"beb5cdc233ca4d513cc37a04183db81a20de8cff","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"from __future__ import annotations\n\nimport sys\nfrom collections.abc import Callable, Generator\nfrom contextlib import contextmanager\nfrom functools import update_wrapper\nfrom threading import Lock\nfrom typing import ContextManager, TypeVar, overload\n\nif sys.version_info >= (3, 10):\n    from typing import ParamSpec\nelse:\n    from typing_extensions import ParamSpec\n\nP = ParamSpec(\"P\")\nT = TypeVar(\"T\")\n\ntype_checks_suppressed = 0\ntype_checks_suppress_lock = Lock()\n\n\n@overload\ndef suppress_type_checks(func: Callable[P, T]) -> Callable[P, T]: ...\n\n\n@overload\ndef suppress_type_checks() -> ContextManager[None]: ...\n\n\ndef suppress_type_checks(\n    func: Callable[P, T] | None = None,\n) -> Callable[P, T] | ContextManager[None]:\n    \"\"\"\n    Temporarily suppress all type checking.\n\n    This function has two operating modes, based on how it's used:\n\n    #. as a context manager (``with suppress_type_checks(): ...``)\n    #. as a decorator (``@suppress_type_checks``)\n\n    When used as a context manager, :func:`check_type` and any automatically\n    instrumented functions skip the actual type checking. These context managers can be\n    nested.\n\n    When used as a decorator, all type checking is suppressed while the function is\n    running.\n\n    Type checking will resume once no more context managers are active and no decorated\n    functions are running.\n\n    Both operating modes are thread-safe.\n\n    \"\"\"\n\n    def wrapper(*args: P.args, **kwargs: P.kwargs) -> T:\n        global type_checks_suppressed\n\n        with type_checks_suppress_lock:\n            type_checks_suppressed += 1\n\n        assert func is not None\n        try:\n            return func(*args, **kwargs)\n        finally:\n            with type_checks_suppress_lock:\n                type_checks_suppressed -= 1\n\n    def cm() -> Generator[None, None, None]:\n        global type_checks_suppressed\n\n        with type_checks_suppress_lock:\n            type_checks_suppressed += 1\n\n        try:\n            yield\n        finally:\n            with type_checks_suppress_lock:\n                type_checks_suppressed -= 1\n\n    if func is None:\n        # Context manager mode\n        return contextmanager(cm)()\n    else:\n        # Decorator mode\n        update_wrapper(wrapper, func)\n        return wrapper\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/_vendor/typeguard/_transformer.py","size":44937,"sha1":"6e78b629a80f2009809f703df4a0716e77db87fa","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"from __future__ import annotations\n\nimport ast\nimport builtins\nimport sys\nimport typing\nfrom ast import (\n    AST,\n    Add,\n    AnnAssign,\n    Assign,\n    AsyncFunctionDef,\n    Attribute,\n    AugAssign,\n    BinOp,\n    BitAnd,\n    BitOr,\n    BitXor,\n    Call,\n    ClassDef,\n    Constant,\n    Dict,\n    Div,\n    Expr,\n    Expression,\n    FloorDiv,\n    FunctionDef,\n    If,\n    Import,\n    ImportFrom,\n    Index,\n    List,\n    Load,\n    LShift,\n    MatMult,\n    Mod,\n    Module,\n    Mult,\n    Name,\n    NamedExpr,\n    NodeTransformer,\n    NodeVisitor,\n    Pass,\n    Pow,\n    Return,\n    RShift,\n    Starred,\n    Store,\n    Sub,\n    Subscript,\n    Tuple,\n    Yield,\n    YieldFrom,\n    alias,\n    copy_location,\n    expr,\n    fix_missing_locations,\n    keyword,\n    walk,\n)\nfrom collections import defaultdict\nfrom collections.abc import Generator, Sequence\nfrom contextlib import contextmanager\nfrom copy import deepcopy\nfrom dataclasses import dataclass, field\nfrom typing import Any, ClassVar, cast, overload\n\ngenerator_names = (\n    \"typing.Generator\",\n    \"collections.abc.Generator\",\n    \"typing.Iterator\",\n    \"collections.abc.Iterator\",\n    \"typing.Iterable\",\n    \"collections.abc.Iterable\",\n    \"typing.AsyncIterator\",\n    \"collections.abc.AsyncIterator\",\n    \"typing.AsyncIterable\",\n    \"collections.abc.AsyncIterable\",\n    \"typing.AsyncGenerator\",\n    \"collections.abc.AsyncGenerator\",\n)\nanytype_names = (\n    \"typing.Any\",\n    \"typing_extensions.Any\",\n)\nliteral_names = (\n    \"typing.Literal\",\n    \"typing_extensions.Literal\",\n)\nannotated_names = (\n    \"typing.Annotated\",\n    \"typing_extensions.Annotated\",\n)\nignore_decorators = (\n    \"typing.no_type_check\",\n    \"typeguard.typeguard_ignore\",\n)\naug_assign_functions = {\n    Add: \"iadd\",\n    Sub: \"isub\",\n    Mult: \"imul\",\n    MatMult: \"imatmul\",\n    Div: \"itruediv\",\n    FloorDiv: \"ifloordiv\",\n    Mod: \"imod\",\n    Pow: \"ipow\",\n    LShift: \"ilshift\",\n    RShift: \"irshift\",\n    BitAnd: \"iand\",\n    BitXor: \"ixor\",\n    BitOr: \"ior\",\n}\n\n\n@dataclass\nclass TransformMemo:\n    node: Module | ClassDef | FunctionDef | AsyncFunctionDef | None\n    parent: TransformMemo | None\n    path: tuple[str, ...]\n    joined_path: Constant = field(init=False)\n    return_annotation: expr | None = None\n    yield_annotation: expr | None = None\n    send_annotation: expr | None = None\n    is_async: bool = False\n    local_names: set[str] = field(init=False, default_factory=set)\n    imported_names: dict[str, str] = field(init=False, default_factory=dict)\n    ignored_names: set[str] = field(init=False, default_factory=set)\n    load_names: defaultdict[str, dict[str, Name]] = field(\n        init=False, default_factory=lambda: defaultdict(dict)\n    )\n    has_yield_expressions: bool = field(init=False, default=False)\n    has_return_expressions: bool = field(init=False, default=False)\n    memo_var_name: Name | None = field(init=False, default=None)\n    should_instrument: bool = field(init=False, default=True)\n    variable_annotations: dict[str, expr] = field(init=False, default_factory=dict)\n    configuration_overrides: dict[str, Any] = field(init=False, default_factory=dict)\n    code_inject_index: int = field(init=False, default=0)\n\n    def __post_init__(self) -> None:\n        elements: list[str] = []\n        memo = self\n        while isinstance(memo.node, (ClassDef, FunctionDef, AsyncFunctionDef)):\n            elements.insert(0, memo.node.name)\n            if not memo.parent:\n                break\n\n            memo = memo.parent\n            if isinstance(memo.node, (FunctionDef, AsyncFunctionDef)):\n                elements.insert(0, \"<locals>\")\n\n        self.joined_path = Constant(\".\".join(elements))\n\n        # Figure out where to insert instrumentation code\n        if self.node:\n            for index, child in enumerate(self.node.body):\n                if isinstance(child, ImportFrom) and child.module == \"__future__\":\n                    # (module only) __future__ imports must come first\n                    continue\n                elif (\n                    isinstance(child, Expr)\n                    and isinstance(child.value, Constant)\n                    and isinstance(child.value.value, str)\n                ):\n                    continue  # docstring\n\n                self.code_inject_index = index\n                break\n\n    def get_unused_name(self, name: str) -> str:\n        memo: TransformMemo | None = self\n        while memo is not None:\n            if name in memo.local_names:\n                memo = self\n                name += \"_\"\n            else:\n                memo = memo.parent\n\n        self.local_names.add(name)\n        return name\n\n    def is_ignored_name(self, expression: expr | Expr | None) -> bool:\n        top_expression = (\n            expression.value if isinstance(expression, Expr) else expression\n        )\n\n        if isinstance(top_expression, Attribute) and isinstance(\n            top_expression.value, Name\n        ):\n            name = top_expression.value.id\n        elif isinstance(top_expression, Name):\n            name = top_expression.id\n        else:\n            return False\n\n        memo: TransformMemo | None = self\n        while memo is not None:\n            if name in memo.ignored_names:\n                return True\n\n            memo = memo.parent\n\n        return False\n\n    def get_memo_name(self) -> Name:\n        if not self.memo_var_name:\n            self.memo_var_name = Name(id=\"memo\", ctx=Load())\n\n        return self.memo_var_name\n\n    def get_import(self, module: str, name: str) -> Name:\n        if module in self.load_names and name in self.load_names[module]:\n            return self.load_names[module][name]\n\n        qualified_name = f\"{module}.{name}\"\n        if name in self.imported_names and self.imported_names[name] == qualified_name:\n            return Name(id=name, ctx=Load())\n\n        alias = self.get_unused_name(name)\n        node = self.load_names[module][name] = Name(id=alias, ctx=Load())\n        self.imported_names[name] = qualified_name\n        return node\n\n    def insert_imports(self, node: Module | FunctionDef | AsyncFunctionDef) -> None:\n        \"\"\"Insert imports needed by injected code.\"\"\"\n        if not self.load_names:\n            return\n\n        # Insert imports after any \"from __future__ ...\" imports and any docstring\n        for modulename, names in self.load_names.items():\n            aliases = [\n                alias(orig_name, new_name.id if orig_name != new_name.id else None)\n                for orig_name, new_name in sorted(names.items())\n            ]\n            node.body.insert(self.code_inject_index, ImportFrom(modulename, aliases, 0))\n\n    def name_matches(self, expression: expr | Expr | None, *names: str) -> bool:\n        if expression is None:\n            return False\n\n        path: list[str] = []\n        top_expression = (\n            expression.value if isinstance(expression, Expr) else expression\n        )\n\n        if isinstance(top_expression, Subscript):\n            top_expression = top_expression.value\n        elif isinstance(top_expression, Call):\n            top_expression = top_expression.func\n\n        while isinstance(top_expression, Attribute):\n            path.insert(0, top_expression.attr)\n            top_expression = top_expression.value\n\n        if not isinstance(top_expression, Name):\n            return False\n\n        if top_expression.id in self.imported_names:\n            translated = self.imported_names[top_expression.id]\n        elif hasattr(builtins, top_expression.id):\n            translated = \"builtins.\" + top_expression.id\n        else:\n            translated = top_expression.id\n\n        path.insert(0, translated)\n        joined_path = \".\".join(path)\n        if joined_path in names:\n            return True\n        elif self.parent:\n            return self.parent.name_matches(expression, *names)\n        else:\n            return False\n\n    def get_config_keywords(self) -> list[keyword]:\n        if self.parent and isinstance(self.parent.node, ClassDef):\n            overrides = self.parent.configuration_overrides.copy()\n        else:\n            overrides = {}\n\n        overrides.update(self.configuration_overrides)\n        return [keyword(key, value) for key, value in overrides.items()]\n\n\nclass NameCollector(NodeVisitor):\n    def __init__(self) -> None:\n        self.names: set[str] = set()\n\n    def visit_Import(self, node: Import) -> None:\n        for name in node.names:\n            self.names.add(name.asname or name.name)\n\n    def visit_ImportFrom(self, node: ImportFrom) -> None:\n        for name in node.names:\n            self.names.add(name.asname or name.name)\n\n    def visit_Assign(self, node: Assign) -> None:\n        for target in node.targets:\n            if isinstance(target, Name):\n                self.names.add(target.id)\n\n    def visit_NamedExpr(self, node: NamedExpr) -> Any:\n        if isinstance(node.target, Name):\n            self.names.add(node.target.id)\n\n    def visit_FunctionDef(self, node: FunctionDef) -> None:\n        pass\n\n    def visit_ClassDef(self, node: ClassDef) -> None:\n        pass\n\n\nclass GeneratorDetector(NodeVisitor):\n    \"\"\"Detects if a function node is a generator function.\"\"\"\n\n    contains_yields: bool = False\n    in_root_function: bool = False\n\n    def visit_Yield(self, node: Yield) -> Any:\n        self.contains_yields = True\n\n    def visit_YieldFrom(self, node: YieldFrom) -> Any:\n        self.contains_yields = True\n\n    def visit_ClassDef(self, node: ClassDef) -> Any:\n        pass\n\n    def visit_FunctionDef(self, node: FunctionDef | AsyncFunctionDef) -> Any:\n        if not self.in_root_function:\n            self.in_root_function = True\n            self.generic_visit(node)\n            self.in_root_function = False\n\n    def visit_AsyncFunctionDef(self, node: AsyncFunctionDef) -> Any:\n        self.visit_FunctionDef(node)\n\n\nclass AnnotationTransformer(NodeTransformer):\n    type_substitutions: ClassVar[dict[str, tuple[str, str]]] = {\n        \"builtins.dict\": (\"typing\", \"Dict\"),\n        \"builtins.list\": (\"typing\", \"List\"),\n        \"builtins.tuple\": (\"typing\", \"Tuple\"),\n        \"builtins.set\": (\"typing\", \"Set\"),\n        \"builtins.frozenset\": (\"typing\", \"FrozenSet\"),\n    }\n\n    def __init__(self, transformer: TypeguardTransformer):\n        self.transformer = transformer\n        self._memo = transformer._memo\n        self._level = 0\n\n    def visit(self, node: AST) -> Any:\n        # Don't process Literals\n        if isinstance(node, expr) and self._memo.name_matches(node, *literal_names):\n            return node\n\n        self._level += 1\n        new_node = super().visit(node)\n        self._level -= 1\n\n        if isinstance(new_node, Expression) and not hasattr(new_node, \"body\"):\n            return None\n\n        # Return None if this new node matches a variation of typing.Any\n        if (\n            self._level == 0\n            and isinstance(new_node, expr)\n            and self._memo.name_matches(new_node, *anytype_names)\n        ):\n            return None\n\n        return new_node\n\n    def visit_BinOp(self, node: BinOp) -> Any:\n        self.generic_visit(node)\n\n        if isinstance(node.op, BitOr):\n            # If either branch of the BinOp has been transformed to `None`, it means\n            # that a type in the union was ignored, so the entire annotation should e\n            # ignored\n            if not hasattr(node, \"left\") or not hasattr(node, \"right\"):\n                return None\n\n            # Return Any if either side is Any\n            if self._memo.name_matches(node.left, *anytype_names):\n                return node.left\n            elif self._memo.name_matches(node.right, *anytype_names):\n                return node.right\n\n            if sys.version_info < (3, 10):\n                union_name = self.transformer._get_import(\"typing\", \"Union\")\n                return Subscript(\n                    value=union_name,\n                    slice=Index(\n                        Tuple(elts=[node.left, node.right], ctx=Load()), ctx=Load()\n                    ),\n                    ctx=Load(),\n                )\n\n        return node\n\n    def visit_Attribute(self, node: Attribute) -> Any:\n        if self._memo.is_ignored_name(node):\n            return None\n\n        return node\n\n    def visit_Subscript(self, node: Subscript) -> Any:\n        if self._memo.is_ignored_name(node.value):\n            return None\n\n        # The subscript of typing(_extensions).Literal can be any arbitrary string, so\n        # don't try to evaluate it as code\n        if node.slice:\n            if isinstance(node.slice, Index):\n                # Python 3.8\n                slice_value = node.slice.value  # type: ignore[attr-defined]\n            else:\n                slice_value = node.slice\n\n            if isinstance(slice_value, Tuple):\n                if self._memo.name_matches(node.value, *annotated_names):\n                    # Only treat the first argument to typing.Annotated as a potential\n                    # forward reference\n                    items = cast(\n                        typing.List[expr],\n                        [self.visit(slice_value.elts[0])] + slice_value.elts[1:],\n                    )\n                else:\n                    items = cast(\n                        typing.List[expr],\n                        [self.visit(item) for item in slice_value.elts],\n                    )\n\n                # If this is a Union and any of the items is Any, erase the entire\n                # annotation\n                if self._memo.name_matches(node.value, \"typing.Union\") and any(\n                    item is None\n                    or (\n                        isinstance(item, expr)\n                        and self._memo.name_matches(item, *anytype_names)\n                    )\n                    for item in items\n                ):\n                    return None\n\n                # If all items in the subscript were Any, erase the subscript entirely\n                if all(item is None for item in items):\n                    return node.value\n\n                for index, item in enumerate(items):\n                    if item is None:\n                        items[index] = self.transformer._get_import(\"typing\", \"Any\")\n\n                slice_value.elts = items\n            else:\n                self.generic_visit(node)\n\n                # If the transformer erased the slice entirely, just return the node\n                # value without the subscript (unless it's Optional, in which case erase\n                # the node entirely\n                if self._memo.name_matches(\n                    node.value, \"typing.Optional\"\n                ) and not hasattr(node, \"slice\"):\n                    return None\n                if sys.version_info >= (3, 9) and not hasattr(node, \"slice\"):\n                    return node.value\n                elif sys.version_info < (3, 9) and not hasattr(node.slice, \"value\"):\n                    return node.value\n\n        return node\n\n    def visit_Name(self, node: Name) -> Any:\n        if self._memo.is_ignored_name(node):\n            return None\n\n        if sys.version_info < (3, 9):\n            for typename, substitute in self.type_substitutions.items():\n                if self._memo.name_matches(node, typename):\n                    new_node = self.transformer._get_import(*substitute)\n                    return copy_location(new_node, node)\n\n        return node\n\n    def visit_Call(self, node: Call) -> Any:\n        # Don't recurse into calls\n        return node\n\n    def visit_Constant(self, node: Constant) -> Any:\n        if isinstance(node.value, str):\n            expression = ast.parse(node.value, mode=\"eval\")\n            new_node = self.visit(expression)\n            if new_node:\n                return copy_location(new_node.body, node)\n            else:\n                return None\n\n        return node\n\n\nclass TypeguardTransformer(NodeTransformer):\n    def __init__(\n        self, target_path: Sequence[str] | None = None, target_lineno: int | None = None\n    ) -> None:\n        self._target_path = tuple(target_path) if target_path else None\n        self._memo = self._module_memo = TransformMemo(None, None, ())\n        self.names_used_in_annotations: set[str] = set()\n        self.target_node: FunctionDef | AsyncFunctionDef | None = None\n        self.target_lineno = target_lineno\n\n    def generic_visit(self, node: AST) -> AST:\n        has_non_empty_body_initially = bool(getattr(node, \"body\", None))\n        initial_type = type(node)\n\n        node = super().generic_visit(node)\n\n        if (\n            type(node) is initial_type\n            and has_non_empty_body_initially\n            and hasattr(node, \"body\")\n            and not node.body\n        ):\n            # If we have still the same node type after transformation\n            # but we've optimised it's body away, we add a `pass` statement.\n            node.body = [Pass()]\n\n        return node\n\n    @contextmanager\n    def _use_memo(\n        self, node: ClassDef | FunctionDef | AsyncFunctionDef\n    ) -> Generator[None, Any, None]:\n        new_memo = TransformMemo(node, self._memo, self._memo.path + (node.name,))\n        old_memo = self._memo\n        self._memo = new_memo\n\n        if isinstance(node, (FunctionDef, AsyncFunctionDef)):\n            new_memo.should_instrument = (\n                self._target_path is None or new_memo.path == self._target_path\n            )\n            if new_memo.should_instrument:\n                # Check if the function is a generator function\n                detector = GeneratorDetector()\n                detector.visit(node)\n\n                # Extract yield, send and return types where possible from a subscripted\n                # annotation like Generator[int, str, bool]\n                return_annotation = deepcopy(node.returns)\n                if detector.contains_yields and new_memo.name_matches(\n                    return_annotation, *generator_names\n                ):\n                    if isinstance(return_annotation, Subscript):\n                        annotation_slice = return_annotation.slice\n\n                        # Python < 3.9\n                        if isinstance(annotation_slice, Index):\n                            annotation_slice = (\n                                annotation_slice.value  # type: ignore[attr-defined]\n                            )\n\n                        if isinstance(annotation_slice, Tuple):\n                            items = annotation_slice.elts\n                        else:\n                            items = [annotation_slice]\n\n                        if len(items) > 0:\n                            new_memo.yield_annotation = self._convert_annotation(\n                                items[0]\n                            )\n\n                        if len(items) > 1:\n                            new_memo.send_annotation = self._convert_annotation(\n                                items[1]\n                            )\n\n                        if len(items) > 2:\n                            new_memo.return_annotation = self._convert_annotation(\n                                items[2]\n                            )\n                else:\n                    new_memo.return_annotation = self._convert_annotation(\n                        return_annotation\n                    )\n\n        if isinstance(node, AsyncFunctionDef):\n            new_memo.is_async = True\n\n        yield\n        self._memo = old_memo\n\n    def _get_import(self, module: str, name: str) -> Name:\n        memo = self._memo if self._target_path else self._module_memo\n        return memo.get_import(module, name)\n\n    @overload\n    def _convert_annotation(self, annotation: None) -> None: ...\n\n    @overload\n    def _convert_annotation(self, annotation: expr) -> expr: ...\n\n    def _convert_annotation(self, annotation: expr | None) -> expr | None:\n        if annotation is None:\n            return None\n\n        # Convert PEP 604 unions (x | y) and generic built-in collections where\n        # necessary, and undo forward references\n        new_annotation = cast(expr, AnnotationTransformer(self).visit(annotation))\n        if isinstance(new_annotation, expr):\n            new_annotation = ast.copy_location(new_annotation, annotation)\n\n            # Store names used in the annotation\n            names = {node.id for node in walk(new_annotation) if isinstance(node, Name)}\n            self.names_used_in_annotations.update(names)\n\n        return new_annotation\n\n    def visit_Name(self, node: Name) -> Name:\n        self._memo.local_names.add(node.id)\n        return node\n\n    def visit_Module(self, node: Module) -> Module:\n        self._module_memo = self._memo = TransformMemo(node, None, ())\n        self.generic_visit(node)\n        self._module_memo.insert_imports(node)\n\n        fix_missing_locations(node)\n        return node\n\n    def visit_Import(self, node: Import) -> Import:\n        for name in node.names:\n            self._memo.local_names.add(name.asname or name.name)\n            self._memo.imported_names[name.asname or name.name] = name.name\n\n        return node\n\n    def visit_ImportFrom(self, node: ImportFrom) -> ImportFrom:\n        for name in node.names:\n            if name.name != \"*\":\n                alias = name.asname or name.name\n                self._memo.local_names.add(alias)\n                self._memo.imported_names[alias] = f\"{node.module}.{name.name}\"\n\n        return node\n\n    def visit_ClassDef(self, node: ClassDef) -> ClassDef | None:\n        self._memo.local_names.add(node.name)\n\n        # Eliminate top level classes not belonging to the target path\n        if (\n            self._target_path is not None\n            and not self._memo.path\n            and node.name != self._target_path[0]\n        ):\n            return None\n\n        with self._use_memo(node):\n            for decorator in node.decorator_list.copy():\n                if self._memo.name_matches(decorator, \"typeguard.typechecked\"):\n                    # Remove the decorator to prevent duplicate instrumentation\n                    node.decorator_list.remove(decorator)\n\n                    # Store any configuration overrides\n                    if isinstance(decorator, Call) and decorator.keywords:\n                        self._memo.configuration_overrides.update(\n                            {kw.arg: kw.value for kw in decorator.keywords if kw.arg}\n                        )\n\n            self.generic_visit(node)\n            return node\n\n    def visit_FunctionDef(\n        self, node: FunctionDef | AsyncFunctionDef\n    ) -> FunctionDef | AsyncFunctionDef | None:\n        \"\"\"\n        Injects type checks for function arguments, and for a return of None if the\n        function is annotated to return something else than Any or None, and the body\n        ends without an explicit \"return\".\n\n        \"\"\"\n        self._memo.local_names.add(node.name)\n\n        # Eliminate top level functions not belonging to the target path\n        if (\n            self._target_path is not None\n            and not self._memo.path\n            and node.name != self._target_path[0]\n        ):\n            return None\n\n        # Skip instrumentation if we're instrumenting the whole module and the function\n        # contains either @no_type_check or @typeguard_ignore\n        if self._target_path is None:\n            for decorator in node.decorator_list:\n                if self._memo.name_matches(decorator, *ignore_decorators):\n                    return node\n\n        with self._use_memo(node):\n            arg_annotations: dict[str, Any] = {}\n            if self._target_path is None or self._memo.path == self._target_path:\n                # Find line number we're supposed to match against\n                if node.decorator_list:\n                    first_lineno = node.decorator_list[0].lineno\n                else:\n                    first_lineno = node.lineno\n\n                for decorator in node.decorator_list.copy():\n                    if self._memo.name_matches(decorator, \"typing.overload\"):\n                        # Remove overloads entirely\n                        return None\n                    elif self._memo.name_matches(decorator, \"typeguard.typechecked\"):\n                        # Remove the decorator to prevent duplicate instrumentation\n                        node.decorator_list.remove(decorator)\n\n                        # Store any configuration overrides\n                        if isinstance(decorator, Call) and decorator.keywords:\n                            self._memo.configuration_overrides = {\n                                kw.arg: kw.value for kw in decorator.keywords if kw.arg\n                            }\n\n                if self.target_lineno == first_lineno:\n                    assert self.target_node is None\n                    self.target_node = node\n                    if node.decorator_list:\n                        self.target_lineno = node.decorator_list[0].lineno\n                    else:\n                        self.target_lineno = node.lineno\n\n                all_args = node.args.args + node.args.kwonlyargs + node.args.posonlyargs\n\n                # Ensure that any type shadowed by the positional or keyword-only\n                # argument names are ignored in this function\n                for arg in all_args:\n                    self._memo.ignored_names.add(arg.arg)\n\n                # Ensure that any type shadowed by the variable positional argument name\n                # (e.g. \"args\" in *args) is ignored this function\n                if node.args.vararg:\n                    self._memo.ignored_names.add(node.args.vararg.arg)\n\n                # Ensure that any type shadowed by the variable keywrod argument name\n                # (e.g. \"kwargs\" in *kwargs) is ignored this function\n                if node.args.kwarg:\n                    self._memo.ignored_names.add(node.args.kwarg.arg)\n\n                for arg in all_args:\n                    annotation = self._convert_annotation(deepcopy(arg.annotation))\n                    if annotation:\n                        arg_annotations[arg.arg] = annotation\n\n                if node.args.vararg:\n                    annotation_ = self._convert_annotation(node.args.vararg.annotation)\n                    if annotation_:\n                        if sys.version_info >= (3, 9):\n                            container = Name(\"tuple\", ctx=Load())\n                        else:\n                            container = self._get_import(\"typing\", \"Tuple\")\n\n                        subscript_slice: Tuple | Index = Tuple(\n                            [\n                                annotation_,\n                                Constant(Ellipsis),\n                            ],\n                            ctx=Load(),\n                        )\n                        if sys.version_info < (3, 9):\n                            subscript_slice = Index(subscript_slice, ctx=Load())\n\n                        arg_annotations[node.args.vararg.arg] = Subscript(\n                            container, subscript_slice, ctx=Load()\n                        )\n\n                if node.args.kwarg:\n                    annotation_ = self._convert_annotation(node.args.kwarg.annotation)\n                    if annotation_:\n                        if sys.version_info >= (3, 9):\n                            container = Name(\"dict\", ctx=Load())\n                        else:\n                            container = self._get_import(\"typing\", \"Dict\")\n\n                        subscript_slice = Tuple(\n                            [\n                                Name(\"str\", ctx=Load()),\n                                annotation_,\n                            ],\n                            ctx=Load(),\n                        )\n                        if sys.version_info < (3, 9):\n                            subscript_slice = Index(subscript_slice, ctx=Load())\n\n                        arg_annotations[node.args.kwarg.arg] = Subscript(\n                            container, subscript_slice, ctx=Load()\n                        )\n\n                if arg_annotations:\n                    self._memo.variable_annotations.update(arg_annotations)\n\n            self.generic_visit(node)\n\n            if arg_annotations:\n                annotations_dict = Dict(\n                    keys=[Constant(key) for key in arg_annotations.keys()],\n                    values=[\n                        Tuple([Name(key, ctx=Load()), annotation], ctx=Load())\n                        for key, annotation in arg_annotations.items()\n                    ],\n                )\n                func_name = self._get_import(\n                    \"typeguard._functions\", \"check_argument_types\"\n                )\n                args = [\n                    self._memo.joined_path,\n                    annotations_dict,\n                    self._memo.get_memo_name(),\n                ]\n                node.body.insert(\n                    self._memo.code_inject_index, Expr(Call(func_name, args, []))\n                )\n\n            # Add a checked \"return None\" to the end if there's no explicit return\n            # Skip if the return annotation is None or Any\n            if (\n                self._memo.return_annotation\n                and (not self._memo.is_async or not self._memo.has_yield_expressions)\n                and not isinstance(node.body[-1], Return)\n                and (\n                    not isinstance(self._memo.return_annotation, Constant)\n                    or self._memo.return_annotation.value is not None\n                )\n            ):\n                func_name = self._get_import(\n                    \"typeguard._functions\", \"check_return_type\"\n                )\n                return_node = Return(\n                    Call(\n                        func_name,\n                        [\n                            self._memo.joined_path,\n                            Constant(None),\n                            self._memo.return_annotation,\n                            self._memo.get_memo_name(),\n                        ],\n                        [],\n                    )\n                )\n\n                # Replace a placeholder \"pass\" at the end\n                if isinstance(node.body[-1], Pass):\n                    copy_location(return_node, node.body[-1])\n                    del node.body[-1]\n\n                node.body.append(return_node)\n\n            # Insert code to create the call memo, if it was ever needed for this\n            # function\n            if self._memo.memo_var_name:\n                memo_kwargs: dict[str, Any] = {}\n                if self._memo.parent and isinstance(self._memo.parent.node, ClassDef):\n                    for decorator in node.decorator_list:\n                        if (\n                            isinstance(decorator, Name)\n                            and decorator.id == \"staticmethod\"\n                        ):\n                            break\n                        elif (\n                            isinstance(decorator, Name)\n                            and decorator.id == \"classmethod\"\n                        ):\n                            memo_kwargs[\"self_type\"] = Name(\n                                id=node.args.args[0].arg, ctx=Load()\n                            )\n                            break\n                    else:\n                        if node.args.args:\n                            if node.name == \"__new__\":\n                                memo_kwargs[\"self_type\"] = Name(\n                                    id=node.args.args[0].arg, ctx=Load()\n                                )\n                            else:\n                                memo_kwargs[\"self_type\"] = Attribute(\n                                    Name(id=node.args.args[0].arg, ctx=Load()),\n                                    \"__class__\",\n                                    ctx=Load(),\n                                )\n\n                # Construct the function reference\n                # Nested functions get special treatment: the function name is added\n                # to free variables (and the closure of the resulting function)\n                names: list[str] = [node.name]\n                memo = self._memo.parent\n                while memo:\n                    if isinstance(memo.node, (FunctionDef, AsyncFunctionDef)):\n                        # This is a nested function. Use the function name as-is.\n                        del names[:-1]\n                        break\n                    elif not isinstance(memo.node, ClassDef):\n                        break\n\n                    names.insert(0, memo.node.name)\n                    memo = memo.parent\n\n                config_keywords = self._memo.get_config_keywords()\n                if config_keywords:\n                    memo_kwargs[\"config\"] = Call(\n                        self._get_import(\"dataclasses\", \"replace\"),\n                        [self._get_import(\"typeguard._config\", \"global_config\")],\n                        config_keywords,\n                    )\n\n                self._memo.memo_var_name.id = self._memo.get_unused_name(\"memo\")\n                memo_store_name = Name(id=self._memo.memo_var_name.id, ctx=Store())\n                globals_call = Call(Name(id=\"globals\", ctx=Load()), [], [])\n                locals_call = Call(Name(id=\"locals\", ctx=Load()), [], [])\n                memo_expr = Call(\n                    self._get_import(\"typeguard\", \"TypeCheckMemo\"),\n                    [globals_call, locals_call],\n                    [keyword(key, value) for key, value in memo_kwargs.items()],\n                )\n                node.body.insert(\n                    self._memo.code_inject_index,\n                    Assign([memo_store_name], memo_expr),\n                )\n\n                self._memo.insert_imports(node)\n\n                # Special case the __new__() method to create a local alias from the\n                # class name to the first argument (usually \"cls\")\n                if (\n                    isinstance(node, FunctionDef)\n                    and node.args\n                    and self._memo.parent is not None\n                    and isinstance(self._memo.parent.node, ClassDef)\n                    and node.name == \"__new__\"\n                ):\n                    first_args_expr = Name(node.args.args[0].arg, ctx=Load())\n                    cls_name = Name(self._memo.parent.node.name, ctx=Store())\n                    node.body.insert(\n                        self._memo.code_inject_index,\n                        Assign([cls_name], first_args_expr),\n                    )\n\n                # Rmove any placeholder \"pass\" at the end\n                if isinstance(node.body[-1], Pass):\n                    del node.body[-1]\n\n        return node\n\n    def visit_AsyncFunctionDef(\n        self, node: AsyncFunctionDef\n    ) -> FunctionDef | AsyncFunctionDef | None:\n        return self.visit_FunctionDef(node)\n\n    def visit_Return(self, node: Return) -> Return:\n        \"\"\"This injects type checks into \"return\" statements.\"\"\"\n        self.generic_visit(node)\n        if (\n            self._memo.return_annotation\n            and self._memo.should_instrument\n            and not self._memo.is_ignored_name(self._memo.return_annotation)\n        ):\n            func_name = self._get_import(\"typeguard._functions\", \"check_return_type\")\n            old_node = node\n            retval = old_node.value or Constant(None)\n            node = Return(\n                Call(\n                    func_name,\n                    [\n                        self._memo.joined_path,\n                        retval,\n                        self._memo.return_annotation,\n                        self._memo.get_memo_name(),\n                    ],\n                    [],\n                )\n            )\n            copy_location(node, old_node)\n\n        return node\n\n    def visit_Yield(self, node: Yield) -> Yield | Call:\n        \"\"\"\n        This injects type checks into \"yield\" expressions, checking both the yielded\n        value and the value sent back to the generator, when appropriate.\n\n        \"\"\"\n        self._memo.has_yield_expressions = True\n        self.generic_visit(node)\n\n        if (\n            self._memo.yield_annotation\n            and self._memo.should_instrument\n            and not self._memo.is_ignored_name(self._memo.yield_annotation)\n        ):\n            func_name = self._get_import(\"typeguard._functions\", \"check_yield_type\")\n            yieldval = node.value or Constant(None)\n            node.value = Call(\n                func_name,\n                [\n                    self._memo.joined_path,\n                    yieldval,\n                    self._memo.yield_annotation,\n                    self._memo.get_memo_name(),\n                ],\n                [],\n            )\n\n        if (\n            self._memo.send_annotation\n            and self._memo.should_instrument\n            and not self._memo.is_ignored_name(self._memo.send_annotation)\n        ):\n            func_name = self._get_import(\"typeguard._functions\", \"check_send_type\")\n            old_node = node\n            call_node = Call(\n                func_name,\n                [\n                    self._memo.joined_path,\n                    old_node,\n                    self._memo.send_annotation,\n                    self._memo.get_memo_name(),\n                ],\n                [],\n            )\n            copy_location(call_node, old_node)\n            return call_node\n\n        return node\n\n    def visit_AnnAssign(self, node: AnnAssign) -> Any:\n        \"\"\"\n        This injects a type check into a local variable annotation-assignment within a\n        function body.\n\n        \"\"\"\n        self.generic_visit(node)\n\n        if (\n            isinstance(self._memo.node, (FunctionDef, AsyncFunctionDef))\n            and node.annotation\n            and isinstance(node.target, Name)\n        ):\n            self._memo.ignored_names.add(node.target.id)\n            annotation = self._convert_annotation(deepcopy(node.annotation))\n            if annotation:\n                self._memo.variable_annotations[node.target.id] = annotation\n                if node.value:\n                    func_name = self._get_import(\n                        \"typeguard._functions\", \"check_variable_assignment\"\n                    )\n                    node.value = Call(\n                        func_name,\n                        [\n                            node.value,\n                            Constant(node.target.id),\n                            annotation,\n                            self._memo.get_memo_name(),\n                        ],\n                        [],\n                    )\n\n        return node\n\n    def visit_Assign(self, node: Assign) -> Any:\n        \"\"\"\n        This injects a type check into a local variable assignment within a function\n        body. The variable must have been annotated earlier in the function body.\n\n        \"\"\"\n        self.generic_visit(node)\n\n        # Only instrument function-local assignments\n        if isinstance(self._memo.node, (FunctionDef, AsyncFunctionDef)):\n            targets: list[dict[Constant, expr | None]] = []\n            check_required = False\n            for target in node.targets:\n                elts: Sequence[expr]\n                if isinstance(target, Name):\n                    elts = [target]\n                elif isinstance(target, Tuple):\n                    elts = target.elts\n                else:\n                    continue\n\n                annotations_: dict[Constant, expr | None] = {}\n                for exp in elts:\n                    prefix = \"\"\n                    if isinstance(exp, Starred):\n                        exp = exp.value\n                        prefix = \"*\"\n\n                    if isinstance(exp, Name):\n                        self._memo.ignored_names.add(exp.id)\n                        name = prefix + exp.id\n                        annotation = self._memo.variable_annotations.get(exp.id)\n                        if annotation:\n                            annotations_[Constant(name)] = annotation\n                            check_required = True\n                        else:\n                            annotations_[Constant(name)] = None\n\n                targets.append(annotations_)\n\n            if check_required:\n                # Replace missing annotations with typing.Any\n                for item in targets:\n                    for key, expression in item.items():\n                        if expression is None:\n                            item[key] = self._get_import(\"typing\", \"Any\")\n\n                if len(targets) == 1 and len(targets[0]) == 1:\n                    func_name = self._get_import(\n                        \"typeguard._functions\", \"check_variable_assignment\"\n                    )\n                    target_varname = next(iter(targets[0]))\n                    node.value = Call(\n                        func_name,\n                        [\n                            node.value,\n                            target_varname,\n                            targets[0][target_varname],\n                            self._memo.get_memo_name(),\n                        ],\n                        [],\n                    )\n                elif targets:\n                    func_name = self._get_import(\n                        \"typeguard._functions\", \"check_multi_variable_assignment\"\n                    )\n                    targets_arg = List(\n                        [\n                            Dict(keys=list(target), values=list(target.values()))\n                            for target in targets\n                        ],\n                        ctx=Load(),\n                    )\n                    node.value = Call(\n                        func_name,\n                        [node.value, targets_arg, self._memo.get_memo_name()],\n                        [],\n                    )\n\n        return node\n\n    def visit_NamedExpr(self, node: NamedExpr) -> Any:\n        \"\"\"This injects a type check into an assignment expression (a := foo()).\"\"\"\n        self.generic_visit(node)\n\n        # Only instrument function-local assignments\n        if isinstance(self._memo.node, (FunctionDef, AsyncFunctionDef)) and isinstance(\n            node.target, Name\n        ):\n            self._memo.ignored_names.add(node.target.id)\n\n            # Bail out if no matching annotation is found\n            annotation = self._memo.variable_annotations.get(node.target.id)\n            if annotation is None:\n                return node\n\n            func_name = self._get_import(\n                \"typeguard._functions\", \"check_variable_assignment\"\n            )\n            node.value = Call(\n                func_name,\n                [\n                    node.value,\n                    Constant(node.target.id),\n                    annotation,\n                    self._memo.get_memo_name(),\n                ],\n                [],\n            )\n\n        return node\n\n    def visit_AugAssign(self, node: AugAssign) -> Any:\n        \"\"\"\n        This injects a type check into an augmented assignment expression (a += 1).\n\n        \"\"\"\n        self.generic_visit(node)\n\n        # Only instrument function-local assignments\n        if isinstance(self._memo.node, (FunctionDef, AsyncFunctionDef)) and isinstance(\n            node.target, Name\n        ):\n            # Bail out if no matching annotation is found\n            annotation = self._memo.variable_annotations.get(node.target.id)\n            if annotation is None:\n                return node\n\n            # Bail out if the operator is not found (newer Python version?)\n            try:\n                operator_func_name = aug_assign_functions[node.op.__class__]\n            except KeyError:\n                return node\n\n            operator_func = self._get_import(\"operator\", operator_func_name)\n            operator_call = Call(\n                operator_func, [Name(node.target.id, ctx=Load()), node.value], []\n            )\n            check_call = Call(\n                self._get_import(\"typeguard._functions\", \"check_variable_assignment\"),\n                [\n                    operator_call,\n                    Constant(node.target.id),\n                    annotation,\n                    self._memo.get_memo_name(),\n                ],\n                [],\n            )\n            return Assign(targets=[node.target], value=check_call)\n\n        return node\n\n    def visit_If(self, node: If) -> Any:\n        \"\"\"\n        This blocks names from being collected from a module-level\n        \"if typing.TYPE_CHECKING:\" block, so that they won't be type checked.\n\n        \"\"\"\n        self.generic_visit(node)\n\n        if (\n            self._memo is self._module_memo\n            and isinstance(node.test, Name)\n            and self._memo.name_matches(node.test, \"typing.TYPE_CHECKING\")\n        ):\n            collector = NameCollector()\n            collector.visit(node)\n            self._memo.ignored_names.update(collector.names)\n\n        return node\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/_vendor/typeguard/_union_transformer.py","size":1354,"sha1":"6f58a2052a0e97dbadb67f5b507dbb3fa83d0bd4","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"\nTransforms lazily evaluated PEP 604 unions into typing.Unions, for compatibility with\nPython versions older than 3.10.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom ast import (\n    BinOp,\n    BitOr,\n    Index,\n    Load,\n    Name,\n    NodeTransformer,\n    Subscript,\n    fix_missing_locations,\n    parse,\n)\nfrom ast import Tuple as ASTTuple\nfrom types import CodeType\nfrom typing import Any, Dict, FrozenSet, List, Set, Tuple, Union\n\ntype_substitutions = {\n    \"dict\": Dict,\n    \"list\": List,\n    \"tuple\": Tuple,\n    \"set\": Set,\n    \"frozenset\": FrozenSet,\n    \"Union\": Union,\n}\n\n\nclass UnionTransformer(NodeTransformer):\n    def __init__(self, union_name: Name | None = None):\n        self.union_name = union_name or Name(id=\"Union\", ctx=Load())\n\n    def visit_BinOp(self, node: BinOp) -> Any:\n        self.generic_visit(node)\n        if isinstance(node.op, BitOr):\n            return Subscript(\n                value=self.union_name,\n                slice=Index(\n                    ASTTuple(elts=[node.left, node.right], ctx=Load()), ctx=Load()\n                ),\n                ctx=Load(),\n            )\n\n        return node\n\n\ndef compile_type_hint(hint: str) -> CodeType:\n    parsed = parse(hint, \"<string>\", \"eval\")\n    UnionTransformer().visit(parsed)\n    fix_missing_locations(parsed)\n    return compile(parsed, \"<string>\", \"eval\", flags=0)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/_vendor/typeguard/_utils.py","size":5270,"sha1":"5e89302ec9f38b282b768d00d8f7509868fb19d7","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"from __future__ import annotations\n\nimport inspect\nimport sys\nfrom importlib import import_module\nfrom inspect import currentframe\nfrom types import CodeType, FrameType, FunctionType\nfrom typing import TYPE_CHECKING, Any, Callable, ForwardRef, Union, cast, final\nfrom weakref import WeakValueDictionary\n\nif TYPE_CHECKING:\n    from ._memo import TypeCheckMemo\n\nif sys.version_info >= (3, 13):\n    from typing import get_args, get_origin\n\n    def evaluate_forwardref(forwardref: ForwardRef, memo: TypeCheckMemo) -> Any:\n        return forwardref._evaluate(\n            memo.globals, memo.locals, type_params=(), recursive_guard=frozenset()\n        )\n\nelif sys.version_info >= (3, 10):\n    from typing import get_args, get_origin\n\n    def evaluate_forwardref(forwardref: ForwardRef, memo: TypeCheckMemo) -> Any:\n        return forwardref._evaluate(\n            memo.globals, memo.locals, recursive_guard=frozenset()\n        )\n\nelse:\n    from typing_extensions import get_args, get_origin\n\n    evaluate_extra_args: tuple[frozenset[Any], ...] = (\n        (frozenset(),) if sys.version_info >= (3, 9) else ()\n    )\n\n    def evaluate_forwardref(forwardref: ForwardRef, memo: TypeCheckMemo) -> Any:\n        from ._union_transformer import compile_type_hint, type_substitutions\n\n        if not forwardref.__forward_evaluated__:\n            forwardref.__forward_code__ = compile_type_hint(forwardref.__forward_arg__)\n\n        try:\n            return forwardref._evaluate(memo.globals, memo.locals, *evaluate_extra_args)\n        except NameError:\n            if sys.version_info < (3, 10):\n                # Try again, with the type substitutions (list -> List etc.) in place\n                new_globals = memo.globals.copy()\n                new_globals.setdefault(\"Union\", Union)\n                if sys.version_info < (3, 9):\n                    new_globals.update(type_substitutions)\n\n                return forwardref._evaluate(\n                    new_globals, memo.locals or new_globals, *evaluate_extra_args\n                )\n\n            raise\n\n\n_functions_map: WeakValueDictionary[CodeType, FunctionType] = WeakValueDictionary()\n\n\ndef get_type_name(type_: Any) -> str:\n    name: str\n    for attrname in \"__name__\", \"_name\", \"__forward_arg__\":\n        candidate = getattr(type_, attrname, None)\n        if isinstance(candidate, str):\n            name = candidate\n            break\n    else:\n        origin = get_origin(type_)\n        candidate = getattr(origin, \"_name\", None)\n        if candidate is None:\n            candidate = type_.__class__.__name__.strip(\"_\")\n\n        if isinstance(candidate, str):\n            name = candidate\n        else:\n            return \"(unknown)\"\n\n    args = get_args(type_)\n    if args:\n        if name == \"Literal\":\n            formatted_args = \", \".join(repr(arg) for arg in args)\n        else:\n            formatted_args = \", \".join(get_type_name(arg) for arg in args)\n\n        name += f\"[{formatted_args}]\"\n\n    module = getattr(type_, \"__module__\", None)\n    if module and module not in (None, \"typing\", \"typing_extensions\", \"builtins\"):\n        name = module + \".\" + name\n\n    return name\n\n\ndef qualified_name(obj: Any, *, add_class_prefix: bool = False) -> str:\n    \"\"\"\n    Return the qualified name (e.g. package.module.Type) for the given object.\n\n    Builtins and types from the :mod:`typing` package get special treatment by having\n    the module name stripped from the generated name.\n\n    \"\"\"\n    if obj is None:\n        return \"None\"\n    elif inspect.isclass(obj):\n        prefix = \"class \" if add_class_prefix else \"\"\n        type_ = obj\n    else:\n        prefix = \"\"\n        type_ = type(obj)\n\n    module = type_.__module__\n    qualname = type_.__qualname__\n    name = qualname if module in (\"typing\", \"builtins\") else f\"{module}.{qualname}\"\n    return prefix + name\n\n\ndef function_name(func: Callable[..., Any]) -> str:\n    \"\"\"\n    Return the qualified name of the given function.\n\n    Builtins and types from the :mod:`typing` package get special treatment by having\n    the module name stripped from the generated name.\n\n    \"\"\"\n    # For partial functions and objects with __call__ defined, __qualname__ does not\n    # exist\n    module = getattr(func, \"__module__\", \"\")\n    qualname = (module + \".\") if module not in (\"builtins\", \"\") else \"\"\n    return qualname + getattr(func, \"__qualname__\", repr(func))\n\n\ndef resolve_reference(reference: str) -> Any:\n    modulename, varname = reference.partition(\":\")[::2]\n    if not modulename or not varname:\n        raise ValueError(f\"{reference!r} is not a module:varname reference\")\n\n    obj = import_module(modulename)\n    for attr in varname.split(\".\"):\n        obj = getattr(obj, attr)\n\n    return obj\n\n\ndef is_method_of(obj: object, cls: type) -> bool:\n    return (\n        inspect.isfunction(obj)\n        and obj.__module__ == cls.__module__\n        and obj.__qualname__.startswith(cls.__qualname__ + \".\")\n    )\n\n\ndef get_stacklevel() -> int:\n    level = 1\n    frame = cast(FrameType, currentframe()).f_back\n    while frame and frame.f_globals.get(\"__name__\", \"\").startswith(\"typeguard.\"):\n        level += 1\n        frame = frame.f_back\n\n    return level\n\n\n@final\nclass Unset:\n    __slots__ = ()\n\n    def __repr__(self) -> str:\n        return \"<unset>\"\n\n\nunset = Unset()\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/_vendor/typing_extensions.py","size":134451,"sha1":"bf10cdbcfe3166c334b905294f06c52c8850e6ed","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import abc\nimport collections\nimport collections.abc\nimport contextlib\nimport functools\nimport inspect\nimport operator\nimport sys\nimport types as _types\nimport typing\nimport warnings\n\n__all__ = [\n    # Super-special typing primitives.\n    'Any',\n    'ClassVar',\n    'Concatenate',\n    'Final',\n    'LiteralString',\n    'ParamSpec',\n    'ParamSpecArgs',\n    'ParamSpecKwargs',\n    'Self',\n    'Type',\n    'TypeVar',\n    'TypeVarTuple',\n    'Unpack',\n\n    # ABCs (from collections.abc).\n    'Awaitable',\n    'AsyncIterator',\n    'AsyncIterable',\n    'Coroutine',\n    'AsyncGenerator',\n    'AsyncContextManager',\n    'Buffer',\n    'ChainMap',\n\n    # Concrete collection types.\n    'ContextManager',\n    'Counter',\n    'Deque',\n    'DefaultDict',\n    'NamedTuple',\n    'OrderedDict',\n    'TypedDict',\n\n    # Structural checks, a.k.a. protocols.\n    'SupportsAbs',\n    'SupportsBytes',\n    'SupportsComplex',\n    'SupportsFloat',\n    'SupportsIndex',\n    'SupportsInt',\n    'SupportsRound',\n\n    # One-off things.\n    'Annotated',\n    'assert_never',\n    'assert_type',\n    'clear_overloads',\n    'dataclass_transform',\n    'deprecated',\n    'Doc',\n    'get_overloads',\n    'final',\n    'get_args',\n    'get_origin',\n    'get_original_bases',\n    'get_protocol_members',\n    'get_type_hints',\n    'IntVar',\n    'is_protocol',\n    'is_typeddict',\n    'Literal',\n    'NewType',\n    'overload',\n    'override',\n    'Protocol',\n    'reveal_type',\n    'runtime',\n    'runtime_checkable',\n    'Text',\n    'TypeAlias',\n    'TypeAliasType',\n    'TypeGuard',\n    'TypeIs',\n    'TYPE_CHECKING',\n    'Never',\n    'NoReturn',\n    'ReadOnly',\n    'Required',\n    'NotRequired',\n\n    # Pure aliases, have always been in typing\n    'AbstractSet',\n    'AnyStr',\n    'BinaryIO',\n    'Callable',\n    'Collection',\n    'Container',\n    'Dict',\n    'ForwardRef',\n    'FrozenSet',\n    'Generator',\n    'Generic',\n    'Hashable',\n    'IO',\n    'ItemsView',\n    'Iterable',\n    'Iterator',\n    'KeysView',\n    'List',\n    'Mapping',\n    'MappingView',\n    'Match',\n    'MutableMapping',\n    'MutableSequence',\n    'MutableSet',\n    'NoDefault',\n    'Optional',\n    'Pattern',\n    'Reversible',\n    'Sequence',\n    'Set',\n    'Sized',\n    'TextIO',\n    'Tuple',\n    'Union',\n    'ValuesView',\n    'cast',\n    'no_type_check',\n    'no_type_check_decorator',\n]\n\n# for backward compatibility\nPEP_560 = True\nGenericMeta = type\n_PEP_696_IMPLEMENTED = sys.version_info >= (3, 13, 0, \"beta\")\n\n# The functions below are modified copies of typing internal helpers.\n# They are needed by _ProtocolMeta and they provide support for PEP 646.\n\n\nclass _Sentinel:\n    def __repr__(self):\n        return \"<sentinel>\"\n\n\n_marker = _Sentinel()\n\n\nif sys.version_info >= (3, 10):\n    def _should_collect_from_parameters(t):\n        return isinstance(\n            t, (typing._GenericAlias, _types.GenericAlias, _types.UnionType)\n        )\nelif sys.version_info >= (3, 9):\n    def _should_collect_from_parameters(t):\n        return isinstance(t, (typing._GenericAlias, _types.GenericAlias))\nelse:\n    def _should_collect_from_parameters(t):\n        return isinstance(t, typing._GenericAlias) and not t._special\n\n\nNoReturn = typing.NoReturn\n\n# Some unconstrained type variables.  These are used by the container types.\n# (These are not for export.)\nT = typing.TypeVar('T')  # Any type.\nKT = typing.TypeVar('KT')  # Key type.\nVT = typing.TypeVar('VT')  # Value type.\nT_co = typing.TypeVar('T_co', covariant=True)  # Any type covariant containers.\nT_contra = typing.TypeVar('T_contra', contravariant=True)  # Ditto contravariant.\n\n\nif sys.version_info >= (3, 11):\n    from typing import Any\nelse:\n\n    class _AnyMeta(type):\n        def __instancecheck__(self, obj):\n            if self is Any:\n                raise TypeError(\"typing_extensions.Any cannot be used with isinstance()\")\n            return super().__instancecheck__(obj)\n\n        def __repr__(self):\n            if self is Any:\n                return \"typing_extensions.Any\"\n            return super().__repr__()\n\n    class Any(metaclass=_AnyMeta):\n        \"\"\"Special type indicating an unconstrained type.\n        - Any is compatible with every type.\n        - Any assumed to have all methods.\n        - All values assumed to be instances of Any.\n        Note that all the above statements are true from the point of view of\n        static type checkers. At runtime, Any should not be used with instance\n        checks.\n        \"\"\"\n        def __new__(cls, *args, **kwargs):\n            if cls is Any:\n                raise TypeError(\"Any cannot be instantiated\")\n            return super().__new__(cls, *args, **kwargs)\n\n\nClassVar = typing.ClassVar\n\n\nclass _ExtensionsSpecialForm(typing._SpecialForm, _root=True):\n    def __repr__(self):\n        return 'typing_extensions.' + self._name\n\n\nFinal = typing.Final\n\nif sys.version_info >= (3, 11):\n    final = typing.final\nelse:\n    # @final exists in 3.8+, but we backport it for all versions\n    # before 3.11 to keep support for the __final__ attribute.\n    # See https://bugs.python.org/issue46342\n    def final(f):\n        \"\"\"This decorator can be used to indicate to type checkers that\n        the decorated method cannot be overridden, and decorated class\n        cannot be subclassed. For example:\n\n            class Base:\n                @final\n                def done(self) -> None:\n                    ...\n            class Sub(Base):\n                def done(self) -> None:  # Error reported by type checker\n                    ...\n            @final\n            class Leaf:\n                ...\n            class Other(Leaf):  # Error reported by type checker\n                ...\n\n        There is no runtime checking of these properties. The decorator\n        sets the ``__final__`` attribute to ``True`` on the decorated object\n        to allow runtime introspection.\n        \"\"\"\n        try:\n            f.__final__ = True\n        except (AttributeError, TypeError):\n            # Skip the attribute silently if it is not writable.\n            # AttributeError happens if the object has __slots__ or a\n            # read-only property, TypeError if it's a builtin class.\n            pass\n        return f\n\n\ndef IntVar(name):\n    return typing.TypeVar(name)\n\n\n# A Literal bug was fixed in 3.11.0, 3.10.1 and 3.9.8\nif sys.version_info >= (3, 10, 1):\n    Literal = typing.Literal\nelse:\n    def _flatten_literal_params(parameters):\n        \"\"\"An internal helper for Literal creation: flatten Literals among parameters\"\"\"\n        params = []\n        for p in parameters:\n            if isinstance(p, _LiteralGenericAlias):\n                params.extend(p.__args__)\n            else:\n                params.append(p)\n        return tuple(params)\n\n    def _value_and_type_iter(params):\n        for p in params:\n            yield p, type(p)\n\n    class _LiteralGenericAlias(typing._GenericAlias, _root=True):\n        def __eq__(self, other):\n            if not isinstance(other, _LiteralGenericAlias):\n                return NotImplemented\n            these_args_deduped = set(_value_and_type_iter(self.__args__))\n            other_args_deduped = set(_value_and_type_iter(other.__args__))\n            return these_args_deduped == other_args_deduped\n\n        def __hash__(self):\n            return hash(frozenset(_value_and_type_iter(self.__args__)))\n\n    class _LiteralForm(_ExtensionsSpecialForm, _root=True):\n        def __init__(self, doc: str):\n            self._name = 'Literal'\n            self._doc = self.__doc__ = doc\n\n        def __getitem__(self, parameters):\n            if not isinstance(parameters, tuple):\n                parameters = (parameters,)\n\n            parameters = _flatten_literal_params(parameters)\n\n            val_type_pairs = list(_value_and_type_iter(parameters))\n            try:\n                deduped_pairs = set(val_type_pairs)\n            except TypeError:\n                # unhashable parameters\n                pass\n            else:\n                # similar logic to typing._deduplicate on Python 3.9+\n                if len(deduped_pairs) < len(val_type_pairs):\n                    new_parameters = []\n                    for pair in val_type_pairs:\n                        if pair in deduped_pairs:\n                            new_parameters.append(pair[0])\n                            deduped_pairs.remove(pair)\n                    assert not deduped_pairs, deduped_pairs\n                    parameters = tuple(new_parameters)\n\n            return _LiteralGenericAlias(self, parameters)\n\n    Literal = _LiteralForm(doc=\"\"\"\\\n                           A type that can be used to indicate to type checkers\n                           that the corresponding value has a value literally equivalent\n                           to the provided parameter. For example:\n\n                               var: Literal[4] = 4\n\n                           The type checker understands that 'var' is literally equal to\n                           the value 4 and no other value.\n\n                           Literal[...] cannot be subclassed. There is no runtime\n                           checking verifying that the parameter is actually a value\n                           instead of a type.\"\"\")\n\n\n_overload_dummy = typing._overload_dummy\n\n\nif hasattr(typing, \"get_overloads\"):  # 3.11+\n    overload = typing.overload\n    get_overloads = typing.get_overloads\n    clear_overloads = typing.clear_overloads\nelse:\n    # {module: {qualname: {firstlineno: func}}}\n    _overload_registry = collections.defaultdict(\n        functools.partial(collections.defaultdict, dict)\n    )\n\n    def overload(func):\n        \"\"\"Decorator for overloaded functions/methods.\n\n        In a stub file, place two or more stub definitions for the same\n        function in a row, each decorated with @overload.  For example:\n\n        @overload\n        def utf8(value: None) -> None: ...\n        @overload\n        def utf8(value: bytes) -> bytes: ...\n        @overload\n        def utf8(value: str) -> bytes: ...\n\n        In a non-stub file (i.e. a regular .py file), do the same but\n        follow it with an implementation.  The implementation should *not*\n        be decorated with @overload.  For example:\n\n        @overload\n        def utf8(value: None) -> None: ...\n        @overload\n        def utf8(value: bytes) -> bytes: ...\n        @overload\n        def utf8(value: str) -> bytes: ...\n        def utf8(value):\n            # implementation goes here\n\n        The overloads for a function can be retrieved at runtime using the\n        get_overloads() function.\n        \"\"\"\n        # classmethod and staticmethod\n        f = getattr(func, \"__func__\", func)\n        try:\n            _overload_registry[f.__module__][f.__qualname__][\n                f.__code__.co_firstlineno\n            ] = func\n        except AttributeError:\n            # Not a normal function; ignore.\n            pass\n        return _overload_dummy\n\n    def get_overloads(func):\n        \"\"\"Return all defined overloads for *func* as a sequence.\"\"\"\n        # classmethod and staticmethod\n        f = getattr(func, \"__func__\", func)\n        if f.__module__ not in _overload_registry:\n            return []\n        mod_dict = _overload_registry[f.__module__]\n        if f.__qualname__ not in mod_dict:\n            return []\n        return list(mod_dict[f.__qualname__].values())\n\n    def clear_overloads():\n        \"\"\"Clear all overloads in the registry.\"\"\"\n        _overload_registry.clear()\n\n\n# This is not a real generic class.  Don't use outside annotations.\nType = typing.Type\n\n# Various ABCs mimicking those in collections.abc.\n# A few are simply re-exported for completeness.\nAwaitable = typing.Awaitable\nCoroutine = typing.Coroutine\nAsyncIterable = typing.AsyncIterable\nAsyncIterator = typing.AsyncIterator\nDeque = typing.Deque\nDefaultDict = typing.DefaultDict\nOrderedDict = typing.OrderedDict\nCounter = typing.Counter\nChainMap = typing.ChainMap\nText = typing.Text\nTYPE_CHECKING = typing.TYPE_CHECKING\n\n\nif sys.version_info >= (3, 13, 0, \"beta\"):\n    from typing import AsyncContextManager, AsyncGenerator, ContextManager, Generator\nelse:\n    def _is_dunder(attr):\n        return attr.startswith('__') and attr.endswith('__')\n\n    # Python <3.9 doesn't have typing._SpecialGenericAlias\n    _special_generic_alias_base = getattr(\n        typing, \"_SpecialGenericAlias\", typing._GenericAlias\n    )\n\n    class _SpecialGenericAlias(_special_generic_alias_base, _root=True):\n        def __init__(self, origin, nparams, *, inst=True, name=None, defaults=()):\n            if _special_generic_alias_base is typing._GenericAlias:\n                # Python <3.9\n                self.__origin__ = origin\n                self._nparams = nparams\n                super().__init__(origin, nparams, special=True, inst=inst, name=name)\n            else:\n                # Python >= 3.9\n                super().__init__(origin, nparams, inst=inst, name=name)\n            self._defaults = defaults\n\n        def __setattr__(self, attr, val):\n            allowed_attrs = {'_name', '_inst', '_nparams', '_defaults'}\n            if _special_generic_alias_base is typing._GenericAlias:\n                # Python <3.9\n                allowed_attrs.add(\"__origin__\")\n            if _is_dunder(attr) or attr in allowed_attrs:\n                object.__setattr__(self, attr, val)\n            else:\n                setattr(self.__origin__, attr, val)\n\n        @typing._tp_cache\n        def __getitem__(self, params):\n            if not isinstance(params, tuple):\n                params = (params,)\n            msg = \"Parameters to generic types must be types.\"\n            params = tuple(typing._type_check(p, msg) for p in params)\n            if (\n                self._defaults\n                and len(params) < self._nparams\n                and len(params) + len(self._defaults) >= self._nparams\n            ):\n                params = (*params, *self._defaults[len(params) - self._nparams:])\n            actual_len = len(params)\n\n            if actual_len != self._nparams:\n                if self._defaults:\n                    expected = f\"at least {self._nparams - len(self._defaults)}\"\n                else:\n                    expected = str(self._nparams)\n                if not self._nparams:\n                    raise TypeError(f\"{self} is not a generic class\")\n                raise TypeError(\n                    f\"Too {'many' if actual_len > self._nparams else 'few'}\"\n                    f\" arguments for {self};\"\n                    f\" actual {actual_len}, expected {expected}\"\n                )\n            return self.copy_with(params)\n\n    _NoneType = type(None)\n    Generator = _SpecialGenericAlias(\n        collections.abc.Generator, 3, defaults=(_NoneType, _NoneType)\n    )\n    AsyncGenerator = _SpecialGenericAlias(\n        collections.abc.AsyncGenerator, 2, defaults=(_NoneType,)\n    )\n    ContextManager = _SpecialGenericAlias(\n        contextlib.AbstractContextManager,\n        2,\n        name=\"ContextManager\",\n        defaults=(typing.Optional[bool],)\n    )\n    AsyncContextManager = _SpecialGenericAlias(\n        contextlib.AbstractAsyncContextManager,\n        2,\n        name=\"AsyncContextManager\",\n        defaults=(typing.Optional[bool],)\n    )\n\n\n_PROTO_ALLOWLIST = {\n    'collections.abc': [\n        'Callable', 'Awaitable', 'Iterable', 'Iterator', 'AsyncIterable',\n        'Hashable', 'Sized', 'Container', 'Collection', 'Reversible', 'Buffer',\n    ],\n    'contextlib': ['AbstractContextManager', 'AbstractAsyncContextManager'],\n    'typing_extensions': ['Buffer'],\n}\n\n\n_EXCLUDED_ATTRS = frozenset(typing.EXCLUDED_ATTRIBUTES) | {\n    \"__match_args__\", \"__protocol_attrs__\", \"__non_callable_proto_members__\",\n    \"__final__\",\n}\n\n\ndef _get_protocol_attrs(cls):\n    attrs = set()\n    for base in cls.__mro__[:-1]:  # without object\n        if base.__name__ in {'Protocol', 'Generic'}:\n            continue\n        annotations = getattr(base, '__annotations__', {})\n        for attr in (*base.__dict__, *annotations):\n            if (not attr.startswith('_abc_') and attr not in _EXCLUDED_ATTRS):\n                attrs.add(attr)\n    return attrs\n\n\ndef _caller(depth=2):\n    try:\n        return sys._getframe(depth).f_globals.get('__name__', '__main__')\n    except (AttributeError, ValueError):  # For platforms without _getframe()\n        return None\n\n\n# `__match_args__` attribute was removed from protocol members in 3.13,\n# we want to backport this change to older Python versions.\nif sys.version_info >= (3, 13):\n    Protocol = typing.Protocol\nelse:\n    def _allow_reckless_class_checks(depth=3):\n        \"\"\"Allow instance and class checks for special stdlib modules.\n        The abc and functools modules indiscriminately call isinstance() and\n        issubclass() on the whole MRO of a user class, which may contain protocols.\n        \"\"\"\n        return _caller(depth) in {'abc', 'functools', None}\n\n    def _no_init(self, *args, **kwargs):\n        if type(self)._is_protocol:\n            raise TypeError('Protocols cannot be instantiated')\n\n    def _type_check_issubclass_arg_1(arg):\n        \"\"\"Raise TypeError if `arg` is not an instance of `type`\n        in `issubclass(arg, <protocol>)`.\n\n        In most cases, this is verified by type.__subclasscheck__.\n        Checking it again unnecessarily would slow down issubclass() checks,\n        so, we don't perform this check unless we absolutely have to.\n\n        For various error paths, however,\n        we want to ensure that *this* error message is shown to the user\n        where relevant, rather than a typing.py-specific error message.\n        \"\"\"\n        if not isinstance(arg, type):\n            # Same error message as for issubclass(1, int).\n            raise TypeError('issubclass() arg 1 must be a class')\n\n    # Inheriting from typing._ProtocolMeta isn't actually desirable,\n    # but is necessary to allow typing.Protocol and typing_extensions.Protocol\n    # to mix without getting TypeErrors about \"metaclass conflict\"\n    class _ProtocolMeta(type(typing.Protocol)):\n        # This metaclass is somewhat unfortunate,\n        # but is necessary for several reasons...\n        #\n        # NOTE: DO NOT call super() in any methods in this class\n        # That would call the methods on typing._ProtocolMeta on Python 3.8-3.11\n        # and those are slow\n        def __new__(mcls, name, bases, namespace, **kwargs):\n            if name == \"Protocol\" and len(bases) < 2:\n                pass\n            elif {Protocol, typing.Protocol} & set(bases):\n                for base in bases:\n                    if not (\n                        base in {object, typing.Generic, Protocol, typing.Protocol}\n                        or base.__name__ in _PROTO_ALLOWLIST.get(base.__module__, [])\n                        or is_protocol(base)\n                    ):\n                        raise TypeError(\n                            f\"Protocols can only inherit from other protocols, \"\n                            f\"got {base!r}\"\n                        )\n            return abc.ABCMeta.__new__(mcls, name, bases, namespace, **kwargs)\n\n        def __init__(cls, *args, **kwargs):\n            abc.ABCMeta.__init__(cls, *args, **kwargs)\n            if getattr(cls, \"_is_protocol\", False):\n                cls.__protocol_attrs__ = _get_protocol_attrs(cls)\n\n        def __subclasscheck__(cls, other):\n            if cls is Protocol:\n                return type.__subclasscheck__(cls, other)\n            if (\n                getattr(cls, '_is_protocol', False)\n                and not _allow_reckless_class_checks()\n            ):\n                if not getattr(cls, '_is_runtime_protocol', False):\n                    _type_check_issubclass_arg_1(other)\n                    raise TypeError(\n                        \"Instance and class checks can only be used with \"\n                        \"@runtime_checkable protocols\"\n                    )\n                if (\n                    # this attribute is set by @runtime_checkable:\n                    cls.__non_callable_proto_members__\n                    and cls.__dict__.get(\"__subclasshook__\") is _proto_hook\n                ):\n                    _type_check_issubclass_arg_1(other)\n                    non_method_attrs = sorted(cls.__non_callable_proto_members__)\n                    raise TypeError(\n                        \"Protocols with non-method members don't support issubclass().\"\n                        f\" Non-method members: {str(non_method_attrs)[1:-1]}.\"\n                    )\n            return abc.ABCMeta.__subclasscheck__(cls, other)\n\n        def __instancecheck__(cls, instance):\n            # We need this method for situations where attributes are\n            # assigned in __init__.\n            if cls is Protocol:\n                return type.__instancecheck__(cls, instance)\n            if not getattr(cls, \"_is_protocol\", False):\n                # i.e., it's a concrete subclass of a protocol\n                return abc.ABCMeta.__instancecheck__(cls, instance)\n\n            if (\n                not getattr(cls, '_is_runtime_protocol', False) and\n                not _allow_reckless_class_checks()\n            ):\n                raise TypeError(\"Instance and class checks can only be used with\"\n                                \" @runtime_checkable protocols\")\n\n            if abc.ABCMeta.__instancecheck__(cls, instance):\n                return True\n\n            for attr in cls.__protocol_attrs__:\n                try:\n                    val = inspect.getattr_static(instance, attr)\n                except AttributeError:\n                    break\n                # this attribute is set by @runtime_checkable:\n                if val is None and attr not in cls.__non_callable_proto_members__:\n                    break\n            else:\n                return True\n\n            return False\n\n        def __eq__(cls, other):\n            # Hack so that typing.Generic.__class_getitem__\n            # treats typing_extensions.Protocol\n            # as equivalent to typing.Protocol\n            if abc.ABCMeta.__eq__(cls, other) is True:\n                return True\n            return cls is Protocol and other is typing.Protocol\n\n        # This has to be defined, or the abc-module cache\n        # complains about classes with this metaclass being unhashable,\n        # if we define only __eq__!\n        def __hash__(cls) -> int:\n            return type.__hash__(cls)\n\n    @classmethod\n    def _proto_hook(cls, other):\n        if not cls.__dict__.get('_is_protocol', False):\n            return NotImplemented\n\n        for attr in cls.__protocol_attrs__:\n            for base in other.__mro__:\n                # Check if the members appears in the class dictionary...\n                if attr in base.__dict__:\n                    if base.__dict__[attr] is None:\n                        return NotImplemented\n                    break\n\n                # ...or in annotations, if it is a sub-protocol.\n                annotations = getattr(base, '__annotations__', {})\n                if (\n                    isinstance(annotations, collections.abc.Mapping)\n                    and attr in annotations\n                    and is_protocol(other)\n                ):\n                    break\n            else:\n                return NotImplemented\n        return True\n\n    class Protocol(typing.Generic, metaclass=_ProtocolMeta):\n        __doc__ = typing.Protocol.__doc__\n        __slots__ = ()\n        _is_protocol = True\n        _is_runtime_protocol = False\n\n        def __init_subclass__(cls, *args, **kwargs):\n            super().__init_subclass__(*args, **kwargs)\n\n            # Determine if this is a protocol or a concrete subclass.\n            if not cls.__dict__.get('_is_protocol', False):\n                cls._is_protocol = any(b is Protocol for b in cls.__bases__)\n\n            # Set (or override) the protocol subclass hook.\n            if '__subclasshook__' not in cls.__dict__:\n                cls.__subclasshook__ = _proto_hook\n\n            # Prohibit instantiation for protocol classes\n            if cls._is_protocol and cls.__init__ is Protocol.__init__:\n                cls.__init__ = _no_init\n\n\nif sys.version_info >= (3, 13):\n    runtime_checkable = typing.runtime_checkable\nelse:\n    def runtime_checkable(cls):\n        \"\"\"Mark a protocol class as a runtime protocol.\n\n        Such protocol can be used with isinstance() and issubclass().\n        Raise TypeError if applied to a non-protocol class.\n        This allows a simple-minded structural check very similar to\n        one trick ponies in collections.abc such as Iterable.\n\n        For example::\n\n            @runtime_checkable\n            class Closable(Protocol):\n                def close(self): ...\n\n            assert isinstance(open('/some/file'), Closable)\n\n        Warning: this will check only the presence of the required methods,\n        not their type signatures!\n        \"\"\"\n        if not issubclass(cls, typing.Generic) or not getattr(cls, '_is_protocol', False):\n            raise TypeError(f'@runtime_checkable can be only applied to protocol classes,'\n                            f' got {cls!r}')\n        cls._is_runtime_protocol = True\n\n        # typing.Protocol classes on <=3.11 break if we execute this block,\n        # because typing.Protocol classes on <=3.11 don't have a\n        # `__protocol_attrs__` attribute, and this block relies on the\n        # `__protocol_attrs__` attribute. Meanwhile, typing.Protocol classes on 3.12.2+\n        # break if we *don't* execute this block, because *they* assume that all\n        # protocol classes have a `__non_callable_proto_members__` attribute\n        # (which this block sets)\n        if isinstance(cls, _ProtocolMeta) or sys.version_info >= (3, 12, 2):\n            # PEP 544 prohibits using issubclass()\n            # with protocols that have non-method members.\n            # See gh-113320 for why we compute this attribute here,\n            # rather than in `_ProtocolMeta.__init__`\n            cls.__non_callable_proto_members__ = set()\n            for attr in cls.__protocol_attrs__:\n                try:\n                    is_callable = callable(getattr(cls, attr, None))\n                except Exception as e:\n                    raise TypeError(\n                        f\"Failed to determine whether protocol member {attr!r} \"\n                        \"is a method member\"\n                    ) from e\n                else:\n                    if not is_callable:\n                        cls.__non_callable_proto_members__.add(attr)\n\n        return cls\n\n\n# The \"runtime\" alias exists for backwards compatibility.\nruntime = runtime_checkable\n\n\n# Our version of runtime-checkable protocols is faster on Python 3.8-3.11\nif sys.version_info >= (3, 12):\n    SupportsInt = typing.SupportsInt\n    SupportsFloat = typing.SupportsFloat\n    SupportsComplex = typing.SupportsComplex\n    SupportsBytes = typing.SupportsBytes\n    SupportsIndex = typing.SupportsIndex\n    SupportsAbs = typing.SupportsAbs\n    SupportsRound = typing.SupportsRound\nelse:\n    @runtime_checkable\n    class SupportsInt(Protocol):\n        \"\"\"An ABC with one abstract method __int__.\"\"\"\n        __slots__ = ()\n\n        @abc.abstractmethod\n        def __int__(self) -> int:\n            pass\n\n    @runtime_checkable\n    class SupportsFloat(Protocol):\n        \"\"\"An ABC with one abstract method __float__.\"\"\"\n        __slots__ = ()\n\n        @abc.abstractmethod\n        def __float__(self) -> float:\n            pass\n\n    @runtime_checkable\n    class SupportsComplex(Protocol):\n        \"\"\"An ABC with one abstract method __complex__.\"\"\"\n        __slots__ = ()\n\n        @abc.abstractmethod\n        def __complex__(self) -> complex:\n            pass\n\n    @runtime_checkable\n    class SupportsBytes(Protocol):\n        \"\"\"An ABC with one abstract method __bytes__.\"\"\"\n        __slots__ = ()\n\n        @abc.abstractmethod\n        def __bytes__(self) -> bytes:\n            pass\n\n    @runtime_checkable\n    class SupportsIndex(Protocol):\n        __slots__ = ()\n\n        @abc.abstractmethod\n        def __index__(self) -> int:\n            pass\n\n    @runtime_checkable\n    class SupportsAbs(Protocol[T_co]):\n        \"\"\"\n        An ABC with one abstract method __abs__ that is covariant in its return type.\n        \"\"\"\n        __slots__ = ()\n\n        @abc.abstractmethod\n        def __abs__(self) -> T_co:\n            pass\n\n    @runtime_checkable\n    class SupportsRound(Protocol[T_co]):\n        \"\"\"\n        An ABC with one abstract method __round__ that is covariant in its return type.\n        \"\"\"\n        __slots__ = ()\n\n        @abc.abstractmethod\n        def __round__(self, ndigits: int = 0) -> T_co:\n            pass\n\n\ndef _ensure_subclassable(mro_entries):\n    def inner(func):\n        if sys.implementation.name == \"pypy\" and sys.version_info < (3, 9):\n            cls_dict = {\n                \"__call__\": staticmethod(func),\n                \"__mro_entries__\": staticmethod(mro_entries)\n            }\n            t = type(func.__name__, (), cls_dict)\n            return functools.update_wrapper(t(), func)\n        else:\n            func.__mro_entries__ = mro_entries\n            return func\n    return inner\n\n\n# Update this to something like >=3.13.0b1 if and when\n# PEP 728 is implemented in CPython\n_PEP_728_IMPLEMENTED = False\n\nif _PEP_728_IMPLEMENTED:\n    # The standard library TypedDict in Python 3.8 does not store runtime information\n    # about which (if any) keys are optional.  See https://bugs.python.org/issue38834\n    # The standard library TypedDict in Python 3.9.0/1 does not honour the \"total\"\n    # keyword with old-style TypedDict().  See https://bugs.python.org/issue42059\n    # The standard library TypedDict below Python 3.11 does not store runtime\n    # information about optional and required keys when using Required or NotRequired.\n    # Generic TypedDicts are also impossible using typing.TypedDict on Python <3.11.\n    # Aaaand on 3.12 we add __orig_bases__ to TypedDict\n    # to enable better runtime introspection.\n    # On 3.13 we deprecate some odd ways of creating TypedDicts.\n    # Also on 3.13, PEP 705 adds the ReadOnly[] qualifier.\n    # PEP 728 (still pending) makes more changes.\n    TypedDict = typing.TypedDict\n    _TypedDictMeta = typing._TypedDictMeta\n    is_typeddict = typing.is_typeddict\nelse:\n    # 3.10.0 and later\n    _TAKES_MODULE = \"module\" in inspect.signature(typing._type_check).parameters\n\n    def _get_typeddict_qualifiers(annotation_type):\n        while True:\n            annotation_origin = get_origin(annotation_type)\n            if annotation_origin is Annotated:\n                annotation_args = get_args(annotation_type)\n                if annotation_args:\n                    annotation_type = annotation_args[0]\n                else:\n                    break\n            elif annotation_origin is Required:\n                yield Required\n                annotation_type, = get_args(annotation_type)\n            elif annotation_origin is NotRequired:\n                yield NotRequired\n                annotation_type, = get_args(annotation_type)\n            elif annotation_origin is ReadOnly:\n                yield ReadOnly\n                annotation_type, = get_args(annotation_type)\n            else:\n                break\n\n    class _TypedDictMeta(type):\n        def __new__(cls, name, bases, ns, *, total=True, closed=False):\n            \"\"\"Create new typed dict class object.\n\n            This method is called when TypedDict is subclassed,\n            or when TypedDict is instantiated. This way\n            TypedDict supports all three syntax forms described in its docstring.\n            Subclasses and instances of TypedDict return actual dictionaries.\n            \"\"\"\n            for base in bases:\n                if type(base) is not _TypedDictMeta and base is not typing.Generic:\n                    raise TypeError('cannot inherit from both a TypedDict type '\n                                    'and a non-TypedDict base class')\n\n            if any(issubclass(b, typing.Generic) for b in bases):\n                generic_base = (typing.Generic,)\n            else:\n                generic_base = ()\n\n            # typing.py generally doesn't let you inherit from plain Generic, unless\n            # the name of the class happens to be \"Protocol\"\n            tp_dict = type.__new__(_TypedDictMeta, \"Protocol\", (*generic_base, dict), ns)\n            tp_dict.__name__ = name\n            if tp_dict.__qualname__ == \"Protocol\":\n                tp_dict.__qualname__ = name\n\n            if not hasattr(tp_dict, '__orig_bases__'):\n                tp_dict.__orig_bases__ = bases\n\n            annotations = {}\n            if \"__annotations__\" in ns:\n                own_annotations = ns[\"__annotations__\"]\n            elif \"__annotate__\" in ns:\n                # TODO: Use inspect.VALUE here, and make the annotations lazily evaluated\n                own_annotations = ns[\"__annotate__\"](1)\n            else:\n                own_annotations = {}\n            msg = \"TypedDict('Name', {f0: t0, f1: t1, ...}); each t must be a type\"\n            if _TAKES_MODULE:\n                own_annotations = {\n                    n: typing._type_check(tp, msg, module=tp_dict.__module__)\n                    for n, tp in own_annotations.items()\n                }\n            else:\n                own_annotations = {\n                    n: typing._type_check(tp, msg)\n                    for n, tp in own_annotations.items()\n                }\n            required_keys = set()\n            optional_keys = set()\n            readonly_keys = set()\n            mutable_keys = set()\n            extra_items_type = None\n\n            for base in bases:\n                base_dict = base.__dict__\n\n                annotations.update(base_dict.get('__annotations__', {}))\n                required_keys.update(base_dict.get('__required_keys__', ()))\n                optional_keys.update(base_dict.get('__optional_keys__', ()))\n                readonly_keys.update(base_dict.get('__readonly_keys__', ()))\n                mutable_keys.update(base_dict.get('__mutable_keys__', ()))\n                base_extra_items_type = base_dict.get('__extra_items__', None)\n                if base_extra_items_type is not None:\n                    extra_items_type = base_extra_items_type\n\n            if closed and extra_items_type is None:\n                extra_items_type = Never\n            if closed and \"__extra_items__\" in own_annotations:\n                annotation_type = own_annotations.pop(\"__extra_items__\")\n                qualifiers = set(_get_typeddict_qualifiers(annotation_type))\n                if Required in qualifiers:\n                    raise TypeError(\n                        \"Special key __extra_items__ does not support \"\n                        \"Required\"\n                    )\n                if NotRequired in qualifiers:\n                    raise TypeError(\n                        \"Special key __extra_items__ does not support \"\n                        \"NotRequired\"\n                    )\n                extra_items_type = annotation_type\n\n            annotations.update(own_annotations)\n            for annotation_key, annotation_type in own_annotations.items():\n                qualifiers = set(_get_typeddict_qualifiers(annotation_type))\n\n                if Required in qualifiers:\n                    required_keys.add(annotation_key)\n                elif NotRequired in qualifiers:\n                    optional_keys.add(annotation_key)\n                elif total:\n                    required_keys.add(annotation_key)\n                else:\n                    optional_keys.add(annotation_key)\n                if ReadOnly in qualifiers:\n                    mutable_keys.discard(annotation_key)\n                    readonly_keys.add(annotation_key)\n                else:\n                    mutable_keys.add(annotation_key)\n                    readonly_keys.discard(annotation_key)\n\n            tp_dict.__annotations__ = annotations\n            tp_dict.__required_keys__ = frozenset(required_keys)\n            tp_dict.__optional_keys__ = frozenset(optional_keys)\n            tp_dict.__readonly_keys__ = frozenset(readonly_keys)\n            tp_dict.__mutable_keys__ = frozenset(mutable_keys)\n            if not hasattr(tp_dict, '__total__'):\n                tp_dict.__total__ = total\n            tp_dict.__closed__ = closed\n            tp_dict.__extra_items__ = extra_items_type\n            return tp_dict\n\n        __call__ = dict  # static method\n\n        def __subclasscheck__(cls, other):\n            # Typed dicts are only for static structural subtyping.\n            raise TypeError('TypedDict does not support instance and class checks')\n\n        __instancecheck__ = __subclasscheck__\n\n    _TypedDict = type.__new__(_TypedDictMeta, 'TypedDict', (), {})\n\n    @_ensure_subclassable(lambda bases: (_TypedDict,))\n    def TypedDict(typename, fields=_marker, /, *, total=True, closed=False, **kwargs):\n        \"\"\"A simple typed namespace. At runtime it is equivalent to a plain dict.\n\n        TypedDict creates a dictionary type such that a type checker will expect all\n        instances to have a certain set of keys, where each key is\n        associated with a value of a consistent type. This expectation\n        is not checked at runtime.\n\n        Usage::\n\n            class Point2D(TypedDict):\n                x: int\n                y: int\n                label: str\n\n            a: Point2D = {'x': 1, 'y': 2, 'label': 'good'}  # OK\n            b: Point2D = {'z': 3, 'label': 'bad'}           # Fails type check\n\n            assert Point2D(x=1, y=2, label='first') == dict(x=1, y=2, label='first')\n\n        The type info can be accessed via the Point2D.__annotations__ dict, and\n        the Point2D.__required_keys__ and Point2D.__optional_keys__ frozensets.\n        TypedDict supports an additional equivalent form::\n\n            Point2D = TypedDict('Point2D', {'x': int, 'y': int, 'label': str})\n\n        By default, all keys must be present in a TypedDict. It is possible\n        to override this by specifying totality::\n\n            class Point2D(TypedDict, total=False):\n                x: int\n                y: int\n\n        This means that a Point2D TypedDict can have any of the keys omitted. A type\n        checker is only expected to support a literal False or True as the value of\n        the total argument. True is the default, and makes all items defined in the\n        class body be required.\n\n        The Required and NotRequired special forms can also be used to mark\n        individual keys as being required or not required::\n\n            class Point2D(TypedDict):\n                x: int  # the \"x\" key must always be present (Required is the default)\n                y: NotRequired[int]  # the \"y\" key can be omitted\n\n        See PEP 655 for more details on Required and NotRequired.\n        \"\"\"\n        if fields is _marker or fields is None:\n            if fields is _marker:\n                deprecated_thing = \"Failing to pass a value for the 'fields' parameter\"\n            else:\n                deprecated_thing = \"Passing `None` as the 'fields' parameter\"\n\n            example = f\"`{typename} = TypedDict({typename!r}, {{}})`\"\n            deprecation_msg = (\n                f\"{deprecated_thing} is deprecated and will be disallowed in \"\n                \"Python 3.15. To create a TypedDict class with 0 fields \"\n                \"using the functional syntax, pass an empty dictionary, e.g. \"\n            ) + example + \".\"\n            warnings.warn(deprecation_msg, DeprecationWarning, stacklevel=2)\n            if closed is not False and closed is not True:\n                kwargs[\"closed\"] = closed\n                closed = False\n            fields = kwargs\n        elif kwargs:\n            raise TypeError(\"TypedDict takes either a dict or keyword arguments,\"\n                            \" but not both\")\n        if kwargs:\n            if sys.version_info >= (3, 13):\n                raise TypeError(\"TypedDict takes no keyword arguments\")\n            warnings.warn(\n                \"The kwargs-based syntax for TypedDict definitions is deprecated \"\n                \"in Python 3.11, will be removed in Python 3.13, and may not be \"\n                \"understood by third-party type checkers.\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n\n        ns = {'__annotations__': dict(fields)}\n        module = _caller()\n        if module is not None:\n            # Setting correct module is necessary to make typed dict classes pickleable.\n            ns['__module__'] = module\n\n        td = _TypedDictMeta(typename, (), ns, total=total, closed=closed)\n        td.__orig_bases__ = (TypedDict,)\n        return td\n\n    if hasattr(typing, \"_TypedDictMeta\"):\n        _TYPEDDICT_TYPES = (typing._TypedDictMeta, _TypedDictMeta)\n    else:\n        _TYPEDDICT_TYPES = (_TypedDictMeta,)\n\n    def is_typeddict(tp):\n        \"\"\"Check if an annotation is a TypedDict class\n\n        For example::\n            class Film(TypedDict):\n                title: str\n                year: int\n\n            is_typeddict(Film)  # => True\n            is_typeddict(Union[list, str])  # => False\n        \"\"\"\n        # On 3.8, this would otherwise return True\n        if hasattr(typing, \"TypedDict\") and tp is typing.TypedDict:\n            return False\n        return isinstance(tp, _TYPEDDICT_TYPES)\n\n\nif hasattr(typing, \"assert_type\"):\n    assert_type = typing.assert_type\n\nelse:\n    def assert_type(val, typ, /):\n        \"\"\"Assert (to the type checker) that the value is of the given type.\n\n        When the type checker encounters a call to assert_type(), it\n        emits an error if the value is not of the specified type::\n\n            def greet(name: str) -> None:\n                assert_type(name, str)  # ok\n                assert_type(name, int)  # type checker error\n\n        At runtime this returns the first argument unchanged and otherwise\n        does nothing.\n        \"\"\"\n        return val\n\n\nif hasattr(typing, \"ReadOnly\"):  # 3.13+\n    get_type_hints = typing.get_type_hints\nelse:  # <=3.13\n    # replaces _strip_annotations()\n    def _strip_extras(t):\n        \"\"\"Strips Annotated, Required and NotRequired from a given type.\"\"\"\n        if isinstance(t, _AnnotatedAlias):\n            return _strip_extras(t.__origin__)\n        if hasattr(t, \"__origin__\") and t.__origin__ in (Required, NotRequired, ReadOnly):\n            return _strip_extras(t.__args__[0])\n        if isinstance(t, typing._GenericAlias):\n            stripped_args = tuple(_strip_extras(a) for a in t.__args__)\n            if stripped_args == t.__args__:\n                return t\n            return t.copy_with(stripped_args)\n        if hasattr(_types, \"GenericAlias\") and isinstance(t, _types.GenericAlias):\n            stripped_args = tuple(_strip_extras(a) for a in t.__args__)\n            if stripped_args == t.__args__:\n                return t\n            return _types.GenericAlias(t.__origin__, stripped_args)\n        if hasattr(_types, \"UnionType\") and isinstance(t, _types.UnionType):\n            stripped_args = tuple(_strip_extras(a) for a in t.__args__)\n            if stripped_args == t.__args__:\n                return t\n            return functools.reduce(operator.or_, stripped_args)\n\n        return t\n\n    def get_type_hints(obj, globalns=None, localns=None, include_extras=False):\n        \"\"\"Return type hints for an object.\n\n        This is often the same as obj.__annotations__, but it handles\n        forward references encoded as string literals, adds Optional[t] if a\n        default value equal to None is set and recursively replaces all\n        'Annotated[T, ...]', 'Required[T]' or 'NotRequired[T]' with 'T'\n        (unless 'include_extras=True').\n\n        The argument may be a module, class, method, or function. The annotations\n        are returned as a dictionary. For classes, annotations include also\n        inherited members.\n\n        TypeError is raised if the argument is not of a type that can contain\n        annotations, and an empty dictionary is returned if no annotations are\n        present.\n\n        BEWARE -- the behavior of globalns and localns is counterintuitive\n        (unless you are familiar with how eval() and exec() work).  The\n        search order is locals first, then globals.\n\n        - If no dict arguments are passed, an attempt is made to use the\n          globals from obj (or the respective module's globals for classes),\n          and these are also used as the locals.  If the object does not appear\n          to have globals, an empty dictionary is used.\n\n        - If one dict argument is passed, it is used for both globals and\n          locals.\n\n        - If two dict arguments are passed, they specify globals and\n          locals, respectively.\n        \"\"\"\n        if hasattr(typing, \"Annotated\"):  # 3.9+\n            hint = typing.get_type_hints(\n                obj, globalns=globalns, localns=localns, include_extras=True\n            )\n        else:  # 3.8\n            hint = typing.get_type_hints(obj, globalns=globalns, localns=localns)\n        if include_extras:\n            return hint\n        return {k: _strip_extras(t) for k, t in hint.items()}\n\n\n# Python 3.9+ has PEP 593 (Annotated)\nif hasattr(typing, 'Annotated'):\n    Annotated = typing.Annotated\n    # Not exported and not a public API, but needed for get_origin() and get_args()\n    # to work.\n    _AnnotatedAlias = typing._AnnotatedAlias\n# 3.8\nelse:\n    class _AnnotatedAlias(typing._GenericAlias, _root=True):\n        \"\"\"Runtime representation of an annotated type.\n\n        At its core 'Annotated[t, dec1, dec2, ...]' is an alias for the type 't'\n        with extra annotations. The alias behaves like a normal typing alias,\n        instantiating is the same as instantiating the underlying type, binding\n        it to types is also the same.\n        \"\"\"\n        def __init__(self, origin, metadata):\n            if isinstance(origin, _AnnotatedAlias):\n                metadata = origin.__metadata__ + metadata\n                origin = origin.__origin__\n            super().__init__(origin, origin)\n            self.__metadata__ = metadata\n\n        def copy_with(self, params):\n            assert len(params) == 1\n            new_type = params[0]\n            return _AnnotatedAlias(new_type, self.__metadata__)\n\n        def __repr__(self):\n            return (f\"typing_extensions.Annotated[{typing._type_repr(self.__origin__)}, \"\n                    f\"{', '.join(repr(a) for a in self.__metadata__)}]\")\n\n        def __reduce__(self):\n            return operator.getitem, (\n                Annotated, (self.__origin__, *self.__metadata__)\n            )\n\n        def __eq__(self, other):\n            if not isinstance(other, _AnnotatedAlias):\n                return NotImplemented\n            if self.__origin__ != other.__origin__:\n                return False\n            return self.__metadata__ == other.__metadata__\n\n        def __hash__(self):\n            return hash((self.__origin__, self.__metadata__))\n\n    class Annotated:\n        \"\"\"Add context specific metadata to a type.\n\n        Example: Annotated[int, runtime_check.Unsigned] indicates to the\n        hypothetical runtime_check module that this type is an unsigned int.\n        Every other consumer of this type can ignore this metadata and treat\n        this type as int.\n\n        The first argument to Annotated must be a valid type (and will be in\n        the __origin__ field), the remaining arguments are kept as a tuple in\n        the __extra__ field.\n\n        Details:\n\n        - It's an error to call `Annotated` with less than two arguments.\n        - Nested Annotated are flattened::\n\n            Annotated[Annotated[T, Ann1, Ann2], Ann3] == Annotated[T, Ann1, Ann2, Ann3]\n\n        - Instantiating an annotated type is equivalent to instantiating the\n        underlying type::\n\n            Annotated[C, Ann1](5) == C(5)\n\n        - Annotated can be used as a generic type alias::\n\n            Optimized = Annotated[T, runtime.Optimize()]\n            Optimized[int] == Annotated[int, runtime.Optimize()]\n\n            OptimizedList = Annotated[List[T], runtime.Optimize()]\n            OptimizedList[int] == Annotated[List[int], runtime.Optimize()]\n        \"\"\"\n\n        __slots__ = ()\n\n        def __new__(cls, *args, **kwargs):\n            raise TypeError(\"Type Annotated cannot be instantiated.\")\n\n        @typing._tp_cache\n        def __class_getitem__(cls, params):\n            if not isinstance(params, tuple) or len(params) < 2:\n                raise TypeError(\"Annotated[...] should be used \"\n                                \"with at least two arguments (a type and an \"\n                                \"annotation).\")\n            allowed_special_forms = (ClassVar, Final)\n            if get_origin(params[0]) in allowed_special_forms:\n                origin = params[0]\n            else:\n                msg = \"Annotated[t, ...]: t must be a type.\"\n                origin = typing._type_check(params[0], msg)\n            metadata = tuple(params[1:])\n            return _AnnotatedAlias(origin, metadata)\n\n        def __init_subclass__(cls, *args, **kwargs):\n            raise TypeError(\n                f\"Cannot subclass {cls.__module__}.Annotated\"\n            )\n\n# Python 3.8 has get_origin() and get_args() but those implementations aren't\n# Annotated-aware, so we can't use those. Python 3.9's versions don't support\n# ParamSpecArgs and ParamSpecKwargs, so only Python 3.10's versions will do.\nif sys.version_info[:2] >= (3, 10):\n    get_origin = typing.get_origin\n    get_args = typing.get_args\n# 3.8-3.9\nelse:\n    try:\n        # 3.9+\n        from typing import _BaseGenericAlias\n    except ImportError:\n        _BaseGenericAlias = typing._GenericAlias\n    try:\n        # 3.9+\n        from typing import GenericAlias as _typing_GenericAlias\n    except ImportError:\n        _typing_GenericAlias = typing._GenericAlias\n\n    def get_origin(tp):\n        \"\"\"Get the unsubscripted version of a type.\n\n        This supports generic types, Callable, Tuple, Union, Literal, Final, ClassVar\n        and Annotated. Return None for unsupported types. Examples::\n\n            get_origin(Literal[42]) is Literal\n            get_origin(int) is None\n            get_origin(ClassVar[int]) is ClassVar\n            get_origin(Generic) is Generic\n            get_origin(Generic[T]) is Generic\n            get_origin(Union[T, int]) is Union\n            get_origin(List[Tuple[T, T]][int]) == list\n            get_origin(P.args) is P\n        \"\"\"\n        if isinstance(tp, _AnnotatedAlias):\n            return Annotated\n        if isinstance(tp, (typing._GenericAlias, _typing_GenericAlias, _BaseGenericAlias,\n                           ParamSpecArgs, ParamSpecKwargs)):\n            return tp.__origin__\n        if tp is typing.Generic:\n            return typing.Generic\n        return None\n\n    def get_args(tp):\n        \"\"\"Get type arguments with all substitutions performed.\n\n        For unions, basic simplifications used by Union constructor are performed.\n        Examples::\n            get_args(Dict[str, int]) == (str, int)\n            get_args(int) == ()\n            get_args(Union[int, Union[T, int], str][int]) == (int, str)\n            get_args(Union[int, Tuple[T, int]][str]) == (int, Tuple[str, int])\n            get_args(Callable[[], T][int]) == ([], int)\n        \"\"\"\n        if isinstance(tp, _AnnotatedAlias):\n            return (tp.__origin__, *tp.__metadata__)\n        if isinstance(tp, (typing._GenericAlias, _typing_GenericAlias)):\n            if getattr(tp, \"_special\", False):\n                return ()\n            res = tp.__args__\n            if get_origin(tp) is collections.abc.Callable and res[0] is not Ellipsis:\n                res = (list(res[:-1]), res[-1])\n            return res\n        return ()\n\n\n# 3.10+\nif hasattr(typing, 'TypeAlias'):\n    TypeAlias = typing.TypeAlias\n# 3.9\nelif sys.version_info[:2] >= (3, 9):\n    @_ExtensionsSpecialForm\n    def TypeAlias(self, parameters):\n        \"\"\"Special marker indicating that an assignment should\n        be recognized as a proper type alias definition by type\n        checkers.\n\n        For example::\n\n            Predicate: TypeAlias = Callable[..., bool]\n\n        It's invalid when used anywhere except as in the example above.\n        \"\"\"\n        raise TypeError(f\"{self} is not subscriptable\")\n# 3.8\nelse:\n    TypeAlias = _ExtensionsSpecialForm(\n        'TypeAlias',\n        doc=\"\"\"Special marker indicating that an assignment should\n        be recognized as a proper type alias definition by type\n        checkers.\n\n        For example::\n\n            Predicate: TypeAlias = Callable[..., bool]\n\n        It's invalid when used anywhere except as in the example\n        above.\"\"\"\n    )\n\n\nif hasattr(typing, \"NoDefault\"):\n    NoDefault = typing.NoDefault\nelse:\n    class NoDefaultTypeMeta(type):\n        def __setattr__(cls, attr, value):\n            # TypeError is consistent with the behavior of NoneType\n            raise TypeError(\n                f\"cannot set {attr!r} attribute of immutable type {cls.__name__!r}\"\n            )\n\n    class NoDefaultType(metaclass=NoDefaultTypeMeta):\n        \"\"\"The type of the NoDefault singleton.\"\"\"\n\n        __slots__ = ()\n\n        def __new__(cls):\n            return globals().get(\"NoDefault\") or object.__new__(cls)\n\n        def __repr__(self):\n            return \"typing_extensions.NoDefault\"\n\n        def __reduce__(self):\n            return \"NoDefault\"\n\n    NoDefault = NoDefaultType()\n    del NoDefaultType, NoDefaultTypeMeta\n\n\ndef _set_default(type_param, default):\n    type_param.has_default = lambda: default is not NoDefault\n    type_param.__default__ = default\n\n\ndef _set_module(typevarlike):\n    # for pickling:\n    def_mod = _caller(depth=3)\n    if def_mod != 'typing_extensions':\n        typevarlike.__module__ = def_mod\n\n\nclass _DefaultMixin:\n    \"\"\"Mixin for TypeVarLike defaults.\"\"\"\n\n    __slots__ = ()\n    __init__ = _set_default\n\n\n# Classes using this metaclass must provide a _backported_typevarlike ClassVar\nclass _TypeVarLikeMeta(type):\n    def __instancecheck__(cls, __instance: Any) -> bool:\n        return isinstance(__instance, cls._backported_typevarlike)\n\n\nif _PEP_696_IMPLEMENTED:\n    from typing import TypeVar\nelse:\n    # Add default and infer_variance parameters from PEP 696 and 695\n    class TypeVar(metaclass=_TypeVarLikeMeta):\n        \"\"\"Type variable.\"\"\"\n\n        _backported_typevarlike = typing.TypeVar\n\n        def __new__(cls, name, *constraints, bound=None,\n                    covariant=False, contravariant=False,\n                    default=NoDefault, infer_variance=False):\n            if hasattr(typing, \"TypeAliasType\"):\n                # PEP 695 implemented (3.12+), can pass infer_variance to typing.TypeVar\n                typevar = typing.TypeVar(name, *constraints, bound=bound,\n                                         covariant=covariant, contravariant=contravariant,\n                                         infer_variance=infer_variance)\n            else:\n                typevar = typing.TypeVar(name, *constraints, bound=bound,\n                                         covariant=covariant, contravariant=contravariant)\n                if infer_variance and (covariant or contravariant):\n                    raise ValueError(\"Variance cannot be specified with infer_variance.\")\n                typevar.__infer_variance__ = infer_variance\n\n            _set_default(typevar, default)\n            _set_module(typevar)\n\n            def _tvar_prepare_subst(alias, args):\n                if (\n                    typevar.has_default()\n                    and alias.__parameters__.index(typevar) == len(args)\n                ):\n                    args += (typevar.__default__,)\n                return args\n\n            typevar.__typing_prepare_subst__ = _tvar_prepare_subst\n            return typevar\n\n        def __init_subclass__(cls) -> None:\n            raise TypeError(f\"type '{__name__}.TypeVar' is not an acceptable base type\")\n\n\n# Python 3.10+ has PEP 612\nif hasattr(typing, 'ParamSpecArgs'):\n    ParamSpecArgs = typing.ParamSpecArgs\n    ParamSpecKwargs = typing.ParamSpecKwargs\n# 3.8-3.9\nelse:\n    class _Immutable:\n        \"\"\"Mixin to indicate that object should not be copied.\"\"\"\n        __slots__ = ()\n\n        def __copy__(self):\n            return self\n\n        def __deepcopy__(self, memo):\n            return self\n\n    class ParamSpecArgs(_Immutable):\n        \"\"\"The args for a ParamSpec object.\n\n        Given a ParamSpec object P, P.args is an instance of ParamSpecArgs.\n\n        ParamSpecArgs objects have a reference back to their ParamSpec:\n\n        P.args.__origin__ is P\n\n        This type is meant for runtime introspection and has no special meaning to\n        static type checkers.\n        \"\"\"\n        def __init__(self, origin):\n            self.__origin__ = origin\n\n        def __repr__(self):\n            return f\"{self.__origin__.__name__}.args\"\n\n        def __eq__(self, other):\n            if not isinstance(other, ParamSpecArgs):\n                return NotImplemented\n            return self.__origin__ == other.__origin__\n\n    class ParamSpecKwargs(_Immutable):\n        \"\"\"The kwargs for a ParamSpec object.\n\n        Given a ParamSpec object P, P.kwargs is an instance of ParamSpecKwargs.\n\n        ParamSpecKwargs objects have a reference back to their ParamSpec:\n\n        P.kwargs.__origin__ is P\n\n        This type is meant for runtime introspection and has no special meaning to\n        static type checkers.\n        \"\"\"\n        def __init__(self, origin):\n            self.__origin__ = origin\n\n        def __repr__(self):\n            return f\"{self.__origin__.__name__}.kwargs\"\n\n        def __eq__(self, other):\n            if not isinstance(other, ParamSpecKwargs):\n                return NotImplemented\n            return self.__origin__ == other.__origin__\n\n\nif _PEP_696_IMPLEMENTED:\n    from typing import ParamSpec\n\n# 3.10+\nelif hasattr(typing, 'ParamSpec'):\n\n    # Add default parameter - PEP 696\n    class ParamSpec(metaclass=_TypeVarLikeMeta):\n        \"\"\"Parameter specification.\"\"\"\n\n        _backported_typevarlike = typing.ParamSpec\n\n        def __new__(cls, name, *, bound=None,\n                    covariant=False, contravariant=False,\n                    infer_variance=False, default=NoDefault):\n            if hasattr(typing, \"TypeAliasType\"):\n                # PEP 695 implemented, can pass infer_variance to typing.TypeVar\n                paramspec = typing.ParamSpec(name, bound=bound,\n                                             covariant=covariant,\n                                             contravariant=contravariant,\n                                             infer_variance=infer_variance)\n            else:\n                paramspec = typing.ParamSpec(name, bound=bound,\n                                             covariant=covariant,\n                                             contravariant=contravariant)\n                paramspec.__infer_variance__ = infer_variance\n\n            _set_default(paramspec, default)\n            _set_module(paramspec)\n\n            def _paramspec_prepare_subst(alias, args):\n                params = alias.__parameters__\n                i = params.index(paramspec)\n                if i == len(args) and paramspec.has_default():\n                    args = [*args, paramspec.__default__]\n                if i >= len(args):\n                    raise TypeError(f\"Too few arguments for {alias}\")\n                # Special case where Z[[int, str, bool]] == Z[int, str, bool] in PEP 612.\n                if len(params) == 1 and not typing._is_param_expr(args[0]):\n                    assert i == 0\n                    args = (args,)\n                # Convert lists to tuples to help other libraries cache the results.\n                elif isinstance(args[i], list):\n                    args = (*args[:i], tuple(args[i]), *args[i + 1:])\n                return args\n\n            paramspec.__typing_prepare_subst__ = _paramspec_prepare_subst\n            return paramspec\n\n        def __init_subclass__(cls) -> None:\n            raise TypeError(f\"type '{__name__}.ParamSpec' is not an acceptable base type\")\n\n# 3.8-3.9\nelse:\n\n    # Inherits from list as a workaround for Callable checks in Python < 3.9.2.\n    class ParamSpec(list, _DefaultMixin):\n        \"\"\"Parameter specification variable.\n\n        Usage::\n\n           P = ParamSpec('P')\n\n        Parameter specification variables exist primarily for the benefit of static\n        type checkers.  They are used to forward the parameter types of one\n        callable to another callable, a pattern commonly found in higher order\n        functions and decorators.  They are only valid when used in ``Concatenate``,\n        or s the first argument to ``Callable``. In Python 3.10 and higher,\n        they are also supported in user-defined Generics at runtime.\n        See class Generic for more information on generic types.  An\n        example for annotating a decorator::\n\n           T = TypeVar('T')\n           P = ParamSpec('P')\n\n           def add_logging(f: Callable[P, T]) -> Callable[P, T]:\n               '''A type-safe decorator to add logging to a function.'''\n               def inner(*args: P.args, **kwargs: P.kwargs) -> T:\n                   logging.info(f'{f.__name__} was called')\n                   return f(*args, **kwargs)\n               return inner\n\n           @add_logging\n           def add_two(x: float, y: float) -> float:\n               '''Add two numbers together.'''\n               return x + y\n\n        Parameter specification variables defined with covariant=True or\n        contravariant=True can be used to declare covariant or contravariant\n        generic types.  These keyword arguments are valid, but their actual semantics\n        are yet to be decided.  See PEP 612 for details.\n\n        Parameter specification variables can be introspected. e.g.:\n\n           P.__name__ == 'T'\n           P.__bound__ == None\n           P.__covariant__ == False\n           P.__contravariant__ == False\n\n        Note that only parameter specification variables defined in global scope can\n        be pickled.\n        \"\"\"\n\n        # Trick Generic __parameters__.\n        __class__ = typing.TypeVar\n\n        @property\n        def args(self):\n            return ParamSpecArgs(self)\n\n        @property\n        def kwargs(self):\n            return ParamSpecKwargs(self)\n\n        def __init__(self, name, *, bound=None, covariant=False, contravariant=False,\n                     infer_variance=False, default=NoDefault):\n            list.__init__(self, [self])\n            self.__name__ = name\n            self.__covariant__ = bool(covariant)\n            self.__contravariant__ = bool(contravariant)\n            self.__infer_variance__ = bool(infer_variance)\n            if bound:\n                self.__bound__ = typing._type_check(bound, 'Bound must be a type.')\n            else:\n                self.__bound__ = None\n            _DefaultMixin.__init__(self, default)\n\n            # for pickling:\n            def_mod = _caller()\n            if def_mod != 'typing_extensions':\n                self.__module__ = def_mod\n\n        def __repr__(self):\n            if self.__infer_variance__:\n                prefix = ''\n            elif self.__covariant__:\n                prefix = '+'\n            elif self.__contravariant__:\n                prefix = '-'\n            else:\n                prefix = '~'\n            return prefix + self.__name__\n\n        def __hash__(self):\n            return object.__hash__(self)\n\n        def __eq__(self, other):\n            return self is other\n\n        def __reduce__(self):\n            return self.__name__\n\n        # Hack to get typing._type_check to pass.\n        def __call__(self, *args, **kwargs):\n            pass\n\n\n# 3.8-3.9\nif not hasattr(typing, 'Concatenate'):\n    # Inherits from list as a workaround for Callable checks in Python < 3.9.2.\n    class _ConcatenateGenericAlias(list):\n\n        # Trick Generic into looking into this for __parameters__.\n        __class__ = typing._GenericAlias\n\n        # Flag in 3.8.\n        _special = False\n\n        def __init__(self, origin, args):\n            super().__init__(args)\n            self.__origin__ = origin\n            self.__args__ = args\n\n        def __repr__(self):\n            _type_repr = typing._type_repr\n            return (f'{_type_repr(self.__origin__)}'\n                    f'[{\", \".join(_type_repr(arg) for arg in self.__args__)}]')\n\n        def __hash__(self):\n            return hash((self.__origin__, self.__args__))\n\n        # Hack to get typing._type_check to pass in Generic.\n        def __call__(self, *args, **kwargs):\n            pass\n\n        @property\n        def __parameters__(self):\n            return tuple(\n                tp for tp in self.__args__ if isinstance(tp, (typing.TypeVar, ParamSpec))\n            )\n\n\n# 3.8-3.9\n@typing._tp_cache\ndef _concatenate_getitem(self, parameters):\n    if parameters == ():\n        raise TypeError(\"Cannot take a Concatenate of no types.\")\n    if not isinstance(parameters, tuple):\n        parameters = (parameters,)\n    if not isinstance(parameters[-1], ParamSpec):\n        raise TypeError(\"The last parameter to Concatenate should be a \"\n                        \"ParamSpec variable.\")\n    msg = \"Concatenate[arg, ...]: each arg must be a type.\"\n    parameters = tuple(typing._type_check(p, msg) for p in parameters)\n    return _ConcatenateGenericAlias(self, parameters)\n\n\n# 3.10+\nif hasattr(typing, 'Concatenate'):\n    Concatenate = typing.Concatenate\n    _ConcatenateGenericAlias = typing._ConcatenateGenericAlias\n# 3.9\nelif sys.version_info[:2] >= (3, 9):\n    @_ExtensionsSpecialForm\n    def Concatenate(self, parameters):\n        \"\"\"Used in conjunction with ``ParamSpec`` and ``Callable`` to represent a\n        higher order function which adds, removes or transforms parameters of a\n        callable.\n\n        For example::\n\n           Callable[Concatenate[int, P], int]\n\n        See PEP 612 for detailed information.\n        \"\"\"\n        return _concatenate_getitem(self, parameters)\n# 3.8\nelse:\n    class _ConcatenateForm(_ExtensionsSpecialForm, _root=True):\n        def __getitem__(self, parameters):\n            return _concatenate_getitem(self, parameters)\n\n    Concatenate = _ConcatenateForm(\n        'Concatenate',\n        doc=\"\"\"Used in conjunction with ``ParamSpec`` and ``Callable`` to represent a\n        higher order function which adds, removes or transforms parameters of a\n        callable.\n\n        For example::\n\n           Callable[Concatenate[int, P], int]\n\n        See PEP 612 for detailed information.\n        \"\"\")\n\n# 3.10+\nif hasattr(typing, 'TypeGuard'):\n    TypeGuard = typing.TypeGuard\n# 3.9\nelif sys.version_info[:2] >= (3, 9):\n    @_ExtensionsSpecialForm\n    def TypeGuard(self, parameters):\n        \"\"\"Special typing form used to annotate the return type of a user-defined\n        type guard function.  ``TypeGuard`` only accepts a single type argument.\n        At runtime, functions marked this way should return a boolean.\n\n        ``TypeGuard`` aims to benefit *type narrowing* -- a technique used by static\n        type checkers to determine a more precise type of an expression within a\n        program's code flow.  Usually type narrowing is done by analyzing\n        conditional code flow and applying the narrowing to a block of code.  The\n        conditional expression here is sometimes referred to as a \"type guard\".\n\n        Sometimes it would be convenient to use a user-defined boolean function\n        as a type guard.  Such a function should use ``TypeGuard[...]`` as its\n        return type to alert static type checkers to this intention.\n\n        Using  ``-> TypeGuard`` tells the static type checker that for a given\n        function:\n\n        1. The return value is a boolean.\n        2. If the return value is ``True``, the type of its argument\n        is the type inside ``TypeGuard``.\n\n        For example::\n\n            def is_str(val: Union[str, float]):\n                # \"isinstance\" type guard\n                if isinstance(val, str):\n                    # Type of ``val`` is narrowed to ``str``\n                    ...\n                else:\n                    # Else, type of ``val`` is narrowed to ``float``.\n                    ...\n\n        Strict type narrowing is not enforced -- ``TypeB`` need not be a narrower\n        form of ``TypeA`` (it can even be a wider form) and this may lead to\n        type-unsafe results.  The main reason is to allow for things like\n        narrowing ``List[object]`` to ``List[str]`` even though the latter is not\n        a subtype of the former, since ``List`` is invariant.  The responsibility of\n        writing type-safe type guards is left to the user.\n\n        ``TypeGuard`` also works with type variables.  For more information, see\n        PEP 647 (User-Defined Type Guards).\n        \"\"\"\n        item = typing._type_check(parameters, f'{self} accepts only a single type.')\n        return typing._GenericAlias(self, (item,))\n# 3.8\nelse:\n    class _TypeGuardForm(_ExtensionsSpecialForm, _root=True):\n        def __getitem__(self, parameters):\n            item = typing._type_check(parameters,\n                                      f'{self._name} accepts only a single type')\n            return typing._GenericAlias(self, (item,))\n\n    TypeGuard = _TypeGuardForm(\n        'TypeGuard',\n        doc=\"\"\"Special typing form used to annotate the return type of a user-defined\n        type guard function.  ``TypeGuard`` only accepts a single type argument.\n        At runtime, functions marked this way should return a boolean.\n\n        ``TypeGuard`` aims to benefit *type narrowing* -- a technique used by static\n        type checkers to determine a more precise type of an expression within a\n        program's code flow.  Usually type narrowing is done by analyzing\n        conditional code flow and applying the narrowing to a block of code.  The\n        conditional expression here is sometimes referred to as a \"type guard\".\n\n        Sometimes it would be convenient to use a user-defined boolean function\n        as a type guard.  Such a function should use ``TypeGuard[...]`` as its\n        return type to alert static type checkers to this intention.\n\n        Using  ``-> TypeGuard`` tells the static type checker that for a given\n        function:\n\n        1. The return value is a boolean.\n        2. If the return value is ``True``, the type of its argument\n        is the type inside ``TypeGuard``.\n\n        For example::\n\n            def is_str(val: Union[str, float]):\n                # \"isinstance\" type guard\n                if isinstance(val, str):\n                    # Type of ``val`` is narrowed to ``str``\n                    ...\n                else:\n                    # Else, type of ``val`` is narrowed to ``float``.\n                    ...\n\n        Strict type narrowing is not enforced -- ``TypeB`` need not be a narrower\n        form of ``TypeA`` (it can even be a wider form) and this may lead to\n        type-unsafe results.  The main reason is to allow for things like\n        narrowing ``List[object]`` to ``List[str]`` even though the latter is not\n        a subtype of the former, since ``List`` is invariant.  The responsibility of\n        writing type-safe type guards is left to the user.\n\n        ``TypeGuard`` also works with type variables.  For more information, see\n        PEP 647 (User-Defined Type Guards).\n        \"\"\")\n\n# 3.13+\nif hasattr(typing, 'TypeIs'):\n    TypeIs = typing.TypeIs\n# 3.9\nelif sys.version_info[:2] >= (3, 9):\n    @_ExtensionsSpecialForm\n    def TypeIs(self, parameters):\n        \"\"\"Special typing form used to annotate the return type of a user-defined\n        type narrower function.  ``TypeIs`` only accepts a single type argument.\n        At runtime, functions marked this way should return a boolean.\n\n        ``TypeIs`` aims to benefit *type narrowing* -- a technique used by static\n        type checkers to determine a more precise type of an expression within a\n        program's code flow.  Usually type narrowing is done by analyzing\n        conditional code flow and applying the narrowing to a block of code.  The\n        conditional expression here is sometimes referred to as a \"type guard\".\n\n        Sometimes it would be convenient to use a user-defined boolean function\n        as a type guard.  Such a function should use ``TypeIs[...]`` as its\n        return type to alert static type checkers to this intention.\n\n        Using  ``-> TypeIs`` tells the static type checker that for a given\n        function:\n\n        1. The return value is a boolean.\n        2. If the return value is ``True``, the type of its argument\n        is the intersection of the type inside ``TypeGuard`` and the argument's\n        previously known type.\n\n        For example::\n\n            def is_awaitable(val: object) -> TypeIs[Awaitable[Any]]:\n                return hasattr(val, '__await__')\n\n            def f(val: Union[int, Awaitable[int]]) -> int:\n                if is_awaitable(val):\n                    assert_type(val, Awaitable[int])\n                else:\n                    assert_type(val, int)\n\n        ``TypeIs`` also works with type variables.  For more information, see\n        PEP 742 (Narrowing types with TypeIs).\n        \"\"\"\n        item = typing._type_check(parameters, f'{self} accepts only a single type.')\n        return typing._GenericAlias(self, (item,))\n# 3.8\nelse:\n    class _TypeIsForm(_ExtensionsSpecialForm, _root=True):\n        def __getitem__(self, parameters):\n            item = typing._type_check(parameters,\n                                      f'{self._name} accepts only a single type')\n            return typing._GenericAlias(self, (item,))\n\n    TypeIs = _TypeIsForm(\n        'TypeIs',\n        doc=\"\"\"Special typing form used to annotate the return type of a user-defined\n        type narrower function.  ``TypeIs`` only accepts a single type argument.\n        At runtime, functions marked this way should return a boolean.\n\n        ``TypeIs`` aims to benefit *type narrowing* -- a technique used by static\n        type checkers to determine a more precise type of an expression within a\n        program's code flow.  Usually type narrowing is done by analyzing\n        conditional code flow and applying the narrowing to a block of code.  The\n        conditional expression here is sometimes referred to as a \"type guard\".\n\n        Sometimes it would be convenient to use a user-defined boolean function\n        as a type guard.  Such a function should use ``TypeIs[...]`` as its\n        return type to alert static type checkers to this intention.\n\n        Using  ``-> TypeIs`` tells the static type checker that for a given\n        function:\n\n        1. The return value is a boolean.\n        2. If the return value is ``True``, the type of its argument\n        is the intersection of the type inside ``TypeGuard`` and the argument's\n        previously known type.\n\n        For example::\n\n            def is_awaitable(val: object) -> TypeIs[Awaitable[Any]]:\n                return hasattr(val, '__await__')\n\n            def f(val: Union[int, Awaitable[int]]) -> int:\n                if is_awaitable(val):\n                    assert_type(val, Awaitable[int])\n                else:\n                    assert_type(val, int)\n\n        ``TypeIs`` also works with type variables.  For more information, see\n        PEP 742 (Narrowing types with TypeIs).\n        \"\"\")\n\n\n# Vendored from cpython typing._SpecialFrom\nclass _SpecialForm(typing._Final, _root=True):\n    __slots__ = ('_name', '__doc__', '_getitem')\n\n    def __init__(self, getitem):\n        self._getitem = getitem\n        self._name = getitem.__name__\n        self.__doc__ = getitem.__doc__\n\n    def __getattr__(self, item):\n        if item in {'__name__', '__qualname__'}:\n            return self._name\n\n        raise AttributeError(item)\n\n    def __mro_entries__(self, bases):\n        raise TypeError(f\"Cannot subclass {self!r}\")\n\n    def __repr__(self):\n        return f'typing_extensions.{self._name}'\n\n    def __reduce__(self):\n        return self._name\n\n    def __call__(self, *args, **kwds):\n        raise TypeError(f\"Cannot instantiate {self!r}\")\n\n    def __or__(self, other):\n        return typing.Union[self, other]\n\n    def __ror__(self, other):\n        return typing.Union[other, self]\n\n    def __instancecheck__(self, obj):\n        raise TypeError(f\"{self} cannot be used with isinstance()\")\n\n    def __subclasscheck__(self, cls):\n        raise TypeError(f\"{self} cannot be used with issubclass()\")\n\n    @typing._tp_cache\n    def __getitem__(self, parameters):\n        return self._getitem(self, parameters)\n\n\nif hasattr(typing, \"LiteralString\"):  # 3.11+\n    LiteralString = typing.LiteralString\nelse:\n    @_SpecialForm\n    def LiteralString(self, params):\n        \"\"\"Represents an arbitrary literal string.\n\n        Example::\n\n          from typing_extensions import LiteralString\n\n          def query(sql: LiteralString) -> ...:\n              ...\n\n          query(\"SELECT * FROM table\")  # ok\n          query(f\"SELECT * FROM {input()}\")  # not ok\n\n        See PEP 675 for details.\n\n        \"\"\"\n        raise TypeError(f\"{self} is not subscriptable\")\n\n\nif hasattr(typing, \"Self\"):  # 3.11+\n    Self = typing.Self\nelse:\n    @_SpecialForm\n    def Self(self, params):\n        \"\"\"Used to spell the type of \"self\" in classes.\n\n        Example::\n\n          from typing import Self\n\n          class ReturnsSelf:\n              def parse(self, data: bytes) -> Self:\n                  ...\n                  return self\n\n        \"\"\"\n\n        raise TypeError(f\"{self} is not subscriptable\")\n\n\nif hasattr(typing, \"Never\"):  # 3.11+\n    Never = typing.Never\nelse:\n    @_SpecialForm\n    def Never(self, params):\n        \"\"\"The bottom type, a type that has no members.\n\n        This can be used to define a function that should never be\n        called, or a function that never returns::\n\n            from typing_extensions import Never\n\n            def never_call_me(arg: Never) -> None:\n                pass\n\n            def int_or_str(arg: int | str) -> None:\n                never_call_me(arg)  # type checker error\n                match arg:\n                    case int():\n                        print(\"It's an int\")\n                    case str():\n                        print(\"It's a str\")\n                    case _:\n                        never_call_me(arg)  # ok, arg is of type Never\n\n        \"\"\"\n\n        raise TypeError(f\"{self} is not subscriptable\")\n\n\nif hasattr(typing, 'Required'):  # 3.11+\n    Required = typing.Required\n    NotRequired = typing.NotRequired\nelif sys.version_info[:2] >= (3, 9):  # 3.9-3.10\n    @_ExtensionsSpecialForm\n    def Required(self, parameters):\n        \"\"\"A special typing construct to mark a key of a total=False TypedDict\n        as required. For example:\n\n            class Movie(TypedDict, total=False):\n                title: Required[str]\n                year: int\n\n            m = Movie(\n                title='The Matrix',  # typechecker error if key is omitted\n                year=1999,\n            )\n\n        There is no runtime checking that a required key is actually provided\n        when instantiating a related TypedDict.\n        \"\"\"\n        item = typing._type_check(parameters, f'{self._name} accepts only a single type.')\n        return typing._GenericAlias(self, (item,))\n\n    @_ExtensionsSpecialForm\n    def NotRequired(self, parameters):\n        \"\"\"A special typing construct to mark a key of a TypedDict as\n        potentially missing. For example:\n\n            class Movie(TypedDict):\n                title: str\n                year: NotRequired[int]\n\n            m = Movie(\n                title='The Matrix',  # typechecker error if key is omitted\n                year=1999,\n            )\n        \"\"\"\n        item = typing._type_check(parameters, f'{self._name} accepts only a single type.')\n        return typing._GenericAlias(self, (item,))\n\nelse:  # 3.8\n    class _RequiredForm(_ExtensionsSpecialForm, _root=True):\n        def __getitem__(self, parameters):\n            item = typing._type_check(parameters,\n                                      f'{self._name} accepts only a single type.')\n            return typing._GenericAlias(self, (item,))\n\n    Required = _RequiredForm(\n        'Required',\n        doc=\"\"\"A special typing construct to mark a key of a total=False TypedDict\n        as required. For example:\n\n            class Movie(TypedDict, total=False):\n                title: Required[str]\n                year: int\n\n            m = Movie(\n                title='The Matrix',  # typechecker error if key is omitted\n                year=1999,\n            )\n\n        There is no runtime checking that a required key is actually provided\n        when instantiating a related TypedDict.\n        \"\"\")\n    NotRequired = _RequiredForm(\n        'NotRequired',\n        doc=\"\"\"A special typing construct to mark a key of a TypedDict as\n        potentially missing. For example:\n\n            class Movie(TypedDict):\n                title: str\n                year: NotRequired[int]\n\n            m = Movie(\n                title='The Matrix',  # typechecker error if key is omitted\n                year=1999,\n            )\n        \"\"\")\n\n\nif hasattr(typing, 'ReadOnly'):\n    ReadOnly = typing.ReadOnly\nelif sys.version_info[:2] >= (3, 9):  # 3.9-3.12\n    @_ExtensionsSpecialForm\n    def ReadOnly(self, parameters):\n        \"\"\"A special typing construct to mark an item of a TypedDict as read-only.\n\n        For example:\n\n            class Movie(TypedDict):\n                title: ReadOnly[str]\n                year: int\n\n            def mutate_movie(m: Movie) -> None:\n                m[\"year\"] = 1992  # allowed\n                m[\"title\"] = \"The Matrix\"  # typechecker error\n\n        There is no runtime checking for this property.\n        \"\"\"\n        item = typing._type_check(parameters, f'{self._name} accepts only a single type.')\n        return typing._GenericAlias(self, (item,))\n\nelse:  # 3.8\n    class _ReadOnlyForm(_ExtensionsSpecialForm, _root=True):\n        def __getitem__(self, parameters):\n            item = typing._type_check(parameters,\n                                      f'{self._name} accepts only a single type.')\n            return typing._GenericAlias(self, (item,))\n\n    ReadOnly = _ReadOnlyForm(\n        'ReadOnly',\n        doc=\"\"\"A special typing construct to mark a key of a TypedDict as read-only.\n\n        For example:\n\n            class Movie(TypedDict):\n                title: ReadOnly[str]\n                year: int\n\n            def mutate_movie(m: Movie) -> None:\n                m[\"year\"] = 1992  # allowed\n                m[\"title\"] = \"The Matrix\"  # typechecker error\n\n        There is no runtime checking for this propery.\n        \"\"\")\n\n\n_UNPACK_DOC = \"\"\"\\\nType unpack operator.\n\nThe type unpack operator takes the child types from some container type,\nsuch as `tuple[int, str]` or a `TypeVarTuple`, and 'pulls them out'. For\nexample:\n\n  # For some generic class `Foo`:\n  Foo[Unpack[tuple[int, str]]]  # Equivalent to Foo[int, str]\n\n  Ts = TypeVarTuple('Ts')\n  # Specifies that `Bar` is generic in an arbitrary number of types.\n  # (Think of `Ts` as a tuple of an arbitrary number of individual\n  #  `TypeVar`s, which the `Unpack` is 'pulling out' directly into the\n  #  `Generic[]`.)\n  class Bar(Generic[Unpack[Ts]]): ...\n  Bar[int]  # Valid\n  Bar[int, str]  # Also valid\n\nFrom Python 3.11, this can also be done using the `*` operator:\n\n    Foo[*tuple[int, str]]\n    class Bar(Generic[*Ts]): ...\n\nThe operator can also be used along with a `TypedDict` to annotate\n`**kwargs` in a function signature. For instance:\n\n  class Movie(TypedDict):\n    name: str\n    year: int\n\n  # This function expects two keyword arguments - *name* of type `str` and\n  # *year* of type `int`.\n  def foo(**kwargs: Unpack[Movie]): ...\n\nNote that there is only some runtime checking of this operator. Not\neverything the runtime allows may be accepted by static type checkers.\n\nFor more information, see PEP 646 and PEP 692.\n\"\"\"\n\n\nif sys.version_info >= (3, 12):  # PEP 692 changed the repr of Unpack[]\n    Unpack = typing.Unpack\n\n    def _is_unpack(obj):\n        return get_origin(obj) is Unpack\n\nelif sys.version_info[:2] >= (3, 9):  # 3.9+\n    class _UnpackSpecialForm(_ExtensionsSpecialForm, _root=True):\n        def __init__(self, getitem):\n            super().__init__(getitem)\n            self.__doc__ = _UNPACK_DOC\n\n    class _UnpackAlias(typing._GenericAlias, _root=True):\n        __class__ = typing.TypeVar\n\n        @property\n        def __typing_unpacked_tuple_args__(self):\n            assert self.__origin__ is Unpack\n            assert len(self.__args__) == 1\n            arg, = self.__args__\n            if isinstance(arg, (typing._GenericAlias, _types.GenericAlias)):\n                if arg.__origin__ is not tuple:\n                    raise TypeError(\"Unpack[...] must be used with a tuple type\")\n                return arg.__args__\n            return None\n\n    @_UnpackSpecialForm\n    def Unpack(self, parameters):\n        item = typing._type_check(parameters, f'{self._name} accepts only a single type.')\n        return _UnpackAlias(self, (item,))\n\n    def _is_unpack(obj):\n        return isinstance(obj, _UnpackAlias)\n\nelse:  # 3.8\n    class _UnpackAlias(typing._GenericAlias, _root=True):\n        __class__ = typing.TypeVar\n\n    class _UnpackForm(_ExtensionsSpecialForm, _root=True):\n        def __getitem__(self, parameters):\n            item = typing._type_check(parameters,\n                                      f'{self._name} accepts only a single type.')\n            return _UnpackAlias(self, (item,))\n\n    Unpack = _UnpackForm('Unpack', doc=_UNPACK_DOC)\n\n    def _is_unpack(obj):\n        return isinstance(obj, _UnpackAlias)\n\n\nif _PEP_696_IMPLEMENTED:\n    from typing import TypeVarTuple\n\nelif hasattr(typing, \"TypeVarTuple\"):  # 3.11+\n\n    def _unpack_args(*args):\n        newargs = []\n        for arg in args:\n            subargs = getattr(arg, '__typing_unpacked_tuple_args__', None)\n            if subargs is not None and not (subargs and subargs[-1] is ...):\n                newargs.extend(subargs)\n            else:\n                newargs.append(arg)\n        return newargs\n\n    # Add default parameter - PEP 696\n    class TypeVarTuple(metaclass=_TypeVarLikeMeta):\n        \"\"\"Type variable tuple.\"\"\"\n\n        _backported_typevarlike = typing.TypeVarTuple\n\n        def __new__(cls, name, *, default=NoDefault):\n            tvt = typing.TypeVarTuple(name)\n            _set_default(tvt, default)\n            _set_module(tvt)\n\n            def _typevartuple_prepare_subst(alias, args):\n                params = alias.__parameters__\n                typevartuple_index = params.index(tvt)\n                for param in params[typevartuple_index + 1:]:\n                    if isinstance(param, TypeVarTuple):\n                        raise TypeError(\n                            f\"More than one TypeVarTuple parameter in {alias}\"\n                        )\n\n                alen = len(args)\n                plen = len(params)\n                left = typevartuple_index\n                right = plen - typevartuple_index - 1\n                var_tuple_index = None\n                fillarg = None\n                for k, arg in enumerate(args):\n                    if not isinstance(arg, type):\n                        subargs = getattr(arg, '__typing_unpacked_tuple_args__', None)\n                        if subargs and len(subargs) == 2 and subargs[-1] is ...:\n                            if var_tuple_index is not None:\n                                raise TypeError(\n                                    \"More than one unpacked \"\n                                    \"arbitrary-length tuple argument\"\n                                )\n                            var_tuple_index = k\n                            fillarg = subargs[0]\n                if var_tuple_index is not None:\n                    left = min(left, var_tuple_index)\n                    right = min(right, alen - var_tuple_index - 1)\n                elif left + right > alen:\n                    raise TypeError(f\"Too few arguments for {alias};\"\n                                    f\" actual {alen}, expected at least {plen - 1}\")\n                if left == alen - right and tvt.has_default():\n                    replacement = _unpack_args(tvt.__default__)\n                else:\n                    replacement = args[left: alen - right]\n\n                return (\n                    *args[:left],\n                    *([fillarg] * (typevartuple_index - left)),\n                    replacement,\n                    *([fillarg] * (plen - right - left - typevartuple_index - 1)),\n                    *args[alen - right:],\n                )\n\n            tvt.__typing_prepare_subst__ = _typevartuple_prepare_subst\n            return tvt\n\n        def __init_subclass__(self, *args, **kwds):\n            raise TypeError(\"Cannot subclass special typing classes\")\n\nelse:  # <=3.10\n    class TypeVarTuple(_DefaultMixin):\n        \"\"\"Type variable tuple.\n\n        Usage::\n\n            Ts = TypeVarTuple('Ts')\n\n        In the same way that a normal type variable is a stand-in for a single\n        type such as ``int``, a type variable *tuple* is a stand-in for a *tuple*\n        type such as ``Tuple[int, str]``.\n\n        Type variable tuples can be used in ``Generic`` declarations.\n        Consider the following example::\n\n            class Array(Generic[*Ts]): ...\n\n        The ``Ts`` type variable tuple here behaves like ``tuple[T1, T2]``,\n        where ``T1`` and ``T2`` are type variables. To use these type variables\n        as type parameters of ``Array``, we must *unpack* the type variable tuple using\n        the star operator: ``*Ts``. The signature of ``Array`` then behaves\n        as if we had simply written ``class Array(Generic[T1, T2]): ...``.\n        In contrast to ``Generic[T1, T2]``, however, ``Generic[*Shape]`` allows\n        us to parameterise the class with an *arbitrary* number of type parameters.\n\n        Type variable tuples can be used anywhere a normal ``TypeVar`` can.\n        This includes class definitions, as shown above, as well as function\n        signatures and variable annotations::\n\n            class Array(Generic[*Ts]):\n\n                def __init__(self, shape: Tuple[*Ts]):\n                    self._shape: Tuple[*Ts] = shape\n\n                def get_shape(self) -> Tuple[*Ts]:\n                    return self._shape\n\n            shape = (Height(480), Width(640))\n            x: Array[Height, Width] = Array(shape)\n            y = abs(x)  # Inferred type is Array[Height, Width]\n            z = x + x   #        ...    is Array[Height, Width]\n            x.get_shape()  #     ...    is tuple[Height, Width]\n\n        \"\"\"\n\n        # Trick Generic __parameters__.\n        __class__ = typing.TypeVar\n\n        def __iter__(self):\n            yield self.__unpacked__\n\n        def __init__(self, name, *, default=NoDefault):\n            self.__name__ = name\n            _DefaultMixin.__init__(self, default)\n\n            # for pickling:\n            def_mod = _caller()\n            if def_mod != 'typing_extensions':\n                self.__module__ = def_mod\n\n            self.__unpacked__ = Unpack[self]\n\n        def __repr__(self):\n            return self.__name__\n\n        def __hash__(self):\n            return object.__hash__(self)\n\n        def __eq__(self, other):\n            return self is other\n\n        def __reduce__(self):\n            return self.__name__\n\n        def __init_subclass__(self, *args, **kwds):\n            if '_root' not in kwds:\n                raise TypeError(\"Cannot subclass special typing classes\")\n\n\nif hasattr(typing, \"reveal_type\"):  # 3.11+\n    reveal_type = typing.reveal_type\nelse:  # <=3.10\n    def reveal_type(obj: T, /) -> T:\n        \"\"\"Reveal the inferred type of a variable.\n\n        When a static type checker encounters a call to ``reveal_type()``,\n        it will emit the inferred type of the argument::\n\n            x: int = 1\n            reveal_type(x)\n\n        Running a static type checker (e.g., ``mypy``) on this example\n        will produce output similar to 'Revealed type is \"builtins.int\"'.\n\n        At runtime, the function prints the runtime type of the\n        argument and returns it unchanged.\n\n        \"\"\"\n        print(f\"Runtime type is {type(obj).__name__!r}\", file=sys.stderr)\n        return obj\n\n\nif hasattr(typing, \"_ASSERT_NEVER_REPR_MAX_LENGTH\"):  # 3.11+\n    _ASSERT_NEVER_REPR_MAX_LENGTH = typing._ASSERT_NEVER_REPR_MAX_LENGTH\nelse:  # <=3.10\n    _ASSERT_NEVER_REPR_MAX_LENGTH = 100\n\n\nif hasattr(typing, \"assert_never\"):  # 3.11+\n    assert_never = typing.assert_never\nelse:  # <=3.10\n    def assert_never(arg: Never, /) -> Never:\n        \"\"\"Assert to the type checker that a line of code is unreachable.\n\n        Example::\n\n            def int_or_str(arg: int | str) -> None:\n                match arg:\n                    case int():\n                        print(\"It's an int\")\n                    case str():\n                        print(\"It's a str\")\n                    case _:\n                        assert_never(arg)\n\n        If a type checker finds that a call to assert_never() is\n        reachable, it will emit an error.\n\n        At runtime, this throws an exception when called.\n\n        \"\"\"\n        value = repr(arg)\n        if len(value) > _ASSERT_NEVER_REPR_MAX_LENGTH:\n            value = value[:_ASSERT_NEVER_REPR_MAX_LENGTH] + '...'\n        raise AssertionError(f\"Expected code to be unreachable, but got: {value}\")\n\n\nif sys.version_info >= (3, 12):  # 3.12+\n    # dataclass_transform exists in 3.11 but lacks the frozen_default parameter\n    dataclass_transform = typing.dataclass_transform\nelse:  # <=3.11\n    def dataclass_transform(\n        *,\n        eq_default: bool = True,\n        order_default: bool = False,\n        kw_only_default: bool = False,\n        frozen_default: bool = False,\n        field_specifiers: typing.Tuple[\n            typing.Union[typing.Type[typing.Any], typing.Callable[..., typing.Any]],\n            ...\n        ] = (),\n        **kwargs: typing.Any,\n    ) -> typing.Callable[[T], T]:\n        \"\"\"Decorator that marks a function, class, or metaclass as providing\n        dataclass-like behavior.\n\n        Example:\n\n            from typing_extensions import dataclass_transform\n\n            _T = TypeVar(\"_T\")\n\n            # Used on a decorator function\n            @dataclass_transform()\n            def create_model(cls: type[_T]) -> type[_T]:\n                ...\n                return cls\n\n            @create_model\n            class CustomerModel:\n                id: int\n                name: str\n\n            # Used on a base class\n            @dataclass_transform()\n            class ModelBase: ...\n\n            class CustomerModel(ModelBase):\n                id: int\n                name: str\n\n            # Used on a metaclass\n            @dataclass_transform()\n            class ModelMeta(type): ...\n\n            class ModelBase(metaclass=ModelMeta): ...\n\n            class CustomerModel(ModelBase):\n                id: int\n                name: str\n\n        Each of the ``CustomerModel`` classes defined in this example will now\n        behave similarly to a dataclass created with the ``@dataclasses.dataclass``\n        decorator. For example, the type checker will synthesize an ``__init__``\n        method.\n\n        The arguments to this decorator can be used to customize this behavior:\n        - ``eq_default`` indicates whether the ``eq`` parameter is assumed to be\n          True or False if it is omitted by the caller.\n        - ``order_default`` indicates whether the ``order`` parameter is\n          assumed to be True or False if it is omitted by the caller.\n        - ``kw_only_default`` indicates whether the ``kw_only`` parameter is\n          assumed to be True or False if it is omitted by the caller.\n        - ``frozen_default`` indicates whether the ``frozen`` parameter is\n          assumed to be True or False if it is omitted by the caller.\n        - ``field_specifiers`` specifies a static list of supported classes\n          or functions that describe fields, similar to ``dataclasses.field()``.\n\n        At runtime, this decorator records its arguments in the\n        ``__dataclass_transform__`` attribute on the decorated object.\n\n        See PEP 681 for details.\n\n        \"\"\"\n        def decorator(cls_or_fn):\n            cls_or_fn.__dataclass_transform__ = {\n                \"eq_default\": eq_default,\n                \"order_default\": order_default,\n                \"kw_only_default\": kw_only_default,\n                \"frozen_default\": frozen_default,\n                \"field_specifiers\": field_specifiers,\n                \"kwargs\": kwargs,\n            }\n            return cls_or_fn\n        return decorator\n\n\nif hasattr(typing, \"override\"):  # 3.12+\n    override = typing.override\nelse:  # <=3.11\n    _F = typing.TypeVar(\"_F\", bound=typing.Callable[..., typing.Any])\n\n    def override(arg: _F, /) -> _F:\n        \"\"\"Indicate that a method is intended to override a method in a base class.\n\n        Usage:\n\n            class Base:\n                def method(self) -> None:\n                    pass\n\n            class Child(Base):\n                @override\n                def method(self) -> None:\n                    super().method()\n\n        When this decorator is applied to a method, the type checker will\n        validate that it overrides a method with the same name on a base class.\n        This helps prevent bugs that may occur when a base class is changed\n        without an equivalent change to a child class.\n\n        There is no runtime checking of these properties. The decorator\n        sets the ``__override__`` attribute to ``True`` on the decorated object\n        to allow runtime introspection.\n\n        See PEP 698 for details.\n\n        \"\"\"\n        try:\n            arg.__override__ = True\n        except (AttributeError, TypeError):\n            # Skip the attribute silently if it is not writable.\n            # AttributeError happens if the object has __slots__ or a\n            # read-only property, TypeError if it's a builtin class.\n            pass\n        return arg\n\n\nif hasattr(warnings, \"deprecated\"):\n    deprecated = warnings.deprecated\nelse:\n    _T = typing.TypeVar(\"_T\")\n\n    class deprecated:\n        \"\"\"Indicate that a class, function or overload is deprecated.\n\n        When this decorator is applied to an object, the type checker\n        will generate a diagnostic on usage of the deprecated object.\n\n        Usage:\n\n            @deprecated(\"Use B instead\")\n            class A:\n                pass\n\n            @deprecated(\"Use g instead\")\n            def f():\n                pass\n\n            @overload\n            @deprecated(\"int support is deprecated\")\n            def g(x: int) -> int: ...\n            @overload\n            def g(x: str) -> int: ...\n\n        The warning specified by *category* will be emitted at runtime\n        on use of deprecated objects. For functions, that happens on calls;\n        for classes, on instantiation and on creation of subclasses.\n        If the *category* is ``None``, no warning is emitted at runtime.\n        The *stacklevel* determines where the\n        warning is emitted. If it is ``1`` (the default), the warning\n        is emitted at the direct caller of the deprecated object; if it\n        is higher, it is emitted further up the stack.\n        Static type checker behavior is not affected by the *category*\n        and *stacklevel* arguments.\n\n        The deprecation message passed to the decorator is saved in the\n        ``__deprecated__`` attribute on the decorated object.\n        If applied to an overload, the decorator\n        must be after the ``@overload`` decorator for the attribute to\n        exist on the overload as returned by ``get_overloads()``.\n\n        See PEP 702 for details.\n\n        \"\"\"\n        def __init__(\n            self,\n            message: str,\n            /,\n            *,\n            category: typing.Optional[typing.Type[Warning]] = DeprecationWarning,\n            stacklevel: int = 1,\n        ) -> None:\n            if not isinstance(message, str):\n                raise TypeError(\n                    \"Expected an object of type str for 'message', not \"\n                    f\"{type(message).__name__!r}\"\n                )\n            self.message = message\n            self.category = category\n            self.stacklevel = stacklevel\n\n        def __call__(self, arg: _T, /) -> _T:\n            # Make sure the inner functions created below don't\n            # retain a reference to self.\n            msg = self.message\n            category = self.category\n            stacklevel = self.stacklevel\n            if category is None:\n                arg.__deprecated__ = msg\n                return arg\n            elif isinstance(arg, type):\n                import functools\n                from types import MethodType\n\n                original_new = arg.__new__\n\n                @functools.wraps(original_new)\n                def __new__(cls, *args, **kwargs):\n                    if cls is arg:\n                        warnings.warn(msg, category=category, stacklevel=stacklevel + 1)\n                    if original_new is not object.__new__:\n                        return original_new(cls, *args, **kwargs)\n                    # Mirrors a similar check in object.__new__.\n                    elif cls.__init__ is object.__init__ and (args or kwargs):\n                        raise TypeError(f\"{cls.__name__}() takes no arguments\")\n                    else:\n                        return original_new(cls)\n\n                arg.__new__ = staticmethod(__new__)\n\n                original_init_subclass = arg.__init_subclass__\n                # We need slightly different behavior if __init_subclass__\n                # is a bound method (likely if it was implemented in Python)\n                if isinstance(original_init_subclass, MethodType):\n                    original_init_subclass = original_init_subclass.__func__\n\n                    @functools.wraps(original_init_subclass)\n                    def __init_subclass__(*args, **kwargs):\n                        warnings.warn(msg, category=category, stacklevel=stacklevel + 1)\n                        return original_init_subclass(*args, **kwargs)\n\n                    arg.__init_subclass__ = classmethod(__init_subclass__)\n                # Or otherwise, which likely means it's a builtin such as\n                # object's implementation of __init_subclass__.\n                else:\n                    @functools.wraps(original_init_subclass)\n                    def __init_subclass__(*args, **kwargs):\n                        warnings.warn(msg, category=category, stacklevel=stacklevel + 1)\n                        return original_init_subclass(*args, **kwargs)\n\n                    arg.__init_subclass__ = __init_subclass__\n\n                arg.__deprecated__ = __new__.__deprecated__ = msg\n                __init_subclass__.__deprecated__ = msg\n                return arg\n            elif callable(arg):\n                import functools\n\n                @functools.wraps(arg)\n                def wrapper(*args, **kwargs):\n                    warnings.warn(msg, category=category, stacklevel=stacklevel + 1)\n                    return arg(*args, **kwargs)\n\n                arg.__deprecated__ = wrapper.__deprecated__ = msg\n                return wrapper\n            else:\n                raise TypeError(\n                    \"@deprecated decorator with non-None category must be applied to \"\n                    f\"a class or callable, not {arg!r}\"\n                )\n\n\n# We have to do some monkey patching to deal with the dual nature of\n# Unpack/TypeVarTuple:\n# - We want Unpack to be a kind of TypeVar so it gets accepted in\n#   Generic[Unpack[Ts]]\n# - We want it to *not* be treated as a TypeVar for the purposes of\n#   counting generic parameters, so that when we subscript a generic,\n#   the runtime doesn't try to substitute the Unpack with the subscripted type.\nif not hasattr(typing, \"TypeVarTuple\"):\n    def _check_generic(cls, parameters, elen=_marker):\n        \"\"\"Check correct count for parameters of a generic cls (internal helper).\n\n        This gives a nice error message in case of count mismatch.\n        \"\"\"\n        if not elen:\n            raise TypeError(f\"{cls} is not a generic class\")\n        if elen is _marker:\n            if not hasattr(cls, \"__parameters__\") or not cls.__parameters__:\n                raise TypeError(f\"{cls} is not a generic class\")\n            elen = len(cls.__parameters__)\n        alen = len(parameters)\n        if alen != elen:\n            expect_val = elen\n            if hasattr(cls, \"__parameters__\"):\n                parameters = [p for p in cls.__parameters__ if not _is_unpack(p)]\n                num_tv_tuples = sum(isinstance(p, TypeVarTuple) for p in parameters)\n                if (num_tv_tuples > 0) and (alen >= elen - num_tv_tuples):\n                    return\n\n                # deal with TypeVarLike defaults\n                # required TypeVarLikes cannot appear after a defaulted one.\n                if alen < elen:\n                    # since we validate TypeVarLike default in _collect_type_vars\n                    # or _collect_parameters we can safely check parameters[alen]\n                    if (\n                        getattr(parameters[alen], '__default__', NoDefault)\n                        is not NoDefault\n                    ):\n                        return\n\n                    num_default_tv = sum(getattr(p, '__default__', NoDefault)\n                                         is not NoDefault for p in parameters)\n\n                    elen -= num_default_tv\n\n                    expect_val = f\"at least {elen}\"\n\n            things = \"arguments\" if sys.version_info >= (3, 10) else \"parameters\"\n            raise TypeError(f\"Too {'many' if alen > elen else 'few'} {things}\"\n                            f\" for {cls}; actual {alen}, expected {expect_val}\")\nelse:\n    # Python 3.11+\n\n    def _check_generic(cls, parameters, elen):\n        \"\"\"Check correct count for parameters of a generic cls (internal helper).\n\n        This gives a nice error message in case of count mismatch.\n        \"\"\"\n        if not elen:\n            raise TypeError(f\"{cls} is not a generic class\")\n        alen = len(parameters)\n        if alen != elen:\n            expect_val = elen\n            if hasattr(cls, \"__parameters__\"):\n                parameters = [p for p in cls.__parameters__ if not _is_unpack(p)]\n\n                # deal with TypeVarLike defaults\n                # required TypeVarLikes cannot appear after a defaulted one.\n                if alen < elen:\n                    # since we validate TypeVarLike default in _collect_type_vars\n                    # or _collect_parameters we can safely check parameters[alen]\n                    if (\n                        getattr(parameters[alen], '__default__', NoDefault)\n                        is not NoDefault\n                    ):\n                        return\n\n                    num_default_tv = sum(getattr(p, '__default__', NoDefault)\n                                         is not NoDefault for p in parameters)\n\n                    elen -= num_default_tv\n\n                    expect_val = f\"at least {elen}\"\n\n            raise TypeError(f\"Too {'many' if alen > elen else 'few'} arguments\"\n                            f\" for {cls}; actual {alen}, expected {expect_val}\")\n\nif not _PEP_696_IMPLEMENTED:\n    typing._check_generic = _check_generic\n\n\ndef _has_generic_or_protocol_as_origin() -> bool:\n    try:\n        frame = sys._getframe(2)\n    # - Catch AttributeError: not all Python implementations have sys._getframe()\n    # - Catch ValueError: maybe we're called from an unexpected module\n    #   and the call stack isn't deep enough\n    except (AttributeError, ValueError):\n        return False  # err on the side of leniency\n    else:\n        # If we somehow get invoked from outside typing.py,\n        # also err on the side of leniency\n        if frame.f_globals.get(\"__name__\") != \"typing\":\n            return False\n        origin = frame.f_locals.get(\"origin\")\n        # Cannot use \"in\" because origin may be an object with a buggy __eq__ that\n        # throws an error.\n        return origin is typing.Generic or origin is Protocol or origin is typing.Protocol\n\n\n_TYPEVARTUPLE_TYPES = {TypeVarTuple, getattr(typing, \"TypeVarTuple\", None)}\n\n\ndef _is_unpacked_typevartuple(x) -> bool:\n    if get_origin(x) is not Unpack:\n        return False\n    args = get_args(x)\n    return (\n        bool(args)\n        and len(args) == 1\n        and type(args[0]) in _TYPEVARTUPLE_TYPES\n    )\n\n\n# Python 3.11+ _collect_type_vars was renamed to _collect_parameters\nif hasattr(typing, '_collect_type_vars'):\n    def _collect_type_vars(types, typevar_types=None):\n        \"\"\"Collect all type variable contained in types in order of\n        first appearance (lexicographic order). For example::\n\n            _collect_type_vars((T, List[S, T])) == (T, S)\n        \"\"\"\n        if typevar_types is None:\n            typevar_types = typing.TypeVar\n        tvars = []\n\n        # A required TypeVarLike cannot appear after a TypeVarLike with a default\n        # if it was a direct call to `Generic[]` or `Protocol[]`\n        enforce_default_ordering = _has_generic_or_protocol_as_origin()\n        default_encountered = False\n\n        # Also, a TypeVarLike with a default cannot appear after a TypeVarTuple\n        type_var_tuple_encountered = False\n\n        for t in types:\n            if _is_unpacked_typevartuple(t):\n                type_var_tuple_encountered = True\n            elif isinstance(t, typevar_types) and t not in tvars:\n                if enforce_default_ordering:\n                    has_default = getattr(t, '__default__', NoDefault) is not NoDefault\n                    if has_default:\n                        if type_var_tuple_encountered:\n                            raise TypeError('Type parameter with a default'\n                                            ' follows TypeVarTuple')\n                        default_encountered = True\n                    elif default_encountered:\n                        raise TypeError(f'Type parameter {t!r} without a default'\n                                        ' follows type parameter with a default')\n\n                tvars.append(t)\n            if _should_collect_from_parameters(t):\n                tvars.extend([t for t in t.__parameters__ if t not in tvars])\n        return tuple(tvars)\n\n    typing._collect_type_vars = _collect_type_vars\nelse:\n    def _collect_parameters(args):\n        \"\"\"Collect all type variables and parameter specifications in args\n        in order of first appearance (lexicographic order).\n\n        For example::\n\n            assert _collect_parameters((T, Callable[P, T])) == (T, P)\n        \"\"\"\n        parameters = []\n\n        # A required TypeVarLike cannot appear after a TypeVarLike with default\n        # if it was a direct call to `Generic[]` or `Protocol[]`\n        enforce_default_ordering = _has_generic_or_protocol_as_origin()\n        default_encountered = False\n\n        # Also, a TypeVarLike with a default cannot appear after a TypeVarTuple\n        type_var_tuple_encountered = False\n\n        for t in args:\n            if isinstance(t, type):\n                # We don't want __parameters__ descriptor of a bare Python class.\n                pass\n            elif isinstance(t, tuple):\n                # `t` might be a tuple, when `ParamSpec` is substituted with\n                # `[T, int]`, or `[int, *Ts]`, etc.\n                for x in t:\n                    for collected in _collect_parameters([x]):\n                        if collected not in parameters:\n                            parameters.append(collected)\n            elif hasattr(t, '__typing_subst__'):\n                if t not in parameters:\n                    if enforce_default_ordering:\n                        has_default = (\n                            getattr(t, '__default__', NoDefault) is not NoDefault\n                        )\n\n                        if type_var_tuple_encountered and has_default:\n                            raise TypeError('Type parameter with a default'\n                                            ' follows TypeVarTuple')\n\n                        if has_default:\n                            default_encountered = True\n                        elif default_encountered:\n                            raise TypeError(f'Type parameter {t!r} without a default'\n                                            ' follows type parameter with a default')\n\n                    parameters.append(t)\n            else:\n                if _is_unpacked_typevartuple(t):\n                    type_var_tuple_encountered = True\n                for x in getattr(t, '__parameters__', ()):\n                    if x not in parameters:\n                        parameters.append(x)\n\n        return tuple(parameters)\n\n    if not _PEP_696_IMPLEMENTED:\n        typing._collect_parameters = _collect_parameters\n\n# Backport typing.NamedTuple as it exists in Python 3.13.\n# In 3.11, the ability to define generic `NamedTuple`s was supported.\n# This was explicitly disallowed in 3.9-3.10, and only half-worked in <=3.8.\n# On 3.12, we added __orig_bases__ to call-based NamedTuples\n# On 3.13, we deprecated kwargs-based NamedTuples\nif sys.version_info >= (3, 13):\n    NamedTuple = typing.NamedTuple\nelse:\n    def _make_nmtuple(name, types, module, defaults=()):\n        fields = [n for n, t in types]\n        annotations = {n: typing._type_check(t, f\"field {n} annotation must be a type\")\n                       for n, t in types}\n        nm_tpl = collections.namedtuple(name, fields,\n                                        defaults=defaults, module=module)\n        nm_tpl.__annotations__ = nm_tpl.__new__.__annotations__ = annotations\n        # The `_field_types` attribute was removed in 3.9;\n        # in earlier versions, it is the same as the `__annotations__` attribute\n        if sys.version_info < (3, 9):\n            nm_tpl._field_types = annotations\n        return nm_tpl\n\n    _prohibited_namedtuple_fields = typing._prohibited\n    _special_namedtuple_fields = frozenset({'__module__', '__name__', '__annotations__'})\n\n    class _NamedTupleMeta(type):\n        def __new__(cls, typename, bases, ns):\n            assert _NamedTuple in bases\n            for base in bases:\n                if base is not _NamedTuple and base is not typing.Generic:\n                    raise TypeError(\n                        'can only inherit from a NamedTuple type and Generic')\n            bases = tuple(tuple if base is _NamedTuple else base for base in bases)\n            if \"__annotations__\" in ns:\n                types = ns[\"__annotations__\"]\n            elif \"__annotate__\" in ns:\n                # TODO: Use inspect.VALUE here, and make the annotations lazily evaluated\n                types = ns[\"__annotate__\"](1)\n            else:\n                types = {}\n            default_names = []\n            for field_name in types:\n                if field_name in ns:\n                    default_names.append(field_name)\n                elif default_names:\n                    raise TypeError(f\"Non-default namedtuple field {field_name} \"\n                                    f\"cannot follow default field\"\n                                    f\"{'s' if len(default_names) > 1 else ''} \"\n                                    f\"{', '.join(default_names)}\")\n            nm_tpl = _make_nmtuple(\n                typename, types.items(),\n                defaults=[ns[n] for n in default_names],\n                module=ns['__module__']\n            )\n            nm_tpl.__bases__ = bases\n            if typing.Generic in bases:\n                if hasattr(typing, '_generic_class_getitem'):  # 3.12+\n                    nm_tpl.__class_getitem__ = classmethod(typing._generic_class_getitem)\n                else:\n                    class_getitem = typing.Generic.__class_getitem__.__func__\n                    nm_tpl.__class_getitem__ = classmethod(class_getitem)\n            # update from user namespace without overriding special namedtuple attributes\n            for key, val in ns.items():\n                if key in _prohibited_namedtuple_fields:\n                    raise AttributeError(\"Cannot overwrite NamedTuple attribute \" + key)\n                elif key not in _special_namedtuple_fields:\n                    if key not in nm_tpl._fields:\n                        setattr(nm_tpl, key, ns[key])\n                    try:\n                        set_name = type(val).__set_name__\n                    except AttributeError:\n                        pass\n                    else:\n                        try:\n                            set_name(val, nm_tpl, key)\n                        except BaseException as e:\n                            msg = (\n                                f\"Error calling __set_name__ on {type(val).__name__!r} \"\n                                f\"instance {key!r} in {typename!r}\"\n                            )\n                            # BaseException.add_note() existed on py311,\n                            # but the __set_name__ machinery didn't start\n                            # using add_note() until py312.\n                            # Making sure exceptions are raised in the same way\n                            # as in \"normal\" classes seems most important here.\n                            if sys.version_info >= (3, 12):\n                                e.add_note(msg)\n                                raise\n                            else:\n                                raise RuntimeError(msg) from e\n\n            if typing.Generic in bases:\n                nm_tpl.__init_subclass__()\n            return nm_tpl\n\n    _NamedTuple = type.__new__(_NamedTupleMeta, 'NamedTuple', (), {})\n\n    def _namedtuple_mro_entries(bases):\n        assert NamedTuple in bases\n        return (_NamedTuple,)\n\n    @_ensure_subclassable(_namedtuple_mro_entries)\n    def NamedTuple(typename, fields=_marker, /, **kwargs):\n        \"\"\"Typed version of namedtuple.\n\n        Usage::\n\n            class Employee(NamedTuple):\n                name: str\n                id: int\n\n        This is equivalent to::\n\n            Employee = collections.namedtuple('Employee', ['name', 'id'])\n\n        The resulting class has an extra __annotations__ attribute, giving a\n        dict that maps field names to types.  (The field names are also in\n        the _fields attribute, which is part of the namedtuple API.)\n        An alternative equivalent functional syntax is also accepted::\n\n            Employee = NamedTuple('Employee', [('name', str), ('id', int)])\n        \"\"\"\n        if fields is _marker:\n            if kwargs:\n                deprecated_thing = \"Creating NamedTuple classes using keyword arguments\"\n                deprecation_msg = (\n                    \"{name} is deprecated and will be disallowed in Python {remove}. \"\n                    \"Use the class-based or functional syntax instead.\"\n                )\n            else:\n                deprecated_thing = \"Failing to pass a value for the 'fields' parameter\"\n                example = f\"`{typename} = NamedTuple({typename!r}, [])`\"\n                deprecation_msg = (\n                    \"{name} is deprecated and will be disallowed in Python {remove}. \"\n                    \"To create a NamedTuple class with 0 fields \"\n                    \"using the functional syntax, \"\n                    \"pass an empty list, e.g. \"\n                ) + example + \".\"\n        elif fields is None:\n            if kwargs:\n                raise TypeError(\n                    \"Cannot pass `None` as the 'fields' parameter \"\n                    \"and also specify fields using keyword arguments\"\n                )\n            else:\n                deprecated_thing = \"Passing `None` as the 'fields' parameter\"\n                example = f\"`{typename} = NamedTuple({typename!r}, [])`\"\n                deprecation_msg = (\n                    \"{name} is deprecated and will be disallowed in Python {remove}. \"\n                    \"To create a NamedTuple class with 0 fields \"\n                    \"using the functional syntax, \"\n                    \"pass an empty list, e.g. \"\n                ) + example + \".\"\n        elif kwargs:\n            raise TypeError(\"Either list of fields or keywords\"\n                            \" can be provided to NamedTuple, not both\")\n        if fields is _marker or fields is None:\n            warnings.warn(\n                deprecation_msg.format(name=deprecated_thing, remove=\"3.15\"),\n                DeprecationWarning,\n                stacklevel=2,\n            )\n            fields = kwargs.items()\n        nt = _make_nmtuple(typename, fields, module=_caller())\n        nt.__orig_bases__ = (NamedTuple,)\n        return nt\n\n\nif hasattr(collections.abc, \"Buffer\"):\n    Buffer = collections.abc.Buffer\nelse:\n    class Buffer(abc.ABC):  # noqa: B024\n        \"\"\"Base class for classes that implement the buffer protocol.\n\n        The buffer protocol allows Python objects to expose a low-level\n        memory buffer interface. Before Python 3.12, it is not possible\n        to implement the buffer protocol in pure Python code, or even\n        to check whether a class implements the buffer protocol. In\n        Python 3.12 and higher, the ``__buffer__`` method allows access\n        to the buffer protocol from Python code, and the\n        ``collections.abc.Buffer`` ABC allows checking whether a class\n        implements the buffer protocol.\n\n        To indicate support for the buffer protocol in earlier versions,\n        inherit from this ABC, either in a stub file or at runtime,\n        or use ABC registration. This ABC provides no methods, because\n        there is no Python-accessible methods shared by pre-3.12 buffer\n        classes. It is useful primarily for static checks.\n\n        \"\"\"\n\n    # As a courtesy, register the most common stdlib buffer classes.\n    Buffer.register(memoryview)\n    Buffer.register(bytearray)\n    Buffer.register(bytes)\n\n\n# Backport of types.get_original_bases, available on 3.12+ in CPython\nif hasattr(_types, \"get_original_bases\"):\n    get_original_bases = _types.get_original_bases\nelse:\n    def get_original_bases(cls, /):\n        \"\"\"Return the class's \"original\" bases prior to modification by `__mro_entries__`.\n\n        Examples::\n\n            from typing import TypeVar, Generic\n            from typing_extensions import NamedTuple, TypedDict\n\n            T = TypeVar(\"T\")\n            class Foo(Generic[T]): ...\n            class Bar(Foo[int], float): ...\n            class Baz(list[str]): ...\n            Eggs = NamedTuple(\"Eggs\", [(\"a\", int), (\"b\", str)])\n            Spam = TypedDict(\"Spam\", {\"a\": int, \"b\": str})\n\n            assert get_original_bases(Bar) == (Foo[int], float)\n            assert get_original_bases(Baz) == (list[str],)\n            assert get_original_bases(Eggs) == (NamedTuple,)\n            assert get_original_bases(Spam) == (TypedDict,)\n            assert get_original_bases(int) == (object,)\n        \"\"\"\n        try:\n            return cls.__dict__.get(\"__orig_bases__\", cls.__bases__)\n        except AttributeError:\n            raise TypeError(\n                f'Expected an instance of type, not {type(cls).__name__!r}'\n            ) from None\n\n\n# NewType is a class on Python 3.10+, making it pickleable\n# The error message for subclassing instances of NewType was improved on 3.11+\nif sys.version_info >= (3, 11):\n    NewType = typing.NewType\nelse:\n    class NewType:\n        \"\"\"NewType creates simple unique types with almost zero\n        runtime overhead. NewType(name, tp) is considered a subtype of tp\n        by static type checkers. At runtime, NewType(name, tp) returns\n        a dummy callable that simply returns its argument. Usage::\n            UserId = NewType('UserId', int)\n            def name_by_id(user_id: UserId) -> str:\n                ...\n            UserId('user')          # Fails type check\n            name_by_id(42)          # Fails type check\n            name_by_id(UserId(42))  # OK\n            num = UserId(5) + 1     # type: int\n        \"\"\"\n\n        def __call__(self, obj, /):\n            return obj\n\n        def __init__(self, name, tp):\n            self.__qualname__ = name\n            if '.' in name:\n                name = name.rpartition('.')[-1]\n            self.__name__ = name\n            self.__supertype__ = tp\n            def_mod = _caller()\n            if def_mod != 'typing_extensions':\n                self.__module__ = def_mod\n\n        def __mro_entries__(self, bases):\n            # We defined __mro_entries__ to get a better error message\n            # if a user attempts to subclass a NewType instance. bpo-46170\n            supercls_name = self.__name__\n\n            class Dummy:\n                def __init_subclass__(cls):\n                    subcls_name = cls.__name__\n                    raise TypeError(\n                        f\"Cannot subclass an instance of NewType. \"\n                        f\"Perhaps you were looking for: \"\n                        f\"`{subcls_name} = NewType({subcls_name!r}, {supercls_name})`\"\n                    )\n\n            return (Dummy,)\n\n        def __repr__(self):\n            return f'{self.__module__}.{self.__qualname__}'\n\n        def __reduce__(self):\n            return self.__qualname__\n\n        if sys.version_info >= (3, 10):\n            # PEP 604 methods\n            # It doesn't make sense to have these methods on Python <3.10\n\n            def __or__(self, other):\n                return typing.Union[self, other]\n\n            def __ror__(self, other):\n                return typing.Union[other, self]\n\n\nif hasattr(typing, \"TypeAliasType\"):\n    TypeAliasType = typing.TypeAliasType\nelse:\n    def _is_unionable(obj):\n        \"\"\"Corresponds to is_unionable() in unionobject.c in CPython.\"\"\"\n        return obj is None or isinstance(obj, (\n            type,\n            _types.GenericAlias,\n            _types.UnionType,\n            TypeAliasType,\n        ))\n\n    class TypeAliasType:\n        \"\"\"Create named, parameterized type aliases.\n\n        This provides a backport of the new `type` statement in Python 3.12:\n\n            type ListOrSet[T] = list[T] | set[T]\n\n        is equivalent to:\n\n            T = TypeVar(\"T\")\n            ListOrSet = TypeAliasType(\"ListOrSet\", list[T] | set[T], type_params=(T,))\n\n        The name ListOrSet can then be used as an alias for the type it refers to.\n\n        The type_params argument should contain all the type parameters used\n        in the value of the type alias. If the alias is not generic, this\n        argument is omitted.\n\n        Static type checkers should only support type aliases declared using\n        TypeAliasType that follow these rules:\n\n        - The first argument (the name) must be a string literal.\n        - The TypeAliasType instance must be immediately assigned to a variable\n          of the same name. (For example, 'X = TypeAliasType(\"Y\", int)' is invalid,\n          as is 'X, Y = TypeAliasType(\"X\", int), TypeAliasType(\"Y\", int)').\n\n        \"\"\"\n\n        def __init__(self, name: str, value, *, type_params=()):\n            if not isinstance(name, str):\n                raise TypeError(\"TypeAliasType name must be a string\")\n            self.__value__ = value\n            self.__type_params__ = type_params\n\n            parameters = []\n            for type_param in type_params:\n                if isinstance(type_param, TypeVarTuple):\n                    parameters.extend(type_param)\n                else:\n                    parameters.append(type_param)\n            self.__parameters__ = tuple(parameters)\n            def_mod = _caller()\n            if def_mod != 'typing_extensions':\n                self.__module__ = def_mod\n            # Setting this attribute closes the TypeAliasType from further modification\n            self.__name__ = name\n\n        def __setattr__(self, name: str, value: object, /) -> None:\n            if hasattr(self, \"__name__\"):\n                self._raise_attribute_error(name)\n            super().__setattr__(name, value)\n\n        def __delattr__(self, name: str, /) -> Never:\n            self._raise_attribute_error(name)\n\n        def _raise_attribute_error(self, name: str) -> Never:\n            # Match the Python 3.12 error messages exactly\n            if name == \"__name__\":\n                raise AttributeError(\"readonly attribute\")\n            elif name in {\"__value__\", \"__type_params__\", \"__parameters__\", \"__module__\"}:\n                raise AttributeError(\n                    f\"attribute '{name}' of 'typing.TypeAliasType' objects \"\n                    \"is not writable\"\n                )\n            else:\n                raise AttributeError(\n                    f\"'typing.TypeAliasType' object has no attribute '{name}'\"\n                )\n\n        def __repr__(self) -> str:\n            return self.__name__\n\n        def __getitem__(self, parameters):\n            if not isinstance(parameters, tuple):\n                parameters = (parameters,)\n            parameters = [\n                typing._type_check(\n                    item, f'Subscripting {self.__name__} requires a type.'\n                )\n                for item in parameters\n            ]\n            return typing._GenericAlias(self, tuple(parameters))\n\n        def __reduce__(self):\n            return self.__name__\n\n        def __init_subclass__(cls, *args, **kwargs):\n            raise TypeError(\n                \"type 'typing_extensions.TypeAliasType' is not an acceptable base type\"\n            )\n\n        # The presence of this method convinces typing._type_check\n        # that TypeAliasTypes are types.\n        def __call__(self):\n            raise TypeError(\"Type alias is not callable\")\n\n        if sys.version_info >= (3, 10):\n            def __or__(self, right):\n                # For forward compatibility with 3.12, reject Unions\n                # that are not accepted by the built-in Union.\n                if not _is_unionable(right):\n                    return NotImplemented\n                return typing.Union[self, right]\n\n            def __ror__(self, left):\n                if not _is_unionable(left):\n                    return NotImplemented\n                return typing.Union[left, self]\n\n\nif hasattr(typing, \"is_protocol\"):\n    is_protocol = typing.is_protocol\n    get_protocol_members = typing.get_protocol_members\nelse:\n    def is_protocol(tp: type, /) -> bool:\n        \"\"\"Return True if the given type is a Protocol.\n\n        Example::\n\n            >>> from typing_extensions import Protocol, is_protocol\n            >>> class P(Protocol):\n            ...     def a(self) -> str: ...\n            ...     b: int\n            >>> is_protocol(P)\n            True\n            >>> is_protocol(int)\n            False\n        \"\"\"\n        return (\n            isinstance(tp, type)\n            and getattr(tp, '_is_protocol', False)\n            and tp is not Protocol\n            and tp is not typing.Protocol\n        )\n\n    def get_protocol_members(tp: type, /) -> typing.FrozenSet[str]:\n        \"\"\"Return the set of members defined in a Protocol.\n\n        Example::\n\n            >>> from typing_extensions import Protocol, get_protocol_members\n            >>> class P(Protocol):\n            ...     def a(self) -> str: ...\n            ...     b: int\n            >>> get_protocol_members(P)\n            frozenset({'a', 'b'})\n\n        Raise a TypeError for arguments that are not Protocols.\n        \"\"\"\n        if not is_protocol(tp):\n            raise TypeError(f'{tp!r} is not a Protocol')\n        if hasattr(tp, '__protocol_attrs__'):\n            return frozenset(tp.__protocol_attrs__)\n        return frozenset(_get_protocol_attrs(tp))\n\n\nif hasattr(typing, \"Doc\"):\n    Doc = typing.Doc\nelse:\n    class Doc:\n        \"\"\"Define the documentation of a type annotation using ``Annotated``, to be\n         used in class attributes, function and method parameters, return values,\n         and variables.\n\n        The value should be a positional-only string literal to allow static tools\n        like editors and documentation generators to use it.\n\n        This complements docstrings.\n\n        The string value passed is available in the attribute ``documentation``.\n\n        Example::\n\n            >>> from typing_extensions import Annotated, Doc\n            >>> def hi(to: Annotated[str, Doc(\"Who to say hi to\")]) -> None: ...\n        \"\"\"\n        def __init__(self, documentation: str, /) -> None:\n            self.documentation = documentation\n\n        def __repr__(self) -> str:\n            return f\"Doc({self.documentation!r})\"\n\n        def __hash__(self) -> int:\n            return hash(self.documentation)\n\n        def __eq__(self, other: object) -> bool:\n            if not isinstance(other, Doc):\n                return NotImplemented\n            return self.documentation == other.documentation\n\n\n_CapsuleType = getattr(_types, \"CapsuleType\", None)\n\nif _CapsuleType is None:\n    try:\n        import _socket\n    except ImportError:\n        pass\n    else:\n        _CAPI = getattr(_socket, \"CAPI\", None)\n        if _CAPI is not None:\n            _CapsuleType = type(_CAPI)\n\nif _CapsuleType is not None:\n    CapsuleType = _CapsuleType\n    __all__.append(\"CapsuleType\")\n\n\n# Aliases for items that have always been in typing.\n# Explicitly assign these (rather than using `from typing import *` at the top),\n# so that we get a CI error if one of these is deleted from typing.py\n# in a future version of Python\nAbstractSet = typing.AbstractSet\nAnyStr = typing.AnyStr\nBinaryIO = typing.BinaryIO\nCallable = typing.Callable\nCollection = typing.Collection\nContainer = typing.Container\nDict = typing.Dict\nForwardRef = typing.ForwardRef\nFrozenSet = typing.FrozenSet\nGeneric = typing.Generic\nHashable = typing.Hashable\nIO = typing.IO\nItemsView = typing.ItemsView\nIterable = typing.Iterable\nIterator = typing.Iterator\nKeysView = typing.KeysView\nList = typing.List\nMapping = typing.Mapping\nMappingView = typing.MappingView\nMatch = typing.Match\nMutableMapping = typing.MutableMapping\nMutableSequence = typing.MutableSequence\nMutableSet = typing.MutableSet\nOptional = typing.Optional\nPattern = typing.Pattern\nReversible = typing.Reversible\nSequence = typing.Sequence\nSet = typing.Set\nSized = typing.Sized\nTextIO = typing.TextIO\nTuple = typing.Tuple\nUnion = typing.Union\nValuesView = typing.ValuesView\ncast = typing.cast\nno_type_check = typing.no_type_check\nno_type_check_decorator = typing.no_type_check_decorator\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/_vendor/wheel/__init__.py","size":59,"sha1":"8e85d1cce98bcd76102066026ca3aadeafdd6440","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"from __future__ import annotations\n\n__version__ = \"0.43.0\"\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/_vendor/wheel/__main__.py","size":455,"sha1":"ffe3e2c7d64812ecfb2becdb41cec1dbb2359618","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"\nWheel command line tool (enable python -m wheel syntax)\n\"\"\"\n\nfrom __future__ import annotations\n\nimport sys\n\n\ndef main():  # needed for console script\n    if __package__ == \"\":\n        # To be able to run 'python wheel-0.9.whl/wheel':\n        import os.path\n\n        path = os.path.dirname(os.path.dirname(__file__))\n        sys.path[0:0] = [path]\n    import wheel.cli\n\n    sys.exit(wheel.cli.main())\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/_vendor/wheel/_setuptools_logging.py","size":746,"sha1":"86cb5d580c8556f743afe3e14974bf4fed88e1e1","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"# copied from setuptools.logging, omitting monkeypatching\nfrom __future__ import annotations\n\nimport logging\nimport sys\n\n\ndef _not_warning(record):\n    return record.levelno < logging.WARNING\n\n\ndef configure():\n    \"\"\"\n    Configure logging to emit warning and above to stderr\n    and everything else to stdout. This behavior is provided\n    for compatibility with distutils.log but may change in\n    the future.\n    \"\"\"\n    err_handler = logging.StreamHandler()\n    err_handler.setLevel(logging.WARNING)\n    out_handler = logging.StreamHandler(sys.stdout)\n    out_handler.addFilter(_not_warning)\n    handlers = err_handler, out_handler\n    logging.basicConfig(\n        format=\"{message}\", style=\"{\", handlers=handlers, level=logging.DEBUG\n    )\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/_vendor/wheel/bdist_wheel.py","size":20938,"sha1":"5ece0340fec2295bd5d0a17e1a972ad8494f67f0","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"\nCreate a wheel (.whl) distribution.\n\nA wheel is a built archive format.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nimport re\nimport shutil\nimport stat\nimport struct\nimport sys\nimport sysconfig\nimport warnings\nfrom email.generator import BytesGenerator, Generator\nfrom email.policy import EmailPolicy\nfrom glob import iglob\nfrom shutil import rmtree\nfrom zipfile import ZIP_DEFLATED, ZIP_STORED\n\nimport setuptools\nfrom setuptools import Command\n\nfrom . import __version__ as wheel_version\nfrom .macosx_libfile import calculate_macosx_platform_tag\nfrom .metadata import pkginfo_to_metadata\nfrom .util import log\nfrom .vendored.packaging import tags\nfrom .vendored.packaging import version as _packaging_version\nfrom .wheelfile import WheelFile\n\n\ndef safe_name(name):\n    \"\"\"Convert an arbitrary string to a standard distribution name\n    Any runs of non-alphanumeric/. characters are replaced with a single '-'.\n    \"\"\"\n    return re.sub(\"[^A-Za-z0-9.]+\", \"-\", name)\n\n\ndef safe_version(version):\n    \"\"\"\n    Convert an arbitrary string to a standard version string\n    \"\"\"\n    try:\n        # normalize the version\n        return str(_packaging_version.Version(version))\n    except _packaging_version.InvalidVersion:\n        version = version.replace(\" \", \".\")\n        return re.sub(\"[^A-Za-z0-9.]+\", \"-\", version)\n\n\nsetuptools_major_version = int(setuptools.__version__.split(\".\")[0])\n\nPY_LIMITED_API_PATTERN = r\"cp3\\d\"\n\n\ndef _is_32bit_interpreter():\n    return struct.calcsize(\"P\") == 4\n\n\ndef python_tag():\n    return f\"py{sys.version_info[0]}\"\n\n\ndef get_platform(archive_root):\n    \"\"\"Return our platform name 'win32', 'linux_x86_64'\"\"\"\n    result = sysconfig.get_platform()\n    if result.startswith(\"macosx\") and archive_root is not None:\n        result = calculate_macosx_platform_tag(archive_root, result)\n    elif _is_32bit_interpreter():\n        if result == \"linux-x86_64\":\n            # pip pull request #3497\n            result = \"linux-i686\"\n        elif result == \"linux-aarch64\":\n            # packaging pull request #234\n            # TODO armv8l, packaging pull request #690 => this did not land\n            # in pip/packaging yet\n            result = \"linux-armv7l\"\n\n    return result.replace(\"-\", \"_\")\n\n\ndef get_flag(var, fallback, expected=True, warn=True):\n    \"\"\"Use a fallback value for determining SOABI flags if the needed config\n    var is unset or unavailable.\"\"\"\n    val = sysconfig.get_config_var(var)\n    if val is None:\n        if warn:\n            warnings.warn(\n                f\"Config variable '{var}' is unset, Python ABI tag may \" \"be incorrect\",\n                RuntimeWarning,\n                stacklevel=2,\n            )\n        return fallback\n    return val == expected\n\n\ndef get_abi_tag():\n    \"\"\"Return the ABI tag based on SOABI (if available) or emulate SOABI (PyPy2).\"\"\"\n    soabi = sysconfig.get_config_var(\"SOABI\")\n    impl = tags.interpreter_name()\n    if not soabi and impl in (\"cp\", \"pp\") and hasattr(sys, \"maxunicode\"):\n        d = \"\"\n        m = \"\"\n        u = \"\"\n        if get_flag(\"Py_DEBUG\", hasattr(sys, \"gettotalrefcount\"), warn=(impl == \"cp\")):\n            d = \"d\"\n\n        if get_flag(\n            \"WITH_PYMALLOC\",\n            impl == \"cp\",\n            warn=(impl == \"cp\" and sys.version_info < (3, 8)),\n        ) and sys.version_info < (3, 8):\n            m = \"m\"\n\n        abi = f\"{impl}{tags.interpreter_version()}{d}{m}{u}\"\n    elif soabi and impl == \"cp\" and soabi.startswith(\"cpython\"):\n        # non-Windows\n        abi = \"cp\" + soabi.split(\"-\")[1]\n    elif soabi and impl == \"cp\" and soabi.startswith(\"cp\"):\n        # Windows\n        abi = soabi.split(\"-\")[0]\n    elif soabi and impl == \"pp\":\n        # we want something like pypy36-pp73\n        abi = \"-\".join(soabi.split(\"-\")[:2])\n        abi = abi.replace(\".\", \"_\").replace(\"-\", \"_\")\n    elif soabi and impl == \"graalpy\":\n        abi = \"-\".join(soabi.split(\"-\")[:3])\n        abi = abi.replace(\".\", \"_\").replace(\"-\", \"_\")\n    elif soabi:\n        abi = soabi.replace(\".\", \"_\").replace(\"-\", \"_\")\n    else:\n        abi = None\n\n    return abi\n\n\ndef safer_name(name):\n    return safe_name(name).replace(\"-\", \"_\")\n\n\ndef safer_version(version):\n    return safe_version(version).replace(\"-\", \"_\")\n\n\ndef remove_readonly(func, path, excinfo):\n    remove_readonly_exc(func, path, excinfo[1])\n\n\ndef remove_readonly_exc(func, path, exc):\n    os.chmod(path, stat.S_IWRITE)\n    func(path)\n\n\nclass bdist_wheel(Command):\n    description = \"create a wheel distribution\"\n\n    supported_compressions = {\n        \"stored\": ZIP_STORED,\n        \"deflated\": ZIP_DEFLATED,\n    }\n\n    user_options = [\n        (\"bdist-dir=\", \"b\", \"temporary directory for creating the distribution\"),\n        (\n            \"plat-name=\",\n            \"p\",\n            \"platform name to embed in generated filenames \"\n            \"(default: %s)\" % get_platform(None),\n        ),\n        (\n            \"keep-temp\",\n            \"k\",\n            \"keep the pseudo-installation tree around after \"\n            \"creating the distribution archive\",\n        ),\n        (\"dist-dir=\", \"d\", \"directory to put final built distributions in\"),\n        (\"skip-build\", None, \"skip rebuilding everything (for testing/debugging)\"),\n        (\n            \"relative\",\n            None,\n            \"build the archive using relative paths \" \"(default: false)\",\n        ),\n        (\n            \"owner=\",\n            \"u\",\n            \"Owner name used when creating a tar file\" \" [default: current user]\",\n        ),\n        (\n            \"group=\",\n            \"g\",\n            \"Group name used when creating a tar file\" \" [default: current group]\",\n        ),\n        (\"universal\", None, \"make a universal wheel\" \" (default: false)\"),\n        (\n            \"compression=\",\n            None,\n            \"zipfile compression (one of: {})\" \" (default: 'deflated')\".format(\n                \", \".join(supported_compressions)\n            ),\n        ),\n        (\n            \"python-tag=\",\n            None,\n            \"Python implementation compatibility tag\"\n            \" (default: '%s')\" % (python_tag()),\n        ),\n        (\n            \"build-number=\",\n            None,\n            \"Build number for this particular version. \"\n            \"As specified in PEP-0427, this must start with a digit. \"\n            \"[default: None]\",\n        ),\n        (\n            \"py-limited-api=\",\n            None,\n            \"Python tag (cp32|cp33|cpNN) for abi3 wheel tag\" \" (default: false)\",\n        ),\n    ]\n\n    boolean_options = [\"keep-temp\", \"skip-build\", \"relative\", \"universal\"]\n\n    def initialize_options(self):\n        self.bdist_dir = None\n        self.data_dir = None\n        self.plat_name = None\n        self.plat_tag = None\n        self.format = \"zip\"\n        self.keep_temp = False\n        self.dist_dir = None\n        self.egginfo_dir = None\n        self.root_is_pure = None\n        self.skip_build = None\n        self.relative = False\n        self.owner = None\n        self.group = None\n        self.universal = False\n        self.compression = \"deflated\"\n        self.python_tag = python_tag()\n        self.build_number = None\n        self.py_limited_api = False\n        self.plat_name_supplied = False\n\n    def finalize_options(self):\n        if self.bdist_dir is None:\n            bdist_base = self.get_finalized_command(\"bdist\").bdist_base\n            self.bdist_dir = os.path.join(bdist_base, \"wheel\")\n\n        egg_info = self.distribution.get_command_obj(\"egg_info\")\n        egg_info.ensure_finalized()  # needed for correct `wheel_dist_name`\n\n        self.data_dir = self.wheel_dist_name + \".data\"\n        self.plat_name_supplied = self.plat_name is not None\n\n        try:\n            self.compression = self.supported_compressions[self.compression]\n        except KeyError:\n            raise ValueError(f\"Unsupported compression: {self.compression}\") from None\n\n        need_options = (\"dist_dir\", \"plat_name\", \"skip_build\")\n\n        self.set_undefined_options(\"bdist\", *zip(need_options, need_options))\n\n        self.root_is_pure = not (\n            self.distribution.has_ext_modules() or self.distribution.has_c_libraries()\n        )\n\n        if self.py_limited_api and not re.match(\n            PY_LIMITED_API_PATTERN, self.py_limited_api\n        ):\n            raise ValueError(\"py-limited-api must match '%s'\" % PY_LIMITED_API_PATTERN)\n\n        # Support legacy [wheel] section for setting universal\n        wheel = self.distribution.get_option_dict(\"wheel\")\n        if \"universal\" in wheel:\n            # please don't define this in your global configs\n            log.warning(\n                \"The [wheel] section is deprecated. Use [bdist_wheel] instead.\",\n            )\n            val = wheel[\"universal\"][1].strip()\n            if val.lower() in (\"1\", \"true\", \"yes\"):\n                self.universal = True\n\n        if self.build_number is not None and not self.build_number[:1].isdigit():\n            raise ValueError(\"Build tag (build-number) must start with a digit.\")\n\n    @property\n    def wheel_dist_name(self):\n        \"\"\"Return distribution full name with - replaced with _\"\"\"\n        components = (\n            safer_name(self.distribution.get_name()),\n            safer_version(self.distribution.get_version()),\n        )\n        if self.build_number:\n            components += (self.build_number,)\n        return \"-\".join(components)\n\n    def get_tag(self):\n        # bdist sets self.plat_name if unset, we should only use it for purepy\n        # wheels if the user supplied it.\n        if self.plat_name_supplied:\n            plat_name = self.plat_name\n        elif self.root_is_pure:\n            plat_name = \"any\"\n        else:\n            # macosx contains system version in platform name so need special handle\n            if self.plat_name and not self.plat_name.startswith(\"macosx\"):\n                plat_name = self.plat_name\n            else:\n                # on macosx always limit the platform name to comply with any\n                # c-extension modules in bdist_dir, since the user can specify\n                # a higher MACOSX_DEPLOYMENT_TARGET via tools like CMake\n\n                # on other platforms, and on macosx if there are no c-extension\n                # modules, use the default platform name.\n                plat_name = get_platform(self.bdist_dir)\n\n            if _is_32bit_interpreter():\n                if plat_name in (\"linux-x86_64\", \"linux_x86_64\"):\n                    plat_name = \"linux_i686\"\n                if plat_name in (\"linux-aarch64\", \"linux_aarch64\"):\n                    # TODO armv8l, packaging pull request #690 => this did not land\n                    # in pip/packaging yet\n                    plat_name = \"linux_armv7l\"\n\n        plat_name = (\n            plat_name.lower().replace(\"-\", \"_\").replace(\".\", \"_\").replace(\" \", \"_\")\n        )\n\n        if self.root_is_pure:\n            if self.universal:\n                impl = \"py2.py3\"\n            else:\n                impl = self.python_tag\n            tag = (impl, \"none\", plat_name)\n        else:\n            impl_name = tags.interpreter_name()\n            impl_ver = tags.interpreter_version()\n            impl = impl_name + impl_ver\n            # We don't work on CPython 3.1, 3.0.\n            if self.py_limited_api and (impl_name + impl_ver).startswith(\"cp3\"):\n                impl = self.py_limited_api\n                abi_tag = \"abi3\"\n            else:\n                abi_tag = str(get_abi_tag()).lower()\n            tag = (impl, abi_tag, plat_name)\n            # issue gh-374: allow overriding plat_name\n            supported_tags = [\n                (t.interpreter, t.abi, plat_name) for t in tags.sys_tags()\n            ]\n            assert (\n                tag in supported_tags\n            ), f\"would build wheel with unsupported tag {tag}\"\n        return tag\n\n    def run(self):\n        build_scripts = self.reinitialize_command(\"build_scripts\")\n        build_scripts.executable = \"python\"\n        build_scripts.force = True\n\n        build_ext = self.reinitialize_command(\"build_ext\")\n        build_ext.inplace = False\n\n        if not self.skip_build:\n            self.run_command(\"build\")\n\n        install = self.reinitialize_command(\"install\", reinit_subcommands=True)\n        install.root = self.bdist_dir\n        install.compile = False\n        install.skip_build = self.skip_build\n        install.warn_dir = False\n\n        # A wheel without setuptools scripts is more cross-platform.\n        # Use the (undocumented) `no_ep` option to setuptools'\n        # install_scripts command to avoid creating entry point scripts.\n        install_scripts = self.reinitialize_command(\"install_scripts\")\n        install_scripts.no_ep = True\n\n        # Use a custom scheme for the archive, because we have to decide\n        # at installation time which scheme to use.\n        for key in (\"headers\", \"scripts\", \"data\", \"purelib\", \"platlib\"):\n            setattr(install, \"install_\" + key, os.path.join(self.data_dir, key))\n\n        basedir_observed = \"\"\n\n        if os.name == \"nt\":\n            # win32 barfs if any of these are ''; could be '.'?\n            # (distutils.command.install:change_roots bug)\n            basedir_observed = os.path.normpath(os.path.join(self.data_dir, \"..\"))\n            self.install_libbase = self.install_lib = basedir_observed\n\n        setattr(\n            install,\n            \"install_purelib\" if self.root_is_pure else \"install_platlib\",\n            basedir_observed,\n        )\n\n        log.info(f\"installing to {self.bdist_dir}\")\n\n        self.run_command(\"install\")\n\n        impl_tag, abi_tag, plat_tag = self.get_tag()\n        archive_basename = f\"{self.wheel_dist_name}-{impl_tag}-{abi_tag}-{plat_tag}\"\n        if not self.relative:\n            archive_root = self.bdist_dir\n        else:\n            archive_root = os.path.join(\n                self.bdist_dir, self._ensure_relative(install.install_base)\n            )\n\n        self.set_undefined_options(\"install_egg_info\", (\"target\", \"egginfo_dir\"))\n        distinfo_dirname = (\n            f\"{safer_name(self.distribution.get_name())}-\"\n            f\"{safer_version(self.distribution.get_version())}.dist-info\"\n        )\n        distinfo_dir = os.path.join(self.bdist_dir, distinfo_dirname)\n        self.egg2dist(self.egginfo_dir, distinfo_dir)\n\n        self.write_wheelfile(distinfo_dir)\n\n        # Make the archive\n        if not os.path.exists(self.dist_dir):\n            os.makedirs(self.dist_dir)\n\n        wheel_path = os.path.join(self.dist_dir, archive_basename + \".whl\")\n        with WheelFile(wheel_path, \"w\", self.compression) as wf:\n            wf.write_files(archive_root)\n\n        # Add to 'Distribution.dist_files' so that the \"upload\" command works\n        getattr(self.distribution, \"dist_files\", []).append(\n            (\n                \"bdist_wheel\",\n                \"{}.{}\".format(*sys.version_info[:2]),  # like 3.7\n                wheel_path,\n            )\n        )\n\n        if not self.keep_temp:\n            log.info(f\"removing {self.bdist_dir}\")\n            if not self.dry_run:\n                if sys.version_info < (3, 12):\n                    rmtree(self.bdist_dir, onerror=remove_readonly)\n                else:\n                    rmtree(self.bdist_dir, onexc=remove_readonly_exc)\n\n    def write_wheelfile(\n        self, wheelfile_base, generator=\"bdist_wheel (\" + wheel_version + \")\"\n    ):\n        from email.message import Message\n\n        msg = Message()\n        msg[\"Wheel-Version\"] = \"1.0\"  # of the spec\n        msg[\"Generator\"] = generator\n        msg[\"Root-Is-Purelib\"] = str(self.root_is_pure).lower()\n        if self.build_number is not None:\n            msg[\"Build\"] = self.build_number\n\n        # Doesn't work for bdist_wininst\n        impl_tag, abi_tag, plat_tag = self.get_tag()\n        for impl in impl_tag.split(\".\"):\n            for abi in abi_tag.split(\".\"):\n                for plat in plat_tag.split(\".\"):\n                    msg[\"Tag\"] = \"-\".join((impl, abi, plat))\n\n        wheelfile_path = os.path.join(wheelfile_base, \"WHEEL\")\n        log.info(f\"creating {wheelfile_path}\")\n        with open(wheelfile_path, \"wb\") as f:\n            BytesGenerator(f, maxheaderlen=0).flatten(msg)\n\n    def _ensure_relative(self, path):\n        # copied from dir_util, deleted\n        drive, path = os.path.splitdrive(path)\n        if path[0:1] == os.sep:\n            path = drive + path[1:]\n        return path\n\n    @property\n    def license_paths(self):\n        if setuptools_major_version >= 57:\n            # Setuptools has resolved any patterns to actual file names\n            return self.distribution.metadata.license_files or ()\n\n        files = set()\n        metadata = self.distribution.get_option_dict(\"metadata\")\n        if setuptools_major_version >= 42:\n            # Setuptools recognizes the license_files option but does not do globbing\n            patterns = self.distribution.metadata.license_files\n        else:\n            # Prior to those, wheel is entirely responsible for handling license files\n            if \"license_files\" in metadata:\n                patterns = metadata[\"license_files\"][1].split()\n            else:\n                patterns = ()\n\n        if \"license_file\" in metadata:\n            warnings.warn(\n                'The \"license_file\" option is deprecated. Use \"license_files\" instead.',\n                DeprecationWarning,\n                stacklevel=2,\n            )\n            files.add(metadata[\"license_file\"][1])\n\n        if not files and not patterns and not isinstance(patterns, list):\n            patterns = (\"LICEN[CS]E*\", \"COPYING*\", \"NOTICE*\", \"AUTHORS*\")\n\n        for pattern in patterns:\n            for path in iglob(pattern):\n                if path.endswith(\"~\"):\n                    log.debug(\n                        f'ignoring license file \"{path}\" as it looks like a backup'\n                    )\n                    continue\n\n                if path not in files and os.path.isfile(path):\n                    log.info(\n                        f'adding license file \"{path}\" (matched pattern \"{pattern}\")'\n                    )\n                    files.add(path)\n\n        return files\n\n    def egg2dist(self, egginfo_path, distinfo_path):\n        \"\"\"Convert an .egg-info directory into a .dist-info directory\"\"\"\n\n        def adios(p):\n            \"\"\"Appropriately delete directory, file or link.\"\"\"\n            if os.path.exists(p) and not os.path.islink(p) and os.path.isdir(p):\n                shutil.rmtree(p)\n            elif os.path.exists(p):\n                os.unlink(p)\n\n        adios(distinfo_path)\n\n        if not os.path.exists(egginfo_path):\n            # There is no egg-info. This is probably because the egg-info\n            # file/directory is not named matching the distribution name used\n            # to name the archive file. Check for this case and report\n            # accordingly.\n            import glob\n\n            pat = os.path.join(os.path.dirname(egginfo_path), \"*.egg-info\")\n            possible = glob.glob(pat)\n            err = f\"Egg metadata expected at {egginfo_path} but not found\"\n            if possible:\n                alt = os.path.basename(possible[0])\n                err += f\" ({alt} found - possible misnamed archive file?)\"\n\n            raise ValueError(err)\n\n        if os.path.isfile(egginfo_path):\n            # .egg-info is a single file\n            pkginfo_path = egginfo_path\n            pkg_info = pkginfo_to_metadata(egginfo_path, egginfo_path)\n            os.mkdir(distinfo_path)\n        else:\n            # .egg-info is a directory\n            pkginfo_path = os.path.join(egginfo_path, \"PKG-INFO\")\n            pkg_info = pkginfo_to_metadata(egginfo_path, pkginfo_path)\n\n            # ignore common egg metadata that is useless to wheel\n            shutil.copytree(\n                egginfo_path,\n                distinfo_path,\n                ignore=lambda x, y: {\n                    \"PKG-INFO\",\n                    \"requires.txt\",\n                    \"SOURCES.txt\",\n                    \"not-zip-safe\",\n                },\n            )\n\n            # delete dependency_links if it is only whitespace\n            dependency_links_path = os.path.join(distinfo_path, \"dependency_links.txt\")\n            with open(dependency_links_path, encoding=\"utf-8\") as dependency_links_file:\n                dependency_links = dependency_links_file.read().strip()\n            if not dependency_links:\n                adios(dependency_links_path)\n\n        pkg_info_path = os.path.join(distinfo_path, \"METADATA\")\n        serialization_policy = EmailPolicy(\n            utf8=True,\n            mangle_from_=False,\n            max_line_length=0,\n        )\n        with open(pkg_info_path, \"w\", encoding=\"utf-8\") as out:\n            Generator(out, policy=serialization_policy).flatten(pkg_info)\n\n        for license_path in self.license_paths:\n            filename = os.path.basename(license_path)\n            shutil.copy(license_path, os.path.join(distinfo_path, filename))\n\n        adios(egginfo_path)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/_vendor/wheel/cli/__init__.py","size":4264,"sha1":"e552e44d40dfb7cdfecc644ede81b1acf5a208e5","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"\nWheel command-line utility.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport os\nimport sys\nfrom argparse import ArgumentTypeError\n\n\nclass WheelError(Exception):\n    pass\n\n\ndef unpack_f(args):\n    from .unpack import unpack\n\n    unpack(args.wheelfile, args.dest)\n\n\ndef pack_f(args):\n    from .pack import pack\n\n    pack(args.directory, args.dest_dir, args.build_number)\n\n\ndef convert_f(args):\n    from .convert import convert\n\n    convert(args.files, args.dest_dir, args.verbose)\n\n\ndef tags_f(args):\n    from .tags import tags\n\n    names = (\n        tags(\n            wheel,\n            args.python_tag,\n            args.abi_tag,\n            args.platform_tag,\n            args.build,\n            args.remove,\n        )\n        for wheel in args.wheel\n    )\n\n    for name in names:\n        print(name)\n\n\ndef version_f(args):\n    from .. import __version__\n\n    print(\"wheel %s\" % __version__)\n\n\ndef parse_build_tag(build_tag: str) -> str:\n    if build_tag and not build_tag[0].isdigit():\n        raise ArgumentTypeError(\"build tag must begin with a digit\")\n    elif \"-\" in build_tag:\n        raise ArgumentTypeError(\"invalid character ('-') in build tag\")\n\n    return build_tag\n\n\nTAGS_HELP = \"\"\"\\\nMake a new wheel with given tags. Any tags unspecified will remain the same.\nStarting the tags with a \"+\" will append to the existing tags. Starting with a\n\"-\" will remove a tag (use --option=-TAG syntax). Multiple tags can be\nseparated by \".\". The original file will remain unless --remove is given.  The\noutput filename(s) will be displayed on stdout for further processing.\n\"\"\"\n\n\ndef parser():\n    p = argparse.ArgumentParser()\n    s = p.add_subparsers(help=\"commands\")\n\n    unpack_parser = s.add_parser(\"unpack\", help=\"Unpack wheel\")\n    unpack_parser.add_argument(\n        \"--dest\", \"-d\", help=\"Destination directory\", default=\".\"\n    )\n    unpack_parser.add_argument(\"wheelfile\", help=\"Wheel file\")\n    unpack_parser.set_defaults(func=unpack_f)\n\n    repack_parser = s.add_parser(\"pack\", help=\"Repack wheel\")\n    repack_parser.add_argument(\"directory\", help=\"Root directory of the unpacked wheel\")\n    repack_parser.add_argument(\n        \"--dest-dir\",\n        \"-d\",\n        default=os.path.curdir,\n        help=\"Directory to store the wheel (default %(default)s)\",\n    )\n    repack_parser.add_argument(\n        \"--build-number\", help=\"Build tag to use in the wheel name\"\n    )\n    repack_parser.set_defaults(func=pack_f)\n\n    convert_parser = s.add_parser(\"convert\", help=\"Convert egg or wininst to wheel\")\n    convert_parser.add_argument(\"files\", nargs=\"*\", help=\"Files to convert\")\n    convert_parser.add_argument(\n        \"--dest-dir\",\n        \"-d\",\n        default=os.path.curdir,\n        help=\"Directory to store wheels (default %(default)s)\",\n    )\n    convert_parser.add_argument(\"--verbose\", \"-v\", action=\"store_true\")\n    convert_parser.set_defaults(func=convert_f)\n\n    tags_parser = s.add_parser(\n        \"tags\", help=\"Add or replace the tags on a wheel\", description=TAGS_HELP\n    )\n    tags_parser.add_argument(\"wheel\", nargs=\"*\", help=\"Existing wheel(s) to retag\")\n    tags_parser.add_argument(\n        \"--remove\",\n        action=\"store_true\",\n        help=\"Remove the original files, keeping only the renamed ones\",\n    )\n    tags_parser.add_argument(\n        \"--python-tag\", metavar=\"TAG\", help=\"Specify an interpreter tag(s)\"\n    )\n    tags_parser.add_argument(\"--abi-tag\", metavar=\"TAG\", help=\"Specify an ABI tag(s)\")\n    tags_parser.add_argument(\n        \"--platform-tag\", metavar=\"TAG\", help=\"Specify a platform tag(s)\"\n    )\n    tags_parser.add_argument(\n        \"--build\", type=parse_build_tag, metavar=\"BUILD\", help=\"Specify a build tag\"\n    )\n    tags_parser.set_defaults(func=tags_f)\n\n    version_parser = s.add_parser(\"version\", help=\"Print version and exit\")\n    version_parser.set_defaults(func=version_f)\n\n    help_parser = s.add_parser(\"help\", help=\"Show this help\")\n    help_parser.set_defaults(func=lambda args: p.print_help())\n\n    return p\n\n\ndef main():\n    p = parser()\n    args = p.parse_args()\n    if not hasattr(args, \"func\"):\n        p.print_help()\n    else:\n        try:\n            args.func(args)\n            return 0\n        except WheelError as e:\n            print(e, file=sys.stderr)\n\n    return 1\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/_vendor/wheel/cli/convert.py","size":9439,"sha1":"3038e1fd7938afdfe88f4b2475456cb0f6fb56f9","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"from __future__ import annotations\n\nimport os.path\nimport re\nimport shutil\nimport tempfile\nimport zipfile\nfrom glob import iglob\n\nfrom ..bdist_wheel import bdist_wheel\nfrom ..wheelfile import WheelFile\nfrom . import WheelError\n\ntry:\n    from setuptools import Distribution\nexcept ImportError:\n    from distutils.dist import Distribution\n\negg_info_re = re.compile(\n    r\"\"\"\n    (?P<name>.+?)-(?P<ver>.+?)\n    (-(?P<pyver>py\\d\\.\\d+)\n     (-(?P<arch>.+?))?\n    )?.egg$\"\"\",\n    re.VERBOSE,\n)\n\n\nclass _bdist_wheel_tag(bdist_wheel):\n    # allow the client to override the default generated wheel tag\n    # The default bdist_wheel implementation uses python and abi tags\n    # of the running python process. This is not suitable for\n    # generating/repackaging prebuild binaries.\n\n    full_tag_supplied = False\n    full_tag = None  # None or a (pytag, soabitag, plattag) triple\n\n    def get_tag(self):\n        if self.full_tag_supplied and self.full_tag is not None:\n            return self.full_tag\n        else:\n            return bdist_wheel.get_tag(self)\n\n\ndef egg2wheel(egg_path: str, dest_dir: str) -> None:\n    filename = os.path.basename(egg_path)\n    match = egg_info_re.match(filename)\n    if not match:\n        raise WheelError(f\"Invalid egg file name: {filename}\")\n\n    egg_info = match.groupdict()\n    dir = tempfile.mkdtemp(suffix=\"_e2w\")\n    if os.path.isfile(egg_path):\n        # assume we have a bdist_egg otherwise\n        with zipfile.ZipFile(egg_path) as egg:\n            egg.extractall(dir)\n    else:\n        # support buildout-style installed eggs directories\n        for pth in os.listdir(egg_path):\n            src = os.path.join(egg_path, pth)\n            if os.path.isfile(src):\n                shutil.copy2(src, dir)\n            else:\n                shutil.copytree(src, os.path.join(dir, pth))\n\n    pyver = egg_info[\"pyver\"]\n    if pyver:\n        pyver = egg_info[\"pyver\"] = pyver.replace(\".\", \"\")\n\n    arch = (egg_info[\"arch\"] or \"any\").replace(\".\", \"_\").replace(\"-\", \"_\")\n\n    # assume all binary eggs are for CPython\n    abi = \"cp\" + pyver[2:] if arch != \"any\" else \"none\"\n\n    root_is_purelib = egg_info[\"arch\"] is None\n    if root_is_purelib:\n        bw = bdist_wheel(Distribution())\n    else:\n        bw = _bdist_wheel_tag(Distribution())\n\n    bw.root_is_pure = root_is_purelib\n    bw.python_tag = pyver\n    bw.plat_name_supplied = True\n    bw.plat_name = egg_info[\"arch\"] or \"any\"\n    if not root_is_purelib:\n        bw.full_tag_supplied = True\n        bw.full_tag = (pyver, abi, arch)\n\n    dist_info_dir = os.path.join(dir, \"{name}-{ver}.dist-info\".format(**egg_info))\n    bw.egg2dist(os.path.join(dir, \"EGG-INFO\"), dist_info_dir)\n    bw.write_wheelfile(dist_info_dir, generator=\"egg2wheel\")\n    wheel_name = \"{name}-{ver}-{pyver}-{}-{}.whl\".format(abi, arch, **egg_info)\n    with WheelFile(os.path.join(dest_dir, wheel_name), \"w\") as wf:\n        wf.write_files(dir)\n\n    shutil.rmtree(dir)\n\n\ndef parse_wininst_info(wininfo_name, egginfo_name):\n    \"\"\"Extract metadata from filenames.\n\n    Extracts the 4 metadataitems needed (name, version, pyversion, arch) from\n    the installer filename and the name of the egg-info directory embedded in\n    the zipfile (if any).\n\n    The egginfo filename has the format::\n\n        name-ver(-pyver)(-arch).egg-info\n\n    The installer filename has the format::\n\n        name-ver.arch(-pyver).exe\n\n    Some things to note:\n\n    1. The installer filename is not definitive. An installer can be renamed\n       and work perfectly well as an installer. So more reliable data should\n       be used whenever possible.\n    2. The egg-info data should be preferred for the name and version, because\n       these come straight from the distutils metadata, and are mandatory.\n    3. The pyver from the egg-info data should be ignored, as it is\n       constructed from the version of Python used to build the installer,\n       which is irrelevant - the installer filename is correct here (even to\n       the point that when it's not there, any version is implied).\n    4. The architecture must be taken from the installer filename, as it is\n       not included in the egg-info data.\n    5. Architecture-neutral installers still have an architecture because the\n       installer format itself (being executable) is architecture-specific. We\n       should therefore ignore the architecture if the content is pure-python.\n    \"\"\"\n\n    egginfo = None\n    if egginfo_name:\n        egginfo = egg_info_re.search(egginfo_name)\n        if not egginfo:\n            raise ValueError(f\"Egg info filename {egginfo_name} is not valid\")\n\n    # Parse the wininst filename\n    # 1. Distribution name (up to the first '-')\n    w_name, sep, rest = wininfo_name.partition(\"-\")\n    if not sep:\n        raise ValueError(f\"Installer filename {wininfo_name} is not valid\")\n\n    # Strip '.exe'\n    rest = rest[:-4]\n    # 2. Python version (from the last '-', must start with 'py')\n    rest2, sep, w_pyver = rest.rpartition(\"-\")\n    if sep and w_pyver.startswith(\"py\"):\n        rest = rest2\n        w_pyver = w_pyver.replace(\".\", \"\")\n    else:\n        # Not version specific - use py2.py3. While it is possible that\n        # pure-Python code is not compatible with both Python 2 and 3, there\n        # is no way of knowing from the wininst format, so we assume the best\n        # here (the user can always manually rename the wheel to be more\n        # restrictive if needed).\n        w_pyver = \"py2.py3\"\n    # 3. Version and architecture\n    w_ver, sep, w_arch = rest.rpartition(\".\")\n    if not sep:\n        raise ValueError(f\"Installer filename {wininfo_name} is not valid\")\n\n    if egginfo:\n        w_name = egginfo.group(\"name\")\n        w_ver = egginfo.group(\"ver\")\n\n    return {\"name\": w_name, \"ver\": w_ver, \"arch\": w_arch, \"pyver\": w_pyver}\n\n\ndef wininst2wheel(path, dest_dir):\n    with zipfile.ZipFile(path) as bdw:\n        # Search for egg-info in the archive\n        egginfo_name = None\n        for filename in bdw.namelist():\n            if \".egg-info\" in filename:\n                egginfo_name = filename\n                break\n\n        info = parse_wininst_info(os.path.basename(path), egginfo_name)\n\n        root_is_purelib = True\n        for zipinfo in bdw.infolist():\n            if zipinfo.filename.startswith(\"PLATLIB\"):\n                root_is_purelib = False\n                break\n        if root_is_purelib:\n            paths = {\"purelib\": \"\"}\n        else:\n            paths = {\"platlib\": \"\"}\n\n        dist_info = \"{name}-{ver}\".format(**info)\n        datadir = \"%s.data/\" % dist_info\n\n        # rewrite paths to trick ZipFile into extracting an egg\n        # XXX grab wininst .ini - between .exe, padding, and first zip file.\n        members = []\n        egginfo_name = \"\"\n        for zipinfo in bdw.infolist():\n            key, basename = zipinfo.filename.split(\"/\", 1)\n            key = key.lower()\n            basepath = paths.get(key, None)\n            if basepath is None:\n                basepath = datadir + key.lower() + \"/\"\n            oldname = zipinfo.filename\n            newname = basepath + basename\n            zipinfo.filename = newname\n            del bdw.NameToInfo[oldname]\n            bdw.NameToInfo[newname] = zipinfo\n            # Collect member names, but omit '' (from an entry like \"PLATLIB/\"\n            if newname:\n                members.append(newname)\n            # Remember egg-info name for the egg2dist call below\n            if not egginfo_name:\n                if newname.endswith(\".egg-info\"):\n                    egginfo_name = newname\n                elif \".egg-info/\" in newname:\n                    egginfo_name, sep, _ = newname.rpartition(\"/\")\n        dir = tempfile.mkdtemp(suffix=\"_b2w\")\n        bdw.extractall(dir, members)\n\n    # egg2wheel\n    abi = \"none\"\n    pyver = info[\"pyver\"]\n    arch = (info[\"arch\"] or \"any\").replace(\".\", \"_\").replace(\"-\", \"_\")\n    # Wininst installers always have arch even if they are not\n    # architecture-specific (because the format itself is).\n    # So, assume the content is architecture-neutral if root is purelib.\n    if root_is_purelib:\n        arch = \"any\"\n    # If the installer is architecture-specific, it's almost certainly also\n    # CPython-specific.\n    if arch != \"any\":\n        pyver = pyver.replace(\"py\", \"cp\")\n    wheel_name = \"-\".join((dist_info, pyver, abi, arch))\n    if root_is_purelib:\n        bw = bdist_wheel(Distribution())\n    else:\n        bw = _bdist_wheel_tag(Distribution())\n\n    bw.root_is_pure = root_is_purelib\n    bw.python_tag = pyver\n    bw.plat_name_supplied = True\n    bw.plat_name = info[\"arch\"] or \"any\"\n\n    if not root_is_purelib:\n        bw.full_tag_supplied = True\n        bw.full_tag = (pyver, abi, arch)\n\n    dist_info_dir = os.path.join(dir, \"%s.dist-info\" % dist_info)\n    bw.egg2dist(os.path.join(dir, egginfo_name), dist_info_dir)\n    bw.write_wheelfile(dist_info_dir, generator=\"wininst2wheel\")\n\n    wheel_path = os.path.join(dest_dir, wheel_name)\n    with WheelFile(wheel_path, \"w\") as wf:\n        wf.write_files(dir)\n\n    shutil.rmtree(dir)\n\n\ndef convert(files, dest_dir, verbose):\n    for pat in files:\n        for installer in iglob(pat):\n            if os.path.splitext(installer)[1] == \".egg\":\n                conv = egg2wheel\n            else:\n                conv = wininst2wheel\n\n            if verbose:\n                print(f\"{installer}... \", flush=True)\n\n            conv(installer, dest_dir)\n            if verbose:\n                print(\"OK\")\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/_vendor/wheel/cli/pack.py","size":3103,"sha1":"8ed643c3d003b233b98aa1ff49c5c7799d73e304","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"from __future__ import annotations\n\nimport email.policy\nimport os.path\nimport re\nfrom email.generator import BytesGenerator\nfrom email.parser import BytesParser\n\nfrom wheel.cli import WheelError\nfrom wheel.wheelfile import WheelFile\n\nDIST_INFO_RE = re.compile(r\"^(?P<namever>(?P<name>.+?)-(?P<ver>\\d.*?))\\.dist-info$\")\n\n\ndef pack(directory: str, dest_dir: str, build_number: str | None) -> None:\n    \"\"\"Repack a previously unpacked wheel directory into a new wheel file.\n\n    The .dist-info/WHEEL file must contain one or more tags so that the target\n    wheel file name can be determined.\n\n    :param directory: The unpacked wheel directory\n    :param dest_dir: Destination directory (defaults to the current directory)\n    \"\"\"\n    # Find the .dist-info directory\n    dist_info_dirs = [\n        fn\n        for fn in os.listdir(directory)\n        if os.path.isdir(os.path.join(directory, fn)) and DIST_INFO_RE.match(fn)\n    ]\n    if len(dist_info_dirs) > 1:\n        raise WheelError(f\"Multiple .dist-info directories found in {directory}\")\n    elif not dist_info_dirs:\n        raise WheelError(f\"No .dist-info directories found in {directory}\")\n\n    # Determine the target wheel filename\n    dist_info_dir = dist_info_dirs[0]\n    name_version = DIST_INFO_RE.match(dist_info_dir).group(\"namever\")\n\n    # Read the tags and the existing build number from .dist-info/WHEEL\n    wheel_file_path = os.path.join(directory, dist_info_dir, \"WHEEL\")\n    with open(wheel_file_path, \"rb\") as f:\n        info = BytesParser(policy=email.policy.compat32).parse(f)\n        tags: list[str] = info.get_all(\"Tag\", [])\n        existing_build_number = info.get(\"Build\")\n\n        if not tags:\n            raise WheelError(\n                f\"No tags present in {dist_info_dir}/WHEEL; cannot determine target \"\n                f\"wheel filename\"\n            )\n\n    # Set the wheel file name and add/replace/remove the Build tag in .dist-info/WHEEL\n    build_number = build_number if build_number is not None else existing_build_number\n    if build_number is not None:\n        del info[\"Build\"]\n        if build_number:\n            info[\"Build\"] = build_number\n            name_version += \"-\" + build_number\n\n        if build_number != existing_build_number:\n            with open(wheel_file_path, \"wb\") as f:\n                BytesGenerator(f, maxheaderlen=0).flatten(info)\n\n    # Reassemble the tags for the wheel file\n    tagline = compute_tagline(tags)\n\n    # Repack the wheel\n    wheel_path = os.path.join(dest_dir, f\"{name_version}-{tagline}.whl\")\n    with WheelFile(wheel_path, \"w\") as wf:\n        print(f\"Repacking wheel as {wheel_path}...\", end=\"\", flush=True)\n        wf.write_files(directory)\n\n    print(\"OK\")\n\n\ndef compute_tagline(tags: list[str]) -> str:\n    \"\"\"Compute a tagline from a list of tags.\n\n    :param tags: A list of tags\n    :return: A tagline\n    \"\"\"\n    impls = sorted({tag.split(\"-\")[0] for tag in tags})\n    abivers = sorted({tag.split(\"-\")[1] for tag in tags})\n    platforms = sorted({tag.split(\"-\")[2] for tag in tags})\n    return \"-\".join([\".\".join(impls), \".\".join(abivers), \".\".join(platforms)])\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/_vendor/wheel/cli/tags.py","size":4760,"sha1":"2c64402ed05e39936574200e18dcf3f53d60ac49","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"from __future__ import annotations\n\nimport email.policy\nimport itertools\nimport os\nfrom collections.abc import Iterable\nfrom email.parser import BytesParser\n\nfrom ..wheelfile import WheelFile\n\n\ndef _compute_tags(original_tags: Iterable[str], new_tags: str | None) -> set[str]:\n    \"\"\"Add or replace tags. Supports dot-separated tags\"\"\"\n    if new_tags is None:\n        return set(original_tags)\n\n    if new_tags.startswith(\"+\"):\n        return {*original_tags, *new_tags[1:].split(\".\")}\n\n    if new_tags.startswith(\"-\"):\n        return set(original_tags) - set(new_tags[1:].split(\".\"))\n\n    return set(new_tags.split(\".\"))\n\n\ndef tags(\n    wheel: str,\n    python_tags: str | None = None,\n    abi_tags: str | None = None,\n    platform_tags: str | None = None,\n    build_tag: str | None = None,\n    remove: bool = False,\n) -> str:\n    \"\"\"Change the tags on a wheel file.\n\n    The tags are left unchanged if they are not specified. To specify \"none\",\n    use [\"none\"]. To append to the previous tags, a tag should start with a\n    \"+\".  If a tag starts with \"-\", it will be removed from existing tags.\n    Processing is done left to right.\n\n    :param wheel: The paths to the wheels\n    :param python_tags: The Python tags to set\n    :param abi_tags: The ABI tags to set\n    :param platform_tags: The platform tags to set\n    :param build_tag: The build tag to set\n    :param remove: Remove the original wheel\n    \"\"\"\n    with WheelFile(wheel, \"r\") as f:\n        assert f.filename, f\"{f.filename} must be available\"\n\n        wheel_info = f.read(f.dist_info_path + \"/WHEEL\")\n        info = BytesParser(policy=email.policy.compat32).parsebytes(wheel_info)\n\n        original_wheel_name = os.path.basename(f.filename)\n        namever = f.parsed_filename.group(\"namever\")\n        build = f.parsed_filename.group(\"build\")\n        original_python_tags = f.parsed_filename.group(\"pyver\").split(\".\")\n        original_abi_tags = f.parsed_filename.group(\"abi\").split(\".\")\n        original_plat_tags = f.parsed_filename.group(\"plat\").split(\".\")\n\n    tags: list[str] = info.get_all(\"Tag\", [])\n    existing_build_tag = info.get(\"Build\")\n\n    impls = {tag.split(\"-\")[0] for tag in tags}\n    abivers = {tag.split(\"-\")[1] for tag in tags}\n    platforms = {tag.split(\"-\")[2] for tag in tags}\n\n    if impls != set(original_python_tags):\n        msg = f\"Wheel internal tags {impls!r} != filename tags {original_python_tags!r}\"\n        raise AssertionError(msg)\n\n    if abivers != set(original_abi_tags):\n        msg = f\"Wheel internal tags {abivers!r} != filename tags {original_abi_tags!r}\"\n        raise AssertionError(msg)\n\n    if platforms != set(original_plat_tags):\n        msg = (\n            f\"Wheel internal tags {platforms!r} != filename tags {original_plat_tags!r}\"\n        )\n        raise AssertionError(msg)\n\n    if existing_build_tag != build:\n        msg = (\n            f\"Incorrect filename '{build}' \"\n            f\"& *.dist-info/WHEEL '{existing_build_tag}' build numbers\"\n        )\n        raise AssertionError(msg)\n\n    # Start changing as needed\n    if build_tag is not None:\n        build = build_tag\n\n    final_python_tags = sorted(_compute_tags(original_python_tags, python_tags))\n    final_abi_tags = sorted(_compute_tags(original_abi_tags, abi_tags))\n    final_plat_tags = sorted(_compute_tags(original_plat_tags, platform_tags))\n\n    final_tags = [\n        namever,\n        \".\".join(final_python_tags),\n        \".\".join(final_abi_tags),\n        \".\".join(final_plat_tags),\n    ]\n    if build:\n        final_tags.insert(1, build)\n\n    final_wheel_name = \"-\".join(final_tags) + \".whl\"\n\n    if original_wheel_name != final_wheel_name:\n        del info[\"Tag\"], info[\"Build\"]\n        for a, b, c in itertools.product(\n            final_python_tags, final_abi_tags, final_plat_tags\n        ):\n            info[\"Tag\"] = f\"{a}-{b}-{c}\"\n        if build:\n            info[\"Build\"] = build\n\n        original_wheel_path = os.path.join(\n            os.path.dirname(f.filename), original_wheel_name\n        )\n        final_wheel_path = os.path.join(os.path.dirname(f.filename), final_wheel_name)\n\n        with WheelFile(original_wheel_path, \"r\") as fin, WheelFile(\n            final_wheel_path, \"w\"\n        ) as fout:\n            fout.comment = fin.comment  # preserve the comment\n            for item in fin.infolist():\n                if item.is_dir():\n                    continue\n                if item.filename == f.dist_info_path + \"/RECORD\":\n                    continue\n                if item.filename == f.dist_info_path + \"/WHEEL\":\n                    fout.writestr(item, info.as_bytes())\n                else:\n                    fout.writestr(item, fin.read(item))\n\n        if remove:\n            os.remove(original_wheel_path)\n\n    return final_wheel_name\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/_vendor/wheel/cli/unpack.py","size":1021,"sha1":"2b8c046ef5b7bd057804871532b93360e67b6a90","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"from __future__ import annotations\n\nfrom pathlib import Path\n\nfrom ..wheelfile import WheelFile\n\n\ndef unpack(path: str, dest: str = \".\") -> None:\n    \"\"\"Unpack a wheel.\n\n    Wheel content will be unpacked to {dest}/{name}-{ver}, where {name}\n    is the package name and {ver} its version.\n\n    :param path: The path to the wheel.\n    :param dest: Destination directory (default to current directory).\n    \"\"\"\n    with WheelFile(path) as wf:\n        namever = wf.parsed_filename.group(\"namever\")\n        destination = Path(dest) / namever\n        print(f\"Unpacking to: {destination}...\", end=\"\", flush=True)\n        for zinfo in wf.filelist:\n            wf.extract(zinfo, destination)\n\n            # Set permissions to the same values as they were set in the archive\n            # We have to do this manually due to\n            # https://github.com/python/cpython/issues/59999\n            permissions = zinfo.external_attr >> 16 & 0o777\n            destination.joinpath(zinfo.filename).chmod(permissions)\n\n    print(\"OK\")\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/_vendor/wheel/macosx_libfile.py","size":16103,"sha1":"4d0a336f5cea8d0d74b9ec35c0a85387084a088b","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"\nThis module contains function to analyse dynamic library\nheaders to extract system information\n\nCurrently only for MacOSX\n\nLibrary file on macosx system starts with Mach-O or Fat field.\nThis can be distinguish by first 32 bites and it is called magic number.\nProper value of magic number is with suffix _MAGIC. Suffix _CIGAM means\nreversed bytes order.\nBoth fields can occur in two types: 32 and 64 bytes.\n\nFAT field inform that this library contains few version of library\n(typically for different types version). It contains\ninformation where Mach-O headers starts.\n\nEach section started with Mach-O header contains one library\n(So if file starts with this field it contains only one version).\n\nAfter filed Mach-O there are section fields.\nEach of them starts with two fields:\ncmd - magic number for this command\ncmdsize - total size occupied by this section information.\n\nIn this case only sections LC_VERSION_MIN_MACOSX (for macosx 10.13 and earlier)\nand LC_BUILD_VERSION (for macosx 10.14 and newer) are interesting,\nbecause them contains information about minimal system version.\n\nImportant remarks:\n- For fat files this implementation looks for maximum number version.\n  It not check if it is 32 or 64 and do not compare it with currently built package.\n  So it is possible to false report higher version that needed.\n- All structures signatures are taken form macosx header files.\n- I think that binary format will be more stable than `otool` output.\n  and if apple introduce some changes both implementation will need to be updated.\n- The system compile will set the deployment target no lower than\n  11.0 for arm64 builds. For \"Universal 2\" builds use the x86_64 deployment\n  target when the arm64 target is 11.0.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport ctypes\nimport os\nimport sys\n\n\"\"\"here the needed const and struct from mach-o header files\"\"\"\n\nFAT_MAGIC = 0xCAFEBABE\nFAT_CIGAM = 0xBEBAFECA\nFAT_MAGIC_64 = 0xCAFEBABF\nFAT_CIGAM_64 = 0xBFBAFECA\nMH_MAGIC = 0xFEEDFACE\nMH_CIGAM = 0xCEFAEDFE\nMH_MAGIC_64 = 0xFEEDFACF\nMH_CIGAM_64 = 0xCFFAEDFE\n\nLC_VERSION_MIN_MACOSX = 0x24\nLC_BUILD_VERSION = 0x32\n\nCPU_TYPE_ARM64 = 0x0100000C\n\nmach_header_fields = [\n    (\"magic\", ctypes.c_uint32),\n    (\"cputype\", ctypes.c_int),\n    (\"cpusubtype\", ctypes.c_int),\n    (\"filetype\", ctypes.c_uint32),\n    (\"ncmds\", ctypes.c_uint32),\n    (\"sizeofcmds\", ctypes.c_uint32),\n    (\"flags\", ctypes.c_uint32),\n]\n\"\"\"\nstruct mach_header {\n    uint32_t\tmagic;\t\t/* mach magic number identifier */\n    cpu_type_t\tcputype;\t/* cpu specifier */\n    cpu_subtype_t\tcpusubtype;\t/* machine specifier */\n    uint32_t\tfiletype;\t/* type of file */\n    uint32_t\tncmds;\t\t/* number of load commands */\n    uint32_t\tsizeofcmds;\t/* the size of all the load commands */\n    uint32_t\tflags;\t\t/* flags */\n};\ntypedef integer_t cpu_type_t;\ntypedef integer_t cpu_subtype_t;\n\"\"\"\n\nmach_header_fields_64 = mach_header_fields + [(\"reserved\", ctypes.c_uint32)]\n\"\"\"\nstruct mach_header_64 {\n    uint32_t\tmagic;\t\t/* mach magic number identifier */\n    cpu_type_t\tcputype;\t/* cpu specifier */\n    cpu_subtype_t\tcpusubtype;\t/* machine specifier */\n    uint32_t\tfiletype;\t/* type of file */\n    uint32_t\tncmds;\t\t/* number of load commands */\n    uint32_t\tsizeofcmds;\t/* the size of all the load commands */\n    uint32_t\tflags;\t\t/* flags */\n    uint32_t\treserved;\t/* reserved */\n};\n\"\"\"\n\nfat_header_fields = [(\"magic\", ctypes.c_uint32), (\"nfat_arch\", ctypes.c_uint32)]\n\"\"\"\nstruct fat_header {\n    uint32_t\tmagic;\t\t/* FAT_MAGIC or FAT_MAGIC_64 */\n    uint32_t\tnfat_arch;\t/* number of structs that follow */\n};\n\"\"\"\n\nfat_arch_fields = [\n    (\"cputype\", ctypes.c_int),\n    (\"cpusubtype\", ctypes.c_int),\n    (\"offset\", ctypes.c_uint32),\n    (\"size\", ctypes.c_uint32),\n    (\"align\", ctypes.c_uint32),\n]\n\"\"\"\nstruct fat_arch {\n    cpu_type_t\tcputype;\t/* cpu specifier (int) */\n    cpu_subtype_t\tcpusubtype;\t/* machine specifier (int) */\n    uint32_t\toffset;\t\t/* file offset to this object file */\n    uint32_t\tsize;\t\t/* size of this object file */\n    uint32_t\talign;\t\t/* alignment as a power of 2 */\n};\n\"\"\"\n\nfat_arch_64_fields = [\n    (\"cputype\", ctypes.c_int),\n    (\"cpusubtype\", ctypes.c_int),\n    (\"offset\", ctypes.c_uint64),\n    (\"size\", ctypes.c_uint64),\n    (\"align\", ctypes.c_uint32),\n    (\"reserved\", ctypes.c_uint32),\n]\n\"\"\"\nstruct fat_arch_64 {\n    cpu_type_t\tcputype;\t/* cpu specifier (int) */\n    cpu_subtype_t\tcpusubtype;\t/* machine specifier (int) */\n    uint64_t\toffset;\t\t/* file offset to this object file */\n    uint64_t\tsize;\t\t/* size of this object file */\n    uint32_t\talign;\t\t/* alignment as a power of 2 */\n    uint32_t\treserved;\t/* reserved */\n};\n\"\"\"\n\nsegment_base_fields = [(\"cmd\", ctypes.c_uint32), (\"cmdsize\", ctypes.c_uint32)]\n\"\"\"base for reading segment info\"\"\"\n\nsegment_command_fields = [\n    (\"cmd\", ctypes.c_uint32),\n    (\"cmdsize\", ctypes.c_uint32),\n    (\"segname\", ctypes.c_char * 16),\n    (\"vmaddr\", ctypes.c_uint32),\n    (\"vmsize\", ctypes.c_uint32),\n    (\"fileoff\", ctypes.c_uint32),\n    (\"filesize\", ctypes.c_uint32),\n    (\"maxprot\", ctypes.c_int),\n    (\"initprot\", ctypes.c_int),\n    (\"nsects\", ctypes.c_uint32),\n    (\"flags\", ctypes.c_uint32),\n]\n\"\"\"\nstruct segment_command { /* for 32-bit architectures */\n    uint32_t\tcmd;\t\t/* LC_SEGMENT */\n    uint32_t\tcmdsize;\t/* includes sizeof section structs */\n    char\t\tsegname[16];\t/* segment name */\n    uint32_t\tvmaddr;\t\t/* memory address of this segment */\n    uint32_t\tvmsize;\t\t/* memory size of this segment */\n    uint32_t\tfileoff;\t/* file offset of this segment */\n    uint32_t\tfilesize;\t/* amount to map from the file */\n    vm_prot_t\tmaxprot;\t/* maximum VM protection */\n    vm_prot_t\tinitprot;\t/* initial VM protection */\n    uint32_t\tnsects;\t\t/* number of sections in segment */\n    uint32_t\tflags;\t\t/* flags */\n};\ntypedef int vm_prot_t;\n\"\"\"\n\nsegment_command_fields_64 = [\n    (\"cmd\", ctypes.c_uint32),\n    (\"cmdsize\", ctypes.c_uint32),\n    (\"segname\", ctypes.c_char * 16),\n    (\"vmaddr\", ctypes.c_uint64),\n    (\"vmsize\", ctypes.c_uint64),\n    (\"fileoff\", ctypes.c_uint64),\n    (\"filesize\", ctypes.c_uint64),\n    (\"maxprot\", ctypes.c_int),\n    (\"initprot\", ctypes.c_int),\n    (\"nsects\", ctypes.c_uint32),\n    (\"flags\", ctypes.c_uint32),\n]\n\"\"\"\nstruct segment_command_64 { /* for 64-bit architectures */\n    uint32_t\tcmd;\t\t/* LC_SEGMENT_64 */\n    uint32_t\tcmdsize;\t/* includes sizeof section_64 structs */\n    char\t\tsegname[16];\t/* segment name */\n    uint64_t\tvmaddr;\t\t/* memory address of this segment */\n    uint64_t\tvmsize;\t\t/* memory size of this segment */\n    uint64_t\tfileoff;\t/* file offset of this segment */\n    uint64_t\tfilesize;\t/* amount to map from the file */\n    vm_prot_t\tmaxprot;\t/* maximum VM protection */\n    vm_prot_t\tinitprot;\t/* initial VM protection */\n    uint32_t\tnsects;\t\t/* number of sections in segment */\n    uint32_t\tflags;\t\t/* flags */\n};\n\"\"\"\n\nversion_min_command_fields = segment_base_fields + [\n    (\"version\", ctypes.c_uint32),\n    (\"sdk\", ctypes.c_uint32),\n]\n\"\"\"\nstruct version_min_command {\n    uint32_t\tcmd;\t\t/* LC_VERSION_MIN_MACOSX or\n                               LC_VERSION_MIN_IPHONEOS or\n                               LC_VERSION_MIN_WATCHOS or\n                               LC_VERSION_MIN_TVOS */\n    uint32_t\tcmdsize;\t/* sizeof(struct min_version_command) */\n    uint32_t\tversion;\t/* X.Y.Z is encoded in nibbles xxxx.yy.zz */\n    uint32_t\tsdk;\t\t/* X.Y.Z is encoded in nibbles xxxx.yy.zz */\n};\n\"\"\"\n\nbuild_version_command_fields = segment_base_fields + [\n    (\"platform\", ctypes.c_uint32),\n    (\"minos\", ctypes.c_uint32),\n    (\"sdk\", ctypes.c_uint32),\n    (\"ntools\", ctypes.c_uint32),\n]\n\"\"\"\nstruct build_version_command {\n    uint32_t\tcmd;\t\t/* LC_BUILD_VERSION */\n    uint32_t\tcmdsize;\t/* sizeof(struct build_version_command) plus */\n                                /* ntools * sizeof(struct build_tool_version) */\n    uint32_t\tplatform;\t/* platform */\n    uint32_t\tminos;\t\t/* X.Y.Z is encoded in nibbles xxxx.yy.zz */\n    uint32_t\tsdk;\t\t/* X.Y.Z is encoded in nibbles xxxx.yy.zz */\n    uint32_t\tntools;\t\t/* number of tool entries following this */\n};\n\"\"\"\n\n\ndef swap32(x):\n    return (\n        ((x << 24) & 0xFF000000)\n        | ((x << 8) & 0x00FF0000)\n        | ((x >> 8) & 0x0000FF00)\n        | ((x >> 24) & 0x000000FF)\n    )\n\n\ndef get_base_class_and_magic_number(lib_file, seek=None):\n    if seek is None:\n        seek = lib_file.tell()\n    else:\n        lib_file.seek(seek)\n    magic_number = ctypes.c_uint32.from_buffer_copy(\n        lib_file.read(ctypes.sizeof(ctypes.c_uint32))\n    ).value\n\n    # Handle wrong byte order\n    if magic_number in [FAT_CIGAM, FAT_CIGAM_64, MH_CIGAM, MH_CIGAM_64]:\n        if sys.byteorder == \"little\":\n            BaseClass = ctypes.BigEndianStructure\n        else:\n            BaseClass = ctypes.LittleEndianStructure\n\n        magic_number = swap32(magic_number)\n    else:\n        BaseClass = ctypes.Structure\n\n    lib_file.seek(seek)\n    return BaseClass, magic_number\n\n\ndef read_data(struct_class, lib_file):\n    return struct_class.from_buffer_copy(lib_file.read(ctypes.sizeof(struct_class)))\n\n\ndef extract_macosx_min_system_version(path_to_lib):\n    with open(path_to_lib, \"rb\") as lib_file:\n        BaseClass, magic_number = get_base_class_and_magic_number(lib_file, 0)\n        if magic_number not in [FAT_MAGIC, FAT_MAGIC_64, MH_MAGIC, MH_MAGIC_64]:\n            return\n\n        if magic_number in [FAT_MAGIC, FAT_CIGAM_64]:\n\n            class FatHeader(BaseClass):\n                _fields_ = fat_header_fields\n\n            fat_header = read_data(FatHeader, lib_file)\n            if magic_number == FAT_MAGIC:\n\n                class FatArch(BaseClass):\n                    _fields_ = fat_arch_fields\n\n            else:\n\n                class FatArch(BaseClass):\n                    _fields_ = fat_arch_64_fields\n\n            fat_arch_list = [\n                read_data(FatArch, lib_file) for _ in range(fat_header.nfat_arch)\n            ]\n\n            versions_list = []\n            for el in fat_arch_list:\n                try:\n                    version = read_mach_header(lib_file, el.offset)\n                    if version is not None:\n                        if el.cputype == CPU_TYPE_ARM64 and len(fat_arch_list) != 1:\n                            # Xcode will not set the deployment target below 11.0.0\n                            # for the arm64 architecture. Ignore the arm64 deployment\n                            # in fat binaries when the target is 11.0.0, that way\n                            # the other architectures can select a lower deployment\n                            # target.\n                            # This is safe because there is no arm64 variant for\n                            # macOS 10.15 or earlier.\n                            if version == (11, 0, 0):\n                                continue\n                        versions_list.append(version)\n                except ValueError:\n                    pass\n\n            if len(versions_list) > 0:\n                return max(versions_list)\n            else:\n                return None\n\n        else:\n            try:\n                return read_mach_header(lib_file, 0)\n            except ValueError:\n                \"\"\"when some error during read library files\"\"\"\n                return None\n\n\ndef read_mach_header(lib_file, seek=None):\n    \"\"\"\n    This function parses a Mach-O header and extracts\n    information about the minimal macOS version.\n\n    :param lib_file: reference to opened library file with pointer\n    \"\"\"\n    base_class, magic_number = get_base_class_and_magic_number(lib_file, seek)\n    arch = \"32\" if magic_number == MH_MAGIC else \"64\"\n\n    class SegmentBase(base_class):\n        _fields_ = segment_base_fields\n\n    if arch == \"32\":\n\n        class MachHeader(base_class):\n            _fields_ = mach_header_fields\n\n    else:\n\n        class MachHeader(base_class):\n            _fields_ = mach_header_fields_64\n\n    mach_header = read_data(MachHeader, lib_file)\n    for _i in range(mach_header.ncmds):\n        pos = lib_file.tell()\n        segment_base = read_data(SegmentBase, lib_file)\n        lib_file.seek(pos)\n        if segment_base.cmd == LC_VERSION_MIN_MACOSX:\n\n            class VersionMinCommand(base_class):\n                _fields_ = version_min_command_fields\n\n            version_info = read_data(VersionMinCommand, lib_file)\n            return parse_version(version_info.version)\n        elif segment_base.cmd == LC_BUILD_VERSION:\n\n            class VersionBuild(base_class):\n                _fields_ = build_version_command_fields\n\n            version_info = read_data(VersionBuild, lib_file)\n            return parse_version(version_info.minos)\n        else:\n            lib_file.seek(pos + segment_base.cmdsize)\n            continue\n\n\ndef parse_version(version):\n    x = (version & 0xFFFF0000) >> 16\n    y = (version & 0x0000FF00) >> 8\n    z = version & 0x000000FF\n    return x, y, z\n\n\ndef calculate_macosx_platform_tag(archive_root, platform_tag):\n    \"\"\"\n    Calculate proper macosx platform tag basing on files which are included to wheel\n\n    Example platform tag `macosx-10.14-x86_64`\n    \"\"\"\n    prefix, base_version, suffix = platform_tag.split(\"-\")\n    base_version = tuple(int(x) for x in base_version.split(\".\"))\n    base_version = base_version[:2]\n    if base_version[0] > 10:\n        base_version = (base_version[0], 0)\n    assert len(base_version) == 2\n    if \"MACOSX_DEPLOYMENT_TARGET\" in os.environ:\n        deploy_target = tuple(\n            int(x) for x in os.environ[\"MACOSX_DEPLOYMENT_TARGET\"].split(\".\")\n        )\n        deploy_target = deploy_target[:2]\n        if deploy_target[0] > 10:\n            deploy_target = (deploy_target[0], 0)\n        if deploy_target < base_version:\n            sys.stderr.write(\n                \"[WARNING] MACOSX_DEPLOYMENT_TARGET is set to a lower value ({}) than \"\n                \"the version on which the Python interpreter was compiled ({}), and \"\n                \"will be ignored.\\n\".format(\n                    \".\".join(str(x) for x in deploy_target),\n                    \".\".join(str(x) for x in base_version),\n                )\n            )\n        else:\n            base_version = deploy_target\n\n    assert len(base_version) == 2\n    start_version = base_version\n    versions_dict = {}\n    for dirpath, _dirnames, filenames in os.walk(archive_root):\n        for filename in filenames:\n            if filename.endswith(\".dylib\") or filename.endswith(\".so\"):\n                lib_path = os.path.join(dirpath, filename)\n                min_ver = extract_macosx_min_system_version(lib_path)\n                if min_ver is not None:\n                    min_ver = min_ver[0:2]\n                    if min_ver[0] > 10:\n                        min_ver = (min_ver[0], 0)\n                    versions_dict[lib_path] = min_ver\n\n    if len(versions_dict) > 0:\n        base_version = max(base_version, max(versions_dict.values()))\n\n    # macosx platform tag do not support minor bugfix release\n    fin_base_version = \"_\".join([str(x) for x in base_version])\n    if start_version < base_version:\n        problematic_files = [k for k, v in versions_dict.items() if v > start_version]\n        problematic_files = \"\\n\".join(problematic_files)\n        if len(problematic_files) == 1:\n            files_form = \"this file\"\n        else:\n            files_form = \"these files\"\n        error_message = (\n            \"[WARNING] This wheel needs a higher macOS version than {}  \"\n            \"To silence this warning, set MACOSX_DEPLOYMENT_TARGET to at least \"\n            + fin_base_version\n            + \" or recreate \"\n            + files_form\n            + \" with lower \"\n            \"MACOSX_DEPLOYMENT_TARGET:  \\n\" + problematic_files\n        )\n\n        if \"MACOSX_DEPLOYMENT_TARGET\" in os.environ:\n            error_message = error_message.format(\n                \"is set in MACOSX_DEPLOYMENT_TARGET variable.\"\n            )\n        else:\n            error_message = error_message.format(\n                \"the version your Python interpreter is compiled against.\"\n            )\n\n        sys.stderr.write(error_message)\n\n    platform_tag = prefix + \"_\" + fin_base_version + \"_\" + suffix\n    return platform_tag\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/_vendor/wheel/metadata.py","size":5884,"sha1":"1187c394400c40d02d318ec2b94144d7870576fd","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"\nTools for converting old- to new-style metadata.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport functools\nimport itertools\nimport os.path\nimport re\nimport textwrap\nfrom email.message import Message\nfrom email.parser import Parser\nfrom typing import Iterator\n\nfrom .vendored.packaging.requirements import Requirement\n\n\ndef _nonblank(str):\n    return str and not str.startswith(\"#\")\n\n\n@functools.singledispatch\ndef yield_lines(iterable):\n    r\"\"\"\n    Yield valid lines of a string or iterable.\n    >>> list(yield_lines(''))\n    []\n    >>> list(yield_lines(['foo', 'bar']))\n    ['foo', 'bar']\n    >>> list(yield_lines('foo\\nbar'))\n    ['foo', 'bar']\n    >>> list(yield_lines('\\nfoo\\n#bar\\nbaz #comment'))\n    ['foo', 'baz #comment']\n    >>> list(yield_lines(['foo\\nbar', 'baz', 'bing\\n\\n\\n']))\n    ['foo', 'bar', 'baz', 'bing']\n    \"\"\"\n    return itertools.chain.from_iterable(map(yield_lines, iterable))\n\n\n@yield_lines.register(str)\ndef _(text):\n    return filter(_nonblank, map(str.strip, text.splitlines()))\n\n\ndef split_sections(s):\n    \"\"\"Split a string or iterable thereof into (section, content) pairs\n    Each ``section`` is a stripped version of the section header (\"[section]\")\n    and each ``content`` is a list of stripped lines excluding blank lines and\n    comment-only lines.  If there are any such lines before the first section\n    header, they're returned in a first ``section`` of ``None``.\n    \"\"\"\n    section = None\n    content = []\n    for line in yield_lines(s):\n        if line.startswith(\"[\"):\n            if line.endswith(\"]\"):\n                if section or content:\n                    yield section, content\n                section = line[1:-1].strip()\n                content = []\n            else:\n                raise ValueError(\"Invalid section heading\", line)\n        else:\n            content.append(line)\n\n    # wrap up last segment\n    yield section, content\n\n\ndef safe_extra(extra):\n    \"\"\"Convert an arbitrary string to a standard 'extra' name\n    Any runs of non-alphanumeric characters are replaced with a single '_',\n    and the result is always lowercased.\n    \"\"\"\n    return re.sub(\"[^A-Za-z0-9.-]+\", \"_\", extra).lower()\n\n\ndef safe_name(name):\n    \"\"\"Convert an arbitrary string to a standard distribution name\n    Any runs of non-alphanumeric/. characters are replaced with a single '-'.\n    \"\"\"\n    return re.sub(\"[^A-Za-z0-9.]+\", \"-\", name)\n\n\ndef requires_to_requires_dist(requirement: Requirement) -> str:\n    \"\"\"Return the version specifier for a requirement in PEP 345/566 fashion.\"\"\"\n    if getattr(requirement, \"url\", None):\n        return \" @ \" + requirement.url\n\n    requires_dist = []\n    for spec in requirement.specifier:\n        requires_dist.append(spec.operator + spec.version)\n\n    if requires_dist:\n        return \" \" + \",\".join(sorted(requires_dist))\n    else:\n        return \"\"\n\n\ndef convert_requirements(requirements: list[str]) -> Iterator[str]:\n    \"\"\"Yield Requires-Dist: strings for parsed requirements strings.\"\"\"\n    for req in requirements:\n        parsed_requirement = Requirement(req)\n        spec = requires_to_requires_dist(parsed_requirement)\n        extras = \",\".join(sorted(safe_extra(e) for e in parsed_requirement.extras))\n        if extras:\n            extras = f\"[{extras}]\"\n\n        yield safe_name(parsed_requirement.name) + extras + spec\n\n\ndef generate_requirements(\n    extras_require: dict[str, list[str]],\n) -> Iterator[tuple[str, str]]:\n    \"\"\"\n    Convert requirements from a setup()-style dictionary to\n    ('Requires-Dist', 'requirement') and ('Provides-Extra', 'extra') tuples.\n\n    extras_require is a dictionary of {extra: [requirements]} as passed to setup(),\n    using the empty extra {'': [requirements]} to hold install_requires.\n    \"\"\"\n    for extra, depends in extras_require.items():\n        condition = \"\"\n        extra = extra or \"\"\n        if \":\" in extra:  # setuptools extra:condition syntax\n            extra, condition = extra.split(\":\", 1)\n\n        extra = safe_extra(extra)\n        if extra:\n            yield \"Provides-Extra\", extra\n            if condition:\n                condition = \"(\" + condition + \") and \"\n            condition += \"extra == '%s'\" % extra\n\n        if condition:\n            condition = \" ; \" + condition\n\n        for new_req in convert_requirements(depends):\n            yield \"Requires-Dist\", new_req + condition\n\n\ndef pkginfo_to_metadata(egg_info_path: str, pkginfo_path: str) -> Message:\n    \"\"\"\n    Convert .egg-info directory with PKG-INFO to the Metadata 2.1 format\n    \"\"\"\n    with open(pkginfo_path, encoding=\"utf-8\") as headers:\n        pkg_info = Parser().parse(headers)\n\n    pkg_info.replace_header(\"Metadata-Version\", \"2.1\")\n    # Those will be regenerated from `requires.txt`.\n    del pkg_info[\"Provides-Extra\"]\n    del pkg_info[\"Requires-Dist\"]\n    requires_path = os.path.join(egg_info_path, \"requires.txt\")\n    if os.path.exists(requires_path):\n        with open(requires_path, encoding=\"utf-8\") as requires_file:\n            requires = requires_file.read()\n\n        parsed_requirements = sorted(split_sections(requires), key=lambda x: x[0] or \"\")\n        for extra, reqs in parsed_requirements:\n            for key, value in generate_requirements({extra: reqs}):\n                if (key, value) not in pkg_info.items():\n                    pkg_info[key] = value\n\n    description = pkg_info[\"Description\"]\n    if description:\n        description_lines = pkg_info[\"Description\"].splitlines()\n        dedented_description = \"\\n\".join(\n            # if the first line of long_description is blank,\n            # the first line here will be indented.\n            (\n                description_lines[0].lstrip(),\n                textwrap.dedent(\"\\n\".join(description_lines[1:])),\n                \"\\n\",\n            )\n        )\n        pkg_info.set_payload(dedented_description)\n        del pkg_info[\"Description\"]\n\n    return pkg_info\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/_vendor/wheel/util.py","size":621,"sha1":"88baf1e97e55cce4d06683a204834465eaf981c0","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"from __future__ import annotations\n\nimport base64\nimport logging\n\nlog = logging.getLogger(\"wheel\")\n\n# ensure Python logging is configured\ntry:\n    __import__(\"setuptools.logging\")\nexcept ImportError:\n    # setuptools < ??\n    from . import _setuptools_logging\n\n    _setuptools_logging.configure()\n\n\ndef urlsafe_b64encode(data: bytes) -> bytes:\n    \"\"\"urlsafe_b64encode without padding\"\"\"\n    return base64.urlsafe_b64encode(data).rstrip(b\"=\")\n\n\ndef urlsafe_b64decode(data: bytes) -> bytes:\n    \"\"\"urlsafe_b64decode without padding\"\"\"\n    pad = b\"=\" * (4 - (len(data) & 3))\n    return base64.urlsafe_b64decode(data + pad)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/_vendor/wheel/vendored/__init__.py","size":0,"sha1":"da39a3ee5e6b4b0d3255bfef95601890afd80709","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":""},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/_vendor/wheel/vendored/packaging/__init__.py","size":0,"sha1":"da39a3ee5e6b4b0d3255bfef95601890afd80709","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":""},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/_vendor/wheel/vendored/packaging/_elffile.py","size":3266,"sha1":"34190a771dc51364fc58f05326e0fed1f37eac61","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"\nELF file parser.\n\nThis provides a class ``ELFFile`` that parses an ELF executable in a similar\ninterface to ``ZipFile``. Only the read interface is implemented.\n\nBased on: https://gist.github.com/lyssdod/f51579ae8d93c8657a5564aefc2ffbca\nELF header: https://refspecs.linuxfoundation.org/elf/gabi4+/ch4.eheader.html\n\"\"\"\n\nimport enum\nimport os\nimport struct\nfrom typing import IO, Optional, Tuple\n\n\nclass ELFInvalid(ValueError):\n    pass\n\n\nclass EIClass(enum.IntEnum):\n    C32 = 1\n    C64 = 2\n\n\nclass EIData(enum.IntEnum):\n    Lsb = 1\n    Msb = 2\n\n\nclass EMachine(enum.IntEnum):\n    I386 = 3\n    S390 = 22\n    Arm = 40\n    X8664 = 62\n    AArc64 = 183\n\n\nclass ELFFile:\n    \"\"\"\n    Representation of an ELF executable.\n    \"\"\"\n\n    def __init__(self, f: IO[bytes]) -> None:\n        self._f = f\n\n        try:\n            ident = self._read(\"16B\")\n        except struct.error:\n            raise ELFInvalid(\"unable to parse identification\")\n        magic = bytes(ident[:4])\n        if magic != b\"\\x7fELF\":\n            raise ELFInvalid(f\"invalid magic: {magic!r}\")\n\n        self.capacity = ident[4]  # Format for program header (bitness).\n        self.encoding = ident[5]  # Data structure encoding (endianness).\n\n        try:\n            # e_fmt: Format for program header.\n            # p_fmt: Format for section header.\n            # p_idx: Indexes to find p_type, p_offset, and p_filesz.\n            e_fmt, self._p_fmt, self._p_idx = {\n                (1, 1): (\"<HHIIIIIHHH\", \"<IIIIIIII\", (0, 1, 4)),  # 32-bit LSB.\n                (1, 2): (\">HHIIIIIHHH\", \">IIIIIIII\", (0, 1, 4)),  # 32-bit MSB.\n                (2, 1): (\"<HHIQQQIHHH\", \"<IIQQQQQQ\", (0, 2, 5)),  # 64-bit LSB.\n                (2, 2): (\">HHIQQQIHHH\", \">IIQQQQQQ\", (0, 2, 5)),  # 64-bit MSB.\n            }[(self.capacity, self.encoding)]\n        except KeyError:\n            raise ELFInvalid(\n                f\"unrecognized capacity ({self.capacity}) or \"\n                f\"encoding ({self.encoding})\"\n            )\n\n        try:\n            (\n                _,\n                self.machine,  # Architecture type.\n                _,\n                _,\n                self._e_phoff,  # Offset of program header.\n                _,\n                self.flags,  # Processor-specific flags.\n                _,\n                self._e_phentsize,  # Size of section.\n                self._e_phnum,  # Number of sections.\n            ) = self._read(e_fmt)\n        except struct.error as e:\n            raise ELFInvalid(\"unable to parse machine and section information\") from e\n\n    def _read(self, fmt: str) -> Tuple[int, ...]:\n        return struct.unpack(fmt, self._f.read(struct.calcsize(fmt)))\n\n    @property\n    def interpreter(self) -> Optional[str]:\n        \"\"\"\n        The path recorded in the ``PT_INTERP`` section header.\n        \"\"\"\n        for index in range(self._e_phnum):\n            self._f.seek(self._e_phoff + self._e_phentsize * index)\n            try:\n                data = self._read(self._p_fmt)\n            except struct.error:\n                continue\n            if data[self._p_idx[0]] != 3:  # Not PT_INTERP.\n                continue\n            self._f.seek(data[self._p_idx[1]])\n            return os.fsdecode(self._f.read(data[self._p_idx[2]])).strip(\"\\0\")\n        return None\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/_vendor/wheel/vendored/packaging/_manylinux.py","size":9588,"sha1":"4f06dcc509cf427a7294036631b07a36af765208","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import collections\nimport contextlib\nimport functools\nimport os\nimport re\nimport sys\nimport warnings\nfrom typing import Dict, Generator, Iterator, NamedTuple, Optional, Sequence, Tuple\n\nfrom ._elffile import EIClass, EIData, ELFFile, EMachine\n\nEF_ARM_ABIMASK = 0xFF000000\nEF_ARM_ABI_VER5 = 0x05000000\nEF_ARM_ABI_FLOAT_HARD = 0x00000400\n\n\n# `os.PathLike` not a generic type until Python 3.9, so sticking with `str`\n# as the type for `path` until then.\n@contextlib.contextmanager\ndef _parse_elf(path: str) -> Generator[Optional[ELFFile], None, None]:\n    try:\n        with open(path, \"rb\") as f:\n            yield ELFFile(f)\n    except (OSError, TypeError, ValueError):\n        yield None\n\n\ndef _is_linux_armhf(executable: str) -> bool:\n    # hard-float ABI can be detected from the ELF header of the running\n    # process\n    # https://static.docs.arm.com/ihi0044/g/aaelf32.pdf\n    with _parse_elf(executable) as f:\n        return (\n            f is not None\n            and f.capacity == EIClass.C32\n            and f.encoding == EIData.Lsb\n            and f.machine == EMachine.Arm\n            and f.flags & EF_ARM_ABIMASK == EF_ARM_ABI_VER5\n            and f.flags & EF_ARM_ABI_FLOAT_HARD == EF_ARM_ABI_FLOAT_HARD\n        )\n\n\ndef _is_linux_i686(executable: str) -> bool:\n    with _parse_elf(executable) as f:\n        return (\n            f is not None\n            and f.capacity == EIClass.C32\n            and f.encoding == EIData.Lsb\n            and f.machine == EMachine.I386\n        )\n\n\ndef _have_compatible_abi(executable: str, archs: Sequence[str]) -> bool:\n    if \"armv7l\" in archs:\n        return _is_linux_armhf(executable)\n    if \"i686\" in archs:\n        return _is_linux_i686(executable)\n    allowed_archs = {\n        \"x86_64\",\n        \"aarch64\",\n        \"ppc64\",\n        \"ppc64le\",\n        \"s390x\",\n        \"loongarch64\",\n        \"riscv64\",\n    }\n    return any(arch in allowed_archs for arch in archs)\n\n\n# If glibc ever changes its major version, we need to know what the last\n# minor version was, so we can build the complete list of all versions.\n# For now, guess what the highest minor version might be, assume it will\n# be 50 for testing. Once this actually happens, update the dictionary\n# with the actual value.\n_LAST_GLIBC_MINOR: Dict[int, int] = collections.defaultdict(lambda: 50)\n\n\nclass _GLibCVersion(NamedTuple):\n    major: int\n    minor: int\n\n\ndef _glibc_version_string_confstr() -> Optional[str]:\n    \"\"\"\n    Primary implementation of glibc_version_string using os.confstr.\n    \"\"\"\n    # os.confstr is quite a bit faster than ctypes.DLL. It's also less likely\n    # to be broken or missing. This strategy is used in the standard library\n    # platform module.\n    # https://github.com/python/cpython/blob/fcf1d003bf4f0100c/Lib/platform.py#L175-L183\n    try:\n        # Should be a string like \"glibc 2.17\".\n        version_string: Optional[str] = os.confstr(\"CS_GNU_LIBC_VERSION\")\n        assert version_string is not None\n        _, version = version_string.rsplit()\n    except (AssertionError, AttributeError, OSError, ValueError):\n        # os.confstr() or CS_GNU_LIBC_VERSION not available (or a bad value)...\n        return None\n    return version\n\n\ndef _glibc_version_string_ctypes() -> Optional[str]:\n    \"\"\"\n    Fallback implementation of glibc_version_string using ctypes.\n    \"\"\"\n    try:\n        import ctypes\n    except ImportError:\n        return None\n\n    # ctypes.CDLL(None) internally calls dlopen(NULL), and as the dlopen\n    # manpage says, \"If filename is NULL, then the returned handle is for the\n    # main program\". This way we can let the linker do the work to figure out\n    # which libc our process is actually using.\n    #\n    # We must also handle the special case where the executable is not a\n    # dynamically linked executable. This can occur when using musl libc,\n    # for example. In this situation, dlopen() will error, leading to an\n    # OSError. Interestingly, at least in the case of musl, there is no\n    # errno set on the OSError. The single string argument used to construct\n    # OSError comes from libc itself and is therefore not portable to\n    # hard code here. In any case, failure to call dlopen() means we\n    # can proceed, so we bail on our attempt.\n    try:\n        process_namespace = ctypes.CDLL(None)\n    except OSError:\n        return None\n\n    try:\n        gnu_get_libc_version = process_namespace.gnu_get_libc_version\n    except AttributeError:\n        # Symbol doesn't exist -> therefore, we are not linked to\n        # glibc.\n        return None\n\n    # Call gnu_get_libc_version, which returns a string like \"2.5\"\n    gnu_get_libc_version.restype = ctypes.c_char_p\n    version_str: str = gnu_get_libc_version()\n    # py2 / py3 compatibility:\n    if not isinstance(version_str, str):\n        version_str = version_str.decode(\"ascii\")\n\n    return version_str\n\n\ndef _glibc_version_string() -> Optional[str]:\n    \"\"\"Returns glibc version string, or None if not using glibc.\"\"\"\n    return _glibc_version_string_confstr() or _glibc_version_string_ctypes()\n\n\ndef _parse_glibc_version(version_str: str) -> Tuple[int, int]:\n    \"\"\"Parse glibc version.\n\n    We use a regexp instead of str.split because we want to discard any\n    random junk that might come after the minor version -- this might happen\n    in patched/forked versions of glibc (e.g. Linaro's version of glibc\n    uses version strings like \"2.20-2014.11\"). See gh-3588.\n    \"\"\"\n    m = re.match(r\"(?P<major>[0-9]+)\\.(?P<minor>[0-9]+)\", version_str)\n    if not m:\n        warnings.warn(\n            f\"Expected glibc version with 2 components major.minor,\"\n            f\" got: {version_str}\",\n            RuntimeWarning,\n        )\n        return -1, -1\n    return int(m.group(\"major\")), int(m.group(\"minor\"))\n\n\n@functools.lru_cache\ndef _get_glibc_version() -> Tuple[int, int]:\n    version_str = _glibc_version_string()\n    if version_str is None:\n        return (-1, -1)\n    return _parse_glibc_version(version_str)\n\n\n# From PEP 513, PEP 600\ndef _is_compatible(arch: str, version: _GLibCVersion) -> bool:\n    sys_glibc = _get_glibc_version()\n    if sys_glibc < version:\n        return False\n    # Check for presence of _manylinux module.\n    try:\n        import _manylinux\n    except ImportError:\n        return True\n    if hasattr(_manylinux, \"manylinux_compatible\"):\n        result = _manylinux.manylinux_compatible(version[0], version[1], arch)\n        if result is not None:\n            return bool(result)\n        return True\n    if version == _GLibCVersion(2, 5):\n        if hasattr(_manylinux, \"manylinux1_compatible\"):\n            return bool(_manylinux.manylinux1_compatible)\n    if version == _GLibCVersion(2, 12):\n        if hasattr(_manylinux, \"manylinux2010_compatible\"):\n            return bool(_manylinux.manylinux2010_compatible)\n    if version == _GLibCVersion(2, 17):\n        if hasattr(_manylinux, \"manylinux2014_compatible\"):\n            return bool(_manylinux.manylinux2014_compatible)\n    return True\n\n\n_LEGACY_MANYLINUX_MAP = {\n    # CentOS 7 w/ glibc 2.17 (PEP 599)\n    (2, 17): \"manylinux2014\",\n    # CentOS 6 w/ glibc 2.12 (PEP 571)\n    (2, 12): \"manylinux2010\",\n    # CentOS 5 w/ glibc 2.5 (PEP 513)\n    (2, 5): \"manylinux1\",\n}\n\n\ndef platform_tags(archs: Sequence[str]) -> Iterator[str]:\n    \"\"\"Generate manylinux tags compatible to the current platform.\n\n    :param archs: Sequence of compatible architectures.\n        The first one shall be the closest to the actual architecture and be the part of\n        platform tag after the ``linux_`` prefix, e.g. ``x86_64``.\n        The ``linux_`` prefix is assumed as a prerequisite for the current platform to\n        be manylinux-compatible.\n\n    :returns: An iterator of compatible manylinux tags.\n    \"\"\"\n    if not _have_compatible_abi(sys.executable, archs):\n        return\n    # Oldest glibc to be supported regardless of architecture is (2, 17).\n    too_old_glibc2 = _GLibCVersion(2, 16)\n    if set(archs) & {\"x86_64\", \"i686\"}:\n        # On x86/i686 also oldest glibc to be supported is (2, 5).\n        too_old_glibc2 = _GLibCVersion(2, 4)\n    current_glibc = _GLibCVersion(*_get_glibc_version())\n    glibc_max_list = [current_glibc]\n    # We can assume compatibility across glibc major versions.\n    # https://sourceware.org/bugzilla/show_bug.cgi?id=24636\n    #\n    # Build a list of maximum glibc versions so that we can\n    # output the canonical list of all glibc from current_glibc\n    # down to too_old_glibc2, including all intermediary versions.\n    for glibc_major in range(current_glibc.major - 1, 1, -1):\n        glibc_minor = _LAST_GLIBC_MINOR[glibc_major]\n        glibc_max_list.append(_GLibCVersion(glibc_major, glibc_minor))\n    for arch in archs:\n        for glibc_max in glibc_max_list:\n            if glibc_max.major == too_old_glibc2.major:\n                min_minor = too_old_glibc2.minor\n            else:\n                # For other glibc major versions oldest supported is (x, 0).\n                min_minor = -1\n            for glibc_minor in range(glibc_max.minor, min_minor, -1):\n                glibc_version = _GLibCVersion(glibc_max.major, glibc_minor)\n                tag = \"manylinux_{}_{}\".format(*glibc_version)\n                if _is_compatible(arch, glibc_version):\n                    yield f\"{tag}_{arch}\"\n                # Handle the legacy manylinux1, manylinux2010, manylinux2014 tags.\n                if glibc_version in _LEGACY_MANYLINUX_MAP:\n                    legacy_tag = _LEGACY_MANYLINUX_MAP[glibc_version]\n                    if _is_compatible(arch, glibc_version):\n                        yield f\"{legacy_tag}_{arch}\"\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/_vendor/wheel/vendored/packaging/_musllinux.py","size":2674,"sha1":"b1e7ad6daec419442bc20ce5e15269e2d3fe43cf","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"PEP 656 support.\n\nThis module implements logic to detect if the currently running Python is\nlinked against musl, and what musl version is used.\n\"\"\"\n\nimport functools\nimport re\nimport subprocess\nimport sys\nfrom typing import Iterator, NamedTuple, Optional, Sequence\n\nfrom ._elffile import ELFFile\n\n\nclass _MuslVersion(NamedTuple):\n    major: int\n    minor: int\n\n\ndef _parse_musl_version(output: str) -> Optional[_MuslVersion]:\n    lines = [n for n in (n.strip() for n in output.splitlines()) if n]\n    if len(lines) < 2 or lines[0][:4] != \"musl\":\n        return None\n    m = re.match(r\"Version (\\d+)\\.(\\d+)\", lines[1])\n    if not m:\n        return None\n    return _MuslVersion(major=int(m.group(1)), minor=int(m.group(2)))\n\n\n@functools.lru_cache\ndef _get_musl_version(executable: str) -> Optional[_MuslVersion]:\n    \"\"\"Detect currently-running musl runtime version.\n\n    This is done by checking the specified executable's dynamic linking\n    information, and invoking the loader to parse its output for a version\n    string. If the loader is musl, the output would be something like::\n\n        musl libc (x86_64)\n        Version 1.2.2\n        Dynamic Program Loader\n    \"\"\"\n    try:\n        with open(executable, \"rb\") as f:\n            ld = ELFFile(f).interpreter\n    except (OSError, TypeError, ValueError):\n        return None\n    if ld is None or \"musl\" not in ld:\n        return None\n    proc = subprocess.run([ld], stderr=subprocess.PIPE, text=True)\n    return _parse_musl_version(proc.stderr)\n\n\ndef platform_tags(archs: Sequence[str]) -> Iterator[str]:\n    \"\"\"Generate musllinux tags compatible to the current platform.\n\n    :param archs: Sequence of compatible architectures.\n        The first one shall be the closest to the actual architecture and be the part of\n        platform tag after the ``linux_`` prefix, e.g. ``x86_64``.\n        The ``linux_`` prefix is assumed as a prerequisite for the current platform to\n        be musllinux-compatible.\n\n    :returns: An iterator of compatible musllinux tags.\n    \"\"\"\n    sys_musl = _get_musl_version(sys.executable)\n    if sys_musl is None:  # Python not dynamically linked against musl.\n        return\n    for arch in archs:\n        for minor in range(sys_musl.minor, -1, -1):\n            yield f\"musllinux_{sys_musl.major}_{minor}_{arch}\"\n\n\nif __name__ == \"__main__\":  # pragma: no cover\n    import sysconfig\n\n    plat = sysconfig.get_platform()\n    assert plat.startswith(\"linux-\"), \"not linux\"\n\n    print(\"plat:\", plat)\n    print(\"musl:\", _get_musl_version(sys.executable))\n    print(\"tags:\", end=\" \")\n    for t in platform_tags(re.sub(r\"[.-]\", \"_\", plat.split(\"-\", 1)[-1])):\n        print(t, end=\"\\n      \")\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/_vendor/wheel/vendored/packaging/_parser.py","size":10347,"sha1":"1157c9f4e715af26ea88c794f8e552fd28afcb05","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"Handwritten parser of dependency specifiers.\n\nThe docstring for each __parse_* function contains EBNF-inspired grammar representing\nthe implementation.\n\"\"\"\n\nimport ast\nfrom typing import Any, List, NamedTuple, Optional, Tuple, Union\n\nfrom ._tokenizer import DEFAULT_RULES, Tokenizer\n\n\nclass Node:\n    def __init__(self, value: str) -> None:\n        self.value = value\n\n    def __str__(self) -> str:\n        return self.value\n\n    def __repr__(self) -> str:\n        return f\"<{self.__class__.__name__}('{self}')>\"\n\n    def serialize(self) -> str:\n        raise NotImplementedError\n\n\nclass Variable(Node):\n    def serialize(self) -> str:\n        return str(self)\n\n\nclass Value(Node):\n    def serialize(self) -> str:\n        return f'\"{self}\"'\n\n\nclass Op(Node):\n    def serialize(self) -> str:\n        return str(self)\n\n\nMarkerVar = Union[Variable, Value]\nMarkerItem = Tuple[MarkerVar, Op, MarkerVar]\n# MarkerAtom = Union[MarkerItem, List[\"MarkerAtom\"]]\n# MarkerList = List[Union[\"MarkerList\", MarkerAtom, str]]\n# mypy does not support recursive type definition\n# https://github.com/python/mypy/issues/731\nMarkerAtom = Any\nMarkerList = List[Any]\n\n\nclass ParsedRequirement(NamedTuple):\n    name: str\n    url: str\n    extras: List[str]\n    specifier: str\n    marker: Optional[MarkerList]\n\n\n# --------------------------------------------------------------------------------------\n# Recursive descent parser for dependency specifier\n# --------------------------------------------------------------------------------------\ndef parse_requirement(source: str) -> ParsedRequirement:\n    return _parse_requirement(Tokenizer(source, rules=DEFAULT_RULES))\n\n\ndef _parse_requirement(tokenizer: Tokenizer) -> ParsedRequirement:\n    \"\"\"\n    requirement = WS? IDENTIFIER WS? extras WS? requirement_details\n    \"\"\"\n    tokenizer.consume(\"WS\")\n\n    name_token = tokenizer.expect(\n        \"IDENTIFIER\", expected=\"package name at the start of dependency specifier\"\n    )\n    name = name_token.text\n    tokenizer.consume(\"WS\")\n\n    extras = _parse_extras(tokenizer)\n    tokenizer.consume(\"WS\")\n\n    url, specifier, marker = _parse_requirement_details(tokenizer)\n    tokenizer.expect(\"END\", expected=\"end of dependency specifier\")\n\n    return ParsedRequirement(name, url, extras, specifier, marker)\n\n\ndef _parse_requirement_details(\n    tokenizer: Tokenizer,\n) -> Tuple[str, str, Optional[MarkerList]]:\n    \"\"\"\n    requirement_details = AT URL (WS requirement_marker?)?\n                        | specifier WS? (requirement_marker)?\n    \"\"\"\n\n    specifier = \"\"\n    url = \"\"\n    marker = None\n\n    if tokenizer.check(\"AT\"):\n        tokenizer.read()\n        tokenizer.consume(\"WS\")\n\n        url_start = tokenizer.position\n        url = tokenizer.expect(\"URL\", expected=\"URL after @\").text\n        if tokenizer.check(\"END\", peek=True):\n            return (url, specifier, marker)\n\n        tokenizer.expect(\"WS\", expected=\"whitespace after URL\")\n\n        # The input might end after whitespace.\n        if tokenizer.check(\"END\", peek=True):\n            return (url, specifier, marker)\n\n        marker = _parse_requirement_marker(\n            tokenizer, span_start=url_start, after=\"URL and whitespace\"\n        )\n    else:\n        specifier_start = tokenizer.position\n        specifier = _parse_specifier(tokenizer)\n        tokenizer.consume(\"WS\")\n\n        if tokenizer.check(\"END\", peek=True):\n            return (url, specifier, marker)\n\n        marker = _parse_requirement_marker(\n            tokenizer,\n            span_start=specifier_start,\n            after=(\n                \"version specifier\"\n                if specifier\n                else \"name and no valid version specifier\"\n            ),\n        )\n\n    return (url, specifier, marker)\n\n\ndef _parse_requirement_marker(\n    tokenizer: Tokenizer, *, span_start: int, after: str\n) -> MarkerList:\n    \"\"\"\n    requirement_marker = SEMICOLON marker WS?\n    \"\"\"\n\n    if not tokenizer.check(\"SEMICOLON\"):\n        tokenizer.raise_syntax_error(\n            f\"Expected end or semicolon (after {after})\",\n            span_start=span_start,\n        )\n    tokenizer.read()\n\n    marker = _parse_marker(tokenizer)\n    tokenizer.consume(\"WS\")\n\n    return marker\n\n\ndef _parse_extras(tokenizer: Tokenizer) -> List[str]:\n    \"\"\"\n    extras = (LEFT_BRACKET wsp* extras_list? wsp* RIGHT_BRACKET)?\n    \"\"\"\n    if not tokenizer.check(\"LEFT_BRACKET\", peek=True):\n        return []\n\n    with tokenizer.enclosing_tokens(\n        \"LEFT_BRACKET\",\n        \"RIGHT_BRACKET\",\n        around=\"extras\",\n    ):\n        tokenizer.consume(\"WS\")\n        extras = _parse_extras_list(tokenizer)\n        tokenizer.consume(\"WS\")\n\n    return extras\n\n\ndef _parse_extras_list(tokenizer: Tokenizer) -> List[str]:\n    \"\"\"\n    extras_list = identifier (wsp* ',' wsp* identifier)*\n    \"\"\"\n    extras: List[str] = []\n\n    if not tokenizer.check(\"IDENTIFIER\"):\n        return extras\n\n    extras.append(tokenizer.read().text)\n\n    while True:\n        tokenizer.consume(\"WS\")\n        if tokenizer.check(\"IDENTIFIER\", peek=True):\n            tokenizer.raise_syntax_error(\"Expected comma between extra names\")\n        elif not tokenizer.check(\"COMMA\"):\n            break\n\n        tokenizer.read()\n        tokenizer.consume(\"WS\")\n\n        extra_token = tokenizer.expect(\"IDENTIFIER\", expected=\"extra name after comma\")\n        extras.append(extra_token.text)\n\n    return extras\n\n\ndef _parse_specifier(tokenizer: Tokenizer) -> str:\n    \"\"\"\n    specifier = LEFT_PARENTHESIS WS? version_many WS? RIGHT_PARENTHESIS\n              | WS? version_many WS?\n    \"\"\"\n    with tokenizer.enclosing_tokens(\n        \"LEFT_PARENTHESIS\",\n        \"RIGHT_PARENTHESIS\",\n        around=\"version specifier\",\n    ):\n        tokenizer.consume(\"WS\")\n        parsed_specifiers = _parse_version_many(tokenizer)\n        tokenizer.consume(\"WS\")\n\n    return parsed_specifiers\n\n\ndef _parse_version_many(tokenizer: Tokenizer) -> str:\n    \"\"\"\n    version_many = (SPECIFIER (WS? COMMA WS? SPECIFIER)*)?\n    \"\"\"\n    parsed_specifiers = \"\"\n    while tokenizer.check(\"SPECIFIER\"):\n        span_start = tokenizer.position\n        parsed_specifiers += tokenizer.read().text\n        if tokenizer.check(\"VERSION_PREFIX_TRAIL\", peek=True):\n            tokenizer.raise_syntax_error(\n                \".* suffix can only be used with `==` or `!=` operators\",\n                span_start=span_start,\n                span_end=tokenizer.position + 1,\n            )\n        if tokenizer.check(\"VERSION_LOCAL_LABEL_TRAIL\", peek=True):\n            tokenizer.raise_syntax_error(\n                \"Local version label can only be used with `==` or `!=` operators\",\n                span_start=span_start,\n                span_end=tokenizer.position,\n            )\n        tokenizer.consume(\"WS\")\n        if not tokenizer.check(\"COMMA\"):\n            break\n        parsed_specifiers += tokenizer.read().text\n        tokenizer.consume(\"WS\")\n\n    return parsed_specifiers\n\n\n# --------------------------------------------------------------------------------------\n# Recursive descent parser for marker expression\n# --------------------------------------------------------------------------------------\ndef parse_marker(source: str) -> MarkerList:\n    return _parse_full_marker(Tokenizer(source, rules=DEFAULT_RULES))\n\n\ndef _parse_full_marker(tokenizer: Tokenizer) -> MarkerList:\n    retval = _parse_marker(tokenizer)\n    tokenizer.expect(\"END\", expected=\"end of marker expression\")\n    return retval\n\n\ndef _parse_marker(tokenizer: Tokenizer) -> MarkerList:\n    \"\"\"\n    marker = marker_atom (BOOLOP marker_atom)+\n    \"\"\"\n    expression = [_parse_marker_atom(tokenizer)]\n    while tokenizer.check(\"BOOLOP\"):\n        token = tokenizer.read()\n        expr_right = _parse_marker_atom(tokenizer)\n        expression.extend((token.text, expr_right))\n    return expression\n\n\ndef _parse_marker_atom(tokenizer: Tokenizer) -> MarkerAtom:\n    \"\"\"\n    marker_atom = WS? LEFT_PARENTHESIS WS? marker WS? RIGHT_PARENTHESIS WS?\n                | WS? marker_item WS?\n    \"\"\"\n\n    tokenizer.consume(\"WS\")\n    if tokenizer.check(\"LEFT_PARENTHESIS\", peek=True):\n        with tokenizer.enclosing_tokens(\n            \"LEFT_PARENTHESIS\",\n            \"RIGHT_PARENTHESIS\",\n            around=\"marker expression\",\n        ):\n            tokenizer.consume(\"WS\")\n            marker: MarkerAtom = _parse_marker(tokenizer)\n            tokenizer.consume(\"WS\")\n    else:\n        marker = _parse_marker_item(tokenizer)\n    tokenizer.consume(\"WS\")\n    return marker\n\n\ndef _parse_marker_item(tokenizer: Tokenizer) -> MarkerItem:\n    \"\"\"\n    marker_item = WS? marker_var WS? marker_op WS? marker_var WS?\n    \"\"\"\n    tokenizer.consume(\"WS\")\n    marker_var_left = _parse_marker_var(tokenizer)\n    tokenizer.consume(\"WS\")\n    marker_op = _parse_marker_op(tokenizer)\n    tokenizer.consume(\"WS\")\n    marker_var_right = _parse_marker_var(tokenizer)\n    tokenizer.consume(\"WS\")\n    return (marker_var_left, marker_op, marker_var_right)\n\n\ndef _parse_marker_var(tokenizer: Tokenizer) -> MarkerVar:\n    \"\"\"\n    marker_var = VARIABLE | QUOTED_STRING\n    \"\"\"\n    if tokenizer.check(\"VARIABLE\"):\n        return process_env_var(tokenizer.read().text.replace(\".\", \"_\"))\n    elif tokenizer.check(\"QUOTED_STRING\"):\n        return process_python_str(tokenizer.read().text)\n    else:\n        tokenizer.raise_syntax_error(\n            message=\"Expected a marker variable or quoted string\"\n        )\n\n\ndef process_env_var(env_var: str) -> Variable:\n    if env_var in (\"platform_python_implementation\", \"python_implementation\"):\n        return Variable(\"platform_python_implementation\")\n    else:\n        return Variable(env_var)\n\n\ndef process_python_str(python_str: str) -> Value:\n    value = ast.literal_eval(python_str)\n    return Value(str(value))\n\n\ndef _parse_marker_op(tokenizer: Tokenizer) -> Op:\n    \"\"\"\n    marker_op = IN | NOT IN | OP\n    \"\"\"\n    if tokenizer.check(\"IN\"):\n        tokenizer.read()\n        return Op(\"in\")\n    elif tokenizer.check(\"NOT\"):\n        tokenizer.read()\n        tokenizer.expect(\"WS\", expected=\"whitespace after 'not'\")\n        tokenizer.expect(\"IN\", expected=\"'in' after 'not'\")\n        return Op(\"not in\")\n    elif tokenizer.check(\"OP\"):\n        return Op(tokenizer.read().text)\n    else:\n        return tokenizer.raise_syntax_error(\n            \"Expected marker operator, one of \"\n            \"<=, <, !=, ==, >=, >, ~=, ===, in, not in\"\n        )\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/_vendor/wheel/vendored/packaging/_structures.py","size":1431,"sha1":"fe0c3747cf14e696276cb6806c6775503de002b8","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"# This file is dual licensed under the terms of the Apache License, Version\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\n# for complete details.\n\n\nclass InfinityType:\n    def __repr__(self) -> str:\n        return \"Infinity\"\n\n    def __hash__(self) -> int:\n        return hash(repr(self))\n\n    def __lt__(self, other: object) -> bool:\n        return False\n\n    def __le__(self, other: object) -> bool:\n        return False\n\n    def __eq__(self, other: object) -> bool:\n        return isinstance(other, self.__class__)\n\n    def __gt__(self, other: object) -> bool:\n        return True\n\n    def __ge__(self, other: object) -> bool:\n        return True\n\n    def __neg__(self: object) -> \"NegativeInfinityType\":\n        return NegativeInfinity\n\n\nInfinity = InfinityType()\n\n\nclass NegativeInfinityType:\n    def __repr__(self) -> str:\n        return \"-Infinity\"\n\n    def __hash__(self) -> int:\n        return hash(repr(self))\n\n    def __lt__(self, other: object) -> bool:\n        return True\n\n    def __le__(self, other: object) -> bool:\n        return True\n\n    def __eq__(self, other: object) -> bool:\n        return isinstance(other, self.__class__)\n\n    def __gt__(self, other: object) -> bool:\n        return False\n\n    def __ge__(self, other: object) -> bool:\n        return False\n\n    def __neg__(self: object) -> InfinityType:\n        return Infinity\n\n\nNegativeInfinity = NegativeInfinityType()\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py","size":5292,"sha1":"7e894dba389a70c4e5e3916705b5525788066a62","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import contextlib\nimport re\nfrom dataclasses import dataclass\nfrom typing import Dict, Iterator, NoReturn, Optional, Tuple, Union\n\nfrom .specifiers import Specifier\n\n\n@dataclass\nclass Token:\n    name: str\n    text: str\n    position: int\n\n\nclass ParserSyntaxError(Exception):\n    \"\"\"The provided source text could not be parsed correctly.\"\"\"\n\n    def __init__(\n        self,\n        message: str,\n        *,\n        source: str,\n        span: Tuple[int, int],\n    ) -> None:\n        self.span = span\n        self.message = message\n        self.source = source\n\n        super().__init__()\n\n    def __str__(self) -> str:\n        marker = \" \" * self.span[0] + \"~\" * (self.span[1] - self.span[0]) + \"^\"\n        return \"\\n    \".join([self.message, self.source, marker])\n\n\nDEFAULT_RULES: \"Dict[str, Union[str, re.Pattern[str]]]\" = {\n    \"LEFT_PARENTHESIS\": r\"\\(\",\n    \"RIGHT_PARENTHESIS\": r\"\\)\",\n    \"LEFT_BRACKET\": r\"\\[\",\n    \"RIGHT_BRACKET\": r\"\\]\",\n    \"SEMICOLON\": r\";\",\n    \"COMMA\": r\",\",\n    \"QUOTED_STRING\": re.compile(\n        r\"\"\"\n            (\n                ('[^']*')\n                |\n                (\"[^\"]*\")\n            )\n        \"\"\",\n        re.VERBOSE,\n    ),\n    \"OP\": r\"(===|==|~=|!=|<=|>=|<|>)\",\n    \"BOOLOP\": r\"\\b(or|and)\\b\",\n    \"IN\": r\"\\bin\\b\",\n    \"NOT\": r\"\\bnot\\b\",\n    \"VARIABLE\": re.compile(\n        r\"\"\"\n            \\b(\n                python_version\n                |python_full_version\n                |os[._]name\n                |sys[._]platform\n                |platform_(release|system)\n                |platform[._](version|machine|python_implementation)\n                |python_implementation\n                |implementation_(name|version)\n                |extra\n            )\\b\n        \"\"\",\n        re.VERBOSE,\n    ),\n    \"SPECIFIER\": re.compile(\n        Specifier._operator_regex_str + Specifier._version_regex_str,\n        re.VERBOSE | re.IGNORECASE,\n    ),\n    \"AT\": r\"\\@\",\n    \"URL\": r\"[^ \\t]+\",\n    \"IDENTIFIER\": r\"\\b[a-zA-Z0-9][a-zA-Z0-9._-]*\\b\",\n    \"VERSION_PREFIX_TRAIL\": r\"\\.\\*\",\n    \"VERSION_LOCAL_LABEL_TRAIL\": r\"\\+[a-z0-9]+(?:[-_\\.][a-z0-9]+)*\",\n    \"WS\": r\"[ \\t]+\",\n    \"END\": r\"$\",\n}\n\n\nclass Tokenizer:\n    \"\"\"Context-sensitive token parsing.\n\n    Provides methods to examine the input stream to check whether the next token\n    matches.\n    \"\"\"\n\n    def __init__(\n        self,\n        source: str,\n        *,\n        rules: \"Dict[str, Union[str, re.Pattern[str]]]\",\n    ) -> None:\n        self.source = source\n        self.rules: Dict[str, re.Pattern[str]] = {\n            name: re.compile(pattern) for name, pattern in rules.items()\n        }\n        self.next_token: Optional[Token] = None\n        self.position = 0\n\n    def consume(self, name: str) -> None:\n        \"\"\"Move beyond provided token name, if at current position.\"\"\"\n        if self.check(name):\n            self.read()\n\n    def check(self, name: str, *, peek: bool = False) -> bool:\n        \"\"\"Check whether the next token has the provided name.\n\n        By default, if the check succeeds, the token *must* be read before\n        another check. If `peek` is set to `True`, the token is not loaded and\n        would need to be checked again.\n        \"\"\"\n        assert (\n            self.next_token is None\n        ), f\"Cannot check for {name!r}, already have {self.next_token!r}\"\n        assert name in self.rules, f\"Unknown token name: {name!r}\"\n\n        expression = self.rules[name]\n\n        match = expression.match(self.source, self.position)\n        if match is None:\n            return False\n        if not peek:\n            self.next_token = Token(name, match[0], self.position)\n        return True\n\n    def expect(self, name: str, *, expected: str) -> Token:\n        \"\"\"Expect a certain token name next, failing with a syntax error otherwise.\n\n        The token is *not* read.\n        \"\"\"\n        if not self.check(name):\n            raise self.raise_syntax_error(f\"Expected {expected}\")\n        return self.read()\n\n    def read(self) -> Token:\n        \"\"\"Consume the next token and return it.\"\"\"\n        token = self.next_token\n        assert token is not None\n\n        self.position += len(token.text)\n        self.next_token = None\n\n        return token\n\n    def raise_syntax_error(\n        self,\n        message: str,\n        *,\n        span_start: Optional[int] = None,\n        span_end: Optional[int] = None,\n    ) -> NoReturn:\n        \"\"\"Raise ParserSyntaxError at the given position.\"\"\"\n        span = (\n            self.position if span_start is None else span_start,\n            self.position if span_end is None else span_end,\n        )\n        raise ParserSyntaxError(\n            message,\n            source=self.source,\n            span=span,\n        )\n\n    @contextlib.contextmanager\n    def enclosing_tokens(\n        self, open_token: str, close_token: str, *, around: str\n    ) -> Iterator[None]:\n        if self.check(open_token):\n            open_position = self.position\n            self.read()\n        else:\n            open_position = None\n\n        yield\n\n        if open_position is None:\n            return\n\n        if not self.check(close_token):\n            self.raise_syntax_error(\n                f\"Expected matching {close_token} for {open_token}, after {around}\",\n                span_start=open_position,\n            )\n\n        self.read()\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/_vendor/wheel/vendored/packaging/markers.py","size":8232,"sha1":"bcbf9f914c03309d2e07d05b0e4d277d13b11411","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"# This file is dual licensed under the terms of the Apache License, Version\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\n# for complete details.\n\nimport operator\nimport os\nimport platform\nimport sys\nfrom typing import Any, Callable, Dict, List, Optional, Tuple, Union\n\nfrom ._parser import (\n    MarkerAtom,\n    MarkerList,\n    Op,\n    Value,\n    Variable,\n)\nfrom ._parser import (\n    parse_marker as _parse_marker,\n)\nfrom ._tokenizer import ParserSyntaxError\nfrom .specifiers import InvalidSpecifier, Specifier\nfrom .utils import canonicalize_name\n\n__all__ = [\n    \"InvalidMarker\",\n    \"UndefinedComparison\",\n    \"UndefinedEnvironmentName\",\n    \"Marker\",\n    \"default_environment\",\n]\n\nOperator = Callable[[str, str], bool]\n\n\nclass InvalidMarker(ValueError):\n    \"\"\"\n    An invalid marker was found, users should refer to PEP 508.\n    \"\"\"\n\n\nclass UndefinedComparison(ValueError):\n    \"\"\"\n    An invalid operation was attempted on a value that doesn't support it.\n    \"\"\"\n\n\nclass UndefinedEnvironmentName(ValueError):\n    \"\"\"\n    A name was attempted to be used that does not exist inside of the\n    environment.\n    \"\"\"\n\n\ndef _normalize_extra_values(results: Any) -> Any:\n    \"\"\"\n    Normalize extra values.\n    \"\"\"\n    if isinstance(results[0], tuple):\n        lhs, op, rhs = results[0]\n        if isinstance(lhs, Variable) and lhs.value == \"extra\":\n            normalized_extra = canonicalize_name(rhs.value)\n            rhs = Value(normalized_extra)\n        elif isinstance(rhs, Variable) and rhs.value == \"extra\":\n            normalized_extra = canonicalize_name(lhs.value)\n            lhs = Value(normalized_extra)\n        results[0] = lhs, op, rhs\n    return results\n\n\ndef _format_marker(\n    marker: Union[List[str], MarkerAtom, str], first: Optional[bool] = True\n) -> str:\n    assert isinstance(marker, (list, tuple, str))\n\n    # Sometimes we have a structure like [[...]] which is a single item list\n    # where the single item is itself it's own list. In that case we want skip\n    # the rest of this function so that we don't get extraneous () on the\n    # outside.\n    if (\n        isinstance(marker, list)\n        and len(marker) == 1\n        and isinstance(marker[0], (list, tuple))\n    ):\n        return _format_marker(marker[0])\n\n    if isinstance(marker, list):\n        inner = (_format_marker(m, first=False) for m in marker)\n        if first:\n            return \" \".join(inner)\n        else:\n            return \"(\" + \" \".join(inner) + \")\"\n    elif isinstance(marker, tuple):\n        return \" \".join([m.serialize() for m in marker])\n    else:\n        return marker\n\n\n_operators: Dict[str, Operator] = {\n    \"in\": lambda lhs, rhs: lhs in rhs,\n    \"not in\": lambda lhs, rhs: lhs not in rhs,\n    \"<\": operator.lt,\n    \"<=\": operator.le,\n    \"==\": operator.eq,\n    \"!=\": operator.ne,\n    \">=\": operator.ge,\n    \">\": operator.gt,\n}\n\n\ndef _eval_op(lhs: str, op: Op, rhs: str) -> bool:\n    try:\n        spec = Specifier(\"\".join([op.serialize(), rhs]))\n    except InvalidSpecifier:\n        pass\n    else:\n        return spec.contains(lhs, prereleases=True)\n\n    oper: Optional[Operator] = _operators.get(op.serialize())\n    if oper is None:\n        raise UndefinedComparison(f\"Undefined {op!r} on {lhs!r} and {rhs!r}.\")\n\n    return oper(lhs, rhs)\n\n\ndef _normalize(*values: str, key: str) -> Tuple[str, ...]:\n    # PEP 685  Comparison of extra names for optional distribution dependencies\n    # https://peps.python.org/pep-0685/\n    # > When comparing extra names, tools MUST normalize the names being\n    # > compared using the semantics outlined in PEP 503 for names\n    if key == \"extra\":\n        return tuple(canonicalize_name(v) for v in values)\n\n    # other environment markers don't have such standards\n    return values\n\n\ndef _evaluate_markers(markers: MarkerList, environment: Dict[str, str]) -> bool:\n    groups: List[List[bool]] = [[]]\n\n    for marker in markers:\n        assert isinstance(marker, (list, tuple, str))\n\n        if isinstance(marker, list):\n            groups[-1].append(_evaluate_markers(marker, environment))\n        elif isinstance(marker, tuple):\n            lhs, op, rhs = marker\n\n            if isinstance(lhs, Variable):\n                environment_key = lhs.value\n                lhs_value = environment[environment_key]\n                rhs_value = rhs.value\n            else:\n                lhs_value = lhs.value\n                environment_key = rhs.value\n                rhs_value = environment[environment_key]\n\n            lhs_value, rhs_value = _normalize(lhs_value, rhs_value, key=environment_key)\n            groups[-1].append(_eval_op(lhs_value, op, rhs_value))\n        else:\n            assert marker in [\"and\", \"or\"]\n            if marker == \"or\":\n                groups.append([])\n\n    return any(all(item) for item in groups)\n\n\ndef format_full_version(info: \"sys._version_info\") -> str:\n    version = \"{0.major}.{0.minor}.{0.micro}\".format(info)\n    kind = info.releaselevel\n    if kind != \"final\":\n        version += kind[0] + str(info.serial)\n    return version\n\n\ndef default_environment() -> Dict[str, str]:\n    iver = format_full_version(sys.implementation.version)\n    implementation_name = sys.implementation.name\n    return {\n        \"implementation_name\": implementation_name,\n        \"implementation_version\": iver,\n        \"os_name\": os.name,\n        \"platform_machine\": platform.machine(),\n        \"platform_release\": platform.release(),\n        \"platform_system\": platform.system(),\n        \"platform_version\": platform.version(),\n        \"python_full_version\": platform.python_version(),\n        \"platform_python_implementation\": platform.python_implementation(),\n        \"python_version\": \".\".join(platform.python_version_tuple()[:2]),\n        \"sys_platform\": sys.platform,\n    }\n\n\nclass Marker:\n    def __init__(self, marker: str) -> None:\n        # Note: We create a Marker object without calling this constructor in\n        #       packaging.requirements.Requirement. If any additional logic is\n        #       added here, make sure to mirror/adapt Requirement.\n        try:\n            self._markers = _normalize_extra_values(_parse_marker(marker))\n            # The attribute `_markers` can be described in terms of a recursive type:\n            # MarkerList = List[Union[Tuple[Node, ...], str, MarkerList]]\n            #\n            # For example, the following expression:\n            # python_version > \"3.6\" or (python_version == \"3.6\" and os_name == \"unix\")\n            #\n            # is parsed into:\n            # [\n            #     (<Variable('python_version')>, <Op('>')>, <Value('3.6')>),\n            #     'and',\n            #     [\n            #         (<Variable('python_version')>, <Op('==')>, <Value('3.6')>),\n            #         'or',\n            #         (<Variable('os_name')>, <Op('==')>, <Value('unix')>)\n            #     ]\n            # ]\n        except ParserSyntaxError as e:\n            raise InvalidMarker(str(e)) from e\n\n    def __str__(self) -> str:\n        return _format_marker(self._markers)\n\n    def __repr__(self) -> str:\n        return f\"<Marker('{self}')>\"\n\n    def __hash__(self) -> int:\n        return hash((self.__class__.__name__, str(self)))\n\n    def __eq__(self, other: Any) -> bool:\n        if not isinstance(other, Marker):\n            return NotImplemented\n\n        return str(self) == str(other)\n\n    def evaluate(self, environment: Optional[Dict[str, str]] = None) -> bool:\n        \"\"\"Evaluate a marker.\n\n        Return the boolean from evaluating the given marker against the\n        environment. environment is an optional argument to override all or\n        part of the determined environment.\n\n        The environment is determined from the current Python process.\n        \"\"\"\n        current_environment = default_environment()\n        current_environment[\"extra\"] = \"\"\n        if environment is not None:\n            current_environment.update(environment)\n            # The API used to allow setting extra to None. We need to handle this\n            # case for backwards compatibility.\n            if current_environment[\"extra\"] is None:\n                current_environment[\"extra\"] = \"\"\n\n        return _evaluate_markers(self._markers, current_environment)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/_vendor/wheel/vendored/packaging/requirements.py","size":2933,"sha1":"fdea96ee084c035525face93abe03e82d55a8f8e","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"# This file is dual licensed under the terms of the Apache License, Version\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\n# for complete details.\n\nfrom typing import Any, Iterator, Optional, Set\n\nfrom ._parser import parse_requirement as _parse_requirement\nfrom ._tokenizer import ParserSyntaxError\nfrom .markers import Marker, _normalize_extra_values\nfrom .specifiers import SpecifierSet\nfrom .utils import canonicalize_name\n\n\nclass InvalidRequirement(ValueError):\n    \"\"\"\n    An invalid requirement was found, users should refer to PEP 508.\n    \"\"\"\n\n\nclass Requirement:\n    \"\"\"Parse a requirement.\n\n    Parse a given requirement string into its parts, such as name, specifier,\n    URL, and extras. Raises InvalidRequirement on a badly-formed requirement\n    string.\n    \"\"\"\n\n    # TODO: Can we test whether something is contained within a requirement?\n    #       If so how do we do that? Do we need to test against the _name_ of\n    #       the thing as well as the version? What about the markers?\n    # TODO: Can we normalize the name and extra name?\n\n    def __init__(self, requirement_string: str) -> None:\n        try:\n            parsed = _parse_requirement(requirement_string)\n        except ParserSyntaxError as e:\n            raise InvalidRequirement(str(e)) from e\n\n        self.name: str = parsed.name\n        self.url: Optional[str] = parsed.url or None\n        self.extras: Set[str] = set(parsed.extras or [])\n        self.specifier: SpecifierSet = SpecifierSet(parsed.specifier)\n        self.marker: Optional[Marker] = None\n        if parsed.marker is not None:\n            self.marker = Marker.__new__(Marker)\n            self.marker._markers = _normalize_extra_values(parsed.marker)\n\n    def _iter_parts(self, name: str) -> Iterator[str]:\n        yield name\n\n        if self.extras:\n            formatted_extras = \",\".join(sorted(self.extras))\n            yield f\"[{formatted_extras}]\"\n\n        if self.specifier:\n            yield str(self.specifier)\n\n        if self.url:\n            yield f\"@ {self.url}\"\n            if self.marker:\n                yield \" \"\n\n        if self.marker:\n            yield f\"; {self.marker}\"\n\n    def __str__(self) -> str:\n        return \"\".join(self._iter_parts(self.name))\n\n    def __repr__(self) -> str:\n        return f\"<Requirement('{self}')>\"\n\n    def __hash__(self) -> int:\n        return hash(\n            (\n                self.__class__.__name__,\n                *self._iter_parts(canonicalize_name(self.name)),\n            )\n        )\n\n    def __eq__(self, other: Any) -> bool:\n        if not isinstance(other, Requirement):\n            return NotImplemented\n\n        return (\n            canonicalize_name(self.name) == canonicalize_name(other.name)\n            and self.extras == other.extras\n            and self.specifier == other.specifier\n            and self.url == other.url\n            and self.marker == other.marker\n        )\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/_vendor/wheel/vendored/packaging/specifiers.py","size":39778,"sha1":"dcbbddd7b97bcbeefa4e78ad4654cd4c7c2c1c1a","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"# This file is dual licensed under the terms of the Apache License, Version\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\n# for complete details.\n\"\"\"\n.. testsetup::\n\n    from packaging.specifiers import Specifier, SpecifierSet, InvalidSpecifier\n    from packaging.version import Version\n\"\"\"\n\nimport abc\nimport itertools\nimport re\nfrom typing import Callable, Iterable, Iterator, List, Optional, Tuple, TypeVar, Union\n\nfrom .utils import canonicalize_version\nfrom .version import Version\n\nUnparsedVersion = Union[Version, str]\nUnparsedVersionVar = TypeVar(\"UnparsedVersionVar\", bound=UnparsedVersion)\nCallableOperator = Callable[[Version, str], bool]\n\n\ndef _coerce_version(version: UnparsedVersion) -> Version:\n    if not isinstance(version, Version):\n        version = Version(version)\n    return version\n\n\nclass InvalidSpecifier(ValueError):\n    \"\"\"\n    Raised when attempting to create a :class:`Specifier` with a specifier\n    string that is invalid.\n\n    >>> Specifier(\"lolwat\")\n    Traceback (most recent call last):\n        ...\n    packaging.specifiers.InvalidSpecifier: Invalid specifier: 'lolwat'\n    \"\"\"\n\n\nclass BaseSpecifier(metaclass=abc.ABCMeta):\n    @abc.abstractmethod\n    def __str__(self) -> str:\n        \"\"\"\n        Returns the str representation of this Specifier-like object. This\n        should be representative of the Specifier itself.\n        \"\"\"\n\n    @abc.abstractmethod\n    def __hash__(self) -> int:\n        \"\"\"\n        Returns a hash value for this Specifier-like object.\n        \"\"\"\n\n    @abc.abstractmethod\n    def __eq__(self, other: object) -> bool:\n        \"\"\"\n        Returns a boolean representing whether or not the two Specifier-like\n        objects are equal.\n\n        :param other: The other object to check against.\n        \"\"\"\n\n    @property\n    @abc.abstractmethod\n    def prereleases(self) -> Optional[bool]:\n        \"\"\"Whether or not pre-releases as a whole are allowed.\n\n        This can be set to either ``True`` or ``False`` to explicitly enable or disable\n        prereleases or it can be set to ``None`` (the default) to use default semantics.\n        \"\"\"\n\n    @prereleases.setter\n    def prereleases(self, value: bool) -> None:\n        \"\"\"Setter for :attr:`prereleases`.\n\n        :param value: The value to set.\n        \"\"\"\n\n    @abc.abstractmethod\n    def contains(self, item: str, prereleases: Optional[bool] = None) -> bool:\n        \"\"\"\n        Determines if the given item is contained within this specifier.\n        \"\"\"\n\n    @abc.abstractmethod\n    def filter(\n        self, iterable: Iterable[UnparsedVersionVar], prereleases: Optional[bool] = None\n    ) -> Iterator[UnparsedVersionVar]:\n        \"\"\"\n        Takes an iterable of items and filters them so that only items which\n        are contained within this specifier are allowed in it.\n        \"\"\"\n\n\nclass Specifier(BaseSpecifier):\n    \"\"\"This class abstracts handling of version specifiers.\n\n    .. tip::\n\n        It is generally not required to instantiate this manually. You should instead\n        prefer to work with :class:`SpecifierSet` instead, which can parse\n        comma-separated version specifiers (which is what package metadata contains).\n    \"\"\"\n\n    _operator_regex_str = r\"\"\"\n        (?P<operator>(~=|==|!=|<=|>=|<|>|===))\n        \"\"\"\n    _version_regex_str = r\"\"\"\n        (?P<version>\n            (?:\n                # The identity operators allow for an escape hatch that will\n                # do an exact string match of the version you wish to install.\n                # This will not be parsed by PEP 440 and we cannot determine\n                # any semantic meaning from it. This operator is discouraged\n                # but included entirely as an escape hatch.\n                (?<====)  # Only match for the identity operator\n                \\s*\n                [^\\s;)]*  # The arbitrary version can be just about anything,\n                          # we match everything except for whitespace, a\n                          # semi-colon for marker support, and a closing paren\n                          # since versions can be enclosed in them.\n            )\n            |\n            (?:\n                # The (non)equality operators allow for wild card and local\n                # versions to be specified so we have to define these two\n                # operators separately to enable that.\n                (?<===|!=)            # Only match for equals and not equals\n\n                \\s*\n                v?\n                (?:[0-9]+!)?          # epoch\n                [0-9]+(?:\\.[0-9]+)*   # release\n\n                # You cannot use a wild card and a pre-release, post-release, a dev or\n                # local version together so group them with a | and make them optional.\n                (?:\n                    \\.\\*  # Wild card syntax of .*\n                    |\n                    (?:                                  # pre release\n                        [-_\\.]?\n                        (alpha|beta|preview|pre|a|b|c|rc)\n                        [-_\\.]?\n                        [0-9]*\n                    )?\n                    (?:                                  # post release\n                        (?:-[0-9]+)|(?:[-_\\.]?(post|rev|r)[-_\\.]?[0-9]*)\n                    )?\n                    (?:[-_\\.]?dev[-_\\.]?[0-9]*)?         # dev release\n                    (?:\\+[a-z0-9]+(?:[-_\\.][a-z0-9]+)*)? # local\n                )?\n            )\n            |\n            (?:\n                # The compatible operator requires at least two digits in the\n                # release segment.\n                (?<=~=)               # Only match for the compatible operator\n\n                \\s*\n                v?\n                (?:[0-9]+!)?          # epoch\n                [0-9]+(?:\\.[0-9]+)+   # release  (We have a + instead of a *)\n                (?:                   # pre release\n                    [-_\\.]?\n                    (alpha|beta|preview|pre|a|b|c|rc)\n                    [-_\\.]?\n                    [0-9]*\n                )?\n                (?:                                   # post release\n                    (?:-[0-9]+)|(?:[-_\\.]?(post|rev|r)[-_\\.]?[0-9]*)\n                )?\n                (?:[-_\\.]?dev[-_\\.]?[0-9]*)?          # dev release\n            )\n            |\n            (?:\n                # All other operators only allow a sub set of what the\n                # (non)equality operators do. Specifically they do not allow\n                # local versions to be specified nor do they allow the prefix\n                # matching wild cards.\n                (?<!==|!=|~=)         # We have special cases for these\n                                      # operators so we want to make sure they\n                                      # don't match here.\n\n                \\s*\n                v?\n                (?:[0-9]+!)?          # epoch\n                [0-9]+(?:\\.[0-9]+)*   # release\n                (?:                   # pre release\n                    [-_\\.]?\n                    (alpha|beta|preview|pre|a|b|c|rc)\n                    [-_\\.]?\n                    [0-9]*\n                )?\n                (?:                                   # post release\n                    (?:-[0-9]+)|(?:[-_\\.]?(post|rev|r)[-_\\.]?[0-9]*)\n                )?\n                (?:[-_\\.]?dev[-_\\.]?[0-9]*)?          # dev release\n            )\n        )\n        \"\"\"\n\n    _regex = re.compile(\n        r\"^\\s*\" + _operator_regex_str + _version_regex_str + r\"\\s*$\",\n        re.VERBOSE | re.IGNORECASE,\n    )\n\n    _operators = {\n        \"~=\": \"compatible\",\n        \"==\": \"equal\",\n        \"!=\": \"not_equal\",\n        \"<=\": \"less_than_equal\",\n        \">=\": \"greater_than_equal\",\n        \"<\": \"less_than\",\n        \">\": \"greater_than\",\n        \"===\": \"arbitrary\",\n    }\n\n    def __init__(self, spec: str = \"\", prereleases: Optional[bool] = None) -> None:\n        \"\"\"Initialize a Specifier instance.\n\n        :param spec:\n            The string representation of a specifier which will be parsed and\n            normalized before use.\n        :param prereleases:\n            This tells the specifier if it should accept prerelease versions if\n            applicable or not. The default of ``None`` will autodetect it from the\n            given specifiers.\n        :raises InvalidSpecifier:\n            If the given specifier is invalid (i.e. bad syntax).\n        \"\"\"\n        match = self._regex.search(spec)\n        if not match:\n            raise InvalidSpecifier(f\"Invalid specifier: '{spec}'\")\n\n        self._spec: Tuple[str, str] = (\n            match.group(\"operator\").strip(),\n            match.group(\"version\").strip(),\n        )\n\n        # Store whether or not this Specifier should accept prereleases\n        self._prereleases = prereleases\n\n    # https://github.com/python/mypy/pull/13475#pullrequestreview-1079784515\n    @property  # type: ignore[override]\n    def prereleases(self) -> bool:\n        # If there is an explicit prereleases set for this, then we'll just\n        # blindly use that.\n        if self._prereleases is not None:\n            return self._prereleases\n\n        # Look at all of our specifiers and determine if they are inclusive\n        # operators, and if they are if they are including an explicit\n        # prerelease.\n        operator, version = self._spec\n        if operator in [\"==\", \">=\", \"<=\", \"~=\", \"===\"]:\n            # The == specifier can include a trailing .*, if it does we\n            # want to remove before parsing.\n            if operator == \"==\" and version.endswith(\".*\"):\n                version = version[:-2]\n\n            # Parse the version, and if it is a pre-release than this\n            # specifier allows pre-releases.\n            if Version(version).is_prerelease:\n                return True\n\n        return False\n\n    @prereleases.setter\n    def prereleases(self, value: bool) -> None:\n        self._prereleases = value\n\n    @property\n    def operator(self) -> str:\n        \"\"\"The operator of this specifier.\n\n        >>> Specifier(\"==1.2.3\").operator\n        '=='\n        \"\"\"\n        return self._spec[0]\n\n    @property\n    def version(self) -> str:\n        \"\"\"The version of this specifier.\n\n        >>> Specifier(\"==1.2.3\").version\n        '1.2.3'\n        \"\"\"\n        return self._spec[1]\n\n    def __repr__(self) -> str:\n        \"\"\"A representation of the Specifier that shows all internal state.\n\n        >>> Specifier('>=1.0.0')\n        <Specifier('>=1.0.0')>\n        >>> Specifier('>=1.0.0', prereleases=False)\n        <Specifier('>=1.0.0', prereleases=False)>\n        >>> Specifier('>=1.0.0', prereleases=True)\n        <Specifier('>=1.0.0', prereleases=True)>\n        \"\"\"\n        pre = (\n            f\", prereleases={self.prereleases!r}\"\n            if self._prereleases is not None\n            else \"\"\n        )\n\n        return f\"<{self.__class__.__name__}({str(self)!r}{pre})>\"\n\n    def __str__(self) -> str:\n        \"\"\"A string representation of the Specifier that can be round-tripped.\n\n        >>> str(Specifier('>=1.0.0'))\n        '>=1.0.0'\n        >>> str(Specifier('>=1.0.0', prereleases=False))\n        '>=1.0.0'\n        \"\"\"\n        return \"{}{}\".format(*self._spec)\n\n    @property\n    def _canonical_spec(self) -> Tuple[str, str]:\n        canonical_version = canonicalize_version(\n            self._spec[1],\n            strip_trailing_zero=(self._spec[0] != \"~=\"),\n        )\n        return self._spec[0], canonical_version\n\n    def __hash__(self) -> int:\n        return hash(self._canonical_spec)\n\n    def __eq__(self, other: object) -> bool:\n        \"\"\"Whether or not the two Specifier-like objects are equal.\n\n        :param other: The other object to check against.\n\n        The value of :attr:`prereleases` is ignored.\n\n        >>> Specifier(\"==1.2.3\") == Specifier(\"== 1.2.3.0\")\n        True\n        >>> (Specifier(\"==1.2.3\", prereleases=False) ==\n        ...  Specifier(\"==1.2.3\", prereleases=True))\n        True\n        >>> Specifier(\"==1.2.3\") == \"==1.2.3\"\n        True\n        >>> Specifier(\"==1.2.3\") == Specifier(\"==1.2.4\")\n        False\n        >>> Specifier(\"==1.2.3\") == Specifier(\"~=1.2.3\")\n        False\n        \"\"\"\n        if isinstance(other, str):\n            try:\n                other = self.__class__(str(other))\n            except InvalidSpecifier:\n                return NotImplemented\n        elif not isinstance(other, self.__class__):\n            return NotImplemented\n\n        return self._canonical_spec == other._canonical_spec\n\n    def _get_operator(self, op: str) -> CallableOperator:\n        operator_callable: CallableOperator = getattr(\n            self, f\"_compare_{self._operators[op]}\"\n        )\n        return operator_callable\n\n    def _compare_compatible(self, prospective: Version, spec: str) -> bool:\n        # Compatible releases have an equivalent combination of >= and ==. That\n        # is that ~=2.2 is equivalent to >=2.2,==2.*. This allows us to\n        # implement this in terms of the other specifiers instead of\n        # implementing it ourselves. The only thing we need to do is construct\n        # the other specifiers.\n\n        # We want everything but the last item in the version, but we want to\n        # ignore suffix segments.\n        prefix = _version_join(\n            list(itertools.takewhile(_is_not_suffix, _version_split(spec)))[:-1]\n        )\n\n        # Add the prefix notation to the end of our string\n        prefix += \".*\"\n\n        return self._get_operator(\">=\")(prospective, spec) and self._get_operator(\"==\")(\n            prospective, prefix\n        )\n\n    def _compare_equal(self, prospective: Version, spec: str) -> bool:\n        # We need special logic to handle prefix matching\n        if spec.endswith(\".*\"):\n            # In the case of prefix matching we want to ignore local segment.\n            normalized_prospective = canonicalize_version(\n                prospective.public, strip_trailing_zero=False\n            )\n            # Get the normalized version string ignoring the trailing .*\n            normalized_spec = canonicalize_version(spec[:-2], strip_trailing_zero=False)\n            # Split the spec out by bangs and dots, and pretend that there is\n            # an implicit dot in between a release segment and a pre-release segment.\n            split_spec = _version_split(normalized_spec)\n\n            # Split the prospective version out by bangs and dots, and pretend\n            # that there is an implicit dot in between a release segment and\n            # a pre-release segment.\n            split_prospective = _version_split(normalized_prospective)\n\n            # 0-pad the prospective version before shortening it to get the correct\n            # shortened version.\n            padded_prospective, _ = _pad_version(split_prospective, split_spec)\n\n            # Shorten the prospective version to be the same length as the spec\n            # so that we can determine if the specifier is a prefix of the\n            # prospective version or not.\n            shortened_prospective = padded_prospective[: len(split_spec)]\n\n            return shortened_prospective == split_spec\n        else:\n            # Convert our spec string into a Version\n            spec_version = Version(spec)\n\n            # If the specifier does not have a local segment, then we want to\n            # act as if the prospective version also does not have a local\n            # segment.\n            if not spec_version.local:\n                prospective = Version(prospective.public)\n\n            return prospective == spec_version\n\n    def _compare_not_equal(self, prospective: Version, spec: str) -> bool:\n        return not self._compare_equal(prospective, spec)\n\n    def _compare_less_than_equal(self, prospective: Version, spec: str) -> bool:\n        # NB: Local version identifiers are NOT permitted in the version\n        # specifier, so local version labels can be universally removed from\n        # the prospective version.\n        return Version(prospective.public) <= Version(spec)\n\n    def _compare_greater_than_equal(self, prospective: Version, spec: str) -> bool:\n        # NB: Local version identifiers are NOT permitted in the version\n        # specifier, so local version labels can be universally removed from\n        # the prospective version.\n        return Version(prospective.public) >= Version(spec)\n\n    def _compare_less_than(self, prospective: Version, spec_str: str) -> bool:\n        # Convert our spec to a Version instance, since we'll want to work with\n        # it as a version.\n        spec = Version(spec_str)\n\n        # Check to see if the prospective version is less than the spec\n        # version. If it's not we can short circuit and just return False now\n        # instead of doing extra unneeded work.\n        if not prospective < spec:\n            return False\n\n        # This special case is here so that, unless the specifier itself\n        # includes is a pre-release version, that we do not accept pre-release\n        # versions for the version mentioned in the specifier (e.g. <3.1 should\n        # not match 3.1.dev0, but should match 3.0.dev0).\n        if not spec.is_prerelease and prospective.is_prerelease:\n            if Version(prospective.base_version) == Version(spec.base_version):\n                return False\n\n        # If we've gotten to here, it means that prospective version is both\n        # less than the spec version *and* it's not a pre-release of the same\n        # version in the spec.\n        return True\n\n    def _compare_greater_than(self, prospective: Version, spec_str: str) -> bool:\n        # Convert our spec to a Version instance, since we'll want to work with\n        # it as a version.\n        spec = Version(spec_str)\n\n        # Check to see if the prospective version is greater than the spec\n        # version. If it's not we can short circuit and just return False now\n        # instead of doing extra unneeded work.\n        if not prospective > spec:\n            return False\n\n        # This special case is here so that, unless the specifier itself\n        # includes is a post-release version, that we do not accept\n        # post-release versions for the version mentioned in the specifier\n        # (e.g. >3.1 should not match 3.0.post0, but should match 3.2.post0).\n        if not spec.is_postrelease and prospective.is_postrelease:\n            if Version(prospective.base_version) == Version(spec.base_version):\n                return False\n\n        # Ensure that we do not allow a local version of the version mentioned\n        # in the specifier, which is technically greater than, to match.\n        if prospective.local is not None:\n            if Version(prospective.base_version) == Version(spec.base_version):\n                return False\n\n        # If we've gotten to here, it means that prospective version is both\n        # greater than the spec version *and* it's not a pre-release of the\n        # same version in the spec.\n        return True\n\n    def _compare_arbitrary(self, prospective: Version, spec: str) -> bool:\n        return str(prospective).lower() == str(spec).lower()\n\n    def __contains__(self, item: Union[str, Version]) -> bool:\n        \"\"\"Return whether or not the item is contained in this specifier.\n\n        :param item: The item to check for.\n\n        This is used for the ``in`` operator and behaves the same as\n        :meth:`contains` with no ``prereleases`` argument passed.\n\n        >>> \"1.2.3\" in Specifier(\">=1.2.3\")\n        True\n        >>> Version(\"1.2.3\") in Specifier(\">=1.2.3\")\n        True\n        >>> \"1.0.0\" in Specifier(\">=1.2.3\")\n        False\n        >>> \"1.3.0a1\" in Specifier(\">=1.2.3\")\n        False\n        >>> \"1.3.0a1\" in Specifier(\">=1.2.3\", prereleases=True)\n        True\n        \"\"\"\n        return self.contains(item)\n\n    def contains(\n        self, item: UnparsedVersion, prereleases: Optional[bool] = None\n    ) -> bool:\n        \"\"\"Return whether or not the item is contained in this specifier.\n\n        :param item:\n            The item to check for, which can be a version string or a\n            :class:`Version` instance.\n        :param prereleases:\n            Whether or not to match prereleases with this Specifier. If set to\n            ``None`` (the default), it uses :attr:`prereleases` to determine\n            whether or not prereleases are allowed.\n\n        >>> Specifier(\">=1.2.3\").contains(\"1.2.3\")\n        True\n        >>> Specifier(\">=1.2.3\").contains(Version(\"1.2.3\"))\n        True\n        >>> Specifier(\">=1.2.3\").contains(\"1.0.0\")\n        False\n        >>> Specifier(\">=1.2.3\").contains(\"1.3.0a1\")\n        False\n        >>> Specifier(\">=1.2.3\", prereleases=True).contains(\"1.3.0a1\")\n        True\n        >>> Specifier(\">=1.2.3\").contains(\"1.3.0a1\", prereleases=True)\n        True\n        \"\"\"\n\n        # Determine if prereleases are to be allowed or not.\n        if prereleases is None:\n            prereleases = self.prereleases\n\n        # Normalize item to a Version, this allows us to have a shortcut for\n        # \"2.0\" in Specifier(\">=2\")\n        normalized_item = _coerce_version(item)\n\n        # Determine if we should be supporting prereleases in this specifier\n        # or not, if we do not support prereleases than we can short circuit\n        # logic if this version is a prereleases.\n        if normalized_item.is_prerelease and not prereleases:\n            return False\n\n        # Actually do the comparison to determine if this item is contained\n        # within this Specifier or not.\n        operator_callable: CallableOperator = self._get_operator(self.operator)\n        return operator_callable(normalized_item, self.version)\n\n    def filter(\n        self, iterable: Iterable[UnparsedVersionVar], prereleases: Optional[bool] = None\n    ) -> Iterator[UnparsedVersionVar]:\n        \"\"\"Filter items in the given iterable, that match the specifier.\n\n        :param iterable:\n            An iterable that can contain version strings and :class:`Version` instances.\n            The items in the iterable will be filtered according to the specifier.\n        :param prereleases:\n            Whether or not to allow prereleases in the returned iterator. If set to\n            ``None`` (the default), it will be intelligently decide whether to allow\n            prereleases or not (based on the :attr:`prereleases` attribute, and\n            whether the only versions matching are prereleases).\n\n        This method is smarter than just ``filter(Specifier().contains, [...])``\n        because it implements the rule from :pep:`440` that a prerelease item\n        SHOULD be accepted if no other versions match the given specifier.\n\n        >>> list(Specifier(\">=1.2.3\").filter([\"1.2\", \"1.3\", \"1.5a1\"]))\n        ['1.3']\n        >>> list(Specifier(\">=1.2.3\").filter([\"1.2\", \"1.2.3\", \"1.3\", Version(\"1.4\")]))\n        ['1.2.3', '1.3', <Version('1.4')>]\n        >>> list(Specifier(\">=1.2.3\").filter([\"1.2\", \"1.5a1\"]))\n        ['1.5a1']\n        >>> list(Specifier(\">=1.2.3\").filter([\"1.3\", \"1.5a1\"], prereleases=True))\n        ['1.3', '1.5a1']\n        >>> list(Specifier(\">=1.2.3\", prereleases=True).filter([\"1.3\", \"1.5a1\"]))\n        ['1.3', '1.5a1']\n        \"\"\"\n\n        yielded = False\n        found_prereleases = []\n\n        kw = {\"prereleases\": prereleases if prereleases is not None else True}\n\n        # Attempt to iterate over all the values in the iterable and if any of\n        # them match, yield them.\n        for version in iterable:\n            parsed_version = _coerce_version(version)\n\n            if self.contains(parsed_version, **kw):\n                # If our version is a prerelease, and we were not set to allow\n                # prereleases, then we'll store it for later in case nothing\n                # else matches this specifier.\n                if parsed_version.is_prerelease and not (\n                    prereleases or self.prereleases\n                ):\n                    found_prereleases.append(version)\n                # Either this is not a prerelease, or we should have been\n                # accepting prereleases from the beginning.\n                else:\n                    yielded = True\n                    yield version\n\n        # Now that we've iterated over everything, determine if we've yielded\n        # any values, and if we have not and we have any prereleases stored up\n        # then we will go ahead and yield the prereleases.\n        if not yielded and found_prereleases:\n            for version in found_prereleases:\n                yield version\n\n\n_prefix_regex = re.compile(r\"^([0-9]+)((?:a|b|c|rc)[0-9]+)$\")\n\n\ndef _version_split(version: str) -> List[str]:\n    \"\"\"Split version into components.\n\n    The split components are intended for version comparison. The logic does\n    not attempt to retain the original version string, so joining the\n    components back with :func:`_version_join` may not produce the original\n    version string.\n    \"\"\"\n    result: List[str] = []\n\n    epoch, _, rest = version.rpartition(\"!\")\n    result.append(epoch or \"0\")\n\n    for item in rest.split(\".\"):\n        match = _prefix_regex.search(item)\n        if match:\n            result.extend(match.groups())\n        else:\n            result.append(item)\n    return result\n\n\ndef _version_join(components: List[str]) -> str:\n    \"\"\"Join split version components into a version string.\n\n    This function assumes the input came from :func:`_version_split`, where the\n    first component must be the epoch (either empty or numeric), and all other\n    components numeric.\n    \"\"\"\n    epoch, *rest = components\n    return f\"{epoch}!{'.'.join(rest)}\"\n\n\ndef _is_not_suffix(segment: str) -> bool:\n    return not any(\n        segment.startswith(prefix) for prefix in (\"dev\", \"a\", \"b\", \"rc\", \"post\")\n    )\n\n\ndef _pad_version(left: List[str], right: List[str]) -> Tuple[List[str], List[str]]:\n    left_split, right_split = [], []\n\n    # Get the release segment of our versions\n    left_split.append(list(itertools.takewhile(lambda x: x.isdigit(), left)))\n    right_split.append(list(itertools.takewhile(lambda x: x.isdigit(), right)))\n\n    # Get the rest of our versions\n    left_split.append(left[len(left_split[0]) :])\n    right_split.append(right[len(right_split[0]) :])\n\n    # Insert our padding\n    left_split.insert(1, [\"0\"] * max(0, len(right_split[0]) - len(left_split[0])))\n    right_split.insert(1, [\"0\"] * max(0, len(left_split[0]) - len(right_split[0])))\n\n    return (\n        list(itertools.chain.from_iterable(left_split)),\n        list(itertools.chain.from_iterable(right_split)),\n    )\n\n\nclass SpecifierSet(BaseSpecifier):\n    \"\"\"This class abstracts handling of a set of version specifiers.\n\n    It can be passed a single specifier (``>=3.0``), a comma-separated list of\n    specifiers (``>=3.0,!=3.1``), or no specifier at all.\n    \"\"\"\n\n    def __init__(\n        self, specifiers: str = \"\", prereleases: Optional[bool] = None\n    ) -> None:\n        \"\"\"Initialize a SpecifierSet instance.\n\n        :param specifiers:\n            The string representation of a specifier or a comma-separated list of\n            specifiers which will be parsed and normalized before use.\n        :param prereleases:\n            This tells the SpecifierSet if it should accept prerelease versions if\n            applicable or not. The default of ``None`` will autodetect it from the\n            given specifiers.\n\n        :raises InvalidSpecifier:\n            If the given ``specifiers`` are not parseable than this exception will be\n            raised.\n        \"\"\"\n\n        # Split on `,` to break each individual specifier into it's own item, and\n        # strip each item to remove leading/trailing whitespace.\n        split_specifiers = [s.strip() for s in specifiers.split(\",\") if s.strip()]\n\n        # Make each individual specifier a Specifier and save in a frozen set for later.\n        self._specs = frozenset(map(Specifier, split_specifiers))\n\n        # Store our prereleases value so we can use it later to determine if\n        # we accept prereleases or not.\n        self._prereleases = prereleases\n\n    @property\n    def prereleases(self) -> Optional[bool]:\n        # If we have been given an explicit prerelease modifier, then we'll\n        # pass that through here.\n        if self._prereleases is not None:\n            return self._prereleases\n\n        # If we don't have any specifiers, and we don't have a forced value,\n        # then we'll just return None since we don't know if this should have\n        # pre-releases or not.\n        if not self._specs:\n            return None\n\n        # Otherwise we'll see if any of the given specifiers accept\n        # prereleases, if any of them do we'll return True, otherwise False.\n        return any(s.prereleases for s in self._specs)\n\n    @prereleases.setter\n    def prereleases(self, value: bool) -> None:\n        self._prereleases = value\n\n    def __repr__(self) -> str:\n        \"\"\"A representation of the specifier set that shows all internal state.\n\n        Note that the ordering of the individual specifiers within the set may not\n        match the input string.\n\n        >>> SpecifierSet('>=1.0.0,!=2.0.0')\n        <SpecifierSet('!=2.0.0,>=1.0.0')>\n        >>> SpecifierSet('>=1.0.0,!=2.0.0', prereleases=False)\n        <SpecifierSet('!=2.0.0,>=1.0.0', prereleases=False)>\n        >>> SpecifierSet('>=1.0.0,!=2.0.0', prereleases=True)\n        <SpecifierSet('!=2.0.0,>=1.0.0', prereleases=True)>\n        \"\"\"\n        pre = (\n            f\", prereleases={self.prereleases!r}\"\n            if self._prereleases is not None\n            else \"\"\n        )\n\n        return f\"<SpecifierSet({str(self)!r}{pre})>\"\n\n    def __str__(self) -> str:\n        \"\"\"A string representation of the specifier set that can be round-tripped.\n\n        Note that the ordering of the individual specifiers within the set may not\n        match the input string.\n\n        >>> str(SpecifierSet(\">=1.0.0,!=1.0.1\"))\n        '!=1.0.1,>=1.0.0'\n        >>> str(SpecifierSet(\">=1.0.0,!=1.0.1\", prereleases=False))\n        '!=1.0.1,>=1.0.0'\n        \"\"\"\n        return \",\".join(sorted(str(s) for s in self._specs))\n\n    def __hash__(self) -> int:\n        return hash(self._specs)\n\n    def __and__(self, other: Union[\"SpecifierSet\", str]) -> \"SpecifierSet\":\n        \"\"\"Return a SpecifierSet which is a combination of the two sets.\n\n        :param other: The other object to combine with.\n\n        >>> SpecifierSet(\">=1.0.0,!=1.0.1\") & '<=2.0.0,!=2.0.1'\n        <SpecifierSet('!=1.0.1,!=2.0.1,<=2.0.0,>=1.0.0')>\n        >>> SpecifierSet(\">=1.0.0,!=1.0.1\") & SpecifierSet('<=2.0.0,!=2.0.1')\n        <SpecifierSet('!=1.0.1,!=2.0.1,<=2.0.0,>=1.0.0')>\n        \"\"\"\n        if isinstance(other, str):\n            other = SpecifierSet(other)\n        elif not isinstance(other, SpecifierSet):\n            return NotImplemented\n\n        specifier = SpecifierSet()\n        specifier._specs = frozenset(self._specs | other._specs)\n\n        if self._prereleases is None and other._prereleases is not None:\n            specifier._prereleases = other._prereleases\n        elif self._prereleases is not None and other._prereleases is None:\n            specifier._prereleases = self._prereleases\n        elif self._prereleases == other._prereleases:\n            specifier._prereleases = self._prereleases\n        else:\n            raise ValueError(\n                \"Cannot combine SpecifierSets with True and False prerelease \"\n                \"overrides.\"\n            )\n\n        return specifier\n\n    def __eq__(self, other: object) -> bool:\n        \"\"\"Whether or not the two SpecifierSet-like objects are equal.\n\n        :param other: The other object to check against.\n\n        The value of :attr:`prereleases` is ignored.\n\n        >>> SpecifierSet(\">=1.0.0,!=1.0.1\") == SpecifierSet(\">=1.0.0,!=1.0.1\")\n        True\n        >>> (SpecifierSet(\">=1.0.0,!=1.0.1\", prereleases=False) ==\n        ...  SpecifierSet(\">=1.0.0,!=1.0.1\", prereleases=True))\n        True\n        >>> SpecifierSet(\">=1.0.0,!=1.0.1\") == \">=1.0.0,!=1.0.1\"\n        True\n        >>> SpecifierSet(\">=1.0.0,!=1.0.1\") == SpecifierSet(\">=1.0.0\")\n        False\n        >>> SpecifierSet(\">=1.0.0,!=1.0.1\") == SpecifierSet(\">=1.0.0,!=1.0.2\")\n        False\n        \"\"\"\n        if isinstance(other, (str, Specifier)):\n            other = SpecifierSet(str(other))\n        elif not isinstance(other, SpecifierSet):\n            return NotImplemented\n\n        return self._specs == other._specs\n\n    def __len__(self) -> int:\n        \"\"\"Returns the number of specifiers in this specifier set.\"\"\"\n        return len(self._specs)\n\n    def __iter__(self) -> Iterator[Specifier]:\n        \"\"\"\n        Returns an iterator over all the underlying :class:`Specifier` instances\n        in this specifier set.\n\n        >>> sorted(SpecifierSet(\">=1.0.0,!=1.0.1\"), key=str)\n        [<Specifier('!=1.0.1')>, <Specifier('>=1.0.0')>]\n        \"\"\"\n        return iter(self._specs)\n\n    def __contains__(self, item: UnparsedVersion) -> bool:\n        \"\"\"Return whether or not the item is contained in this specifier.\n\n        :param item: The item to check for.\n\n        This is used for the ``in`` operator and behaves the same as\n        :meth:`contains` with no ``prereleases`` argument passed.\n\n        >>> \"1.2.3\" in SpecifierSet(\">=1.0.0,!=1.0.1\")\n        True\n        >>> Version(\"1.2.3\") in SpecifierSet(\">=1.0.0,!=1.0.1\")\n        True\n        >>> \"1.0.1\" in SpecifierSet(\">=1.0.0,!=1.0.1\")\n        False\n        >>> \"1.3.0a1\" in SpecifierSet(\">=1.0.0,!=1.0.1\")\n        False\n        >>> \"1.3.0a1\" in SpecifierSet(\">=1.0.0,!=1.0.1\", prereleases=True)\n        True\n        \"\"\"\n        return self.contains(item)\n\n    def contains(\n        self,\n        item: UnparsedVersion,\n        prereleases: Optional[bool] = None,\n        installed: Optional[bool] = None,\n    ) -> bool:\n        \"\"\"Return whether or not the item is contained in this SpecifierSet.\n\n        :param item:\n            The item to check for, which can be a version string or a\n            :class:`Version` instance.\n        :param prereleases:\n            Whether or not to match prereleases with this SpecifierSet. If set to\n            ``None`` (the default), it uses :attr:`prereleases` to determine\n            whether or not prereleases are allowed.\n\n        >>> SpecifierSet(\">=1.0.0,!=1.0.1\").contains(\"1.2.3\")\n        True\n        >>> SpecifierSet(\">=1.0.0,!=1.0.1\").contains(Version(\"1.2.3\"))\n        True\n        >>> SpecifierSet(\">=1.0.0,!=1.0.1\").contains(\"1.0.1\")\n        False\n        >>> SpecifierSet(\">=1.0.0,!=1.0.1\").contains(\"1.3.0a1\")\n        False\n        >>> SpecifierSet(\">=1.0.0,!=1.0.1\", prereleases=True).contains(\"1.3.0a1\")\n        True\n        >>> SpecifierSet(\">=1.0.0,!=1.0.1\").contains(\"1.3.0a1\", prereleases=True)\n        True\n        \"\"\"\n        # Ensure that our item is a Version instance.\n        if not isinstance(item, Version):\n            item = Version(item)\n\n        # Determine if we're forcing a prerelease or not, if we're not forcing\n        # one for this particular filter call, then we'll use whatever the\n        # SpecifierSet thinks for whether or not we should support prereleases.\n        if prereleases is None:\n            prereleases = self.prereleases\n\n        # We can determine if we're going to allow pre-releases by looking to\n        # see if any of the underlying items supports them. If none of them do\n        # and this item is a pre-release then we do not allow it and we can\n        # short circuit that here.\n        # Note: This means that 1.0.dev1 would not be contained in something\n        #       like >=1.0.devabc however it would be in >=1.0.debabc,>0.0.dev0\n        if not prereleases and item.is_prerelease:\n            return False\n\n        if installed and item.is_prerelease:\n            item = Version(item.base_version)\n\n        # We simply dispatch to the underlying specs here to make sure that the\n        # given version is contained within all of them.\n        # Note: This use of all() here means that an empty set of specifiers\n        #       will always return True, this is an explicit design decision.\n        return all(s.contains(item, prereleases=prereleases) for s in self._specs)\n\n    def filter(\n        self, iterable: Iterable[UnparsedVersionVar], prereleases: Optional[bool] = None\n    ) -> Iterator[UnparsedVersionVar]:\n        \"\"\"Filter items in the given iterable, that match the specifiers in this set.\n\n        :param iterable:\n            An iterable that can contain version strings and :class:`Version` instances.\n            The items in the iterable will be filtered according to the specifier.\n        :param prereleases:\n            Whether or not to allow prereleases in the returned iterator. If set to\n            ``None`` (the default), it will be intelligently decide whether to allow\n            prereleases or not (based on the :attr:`prereleases` attribute, and\n            whether the only versions matching are prereleases).\n\n        This method is smarter than just ``filter(SpecifierSet(...).contains, [...])``\n        because it implements the rule from :pep:`440` that a prerelease item\n        SHOULD be accepted if no other versions match the given specifier.\n\n        >>> list(SpecifierSet(\">=1.2.3\").filter([\"1.2\", \"1.3\", \"1.5a1\"]))\n        ['1.3']\n        >>> list(SpecifierSet(\">=1.2.3\").filter([\"1.2\", \"1.3\", Version(\"1.4\")]))\n        ['1.3', <Version('1.4')>]\n        >>> list(SpecifierSet(\">=1.2.3\").filter([\"1.2\", \"1.5a1\"]))\n        []\n        >>> list(SpecifierSet(\">=1.2.3\").filter([\"1.3\", \"1.5a1\"], prereleases=True))\n        ['1.3', '1.5a1']\n        >>> list(SpecifierSet(\">=1.2.3\", prereleases=True).filter([\"1.3\", \"1.5a1\"]))\n        ['1.3', '1.5a1']\n\n        An \"empty\" SpecifierSet will filter items based on the presence of prerelease\n        versions in the set.\n\n        >>> list(SpecifierSet(\"\").filter([\"1.3\", \"1.5a1\"]))\n        ['1.3']\n        >>> list(SpecifierSet(\"\").filter([\"1.5a1\"]))\n        ['1.5a1']\n        >>> list(SpecifierSet(\"\", prereleases=True).filter([\"1.3\", \"1.5a1\"]))\n        ['1.3', '1.5a1']\n        >>> list(SpecifierSet(\"\").filter([\"1.3\", \"1.5a1\"], prereleases=True))\n        ['1.3', '1.5a1']\n        \"\"\"\n        # Determine if we're forcing a prerelease or not, if we're not forcing\n        # one for this particular filter call, then we'll use whatever the\n        # SpecifierSet thinks for whether or not we should support prereleases.\n        if prereleases is None:\n            prereleases = self.prereleases\n\n        # If we have any specifiers, then we want to wrap our iterable in the\n        # filter method for each one, this will act as a logical AND amongst\n        # each specifier.\n        if self._specs:\n            for spec in self._specs:\n                iterable = spec.filter(iterable, prereleases=bool(prereleases))\n            return iter(iterable)\n        # If we do not have any specifiers, then we need to have a rough filter\n        # which will filter out any pre-releases, unless there are no final\n        # releases.\n        else:\n            filtered: List[UnparsedVersionVar] = []\n            found_prereleases: List[UnparsedVersionVar] = []\n\n            for item in iterable:\n                parsed_version = _coerce_version(item)\n\n                # Store any item which is a pre-release for later unless we've\n                # already found a final version or we are accepting prereleases\n                if parsed_version.is_prerelease and not prereleases:\n                    if not filtered:\n                        found_prereleases.append(item)\n                else:\n                    filtered.append(item)\n\n            # If we've found no items except for pre-releases, then we'll go\n            # ahead and use the pre-releases\n            if not filtered and found_prereleases and prereleases is None:\n                return iter(found_prereleases)\n\n            return iter(filtered)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/_vendor/wheel/vendored/packaging/tags.py","size":18950,"sha1":"d0e8626cb65a650cf790493be9981f427eec05c7","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"# This file is dual licensed under the terms of the Apache License, Version\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\n# for complete details.\n\nimport logging\nimport platform\nimport re\nimport struct\nimport subprocess\nimport sys\nimport sysconfig\nfrom importlib.machinery import EXTENSION_SUFFIXES\nfrom typing import (\n    Dict,\n    FrozenSet,\n    Iterable,\n    Iterator,\n    List,\n    Optional,\n    Sequence,\n    Tuple,\n    Union,\n    cast,\n)\n\nfrom . import _manylinux, _musllinux\n\nlogger = logging.getLogger(__name__)\n\nPythonVersion = Sequence[int]\nMacVersion = Tuple[int, int]\n\nINTERPRETER_SHORT_NAMES: Dict[str, str] = {\n    \"python\": \"py\",  # Generic.\n    \"cpython\": \"cp\",\n    \"pypy\": \"pp\",\n    \"ironpython\": \"ip\",\n    \"jython\": \"jy\",\n}\n\n\n_32_BIT_INTERPRETER = struct.calcsize(\"P\") == 4\n\n\nclass Tag:\n    \"\"\"\n    A representation of the tag triple for a wheel.\n\n    Instances are considered immutable and thus are hashable. Equality checking\n    is also supported.\n    \"\"\"\n\n    __slots__ = [\"_interpreter\", \"_abi\", \"_platform\", \"_hash\"]\n\n    def __init__(self, interpreter: str, abi: str, platform: str) -> None:\n        self._interpreter = interpreter.lower()\n        self._abi = abi.lower()\n        self._platform = platform.lower()\n        # The __hash__ of every single element in a Set[Tag] will be evaluated each time\n        # that a set calls its `.disjoint()` method, which may be called hundreds of\n        # times when scanning a page of links for packages with tags matching that\n        # Set[Tag]. Pre-computing the value here produces significant speedups for\n        # downstream consumers.\n        self._hash = hash((self._interpreter, self._abi, self._platform))\n\n    @property\n    def interpreter(self) -> str:\n        return self._interpreter\n\n    @property\n    def abi(self) -> str:\n        return self._abi\n\n    @property\n    def platform(self) -> str:\n        return self._platform\n\n    def __eq__(self, other: object) -> bool:\n        if not isinstance(other, Tag):\n            return NotImplemented\n\n        return (\n            (self._hash == other._hash)  # Short-circuit ASAP for perf reasons.\n            and (self._platform == other._platform)\n            and (self._abi == other._abi)\n            and (self._interpreter == other._interpreter)\n        )\n\n    def __hash__(self) -> int:\n        return self._hash\n\n    def __str__(self) -> str:\n        return f\"{self._interpreter}-{self._abi}-{self._platform}\"\n\n    def __repr__(self) -> str:\n        return f\"<{self} @ {id(self)}>\"\n\n\ndef parse_tag(tag: str) -> FrozenSet[Tag]:\n    \"\"\"\n    Parses the provided tag (e.g. `py3-none-any`) into a frozenset of Tag instances.\n\n    Returning a set is required due to the possibility that the tag is a\n    compressed tag set.\n    \"\"\"\n    tags = set()\n    interpreters, abis, platforms = tag.split(\"-\")\n    for interpreter in interpreters.split(\".\"):\n        for abi in abis.split(\".\"):\n            for platform_ in platforms.split(\".\"):\n                tags.add(Tag(interpreter, abi, platform_))\n    return frozenset(tags)\n\n\ndef _get_config_var(name: str, warn: bool = False) -> Union[int, str, None]:\n    value: Union[int, str, None] = sysconfig.get_config_var(name)\n    if value is None and warn:\n        logger.debug(\n            \"Config variable '%s' is unset, Python ABI tag may be incorrect\", name\n        )\n    return value\n\n\ndef _normalize_string(string: str) -> str:\n    return string.replace(\".\", \"_\").replace(\"-\", \"_\").replace(\" \", \"_\")\n\n\ndef _is_threaded_cpython(abis: List[str]) -> bool:\n    \"\"\"\n    Determine if the ABI corresponds to a threaded (`--disable-gil`) build.\n\n    The threaded builds are indicated by a \"t\" in the abiflags.\n    \"\"\"\n    if len(abis) == 0:\n        return False\n    # expect e.g., cp313\n    m = re.match(r\"cp\\d+(.*)\", abis[0])\n    if not m:\n        return False\n    abiflags = m.group(1)\n    return \"t\" in abiflags\n\n\ndef _abi3_applies(python_version: PythonVersion, threading: bool) -> bool:\n    \"\"\"\n    Determine if the Python version supports abi3.\n\n    PEP 384 was first implemented in Python 3.2. The threaded (`--disable-gil`)\n    builds do not support abi3.\n    \"\"\"\n    return len(python_version) > 1 and tuple(python_version) >= (3, 2) and not threading\n\n\ndef _cpython_abis(py_version: PythonVersion, warn: bool = False) -> List[str]:\n    py_version = tuple(py_version)  # To allow for version comparison.\n    abis = []\n    version = _version_nodot(py_version[:2])\n    threading = debug = pymalloc = ucs4 = \"\"\n    with_debug = _get_config_var(\"Py_DEBUG\", warn)\n    has_refcount = hasattr(sys, \"gettotalrefcount\")\n    # Windows doesn't set Py_DEBUG, so checking for support of debug-compiled\n    # extension modules is the best option.\n    # https://github.com/pypa/pip/issues/3383#issuecomment-173267692\n    has_ext = \"_d.pyd\" in EXTENSION_SUFFIXES\n    if with_debug or (with_debug is None and (has_refcount or has_ext)):\n        debug = \"d\"\n    if py_version >= (3, 13) and _get_config_var(\"Py_GIL_DISABLED\", warn):\n        threading = \"t\"\n    if py_version < (3, 8):\n        with_pymalloc = _get_config_var(\"WITH_PYMALLOC\", warn)\n        if with_pymalloc or with_pymalloc is None:\n            pymalloc = \"m\"\n        if py_version < (3, 3):\n            unicode_size = _get_config_var(\"Py_UNICODE_SIZE\", warn)\n            if unicode_size == 4 or (\n                unicode_size is None and sys.maxunicode == 0x10FFFF\n            ):\n                ucs4 = \"u\"\n    elif debug:\n        # Debug builds can also load \"normal\" extension modules.\n        # We can also assume no UCS-4 or pymalloc requirement.\n        abis.append(f\"cp{version}{threading}\")\n    abis.insert(0, f\"cp{version}{threading}{debug}{pymalloc}{ucs4}\")\n    return abis\n\n\ndef cpython_tags(\n    python_version: Optional[PythonVersion] = None,\n    abis: Optional[Iterable[str]] = None,\n    platforms: Optional[Iterable[str]] = None,\n    *,\n    warn: bool = False,\n) -> Iterator[Tag]:\n    \"\"\"\n    Yields the tags for a CPython interpreter.\n\n    The tags consist of:\n    - cp<python_version>-<abi>-<platform>\n    - cp<python_version>-abi3-<platform>\n    - cp<python_version>-none-<platform>\n    - cp<less than python_version>-abi3-<platform>  # Older Python versions down to 3.2.\n\n    If python_version only specifies a major version then user-provided ABIs and\n    the 'none' ABItag will be used.\n\n    If 'abi3' or 'none' are specified in 'abis' then they will be yielded at\n    their normal position and not at the beginning.\n    \"\"\"\n    if not python_version:\n        python_version = sys.version_info[:2]\n\n    interpreter = f\"cp{_version_nodot(python_version[:2])}\"\n\n    if abis is None:\n        if len(python_version) > 1:\n            abis = _cpython_abis(python_version, warn)\n        else:\n            abis = []\n    abis = list(abis)\n    # 'abi3' and 'none' are explicitly handled later.\n    for explicit_abi in (\"abi3\", \"none\"):\n        try:\n            abis.remove(explicit_abi)\n        except ValueError:\n            pass\n\n    platforms = list(platforms or platform_tags())\n    for abi in abis:\n        for platform_ in platforms:\n            yield Tag(interpreter, abi, platform_)\n\n    threading = _is_threaded_cpython(abis)\n    use_abi3 = _abi3_applies(python_version, threading)\n    if use_abi3:\n        yield from (Tag(interpreter, \"abi3\", platform_) for platform_ in platforms)\n    yield from (Tag(interpreter, \"none\", platform_) for platform_ in platforms)\n\n    if use_abi3:\n        for minor_version in range(python_version[1] - 1, 1, -1):\n            for platform_ in platforms:\n                interpreter = \"cp{version}\".format(\n                    version=_version_nodot((python_version[0], minor_version))\n                )\n                yield Tag(interpreter, \"abi3\", platform_)\n\n\ndef _generic_abi() -> List[str]:\n    \"\"\"\n    Return the ABI tag based on EXT_SUFFIX.\n    \"\"\"\n    # The following are examples of `EXT_SUFFIX`.\n    # We want to keep the parts which are related to the ABI and remove the\n    # parts which are related to the platform:\n    # - linux:   '.cpython-310-x86_64-linux-gnu.so' => cp310\n    # - mac:     '.cpython-310-darwin.so'           => cp310\n    # - win:     '.cp310-win_amd64.pyd'             => cp310\n    # - win:     '.pyd'                             => cp37 (uses _cpython_abis())\n    # - pypy:    '.pypy38-pp73-x86_64-linux-gnu.so' => pypy38_pp73\n    # - graalpy: '.graalpy-38-native-x86_64-darwin.dylib'\n    #                                               => graalpy_38_native\n\n    ext_suffix = _get_config_var(\"EXT_SUFFIX\", warn=True)\n    if not isinstance(ext_suffix, str) or ext_suffix[0] != \".\":\n        raise SystemError(\"invalid sysconfig.get_config_var('EXT_SUFFIX')\")\n    parts = ext_suffix.split(\".\")\n    if len(parts) < 3:\n        # CPython3.7 and earlier uses \".pyd\" on Windows.\n        return _cpython_abis(sys.version_info[:2])\n    soabi = parts[1]\n    if soabi.startswith(\"cpython\"):\n        # non-windows\n        abi = \"cp\" + soabi.split(\"-\")[1]\n    elif soabi.startswith(\"cp\"):\n        # windows\n        abi = soabi.split(\"-\")[0]\n    elif soabi.startswith(\"pypy\"):\n        abi = \"-\".join(soabi.split(\"-\")[:2])\n    elif soabi.startswith(\"graalpy\"):\n        abi = \"-\".join(soabi.split(\"-\")[:3])\n    elif soabi:\n        # pyston, ironpython, others?\n        abi = soabi\n    else:\n        return []\n    return [_normalize_string(abi)]\n\n\ndef generic_tags(\n    interpreter: Optional[str] = None,\n    abis: Optional[Iterable[str]] = None,\n    platforms: Optional[Iterable[str]] = None,\n    *,\n    warn: bool = False,\n) -> Iterator[Tag]:\n    \"\"\"\n    Yields the tags for a generic interpreter.\n\n    The tags consist of:\n    - <interpreter>-<abi>-<platform>\n\n    The \"none\" ABI will be added if it was not explicitly provided.\n    \"\"\"\n    if not interpreter:\n        interp_name = interpreter_name()\n        interp_version = interpreter_version(warn=warn)\n        interpreter = \"\".join([interp_name, interp_version])\n    if abis is None:\n        abis = _generic_abi()\n    else:\n        abis = list(abis)\n    platforms = list(platforms or platform_tags())\n    if \"none\" not in abis:\n        abis.append(\"none\")\n    for abi in abis:\n        for platform_ in platforms:\n            yield Tag(interpreter, abi, platform_)\n\n\ndef _py_interpreter_range(py_version: PythonVersion) -> Iterator[str]:\n    \"\"\"\n    Yields Python versions in descending order.\n\n    After the latest version, the major-only version will be yielded, and then\n    all previous versions of that major version.\n    \"\"\"\n    if len(py_version) > 1:\n        yield f\"py{_version_nodot(py_version[:2])}\"\n    yield f\"py{py_version[0]}\"\n    if len(py_version) > 1:\n        for minor in range(py_version[1] - 1, -1, -1):\n            yield f\"py{_version_nodot((py_version[0], minor))}\"\n\n\ndef compatible_tags(\n    python_version: Optional[PythonVersion] = None,\n    interpreter: Optional[str] = None,\n    platforms: Optional[Iterable[str]] = None,\n) -> Iterator[Tag]:\n    \"\"\"\n    Yields the sequence of tags that are compatible with a specific version of Python.\n\n    The tags consist of:\n    - py*-none-<platform>\n    - <interpreter>-none-any  # ... if `interpreter` is provided.\n    - py*-none-any\n    \"\"\"\n    if not python_version:\n        python_version = sys.version_info[:2]\n    platforms = list(platforms or platform_tags())\n    for version in _py_interpreter_range(python_version):\n        for platform_ in platforms:\n            yield Tag(version, \"none\", platform_)\n    if interpreter:\n        yield Tag(interpreter, \"none\", \"any\")\n    for version in _py_interpreter_range(python_version):\n        yield Tag(version, \"none\", \"any\")\n\n\ndef _mac_arch(arch: str, is_32bit: bool = _32_BIT_INTERPRETER) -> str:\n    if not is_32bit:\n        return arch\n\n    if arch.startswith(\"ppc\"):\n        return \"ppc\"\n\n    return \"i386\"\n\n\ndef _mac_binary_formats(version: MacVersion, cpu_arch: str) -> List[str]:\n    formats = [cpu_arch]\n    if cpu_arch == \"x86_64\":\n        if version < (10, 4):\n            return []\n        formats.extend([\"intel\", \"fat64\", \"fat32\"])\n\n    elif cpu_arch == \"i386\":\n        if version < (10, 4):\n            return []\n        formats.extend([\"intel\", \"fat32\", \"fat\"])\n\n    elif cpu_arch == \"ppc64\":\n        # TODO: Need to care about 32-bit PPC for ppc64 through 10.2?\n        if version > (10, 5) or version < (10, 4):\n            return []\n        formats.append(\"fat64\")\n\n    elif cpu_arch == \"ppc\":\n        if version > (10, 6):\n            return []\n        formats.extend([\"fat32\", \"fat\"])\n\n    if cpu_arch in {\"arm64\", \"x86_64\"}:\n        formats.append(\"universal2\")\n\n    if cpu_arch in {\"x86_64\", \"i386\", \"ppc64\", \"ppc\", \"intel\"}:\n        formats.append(\"universal\")\n\n    return formats\n\n\ndef mac_platforms(\n    version: Optional[MacVersion] = None, arch: Optional[str] = None\n) -> Iterator[str]:\n    \"\"\"\n    Yields the platform tags for a macOS system.\n\n    The `version` parameter is a two-item tuple specifying the macOS version to\n    generate platform tags for. The `arch` parameter is the CPU architecture to\n    generate platform tags for. Both parameters default to the appropriate value\n    for the current system.\n    \"\"\"\n    version_str, _, cpu_arch = platform.mac_ver()\n    if version is None:\n        version = cast(\"MacVersion\", tuple(map(int, version_str.split(\".\")[:2])))\n        if version == (10, 16):\n            # When built against an older macOS SDK, Python will report macOS 10.16\n            # instead of the real version.\n            version_str = subprocess.run(\n                [\n                    sys.executable,\n                    \"-sS\",\n                    \"-c\",\n                    \"import platform; print(platform.mac_ver()[0])\",\n                ],\n                check=True,\n                env={\"SYSTEM_VERSION_COMPAT\": \"0\"},\n                stdout=subprocess.PIPE,\n                text=True,\n            ).stdout\n            version = cast(\"MacVersion\", tuple(map(int, version_str.split(\".\")[:2])))\n    else:\n        version = version\n    if arch is None:\n        arch = _mac_arch(cpu_arch)\n    else:\n        arch = arch\n\n    if (10, 0) <= version and version < (11, 0):\n        # Prior to Mac OS 11, each yearly release of Mac OS bumped the\n        # \"minor\" version number.  The major version was always 10.\n        for minor_version in range(version[1], -1, -1):\n            compat_version = 10, minor_version\n            binary_formats = _mac_binary_formats(compat_version, arch)\n            for binary_format in binary_formats:\n                yield \"macosx_{major}_{minor}_{binary_format}\".format(\n                    major=10, minor=minor_version, binary_format=binary_format\n                )\n\n    if version >= (11, 0):\n        # Starting with Mac OS 11, each yearly release bumps the major version\n        # number.   The minor versions are now the midyear updates.\n        for major_version in range(version[0], 10, -1):\n            compat_version = major_version, 0\n            binary_formats = _mac_binary_formats(compat_version, arch)\n            for binary_format in binary_formats:\n                yield \"macosx_{major}_{minor}_{binary_format}\".format(\n                    major=major_version, minor=0, binary_format=binary_format\n                )\n\n    if version >= (11, 0):\n        # Mac OS 11 on x86_64 is compatible with binaries from previous releases.\n        # Arm64 support was introduced in 11.0, so no Arm binaries from previous\n        # releases exist.\n        #\n        # However, the \"universal2\" binary format can have a\n        # macOS version earlier than 11.0 when the x86_64 part of the binary supports\n        # that version of macOS.\n        if arch == \"x86_64\":\n            for minor_version in range(16, 3, -1):\n                compat_version = 10, minor_version\n                binary_formats = _mac_binary_formats(compat_version, arch)\n                for binary_format in binary_formats:\n                    yield \"macosx_{major}_{minor}_{binary_format}\".format(\n                        major=compat_version[0],\n                        minor=compat_version[1],\n                        binary_format=binary_format,\n                    )\n        else:\n            for minor_version in range(16, 3, -1):\n                compat_version = 10, minor_version\n                binary_format = \"universal2\"\n                yield \"macosx_{major}_{minor}_{binary_format}\".format(\n                    major=compat_version[0],\n                    minor=compat_version[1],\n                    binary_format=binary_format,\n                )\n\n\ndef _linux_platforms(is_32bit: bool = _32_BIT_INTERPRETER) -> Iterator[str]:\n    linux = _normalize_string(sysconfig.get_platform())\n    if not linux.startswith(\"linux_\"):\n        # we should never be here, just yield the sysconfig one and return\n        yield linux\n        return\n    if is_32bit:\n        if linux == \"linux_x86_64\":\n            linux = \"linux_i686\"\n        elif linux == \"linux_aarch64\":\n            linux = \"linux_armv8l\"\n    _, arch = linux.split(\"_\", 1)\n    archs = {\"armv8l\": [\"armv8l\", \"armv7l\"]}.get(arch, [arch])\n    yield from _manylinux.platform_tags(archs)\n    yield from _musllinux.platform_tags(archs)\n    for arch in archs:\n        yield f\"linux_{arch}\"\n\n\ndef _generic_platforms() -> Iterator[str]:\n    yield _normalize_string(sysconfig.get_platform())\n\n\ndef platform_tags() -> Iterator[str]:\n    \"\"\"\n    Provides the platform tags for this installation.\n    \"\"\"\n    if platform.system() == \"Darwin\":\n        return mac_platforms()\n    elif platform.system() == \"Linux\":\n        return _linux_platforms()\n    else:\n        return _generic_platforms()\n\n\ndef interpreter_name() -> str:\n    \"\"\"\n    Returns the name of the running interpreter.\n\n    Some implementations have a reserved, two-letter abbreviation which will\n    be returned when appropriate.\n    \"\"\"\n    name = sys.implementation.name\n    return INTERPRETER_SHORT_NAMES.get(name) or name\n\n\ndef interpreter_version(*, warn: bool = False) -> str:\n    \"\"\"\n    Returns the version of the running interpreter.\n    \"\"\"\n    version = _get_config_var(\"py_version_nodot\", warn=warn)\n    if version:\n        version = str(version)\n    else:\n        version = _version_nodot(sys.version_info[:2])\n    return version\n\n\ndef _version_nodot(version: PythonVersion) -> str:\n    return \"\".join(map(str, version))\n\n\ndef sys_tags(*, warn: bool = False) -> Iterator[Tag]:\n    \"\"\"\n    Returns the sequence of tag triples for the running interpreter.\n\n    The order of the sequence corresponds to priority order for the\n    interpreter, from most to least important.\n    \"\"\"\n\n    interp_name = interpreter_name()\n    if interp_name == \"cp\":\n        yield from cpython_tags(warn=warn)\n    else:\n        yield from generic_tags()\n\n    if interp_name == \"pp\":\n        interp = \"pp3\"\n    elif interp_name == \"cp\":\n        interp = \"cp\" + interpreter_version(warn=warn)\n    else:\n        interp = None\n    yield from compatible_tags(interpreter=interp)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/_vendor/wheel/vendored/packaging/utils.py","size":5268,"sha1":"832a86edb71c6c5e128f0a4172fd063de7858e71","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"# This file is dual licensed under the terms of the Apache License, Version\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\n# for complete details.\n\nimport re\nfrom typing import FrozenSet, NewType, Tuple, Union, cast\n\nfrom .tags import Tag, parse_tag\nfrom .version import InvalidVersion, Version\n\nBuildTag = Union[Tuple[()], Tuple[int, str]]\nNormalizedName = NewType(\"NormalizedName\", str)\n\n\nclass InvalidName(ValueError):\n    \"\"\"\n    An invalid distribution name; users should refer to the packaging user guide.\n    \"\"\"\n\n\nclass InvalidWheelFilename(ValueError):\n    \"\"\"\n    An invalid wheel filename was found, users should refer to PEP 427.\n    \"\"\"\n\n\nclass InvalidSdistFilename(ValueError):\n    \"\"\"\n    An invalid sdist filename was found, users should refer to the packaging user guide.\n    \"\"\"\n\n\n# Core metadata spec for `Name`\n_validate_regex = re.compile(\n    r\"^([A-Z0-9]|[A-Z0-9][A-Z0-9._-]*[A-Z0-9])$\", re.IGNORECASE\n)\n_canonicalize_regex = re.compile(r\"[-_.]+\")\n_normalized_regex = re.compile(r\"^([a-z0-9]|[a-z0-9]([a-z0-9-](?!--))*[a-z0-9])$\")\n# PEP 427: The build number must start with a digit.\n_build_tag_regex = re.compile(r\"(\\d+)(.*)\")\n\n\ndef canonicalize_name(name: str, *, validate: bool = False) -> NormalizedName:\n    if validate and not _validate_regex.match(name):\n        raise InvalidName(f\"name is invalid: {name!r}\")\n    # This is taken from PEP 503.\n    value = _canonicalize_regex.sub(\"-\", name).lower()\n    return cast(NormalizedName, value)\n\n\ndef is_normalized_name(name: str) -> bool:\n    return _normalized_regex.match(name) is not None\n\n\ndef canonicalize_version(\n    version: Union[Version, str], *, strip_trailing_zero: bool = True\n) -> str:\n    \"\"\"\n    This is very similar to Version.__str__, but has one subtle difference\n    with the way it handles the release segment.\n    \"\"\"\n    if isinstance(version, str):\n        try:\n            parsed = Version(version)\n        except InvalidVersion:\n            # Legacy versions cannot be normalized\n            return version\n    else:\n        parsed = version\n\n    parts = []\n\n    # Epoch\n    if parsed.epoch != 0:\n        parts.append(f\"{parsed.epoch}!\")\n\n    # Release segment\n    release_segment = \".\".join(str(x) for x in parsed.release)\n    if strip_trailing_zero:\n        # NB: This strips trailing '.0's to normalize\n        release_segment = re.sub(r\"(\\.0)+$\", \"\", release_segment)\n    parts.append(release_segment)\n\n    # Pre-release\n    if parsed.pre is not None:\n        parts.append(\"\".join(str(x) for x in parsed.pre))\n\n    # Post-release\n    if parsed.post is not None:\n        parts.append(f\".post{parsed.post}\")\n\n    # Development release\n    if parsed.dev is not None:\n        parts.append(f\".dev{parsed.dev}\")\n\n    # Local version segment\n    if parsed.local is not None:\n        parts.append(f\"+{parsed.local}\")\n\n    return \"\".join(parts)\n\n\ndef parse_wheel_filename(\n    filename: str,\n) -> Tuple[NormalizedName, Version, BuildTag, FrozenSet[Tag]]:\n    if not filename.endswith(\".whl\"):\n        raise InvalidWheelFilename(\n            f\"Invalid wheel filename (extension must be '.whl'): {filename}\"\n        )\n\n    filename = filename[:-4]\n    dashes = filename.count(\"-\")\n    if dashes not in (4, 5):\n        raise InvalidWheelFilename(\n            f\"Invalid wheel filename (wrong number of parts): {filename}\"\n        )\n\n    parts = filename.split(\"-\", dashes - 2)\n    name_part = parts[0]\n    # See PEP 427 for the rules on escaping the project name.\n    if \"__\" in name_part or re.match(r\"^[\\w\\d._]*$\", name_part, re.UNICODE) is None:\n        raise InvalidWheelFilename(f\"Invalid project name: {filename}\")\n    name = canonicalize_name(name_part)\n\n    try:\n        version = Version(parts[1])\n    except InvalidVersion as e:\n        raise InvalidWheelFilename(\n            f\"Invalid wheel filename (invalid version): {filename}\"\n        ) from e\n\n    if dashes == 5:\n        build_part = parts[2]\n        build_match = _build_tag_regex.match(build_part)\n        if build_match is None:\n            raise InvalidWheelFilename(\n                f\"Invalid build number: {build_part} in '{filename}'\"\n            )\n        build = cast(BuildTag, (int(build_match.group(1)), build_match.group(2)))\n    else:\n        build = ()\n    tags = parse_tag(parts[-1])\n    return (name, version, build, tags)\n\n\ndef parse_sdist_filename(filename: str) -> Tuple[NormalizedName, Version]:\n    if filename.endswith(\".tar.gz\"):\n        file_stem = filename[: -len(\".tar.gz\")]\n    elif filename.endswith(\".zip\"):\n        file_stem = filename[: -len(\".zip\")]\n    else:\n        raise InvalidSdistFilename(\n            f\"Invalid sdist filename (extension must be '.tar.gz' or '.zip'):\"\n            f\" {filename}\"\n        )\n\n    # We are requiring a PEP 440 version, which cannot contain dashes,\n    # so we split on the last dash.\n    name_part, sep, version_part = file_stem.rpartition(\"-\")\n    if not sep:\n        raise InvalidSdistFilename(f\"Invalid sdist filename: {filename}\")\n\n    name = canonicalize_name(name_part)\n\n    try:\n        version = Version(version_part)\n    except InvalidVersion as e:\n        raise InvalidSdistFilename(\n            f\"Invalid sdist filename (invalid version): {filename}\"\n        ) from e\n\n    return (name, version)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/_vendor/wheel/vendored/packaging/version.py","size":16234,"sha1":"9e48a008c94d48159224be4714f5f49414873153","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"# This file is dual licensed under the terms of the Apache License, Version\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\n# for complete details.\n\"\"\"\n.. testsetup::\n\n    from packaging.version import parse, Version\n\"\"\"\n\nimport itertools\nimport re\nfrom typing import Any, Callable, NamedTuple, Optional, SupportsInt, Tuple, Union\n\nfrom ._structures import Infinity, InfinityType, NegativeInfinity, NegativeInfinityType\n\n__all__ = [\"VERSION_PATTERN\", \"parse\", \"Version\", \"InvalidVersion\"]\n\nLocalType = Tuple[Union[int, str], ...]\n\nCmpPrePostDevType = Union[InfinityType, NegativeInfinityType, Tuple[str, int]]\nCmpLocalType = Union[\n    NegativeInfinityType,\n    Tuple[Union[Tuple[int, str], Tuple[NegativeInfinityType, Union[int, str]]], ...],\n]\nCmpKey = Tuple[\n    int,\n    Tuple[int, ...],\n    CmpPrePostDevType,\n    CmpPrePostDevType,\n    CmpPrePostDevType,\n    CmpLocalType,\n]\nVersionComparisonMethod = Callable[[CmpKey, CmpKey], bool]\n\n\nclass _Version(NamedTuple):\n    epoch: int\n    release: Tuple[int, ...]\n    dev: Optional[Tuple[str, int]]\n    pre: Optional[Tuple[str, int]]\n    post: Optional[Tuple[str, int]]\n    local: Optional[LocalType]\n\n\ndef parse(version: str) -> \"Version\":\n    \"\"\"Parse the given version string.\n\n    >>> parse('1.0.dev1')\n    <Version('1.0.dev1')>\n\n    :param version: The version string to parse.\n    :raises InvalidVersion: When the version string is not a valid version.\n    \"\"\"\n    return Version(version)\n\n\nclass InvalidVersion(ValueError):\n    \"\"\"Raised when a version string is not a valid version.\n\n    >>> Version(\"invalid\")\n    Traceback (most recent call last):\n        ...\n    packaging.version.InvalidVersion: Invalid version: 'invalid'\n    \"\"\"\n\n\nclass _BaseVersion:\n    _key: Tuple[Any, ...]\n\n    def __hash__(self) -> int:\n        return hash(self._key)\n\n    # Please keep the duplicated `isinstance` check\n    # in the six comparisons hereunder\n    # unless you find a way to avoid adding overhead function calls.\n    def __lt__(self, other: \"_BaseVersion\") -> bool:\n        if not isinstance(other, _BaseVersion):\n            return NotImplemented\n\n        return self._key < other._key\n\n    def __le__(self, other: \"_BaseVersion\") -> bool:\n        if not isinstance(other, _BaseVersion):\n            return NotImplemented\n\n        return self._key <= other._key\n\n    def __eq__(self, other: object) -> bool:\n        if not isinstance(other, _BaseVersion):\n            return NotImplemented\n\n        return self._key == other._key\n\n    def __ge__(self, other: \"_BaseVersion\") -> bool:\n        if not isinstance(other, _BaseVersion):\n            return NotImplemented\n\n        return self._key >= other._key\n\n    def __gt__(self, other: \"_BaseVersion\") -> bool:\n        if not isinstance(other, _BaseVersion):\n            return NotImplemented\n\n        return self._key > other._key\n\n    def __ne__(self, other: object) -> bool:\n        if not isinstance(other, _BaseVersion):\n            return NotImplemented\n\n        return self._key != other._key\n\n\n# Deliberately not anchored to the start and end of the string, to make it\n# easier for 3rd party code to reuse\n_VERSION_PATTERN = r\"\"\"\n    v?\n    (?:\n        (?:(?P<epoch>[0-9]+)!)?                           # epoch\n        (?P<release>[0-9]+(?:\\.[0-9]+)*)                  # release segment\n        (?P<pre>                                          # pre-release\n            [-_\\.]?\n            (?P<pre_l>alpha|a|beta|b|preview|pre|c|rc)\n            [-_\\.]?\n            (?P<pre_n>[0-9]+)?\n        )?\n        (?P<post>                                         # post release\n            (?:-(?P<post_n1>[0-9]+))\n            |\n            (?:\n                [-_\\.]?\n                (?P<post_l>post|rev|r)\n                [-_\\.]?\n                (?P<post_n2>[0-9]+)?\n            )\n        )?\n        (?P<dev>                                          # dev release\n            [-_\\.]?\n            (?P<dev_l>dev)\n            [-_\\.]?\n            (?P<dev_n>[0-9]+)?\n        )?\n    )\n    (?:\\+(?P<local>[a-z0-9]+(?:[-_\\.][a-z0-9]+)*))?       # local version\n\"\"\"\n\nVERSION_PATTERN = _VERSION_PATTERN\n\"\"\"\nA string containing the regular expression used to match a valid version.\n\nThe pattern is not anchored at either end, and is intended for embedding in larger\nexpressions (for example, matching a version number as part of a file name). The\nregular expression should be compiled with the ``re.VERBOSE`` and ``re.IGNORECASE``\nflags set.\n\n:meta hide-value:\n\"\"\"\n\n\nclass Version(_BaseVersion):\n    \"\"\"This class abstracts handling of a project's versions.\n\n    A :class:`Version` instance is comparison aware and can be compared and\n    sorted using the standard Python interfaces.\n\n    >>> v1 = Version(\"1.0a5\")\n    >>> v2 = Version(\"1.0\")\n    >>> v1\n    <Version('1.0a5')>\n    >>> v2\n    <Version('1.0')>\n    >>> v1 < v2\n    True\n    >>> v1 == v2\n    False\n    >>> v1 > v2\n    False\n    >>> v1 >= v2\n    False\n    >>> v1 <= v2\n    True\n    \"\"\"\n\n    _regex = re.compile(r\"^\\s*\" + VERSION_PATTERN + r\"\\s*$\", re.VERBOSE | re.IGNORECASE)\n    _key: CmpKey\n\n    def __init__(self, version: str) -> None:\n        \"\"\"Initialize a Version object.\n\n        :param version:\n            The string representation of a version which will be parsed and normalized\n            before use.\n        :raises InvalidVersion:\n            If the ``version`` does not conform to PEP 440 in any way then this\n            exception will be raised.\n        \"\"\"\n\n        # Validate the version and parse it into pieces\n        match = self._regex.search(version)\n        if not match:\n            raise InvalidVersion(f\"Invalid version: '{version}'\")\n\n        # Store the parsed out pieces of the version\n        self._version = _Version(\n            epoch=int(match.group(\"epoch\")) if match.group(\"epoch\") else 0,\n            release=tuple(int(i) for i in match.group(\"release\").split(\".\")),\n            pre=_parse_letter_version(match.group(\"pre_l\"), match.group(\"pre_n\")),\n            post=_parse_letter_version(\n                match.group(\"post_l\"), match.group(\"post_n1\") or match.group(\"post_n2\")\n            ),\n            dev=_parse_letter_version(match.group(\"dev_l\"), match.group(\"dev_n\")),\n            local=_parse_local_version(match.group(\"local\")),\n        )\n\n        # Generate a key which will be used for sorting\n        self._key = _cmpkey(\n            self._version.epoch,\n            self._version.release,\n            self._version.pre,\n            self._version.post,\n            self._version.dev,\n            self._version.local,\n        )\n\n    def __repr__(self) -> str:\n        \"\"\"A representation of the Version that shows all internal state.\n\n        >>> Version('1.0.0')\n        <Version('1.0.0')>\n        \"\"\"\n        return f\"<Version('{self}')>\"\n\n    def __str__(self) -> str:\n        \"\"\"A string representation of the version that can be rounded-tripped.\n\n        >>> str(Version(\"1.0a5\"))\n        '1.0a5'\n        \"\"\"\n        parts = []\n\n        # Epoch\n        if self.epoch != 0:\n            parts.append(f\"{self.epoch}!\")\n\n        # Release segment\n        parts.append(\".\".join(str(x) for x in self.release))\n\n        # Pre-release\n        if self.pre is not None:\n            parts.append(\"\".join(str(x) for x in self.pre))\n\n        # Post-release\n        if self.post is not None:\n            parts.append(f\".post{self.post}\")\n\n        # Development release\n        if self.dev is not None:\n            parts.append(f\".dev{self.dev}\")\n\n        # Local version segment\n        if self.local is not None:\n            parts.append(f\"+{self.local}\")\n\n        return \"\".join(parts)\n\n    @property\n    def epoch(self) -> int:\n        \"\"\"The epoch of the version.\n\n        >>> Version(\"2.0.0\").epoch\n        0\n        >>> Version(\"1!2.0.0\").epoch\n        1\n        \"\"\"\n        return self._version.epoch\n\n    @property\n    def release(self) -> Tuple[int, ...]:\n        \"\"\"The components of the \"release\" segment of the version.\n\n        >>> Version(\"1.2.3\").release\n        (1, 2, 3)\n        >>> Version(\"2.0.0\").release\n        (2, 0, 0)\n        >>> Version(\"1!2.0.0.post0\").release\n        (2, 0, 0)\n\n        Includes trailing zeroes but not the epoch or any pre-release / development /\n        post-release suffixes.\n        \"\"\"\n        return self._version.release\n\n    @property\n    def pre(self) -> Optional[Tuple[str, int]]:\n        \"\"\"The pre-release segment of the version.\n\n        >>> print(Version(\"1.2.3\").pre)\n        None\n        >>> Version(\"1.2.3a1\").pre\n        ('a', 1)\n        >>> Version(\"1.2.3b1\").pre\n        ('b', 1)\n        >>> Version(\"1.2.3rc1\").pre\n        ('rc', 1)\n        \"\"\"\n        return self._version.pre\n\n    @property\n    def post(self) -> Optional[int]:\n        \"\"\"The post-release number of the version.\n\n        >>> print(Version(\"1.2.3\").post)\n        None\n        >>> Version(\"1.2.3.post1\").post\n        1\n        \"\"\"\n        return self._version.post[1] if self._version.post else None\n\n    @property\n    def dev(self) -> Optional[int]:\n        \"\"\"The development number of the version.\n\n        >>> print(Version(\"1.2.3\").dev)\n        None\n        >>> Version(\"1.2.3.dev1\").dev\n        1\n        \"\"\"\n        return self._version.dev[1] if self._version.dev else None\n\n    @property\n    def local(self) -> Optional[str]:\n        \"\"\"The local version segment of the version.\n\n        >>> print(Version(\"1.2.3\").local)\n        None\n        >>> Version(\"1.2.3+abc\").local\n        'abc'\n        \"\"\"\n        if self._version.local:\n            return \".\".join(str(x) for x in self._version.local)\n        else:\n            return None\n\n    @property\n    def public(self) -> str:\n        \"\"\"The public portion of the version.\n\n        >>> Version(\"1.2.3\").public\n        '1.2.3'\n        >>> Version(\"1.2.3+abc\").public\n        '1.2.3'\n        >>> Version(\"1.2.3+abc.dev1\").public\n        '1.2.3'\n        \"\"\"\n        return str(self).split(\"+\", 1)[0]\n\n    @property\n    def base_version(self) -> str:\n        \"\"\"The \"base version\" of the version.\n\n        >>> Version(\"1.2.3\").base_version\n        '1.2.3'\n        >>> Version(\"1.2.3+abc\").base_version\n        '1.2.3'\n        >>> Version(\"1!1.2.3+abc.dev1\").base_version\n        '1!1.2.3'\n\n        The \"base version\" is the public version of the project without any pre or post\n        release markers.\n        \"\"\"\n        parts = []\n\n        # Epoch\n        if self.epoch != 0:\n            parts.append(f\"{self.epoch}!\")\n\n        # Release segment\n        parts.append(\".\".join(str(x) for x in self.release))\n\n        return \"\".join(parts)\n\n    @property\n    def is_prerelease(self) -> bool:\n        \"\"\"Whether this version is a pre-release.\n\n        >>> Version(\"1.2.3\").is_prerelease\n        False\n        >>> Version(\"1.2.3a1\").is_prerelease\n        True\n        >>> Version(\"1.2.3b1\").is_prerelease\n        True\n        >>> Version(\"1.2.3rc1\").is_prerelease\n        True\n        >>> Version(\"1.2.3dev1\").is_prerelease\n        True\n        \"\"\"\n        return self.dev is not None or self.pre is not None\n\n    @property\n    def is_postrelease(self) -> bool:\n        \"\"\"Whether this version is a post-release.\n\n        >>> Version(\"1.2.3\").is_postrelease\n        False\n        >>> Version(\"1.2.3.post1\").is_postrelease\n        True\n        \"\"\"\n        return self.post is not None\n\n    @property\n    def is_devrelease(self) -> bool:\n        \"\"\"Whether this version is a development release.\n\n        >>> Version(\"1.2.3\").is_devrelease\n        False\n        >>> Version(\"1.2.3.dev1\").is_devrelease\n        True\n        \"\"\"\n        return self.dev is not None\n\n    @property\n    def major(self) -> int:\n        \"\"\"The first item of :attr:`release` or ``0`` if unavailable.\n\n        >>> Version(\"1.2.3\").major\n        1\n        \"\"\"\n        return self.release[0] if len(self.release) >= 1 else 0\n\n    @property\n    def minor(self) -> int:\n        \"\"\"The second item of :attr:`release` or ``0`` if unavailable.\n\n        >>> Version(\"1.2.3\").minor\n        2\n        >>> Version(\"1\").minor\n        0\n        \"\"\"\n        return self.release[1] if len(self.release) >= 2 else 0\n\n    @property\n    def micro(self) -> int:\n        \"\"\"The third item of :attr:`release` or ``0`` if unavailable.\n\n        >>> Version(\"1.2.3\").micro\n        3\n        >>> Version(\"1\").micro\n        0\n        \"\"\"\n        return self.release[2] if len(self.release) >= 3 else 0\n\n\ndef _parse_letter_version(\n    letter: Optional[str], number: Union[str, bytes, SupportsInt, None]\n) -> Optional[Tuple[str, int]]:\n    if letter:\n        # We consider there to be an implicit 0 in a pre-release if there is\n        # not a numeral associated with it.\n        if number is None:\n            number = 0\n\n        # We normalize any letters to their lower case form\n        letter = letter.lower()\n\n        # We consider some words to be alternate spellings of other words and\n        # in those cases we want to normalize the spellings to our preferred\n        # spelling.\n        if letter == \"alpha\":\n            letter = \"a\"\n        elif letter == \"beta\":\n            letter = \"b\"\n        elif letter in [\"c\", \"pre\", \"preview\"]:\n            letter = \"rc\"\n        elif letter in [\"rev\", \"r\"]:\n            letter = \"post\"\n\n        return letter, int(number)\n    if not letter and number:\n        # We assume if we are given a number, but we are not given a letter\n        # then this is using the implicit post release syntax (e.g. 1.0-1)\n        letter = \"post\"\n\n        return letter, int(number)\n\n    return None\n\n\n_local_version_separators = re.compile(r\"[\\._-]\")\n\n\ndef _parse_local_version(local: Optional[str]) -> Optional[LocalType]:\n    \"\"\"\n    Takes a string like abc.1.twelve and turns it into (\"abc\", 1, \"twelve\").\n    \"\"\"\n    if local is not None:\n        return tuple(\n            part.lower() if not part.isdigit() else int(part)\n            for part in _local_version_separators.split(local)\n        )\n    return None\n\n\ndef _cmpkey(\n    epoch: int,\n    release: Tuple[int, ...],\n    pre: Optional[Tuple[str, int]],\n    post: Optional[Tuple[str, int]],\n    dev: Optional[Tuple[str, int]],\n    local: Optional[LocalType],\n) -> CmpKey:\n    # When we compare a release version, we want to compare it with all of the\n    # trailing zeros removed. So we'll use a reverse the list, drop all the now\n    # leading zeros until we come to something non zero, then take the rest\n    # re-reverse it back into the correct order and make it a tuple and use\n    # that for our sorting key.\n    _release = tuple(\n        reversed(list(itertools.dropwhile(lambda x: x == 0, reversed(release))))\n    )\n\n    # We need to \"trick\" the sorting algorithm to put 1.0.dev0 before 1.0a0.\n    # We'll do this by abusing the pre segment, but we _only_ want to do this\n    # if there is not a pre or a post segment. If we have one of those then\n    # the normal sorting rules will handle this case correctly.\n    if pre is None and post is None and dev is not None:\n        _pre: CmpPrePostDevType = NegativeInfinity\n    # Versions without a pre-release (except as noted above) should sort after\n    # those with one.\n    elif pre is None:\n        _pre = Infinity\n    else:\n        _pre = pre\n\n    # Versions without a post segment should sort before those with one.\n    if post is None:\n        _post: CmpPrePostDevType = NegativeInfinity\n\n    else:\n        _post = post\n\n    # Versions without a development segment should sort after those with one.\n    if dev is None:\n        _dev: CmpPrePostDevType = Infinity\n\n    else:\n        _dev = dev\n\n    if local is None:\n        # Versions without a local segment should sort before those with one.\n        _local: CmpLocalType = NegativeInfinity\n    else:\n        # Versions with a local segment need that segment parsed to implement\n        # the sorting rules in PEP440.\n        # - Alpha numeric segments sort before numeric segments\n        # - Alpha numeric segments sort lexicographically\n        # - Numeric segments sort numerically\n        # - Shorter versions sort before longer versions when the prefixes\n        #   match exactly\n        _local = tuple(\n            (i, \"\") if isinstance(i, int) else (NegativeInfinity, i) for i in local\n        )\n\n    return epoch, _release, _pre, _post, _dev, _local\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/_vendor/wheel/wheelfile.py","size":7694,"sha1":"48395608996f9e4a1935d83fa2cd16166ee03e92","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"from __future__ import annotations\n\nimport csv\nimport hashlib\nimport os.path\nimport re\nimport stat\nimport time\nfrom io import StringIO, TextIOWrapper\nfrom zipfile import ZIP_DEFLATED, ZipFile, ZipInfo\n\nfrom wheel.cli import WheelError\nfrom wheel.util import log, urlsafe_b64decode, urlsafe_b64encode\n\n# Non-greedy matching of an optional build number may be too clever (more\n# invalid wheel filenames will match). Separate regex for .dist-info?\nWHEEL_INFO_RE = re.compile(\n    r\"\"\"^(?P<namever>(?P<name>[^\\s-]+?)-(?P<ver>[^\\s-]+?))(-(?P<build>\\d[^\\s-]*))?\n     -(?P<pyver>[^\\s-]+?)-(?P<abi>[^\\s-]+?)-(?P<plat>\\S+)\\.whl$\"\"\",\n    re.VERBOSE,\n)\nMINIMUM_TIMESTAMP = 315532800  # 1980-01-01 00:00:00 UTC\n\n\ndef get_zipinfo_datetime(timestamp=None):\n    # Some applications need reproducible .whl files, but they can't do this without\n    # forcing the timestamp of the individual ZipInfo objects. See issue #143.\n    timestamp = int(os.environ.get(\"SOURCE_DATE_EPOCH\", timestamp or time.time()))\n    timestamp = max(timestamp, MINIMUM_TIMESTAMP)\n    return time.gmtime(timestamp)[0:6]\n\n\nclass WheelFile(ZipFile):\n    \"\"\"A ZipFile derivative class that also reads SHA-256 hashes from\n    .dist-info/RECORD and checks any read files against those.\n    \"\"\"\n\n    _default_algorithm = hashlib.sha256\n\n    def __init__(self, file, mode=\"r\", compression=ZIP_DEFLATED):\n        basename = os.path.basename(file)\n        self.parsed_filename = WHEEL_INFO_RE.match(basename)\n        if not basename.endswith(\".whl\") or self.parsed_filename is None:\n            raise WheelError(f\"Bad wheel filename {basename!r}\")\n\n        ZipFile.__init__(self, file, mode, compression=compression, allowZip64=True)\n\n        self.dist_info_path = \"{}.dist-info\".format(\n            self.parsed_filename.group(\"namever\")\n        )\n        self.record_path = self.dist_info_path + \"/RECORD\"\n        self._file_hashes = {}\n        self._file_sizes = {}\n        if mode == \"r\":\n            # Ignore RECORD and any embedded wheel signatures\n            self._file_hashes[self.record_path] = None, None\n            self._file_hashes[self.record_path + \".jws\"] = None, None\n            self._file_hashes[self.record_path + \".p7s\"] = None, None\n\n            # Fill in the expected hashes by reading them from RECORD\n            try:\n                record = self.open(self.record_path)\n            except KeyError:\n                raise WheelError(f\"Missing {self.record_path} file\") from None\n\n            with record:\n                for line in csv.reader(\n                    TextIOWrapper(record, newline=\"\", encoding=\"utf-8\")\n                ):\n                    path, hash_sum, size = line\n                    if not hash_sum:\n                        continue\n\n                    algorithm, hash_sum = hash_sum.split(\"=\")\n                    try:\n                        hashlib.new(algorithm)\n                    except ValueError:\n                        raise WheelError(\n                            f\"Unsupported hash algorithm: {algorithm}\"\n                        ) from None\n\n                    if algorithm.lower() in {\"md5\", \"sha1\"}:\n                        raise WheelError(\n                            f\"Weak hash algorithm ({algorithm}) is not permitted by \"\n                            f\"PEP 427\"\n                        )\n\n                    self._file_hashes[path] = (\n                        algorithm,\n                        urlsafe_b64decode(hash_sum.encode(\"ascii\")),\n                    )\n\n    def open(self, name_or_info, mode=\"r\", pwd=None):\n        def _update_crc(newdata):\n            eof = ef._eof\n            update_crc_orig(newdata)\n            running_hash.update(newdata)\n            if eof and running_hash.digest() != expected_hash:\n                raise WheelError(f\"Hash mismatch for file '{ef_name}'\")\n\n        ef_name = (\n            name_or_info.filename if isinstance(name_or_info, ZipInfo) else name_or_info\n        )\n        if (\n            mode == \"r\"\n            and not ef_name.endswith(\"/\")\n            and ef_name not in self._file_hashes\n        ):\n            raise WheelError(f\"No hash found for file '{ef_name}'\")\n\n        ef = ZipFile.open(self, name_or_info, mode, pwd)\n        if mode == \"r\" and not ef_name.endswith(\"/\"):\n            algorithm, expected_hash = self._file_hashes[ef_name]\n            if expected_hash is not None:\n                # Monkey patch the _update_crc method to also check for the hash from\n                # RECORD\n                running_hash = hashlib.new(algorithm)\n                update_crc_orig, ef._update_crc = ef._update_crc, _update_crc\n\n        return ef\n\n    def write_files(self, base_dir):\n        log.info(f\"creating '{self.filename}' and adding '{base_dir}' to it\")\n        deferred = []\n        for root, dirnames, filenames in os.walk(base_dir):\n            # Sort the directory names so that `os.walk` will walk them in a\n            # defined order on the next iteration.\n            dirnames.sort()\n            for name in sorted(filenames):\n                path = os.path.normpath(os.path.join(root, name))\n                if os.path.isfile(path):\n                    arcname = os.path.relpath(path, base_dir).replace(os.path.sep, \"/\")\n                    if arcname == self.record_path:\n                        pass\n                    elif root.endswith(\".dist-info\"):\n                        deferred.append((path, arcname))\n                    else:\n                        self.write(path, arcname)\n\n        deferred.sort()\n        for path, arcname in deferred:\n            self.write(path, arcname)\n\n    def write(self, filename, arcname=None, compress_type=None):\n        with open(filename, \"rb\") as f:\n            st = os.fstat(f.fileno())\n            data = f.read()\n\n        zinfo = ZipInfo(\n            arcname or filename, date_time=get_zipinfo_datetime(st.st_mtime)\n        )\n        zinfo.external_attr = (stat.S_IMODE(st.st_mode) | stat.S_IFMT(st.st_mode)) << 16\n        zinfo.compress_type = compress_type or self.compression\n        self.writestr(zinfo, data, compress_type)\n\n    def writestr(self, zinfo_or_arcname, data, compress_type=None):\n        if isinstance(zinfo_or_arcname, str):\n            zinfo_or_arcname = ZipInfo(\n                zinfo_or_arcname, date_time=get_zipinfo_datetime()\n            )\n            zinfo_or_arcname.compress_type = self.compression\n            zinfo_or_arcname.external_attr = (0o664 | stat.S_IFREG) << 16\n\n        if isinstance(data, str):\n            data = data.encode(\"utf-8\")\n\n        ZipFile.writestr(self, zinfo_or_arcname, data, compress_type)\n        fname = (\n            zinfo_or_arcname.filename\n            if isinstance(zinfo_or_arcname, ZipInfo)\n            else zinfo_or_arcname\n        )\n        log.info(f\"adding '{fname}'\")\n        if fname != self.record_path:\n            hash_ = self._default_algorithm(data)\n            self._file_hashes[fname] = (\n                hash_.name,\n                urlsafe_b64encode(hash_.digest()).decode(\"ascii\"),\n            )\n            self._file_sizes[fname] = len(data)\n\n    def close(self):\n        # Write RECORD\n        if self.fp is not None and self.mode == \"w\" and self._file_hashes:\n            data = StringIO()\n            writer = csv.writer(data, delimiter=\",\", quotechar='\"', lineterminator=\"\\n\")\n            writer.writerows(\n                (\n                    (fname, algorithm + \"=\" + hash_, self._file_sizes[fname])\n                    for fname, (algorithm, hash_) in self._file_hashes.items()\n                )\n            )\n            writer.writerow((format(self.record_path), \"\", \"\"))\n            self.writestr(self.record_path, data.getvalue())\n\n        ZipFile.close(self)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/_vendor/zipp/__init__.py","size":13412,"sha1":"3eba9085ae4aeb575659ca4a5e4a7c2c51e6ae5e","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import io\nimport posixpath\nimport zipfile\nimport itertools\nimport contextlib\nimport pathlib\nimport re\nimport stat\nimport sys\n\nfrom .compat.py310 import text_encoding\nfrom .glob import Translator\n\n\n__all__ = ['Path']\n\n\ndef _parents(path):\n    \"\"\"\n    Given a path with elements separated by\n    posixpath.sep, generate all parents of that path.\n\n    >>> list(_parents('b/d'))\n    ['b']\n    >>> list(_parents('/b/d/'))\n    ['/b']\n    >>> list(_parents('b/d/f/'))\n    ['b/d', 'b']\n    >>> list(_parents('b'))\n    []\n    >>> list(_parents(''))\n    []\n    \"\"\"\n    return itertools.islice(_ancestry(path), 1, None)\n\n\ndef _ancestry(path):\n    \"\"\"\n    Given a path with elements separated by\n    posixpath.sep, generate all elements of that path\n\n    >>> list(_ancestry('b/d'))\n    ['b/d', 'b']\n    >>> list(_ancestry('/b/d/'))\n    ['/b/d', '/b']\n    >>> list(_ancestry('b/d/f/'))\n    ['b/d/f', 'b/d', 'b']\n    >>> list(_ancestry('b'))\n    ['b']\n    >>> list(_ancestry(''))\n    []\n    \"\"\"\n    path = path.rstrip(posixpath.sep)\n    while path and path != posixpath.sep:\n        yield path\n        path, tail = posixpath.split(path)\n\n\n_dedupe = dict.fromkeys\n\"\"\"Deduplicate an iterable in original order\"\"\"\n\n\ndef _difference(minuend, subtrahend):\n    \"\"\"\n    Return items in minuend not in subtrahend, retaining order\n    with O(1) lookup.\n    \"\"\"\n    return itertools.filterfalse(set(subtrahend).__contains__, minuend)\n\n\nclass InitializedState:\n    \"\"\"\n    Mix-in to save the initialization state for pickling.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        self.__args = args\n        self.__kwargs = kwargs\n        super().__init__(*args, **kwargs)\n\n    def __getstate__(self):\n        return self.__args, self.__kwargs\n\n    def __setstate__(self, state):\n        args, kwargs = state\n        super().__init__(*args, **kwargs)\n\n\nclass SanitizedNames:\n    \"\"\"\n    ZipFile mix-in to ensure names are sanitized.\n    \"\"\"\n\n    def namelist(self):\n        return list(map(self._sanitize, super().namelist()))\n\n    @staticmethod\n    def _sanitize(name):\n        r\"\"\"\n        Ensure a relative path with posix separators and no dot names.\n\n        Modeled after\n        https://github.com/python/cpython/blob/bcc1be39cb1d04ad9fc0bd1b9193d3972835a57c/Lib/zipfile/__init__.py#L1799-L1813\n        but provides consistent cross-platform behavior.\n\n        >>> san = SanitizedNames._sanitize\n        >>> san('/foo/bar')\n        'foo/bar'\n        >>> san('//foo.txt')\n        'foo.txt'\n        >>> san('foo/.././bar.txt')\n        'foo/bar.txt'\n        >>> san('foo../.bar.txt')\n        'foo../.bar.txt'\n        >>> san('\\\\foo\\\\bar.txt')\n        'foo/bar.txt'\n        >>> san('D:\\\\foo.txt')\n        'D/foo.txt'\n        >>> san('\\\\\\\\server\\\\share\\\\file.txt')\n        'server/share/file.txt'\n        >>> san('\\\\\\\\?\\\\GLOBALROOT\\\\Volume3')\n        '?/GLOBALROOT/Volume3'\n        >>> san('\\\\\\\\.\\\\PhysicalDrive1\\\\root')\n        'PhysicalDrive1/root'\n\n        Retain any trailing slash.\n        >>> san('abc/')\n        'abc/'\n\n        Raises a ValueError if the result is empty.\n        >>> san('../..')\n        Traceback (most recent call last):\n        ...\n        ValueError: Empty filename\n        \"\"\"\n\n        def allowed(part):\n            return part and part not in {'..', '.'}\n\n        # Remove the drive letter.\n        # Don't use ntpath.splitdrive, because that also strips UNC paths\n        bare = re.sub('^([A-Z]):', r'\\1', name, flags=re.IGNORECASE)\n        clean = bare.replace('\\\\', '/')\n        parts = clean.split('/')\n        joined = '/'.join(filter(allowed, parts))\n        if not joined:\n            raise ValueError(\"Empty filename\")\n        return joined + '/' * name.endswith('/')\n\n\nclass CompleteDirs(InitializedState, SanitizedNames, zipfile.ZipFile):\n    \"\"\"\n    A ZipFile subclass that ensures that implied directories\n    are always included in the namelist.\n\n    >>> list(CompleteDirs._implied_dirs(['foo/bar.txt', 'foo/bar/baz.txt']))\n    ['foo/', 'foo/bar/']\n    >>> list(CompleteDirs._implied_dirs(['foo/bar.txt', 'foo/bar/baz.txt', 'foo/bar/']))\n    ['foo/']\n    \"\"\"\n\n    @staticmethod\n    def _implied_dirs(names):\n        parents = itertools.chain.from_iterable(map(_parents, names))\n        as_dirs = (p + posixpath.sep for p in parents)\n        return _dedupe(_difference(as_dirs, names))\n\n    def namelist(self):\n        names = super().namelist()\n        return names + list(self._implied_dirs(names))\n\n    def _name_set(self):\n        return set(self.namelist())\n\n    def resolve_dir(self, name):\n        \"\"\"\n        If the name represents a directory, return that name\n        as a directory (with the trailing slash).\n        \"\"\"\n        names = self._name_set()\n        dirname = name + '/'\n        dir_match = name not in names and dirname in names\n        return dirname if dir_match else name\n\n    def getinfo(self, name):\n        \"\"\"\n        Supplement getinfo for implied dirs.\n        \"\"\"\n        try:\n            return super().getinfo(name)\n        except KeyError:\n            if not name.endswith('/') or name not in self._name_set():\n                raise\n            return zipfile.ZipInfo(filename=name)\n\n    @classmethod\n    def make(cls, source):\n        \"\"\"\n        Given a source (filename or zipfile), return an\n        appropriate CompleteDirs subclass.\n        \"\"\"\n        if isinstance(source, CompleteDirs):\n            return source\n\n        if not isinstance(source, zipfile.ZipFile):\n            return cls(source)\n\n        # Only allow for FastLookup when supplied zipfile is read-only\n        if 'r' not in source.mode:\n            cls = CompleteDirs\n\n        source.__class__ = cls\n        return source\n\n    @classmethod\n    def inject(cls, zf: zipfile.ZipFile) -> zipfile.ZipFile:\n        \"\"\"\n        Given a writable zip file zf, inject directory entries for\n        any directories implied by the presence of children.\n        \"\"\"\n        for name in cls._implied_dirs(zf.namelist()):\n            zf.writestr(name, b\"\")\n        return zf\n\n\nclass FastLookup(CompleteDirs):\n    \"\"\"\n    ZipFile subclass to ensure implicit\n    dirs exist and are resolved rapidly.\n    \"\"\"\n\n    def namelist(self):\n        with contextlib.suppress(AttributeError):\n            return self.__names\n        self.__names = super().namelist()\n        return self.__names\n\n    def _name_set(self):\n        with contextlib.suppress(AttributeError):\n            return self.__lookup\n        self.__lookup = super()._name_set()\n        return self.__lookup\n\n\ndef _extract_text_encoding(encoding=None, *args, **kwargs):\n    # compute stack level so that the caller of the caller sees any warning.\n    is_pypy = sys.implementation.name == 'pypy'\n    stack_level = 3 + is_pypy\n    return text_encoding(encoding, stack_level), args, kwargs\n\n\nclass Path:\n    \"\"\"\n    A :class:`importlib.resources.abc.Traversable` interface for zip files.\n\n    Implements many of the features users enjoy from\n    :class:`pathlib.Path`.\n\n    Consider a zip file with this structure::\n\n        .\n         a.txt\n         b\n             c.txt\n             d\n                 e.txt\n\n    >>> data = io.BytesIO()\n    >>> zf = zipfile.ZipFile(data, 'w')\n    >>> zf.writestr('a.txt', 'content of a')\n    >>> zf.writestr('b/c.txt', 'content of c')\n    >>> zf.writestr('b/d/e.txt', 'content of e')\n    >>> zf.filename = 'mem/abcde.zip'\n\n    Path accepts the zipfile object itself or a filename\n\n    >>> path = Path(zf)\n\n    From there, several path operations are available.\n\n    Directory iteration (including the zip file itself):\n\n    >>> a, b = path.iterdir()\n    >>> a\n    Path('mem/abcde.zip', 'a.txt')\n    >>> b\n    Path('mem/abcde.zip', 'b/')\n\n    name property:\n\n    >>> b.name\n    'b'\n\n    join with divide operator:\n\n    >>> c = b / 'c.txt'\n    >>> c\n    Path('mem/abcde.zip', 'b/c.txt')\n    >>> c.name\n    'c.txt'\n\n    Read text:\n\n    >>> c.read_text(encoding='utf-8')\n    'content of c'\n\n    existence:\n\n    >>> c.exists()\n    True\n    >>> (b / 'missing.txt').exists()\n    False\n\n    Coercion to string:\n\n    >>> import os\n    >>> str(c).replace(os.sep, posixpath.sep)\n    'mem/abcde.zip/b/c.txt'\n\n    At the root, ``name``, ``filename``, and ``parent``\n    resolve to the zipfile.\n\n    >>> str(path)\n    'mem/abcde.zip/'\n    >>> path.name\n    'abcde.zip'\n    >>> path.filename == pathlib.Path('mem/abcde.zip')\n    True\n    >>> str(path.parent)\n    'mem'\n\n    If the zipfile has no filename, such attributes are not\n    valid and accessing them will raise an Exception.\n\n    >>> zf.filename = None\n    >>> path.name\n    Traceback (most recent call last):\n    ...\n    TypeError: ...\n\n    >>> path.filename\n    Traceback (most recent call last):\n    ...\n    TypeError: ...\n\n    >>> path.parent\n    Traceback (most recent call last):\n    ...\n    TypeError: ...\n\n    # workaround python/cpython#106763\n    >>> pass\n    \"\"\"\n\n    __repr = \"{self.__class__.__name__}({self.root.filename!r}, {self.at!r})\"\n\n    def __init__(self, root, at=\"\"):\n        \"\"\"\n        Construct a Path from a ZipFile or filename.\n\n        Note: When the source is an existing ZipFile object,\n        its type (__class__) will be mutated to a\n        specialized type. If the caller wishes to retain the\n        original type, the caller should either create a\n        separate ZipFile object or pass a filename.\n        \"\"\"\n        self.root = FastLookup.make(root)\n        self.at = at\n\n    def __eq__(self, other):\n        \"\"\"\n        >>> Path(zipfile.ZipFile(io.BytesIO(), 'w')) == 'foo'\n        False\n        \"\"\"\n        if self.__class__ is not other.__class__:\n            return NotImplemented\n        return (self.root, self.at) == (other.root, other.at)\n\n    def __hash__(self):\n        return hash((self.root, self.at))\n\n    def open(self, mode='r', *args, pwd=None, **kwargs):\n        \"\"\"\n        Open this entry as text or binary following the semantics\n        of ``pathlib.Path.open()`` by passing arguments through\n        to io.TextIOWrapper().\n        \"\"\"\n        if self.is_dir():\n            raise IsADirectoryError(self)\n        zip_mode = mode[0]\n        if not self.exists() and zip_mode == 'r':\n            raise FileNotFoundError(self)\n        stream = self.root.open(self.at, zip_mode, pwd=pwd)\n        if 'b' in mode:\n            if args or kwargs:\n                raise ValueError(\"encoding args invalid for binary operation\")\n            return stream\n        # Text mode:\n        encoding, args, kwargs = _extract_text_encoding(*args, **kwargs)\n        return io.TextIOWrapper(stream, encoding, *args, **kwargs)\n\n    def _base(self):\n        return pathlib.PurePosixPath(self.at or self.root.filename)\n\n    @property\n    def name(self):\n        return self._base().name\n\n    @property\n    def suffix(self):\n        return self._base().suffix\n\n    @property\n    def suffixes(self):\n        return self._base().suffixes\n\n    @property\n    def stem(self):\n        return self._base().stem\n\n    @property\n    def filename(self):\n        return pathlib.Path(self.root.filename).joinpath(self.at)\n\n    def read_text(self, *args, **kwargs):\n        encoding, args, kwargs = _extract_text_encoding(*args, **kwargs)\n        with self.open('r', encoding, *args, **kwargs) as strm:\n            return strm.read()\n\n    def read_bytes(self):\n        with self.open('rb') as strm:\n            return strm.read()\n\n    def _is_child(self, path):\n        return posixpath.dirname(path.at.rstrip(\"/\")) == self.at.rstrip(\"/\")\n\n    def _next(self, at):\n        return self.__class__(self.root, at)\n\n    def is_dir(self):\n        return not self.at or self.at.endswith(\"/\")\n\n    def is_file(self):\n        return self.exists() and not self.is_dir()\n\n    def exists(self):\n        return self.at in self.root._name_set()\n\n    def iterdir(self):\n        if not self.is_dir():\n            raise ValueError(\"Can't listdir a file\")\n        subs = map(self._next, self.root.namelist())\n        return filter(self._is_child, subs)\n\n    def match(self, path_pattern):\n        return pathlib.PurePosixPath(self.at).match(path_pattern)\n\n    def is_symlink(self):\n        \"\"\"\n        Return whether this path is a symlink.\n        \"\"\"\n        info = self.root.getinfo(self.at)\n        mode = info.external_attr >> 16\n        return stat.S_ISLNK(mode)\n\n    def glob(self, pattern):\n        if not pattern:\n            raise ValueError(f\"Unacceptable pattern: {pattern!r}\")\n\n        prefix = re.escape(self.at)\n        tr = Translator(seps='/')\n        matches = re.compile(prefix + tr.translate(pattern)).fullmatch\n        names = (data.filename for data in self.root.filelist)\n        return map(self._next, filter(matches, names))\n\n    def rglob(self, pattern):\n        return self.glob(f'**/{pattern}')\n\n    def relative_to(self, other, *extra):\n        return posixpath.relpath(str(self), str(other.joinpath(*extra)))\n\n    def __str__(self):\n        return posixpath.join(self.root.filename, self.at)\n\n    def __repr__(self):\n        return self.__repr.format(self=self)\n\n    def joinpath(self, *other):\n        next = posixpath.join(self.at, *other)\n        return self._next(self.root.resolve_dir(next))\n\n    __truediv__ = joinpath\n\n    @property\n    def parent(self):\n        if not self.at:\n            return self.filename.parent\n        parent_at = posixpath.dirname(self.at.rstrip('/'))\n        if parent_at:\n            parent_at += '/'\n        return self._next(parent_at)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/_vendor/zipp/compat/__init__.py","size":0,"sha1":"da39a3ee5e6b4b0d3255bfef95601890afd80709","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":""},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/_vendor/zipp/compat/py310.py","size":219,"sha1":"2321f91dbd8b2842d69de41407e13a7761e5736e","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import sys\nimport io\n\n\ndef _text_encoding(encoding, stacklevel=2, /):  # pragma: no cover\n    return encoding\n\n\ntext_encoding = (\n    io.text_encoding if sys.version_info > (3, 10) else _text_encoding  # type: ignore\n)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/_vendor/zipp/glob.py","size":3082,"sha1":"06d2207e9fa9e09d25a8339d07e7d4763c0ab4eb","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import os\nimport re\n\n\n_default_seps = os.sep + str(os.altsep) * bool(os.altsep)\n\n\nclass Translator:\n    \"\"\"\n    >>> Translator('xyz')\n    Traceback (most recent call last):\n    ...\n    AssertionError: Invalid separators\n\n    >>> Translator('')\n    Traceback (most recent call last):\n    ...\n    AssertionError: Invalid separators\n    \"\"\"\n\n    seps: str\n\n    def __init__(self, seps: str = _default_seps):\n        assert seps and set(seps) <= set(_default_seps), \"Invalid separators\"\n        self.seps = seps\n\n    def translate(self, pattern):\n        \"\"\"\n        Given a glob pattern, produce a regex that matches it.\n        \"\"\"\n        return self.extend(self.translate_core(pattern))\n\n    def extend(self, pattern):\n        r\"\"\"\n        Extend regex for pattern-wide concerns.\n\n        Apply '(?s:)' to create a non-matching group that\n        matches newlines (valid on Unix).\n\n        Append '\\Z' to imply fullmatch even when match is used.\n        \"\"\"\n        return rf'(?s:{pattern})\\Z'\n\n    def translate_core(self, pattern):\n        r\"\"\"\n        Given a glob pattern, produce a regex that matches it.\n\n        >>> t = Translator()\n        >>> t.translate_core('*.txt').replace('\\\\\\\\', '')\n        '[^/]*\\\\.txt'\n        >>> t.translate_core('a?txt')\n        'a[^/]txt'\n        >>> t.translate_core('**/*').replace('\\\\\\\\', '')\n        '.*/[^/][^/]*'\n        \"\"\"\n        self.restrict_rglob(pattern)\n        return ''.join(map(self.replace, separate(self.star_not_empty(pattern))))\n\n    def replace(self, match):\n        \"\"\"\n        Perform the replacements for a match from :func:`separate`.\n        \"\"\"\n        return match.group('set') or (\n            re.escape(match.group(0))\n            .replace('\\\\*\\\\*', r'.*')\n            .replace('\\\\*', rf'[^{re.escape(self.seps)}]*')\n            .replace('\\\\?', r'[^/]')\n        )\n\n    def restrict_rglob(self, pattern):\n        \"\"\"\n        Raise ValueError if ** appears in anything but a full path segment.\n\n        >>> Translator().translate('**foo')\n        Traceback (most recent call last):\n        ...\n        ValueError: ** must appear alone in a path segment\n        \"\"\"\n        seps_pattern = rf'[{re.escape(self.seps)}]+'\n        segments = re.split(seps_pattern, pattern)\n        if any('**' in segment and segment != '**' for segment in segments):\n            raise ValueError(\"** must appear alone in a path segment\")\n\n    def star_not_empty(self, pattern):\n        \"\"\"\n        Ensure that * will not match an empty segment.\n        \"\"\"\n\n        def handle_segment(match):\n            segment = match.group(0)\n            return '?*' if segment == '*' else segment\n\n        not_seps_pattern = rf'[^{re.escape(self.seps)}]+'\n        return re.sub(not_seps_pattern, handle_segment, pattern)\n\n\ndef separate(pattern):\n    \"\"\"\n    Separate out character sets to avoid translating their contents.\n\n    >>> [m.group(0) for m in separate('*.txt')]\n    ['*.txt']\n    >>> [m.group(0) for m in separate('a[?]txt')]\n    ['a', '[?]', 'txt']\n    \"\"\"\n    return re.finditer(r'([^\\[]+)|(?P<set>[\\[].*?[\\]])|([\\[][^\\]]*$)', pattern)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/archive_util.py","size":7356,"sha1":"879aa35c91dbbbe70756e25a5d43ee9ec57bd0a7","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"Utilities for extracting common archive formats\"\"\"\n\nimport contextlib\nimport os\nimport posixpath\nimport shutil\nimport tarfile\nimport zipfile\n\nfrom ._path import ensure_directory\n\nfrom distutils.errors import DistutilsError\n\n__all__ = [\n    \"unpack_archive\",\n    \"unpack_zipfile\",\n    \"unpack_tarfile\",\n    \"default_filter\",\n    \"UnrecognizedFormat\",\n    \"extraction_drivers\",\n    \"unpack_directory\",\n]\n\n\nclass UnrecognizedFormat(DistutilsError):\n    \"\"\"Couldn't recognize the archive type\"\"\"\n\n\ndef default_filter(src, dst):\n    \"\"\"The default progress/filter callback; returns True for all files\"\"\"\n    return dst\n\n\ndef unpack_archive(\n    filename, extract_dir, progress_filter=default_filter, drivers=None\n) -> None:\n    \"\"\"Unpack `filename` to `extract_dir`, or raise ``UnrecognizedFormat``\n\n    `progress_filter` is a function taking two arguments: a source path\n    internal to the archive ('/'-separated), and a filesystem path where it\n    will be extracted.  The callback must return the desired extract path\n    (which may be the same as the one passed in), or else ``None`` to skip\n    that file or directory.  The callback can thus be used to report on the\n    progress of the extraction, as well as to filter the items extracted or\n    alter their extraction paths.\n\n    `drivers`, if supplied, must be a non-empty sequence of functions with the\n    same signature as this function (minus the `drivers` argument), that raise\n    ``UnrecognizedFormat`` if they do not support extracting the designated\n    archive type.  The `drivers` are tried in sequence until one is found that\n    does not raise an error, or until all are exhausted (in which case\n    ``UnrecognizedFormat`` is raised).  If you do not supply a sequence of\n    drivers, the module's ``extraction_drivers`` constant will be used, which\n    means that ``unpack_zipfile`` and ``unpack_tarfile`` will be tried, in that\n    order.\n    \"\"\"\n    for driver in drivers or extraction_drivers:\n        try:\n            driver(filename, extract_dir, progress_filter)\n        except UnrecognizedFormat:\n            continue\n        else:\n            return\n    else:\n        raise UnrecognizedFormat(f\"Not a recognized archive type: {filename}\")\n\n\ndef unpack_directory(filename, extract_dir, progress_filter=default_filter) -> None:\n    \"\"\" \"Unpack\" a directory, using the same interface as for archives\n\n    Raises ``UnrecognizedFormat`` if `filename` is not a directory\n    \"\"\"\n    if not os.path.isdir(filename):\n        raise UnrecognizedFormat(f\"{filename} is not a directory\")\n\n    paths = {\n        filename: ('', extract_dir),\n    }\n    for base, dirs, files in os.walk(filename):\n        src, dst = paths[base]\n        for d in dirs:\n            paths[os.path.join(base, d)] = src + d + '/', os.path.join(dst, d)\n        for f in files:\n            target = os.path.join(dst, f)\n            target = progress_filter(src + f, target)\n            if not target:\n                # skip non-files\n                continue\n            ensure_directory(target)\n            f = os.path.join(base, f)\n            shutil.copyfile(f, target)\n            shutil.copystat(f, target)\n\n\ndef unpack_zipfile(filename, extract_dir, progress_filter=default_filter) -> None:\n    \"\"\"Unpack zip `filename` to `extract_dir`\n\n    Raises ``UnrecognizedFormat`` if `filename` is not a zipfile (as determined\n    by ``zipfile.is_zipfile()``).  See ``unpack_archive()`` for an explanation\n    of the `progress_filter` argument.\n    \"\"\"\n\n    if not zipfile.is_zipfile(filename):\n        raise UnrecognizedFormat(f\"{filename} is not a zip file\")\n\n    with zipfile.ZipFile(filename) as z:\n        _unpack_zipfile_obj(z, extract_dir, progress_filter)\n\n\ndef _unpack_zipfile_obj(zipfile_obj, extract_dir, progress_filter=default_filter):\n    \"\"\"Internal/private API used by other parts of setuptools.\n    Similar to ``unpack_zipfile``, but receives an already opened :obj:`zipfile.ZipFile`\n    object instead of a filename.\n    \"\"\"\n    for info in zipfile_obj.infolist():\n        name = info.filename\n\n        # don't extract absolute paths or ones with .. in them\n        if name.startswith('/') or '..' in name.split('/'):\n            continue\n\n        target = os.path.join(extract_dir, *name.split('/'))\n        target = progress_filter(name, target)\n        if not target:\n            continue\n        if name.endswith('/'):\n            # directory\n            ensure_directory(target)\n        else:\n            # file\n            ensure_directory(target)\n            data = zipfile_obj.read(info.filename)\n            with open(target, 'wb') as f:\n                f.write(data)\n        unix_attributes = info.external_attr >> 16\n        if unix_attributes:\n            os.chmod(target, unix_attributes)\n\n\ndef _resolve_tar_file_or_dir(tar_obj, tar_member_obj):\n    \"\"\"Resolve any links and extract link targets as normal files.\"\"\"\n    while tar_member_obj is not None and (\n        tar_member_obj.islnk() or tar_member_obj.issym()\n    ):\n        linkpath = tar_member_obj.linkname\n        if tar_member_obj.issym():\n            base = posixpath.dirname(tar_member_obj.name)\n            linkpath = posixpath.join(base, linkpath)\n            linkpath = posixpath.normpath(linkpath)\n        tar_member_obj = tar_obj._getmember(linkpath)\n\n    is_file_or_dir = tar_member_obj is not None and (\n        tar_member_obj.isfile() or tar_member_obj.isdir()\n    )\n    if is_file_or_dir:\n        return tar_member_obj\n\n    raise LookupError('Got unknown file type')\n\n\ndef _iter_open_tar(tar_obj, extract_dir, progress_filter):\n    \"\"\"Emit member-destination pairs from a tar archive.\"\"\"\n    # don't do any chowning!\n    tar_obj.chown = lambda *args: None\n\n    with contextlib.closing(tar_obj):\n        for member in tar_obj:\n            name = member.name\n            # don't extract absolute paths or ones with .. in them\n            if name.startswith('/') or '..' in name.split('/'):\n                continue\n\n            prelim_dst = os.path.join(extract_dir, *name.split('/'))\n\n            try:\n                member = _resolve_tar_file_or_dir(tar_obj, member)\n            except LookupError:\n                continue\n\n            final_dst = progress_filter(name, prelim_dst)\n            if not final_dst:\n                continue\n\n            if final_dst.endswith(os.sep):\n                final_dst = final_dst[:-1]\n\n            yield member, final_dst\n\n\ndef unpack_tarfile(filename, extract_dir, progress_filter=default_filter) -> bool:\n    \"\"\"Unpack tar/tar.gz/tar.bz2 `filename` to `extract_dir`\n\n    Raises ``UnrecognizedFormat`` if `filename` is not a tarfile (as determined\n    by ``tarfile.open()``).  See ``unpack_archive()`` for an explanation\n    of the `progress_filter` argument.\n    \"\"\"\n    try:\n        tarobj = tarfile.open(filename)\n    except tarfile.TarError as e:\n        raise UnrecognizedFormat(\n            f\"{filename} is not a compressed or uncompressed tar file\"\n        ) from e\n\n    for member, final_dst in _iter_open_tar(\n        tarobj,\n        extract_dir,\n        progress_filter,\n    ):\n        try:\n            # XXX Ugh\n            tarobj._extract_member(member, final_dst)\n        except tarfile.ExtractError:\n            # chown/chmod/mkfifo/mknode/makedev failed\n            pass\n\n    return True\n\n\nextraction_drivers = unpack_directory, unpack_zipfile, unpack_tarfile\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/build_meta.py","size":20446,"sha1":"21c00895a3ae8e9c70ee2e3cd5d9710911a46454","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"A PEP 517 interface to setuptools\n\nPreviously, when a user or a command line tool (let's call it a \"frontend\")\nneeded to make a request of setuptools to take a certain action, for\nexample, generating a list of installation requirements, the frontend\nwould call \"setup.py egg_info\" or \"setup.py bdist_wheel\" on the command line.\n\nPEP 517 defines a different method of interfacing with setuptools. Rather\nthan calling \"setup.py\" directly, the frontend should:\n\n  1. Set the current directory to the directory with a setup.py file\n  2. Import this module into a safe python interpreter (one in which\n     setuptools can potentially set global variables or crash hard).\n  3. Call one of the functions defined in PEP 517.\n\nWhat each function does is defined in PEP 517. However, here is a \"casual\"\ndefinition of the functions (this definition should not be relied on for\nbug reports or API stability):\n\n  - `build_wheel`: build a wheel in the folder and return the basename\n  - `get_requires_for_build_wheel`: get the `setup_requires` to build\n  - `prepare_metadata_for_build_wheel`: get the `install_requires`\n  - `build_sdist`: build an sdist in the folder and return the basename\n  - `get_requires_for_build_sdist`: get the `setup_requires` to build\n\nAgain, this is not a formal definition! Just a \"taste\" of the module.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport contextlib\nimport io\nimport os\nimport shlex\nimport shutil\nimport sys\nimport tempfile\nimport tokenize\nimport warnings\nfrom collections.abc import Iterable, Iterator, Mapping\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING, Union\n\nimport setuptools\n\nfrom . import errors\nfrom ._path import StrPath, same_path\nfrom ._reqs import parse_strings\nfrom .warnings import SetuptoolsDeprecationWarning\n\nimport distutils\nfrom distutils.util import strtobool\n\nif TYPE_CHECKING:\n    from typing_extensions import TypeAlias\n\n__all__ = [\n    'get_requires_for_build_sdist',\n    'get_requires_for_build_wheel',\n    'prepare_metadata_for_build_wheel',\n    'build_wheel',\n    'build_sdist',\n    'get_requires_for_build_editable',\n    'prepare_metadata_for_build_editable',\n    'build_editable',\n    '__legacy__',\n    'SetupRequirementsError',\n]\n\nSETUPTOOLS_ENABLE_FEATURES = os.getenv(\"SETUPTOOLS_ENABLE_FEATURES\", \"\").lower()\nLEGACY_EDITABLE = \"legacy-editable\" in SETUPTOOLS_ENABLE_FEATURES.replace(\"_\", \"-\")\n\n\nclass SetupRequirementsError(BaseException):\n    def __init__(self, specifiers) -> None:\n        self.specifiers = specifiers\n\n\nclass Distribution(setuptools.dist.Distribution):\n    def fetch_build_eggs(self, specifiers):\n        specifier_list = list(parse_strings(specifiers))\n\n        raise SetupRequirementsError(specifier_list)\n\n    @classmethod\n    @contextlib.contextmanager\n    def patch(cls):\n        \"\"\"\n        Replace\n        distutils.dist.Distribution with this class\n        for the duration of this context.\n        \"\"\"\n        orig = distutils.core.Distribution\n        distutils.core.Distribution = cls  # type: ignore[misc] # monkeypatching\n        try:\n            yield\n        finally:\n            distutils.core.Distribution = orig  # type: ignore[misc] # monkeypatching\n\n\n@contextlib.contextmanager\ndef no_install_setup_requires():\n    \"\"\"Temporarily disable installing setup_requires\n\n    Under PEP 517, the backend reports build dependencies to the frontend,\n    and the frontend is responsible for ensuring they're installed.\n    So setuptools (acting as a backend) should not try to install them.\n    \"\"\"\n    orig = setuptools._install_setup_requires\n    setuptools._install_setup_requires = lambda attrs: None\n    try:\n        yield\n    finally:\n        setuptools._install_setup_requires = orig\n\n\ndef _get_immediate_subdirectories(a_dir):\n    return [\n        name for name in os.listdir(a_dir) if os.path.isdir(os.path.join(a_dir, name))\n    ]\n\n\ndef _file_with_extension(directory: StrPath, extension: str | tuple[str, ...]):\n    matching = (f for f in os.listdir(directory) if f.endswith(extension))\n    try:\n        (file,) = matching\n    except ValueError:\n        raise ValueError(\n            'No distribution was found. Ensure that `setup.py` '\n            'is not empty and that it calls `setup()`.'\n        ) from None\n    return file\n\n\ndef _open_setup_script(setup_script):\n    if not os.path.exists(setup_script):\n        # Supply a default setup.py\n        return io.StringIO(\"from setuptools import setup; setup()\")\n\n    return tokenize.open(setup_script)\n\n\n@contextlib.contextmanager\ndef suppress_known_deprecation():\n    with warnings.catch_warnings():\n        warnings.filterwarnings('ignore', 'setup.py install is deprecated')\n        yield\n\n\n_ConfigSettings: TypeAlias = Union[Mapping[str, Union[str, list[str], None]], None]\n\"\"\"\nCurrently the user can run::\n\n    pip install -e . --config-settings key=value\n    python -m build -C--key=value -C key=value\n\n- pip will pass both key and value as strings and overwriting repeated keys\n  (pypa/pip#11059).\n- build will accumulate values associated with repeated keys in a list.\n  It will also accept keys with no associated value.\n  This means that an option passed by build can be ``str | list[str] | None``.\n- PEP 517 specifies that ``config_settings`` is an optional dict.\n\"\"\"\n\n\nclass _ConfigSettingsTranslator:\n    \"\"\"Translate ``config_settings`` into distutils-style command arguments.\n    Only a limited number of options is currently supported.\n    \"\"\"\n\n    # See pypa/setuptools#1928 pypa/setuptools#2491\n\n    def _get_config(self, key: str, config_settings: _ConfigSettings) -> list[str]:\n        \"\"\"\n        Get the value of a specific key in ``config_settings`` as a list of strings.\n\n        >>> fn = _ConfigSettingsTranslator()._get_config\n        >>> fn(\"--global-option\", None)\n        []\n        >>> fn(\"--global-option\", {})\n        []\n        >>> fn(\"--global-option\", {'--global-option': 'foo'})\n        ['foo']\n        >>> fn(\"--global-option\", {'--global-option': ['foo']})\n        ['foo']\n        >>> fn(\"--global-option\", {'--global-option': 'foo'})\n        ['foo']\n        >>> fn(\"--global-option\", {'--global-option': 'foo bar'})\n        ['foo', 'bar']\n        \"\"\"\n        cfg = config_settings or {}\n        opts = cfg.get(key) or []\n        return shlex.split(opts) if isinstance(opts, str) else opts\n\n    def _global_args(self, config_settings: _ConfigSettings) -> Iterator[str]:\n        \"\"\"\n        Let the user specify ``verbose`` or ``quiet`` + escape hatch via\n        ``--global-option``.\n        Note: ``-v``, ``-vv``, ``-vvv`` have similar effects in setuptools,\n        so we just have to cover the basic scenario ``-v``.\n\n        >>> fn = _ConfigSettingsTranslator()._global_args\n        >>> list(fn(None))\n        []\n        >>> list(fn({\"verbose\": \"False\"}))\n        ['-q']\n        >>> list(fn({\"verbose\": \"1\"}))\n        ['-v']\n        >>> list(fn({\"--verbose\": None}))\n        ['-v']\n        >>> list(fn({\"verbose\": \"true\", \"--global-option\": \"-q --no-user-cfg\"}))\n        ['-v', '-q', '--no-user-cfg']\n        >>> list(fn({\"--quiet\": None}))\n        ['-q']\n        \"\"\"\n        cfg = config_settings or {}\n        falsey = {\"false\", \"no\", \"0\", \"off\"}\n        if \"verbose\" in cfg or \"--verbose\" in cfg:\n            level = str(cfg.get(\"verbose\") or cfg.get(\"--verbose\") or \"1\")\n            yield (\"-q\" if level.lower() in falsey else \"-v\")\n        if \"quiet\" in cfg or \"--quiet\" in cfg:\n            level = str(cfg.get(\"quiet\") or cfg.get(\"--quiet\") or \"1\")\n            yield (\"-v\" if level.lower() in falsey else \"-q\")\n\n        yield from self._get_config(\"--global-option\", config_settings)\n\n    def __dist_info_args(self, config_settings: _ConfigSettings) -> Iterator[str]:\n        \"\"\"\n        The ``dist_info`` command accepts ``tag-date`` and ``tag-build``.\n\n        .. warning::\n           We cannot use this yet as it requires the ``sdist`` and ``bdist_wheel``\n           commands run in ``build_sdist`` and ``build_wheel`` to reuse the egg-info\n           directory created in ``prepare_metadata_for_build_wheel``.\n\n        >>> fn = _ConfigSettingsTranslator()._ConfigSettingsTranslator__dist_info_args\n        >>> list(fn(None))\n        []\n        >>> list(fn({\"tag-date\": \"False\"}))\n        ['--no-date']\n        >>> list(fn({\"tag-date\": None}))\n        ['--no-date']\n        >>> list(fn({\"tag-date\": \"true\", \"tag-build\": \".a\"}))\n        ['--tag-date', '--tag-build', '.a']\n        \"\"\"\n        cfg = config_settings or {}\n        if \"tag-date\" in cfg:\n            val = strtobool(str(cfg[\"tag-date\"] or \"false\"))\n            yield (\"--tag-date\" if val else \"--no-date\")\n        if \"tag-build\" in cfg:\n            yield from [\"--tag-build\", str(cfg[\"tag-build\"])]\n\n    def _editable_args(self, config_settings: _ConfigSettings) -> Iterator[str]:\n        \"\"\"\n        The ``editable_wheel`` command accepts ``editable-mode=strict``.\n\n        >>> fn = _ConfigSettingsTranslator()._editable_args\n        >>> list(fn(None))\n        []\n        >>> list(fn({\"editable-mode\": \"strict\"}))\n        ['--mode', 'strict']\n        \"\"\"\n        cfg = config_settings or {}\n        mode = cfg.get(\"editable-mode\") or cfg.get(\"editable_mode\")\n        if not mode:\n            return\n        yield from [\"--mode\", str(mode)]\n\n    def _arbitrary_args(self, config_settings: _ConfigSettings) -> Iterator[str]:\n        \"\"\"\n        Users may expect to pass arbitrary lists of arguments to a command\n        via \"--global-option\" (example provided in PEP 517 of a \"escape hatch\").\n\n        >>> fn = _ConfigSettingsTranslator()._arbitrary_args\n        >>> list(fn(None))\n        []\n        >>> list(fn({}))\n        []\n        >>> list(fn({'--build-option': 'foo'}))\n        ['foo']\n        >>> list(fn({'--build-option': ['foo']}))\n        ['foo']\n        >>> list(fn({'--build-option': 'foo'}))\n        ['foo']\n        >>> list(fn({'--build-option': 'foo bar'}))\n        ['foo', 'bar']\n        >>> list(fn({'--global-option': 'foo'}))\n        []\n        \"\"\"\n        yield from self._get_config(\"--build-option\", config_settings)\n\n\nclass _BuildMetaBackend(_ConfigSettingsTranslator):\n    def _get_build_requires(\n        self, config_settings: _ConfigSettings, requirements: list[str]\n    ):\n        sys.argv = [\n            *sys.argv[:1],\n            *self._global_args(config_settings),\n            \"egg_info\",\n        ]\n        try:\n            with Distribution.patch():\n                self.run_setup()\n        except SetupRequirementsError as e:\n            requirements += e.specifiers\n\n        return requirements\n\n    def run_setup(self, setup_script: str = 'setup.py'):\n        # Note that we can reuse our build directory between calls\n        # Correctness comes first, then optimization later\n        __file__ = os.path.abspath(setup_script)\n        __name__ = '__main__'\n\n        with _open_setup_script(__file__) as f:\n            code = f.read().replace(r'\\r\\n', r'\\n')\n\n        try:\n            exec(code, locals())\n        except SystemExit as e:\n            if e.code:\n                raise\n            # We ignore exit code indicating success\n            SetuptoolsDeprecationWarning.emit(\n                \"Running `setup.py` directly as CLI tool is deprecated.\",\n                \"Please avoid using `sys.exit(0)` or similar statements \"\n                \"that don't fit in the paradigm of a configuration file.\",\n                see_url=\"https://blog.ganssle.io/articles/2021/10/\"\n                \"setup-py-deprecated.html\",\n            )\n\n    def get_requires_for_build_wheel(self, config_settings: _ConfigSettings = None):\n        return self._get_build_requires(config_settings, requirements=[])\n\n    def get_requires_for_build_sdist(self, config_settings: _ConfigSettings = None):\n        return self._get_build_requires(config_settings, requirements=[])\n\n    def _bubble_up_info_directory(\n        self, metadata_directory: StrPath, suffix: str\n    ) -> str:\n        \"\"\"\n        PEP 517 requires that the .dist-info directory be placed in the\n        metadata_directory. To comply, we MUST copy the directory to the root.\n\n        Returns the basename of the info directory, e.g. `proj-0.0.0.dist-info`.\n        \"\"\"\n        info_dir = self._find_info_directory(metadata_directory, suffix)\n        if not same_path(info_dir.parent, metadata_directory):\n            shutil.move(str(info_dir), metadata_directory)\n            # PEP 517 allow other files and dirs to exist in metadata_directory\n        return info_dir.name\n\n    def _find_info_directory(self, metadata_directory: StrPath, suffix: str) -> Path:\n        for parent, dirs, _ in os.walk(metadata_directory):\n            candidates = [f for f in dirs if f.endswith(suffix)]\n\n            if len(candidates) != 0 or len(dirs) != 1:\n                assert len(candidates) == 1, f\"Multiple {suffix} directories found\"\n                return Path(parent, candidates[0])\n\n        msg = f\"No {suffix} directory found in {metadata_directory}\"\n        raise errors.InternalError(msg)\n\n    def prepare_metadata_for_build_wheel(\n        self, metadata_directory: StrPath, config_settings: _ConfigSettings = None\n    ):\n        sys.argv = [\n            *sys.argv[:1],\n            *self._global_args(config_settings),\n            \"dist_info\",\n            \"--output-dir\",\n            str(metadata_directory),\n            \"--keep-egg-info\",\n        ]\n        with no_install_setup_requires():\n            self.run_setup()\n\n        self._bubble_up_info_directory(metadata_directory, \".egg-info\")\n        return self._bubble_up_info_directory(metadata_directory, \".dist-info\")\n\n    def _build_with_temp_dir(\n        self,\n        setup_command: Iterable[str],\n        result_extension: str | tuple[str, ...],\n        result_directory: StrPath,\n        config_settings: _ConfigSettings,\n        arbitrary_args: Iterable[str] = (),\n    ):\n        result_directory = os.path.abspath(result_directory)\n\n        # Build in a temporary directory, then copy to the target.\n        os.makedirs(result_directory, exist_ok=True)\n\n        with tempfile.TemporaryDirectory(\n            prefix=\".tmp-\", dir=result_directory\n        ) as tmp_dist_dir:\n            sys.argv = [\n                *sys.argv[:1],\n                *self._global_args(config_settings),\n                *setup_command,\n                \"--dist-dir\",\n                tmp_dist_dir,\n                *arbitrary_args,\n            ]\n            with no_install_setup_requires():\n                self.run_setup()\n\n            result_basename = _file_with_extension(tmp_dist_dir, result_extension)\n            result_path = os.path.join(result_directory, result_basename)\n            if os.path.exists(result_path):\n                # os.rename will fail overwriting on non-Unix.\n                os.remove(result_path)\n            os.rename(os.path.join(tmp_dist_dir, result_basename), result_path)\n\n        return result_basename\n\n    def build_wheel(\n        self,\n        wheel_directory: StrPath,\n        config_settings: _ConfigSettings = None,\n        metadata_directory: StrPath | None = None,\n    ):\n        def _build(cmd: list[str]):\n            with suppress_known_deprecation():\n                return self._build_with_temp_dir(\n                    cmd,\n                    '.whl',\n                    wheel_directory,\n                    config_settings,\n                    self._arbitrary_args(config_settings),\n                )\n\n        if metadata_directory is None:\n            return _build(['bdist_wheel'])\n\n        try:\n            return _build(['bdist_wheel', '--dist-info-dir', str(metadata_directory)])\n        except SystemExit as ex:  # pragma: nocover\n            # pypa/setuptools#4683\n            if \"--dist-info-dir not recognized\" not in str(ex):\n                raise\n            _IncompatibleBdistWheel.emit()\n            return _build(['bdist_wheel'])\n\n    def build_sdist(\n        self, sdist_directory: StrPath, config_settings: _ConfigSettings = None\n    ):\n        return self._build_with_temp_dir(\n            ['sdist', '--formats', 'gztar'], '.tar.gz', sdist_directory, config_settings\n        )\n\n    def _get_dist_info_dir(self, metadata_directory: StrPath | None) -> str | None:\n        if not metadata_directory:\n            return None\n        dist_info_candidates = list(Path(metadata_directory).glob(\"*.dist-info\"))\n        assert len(dist_info_candidates) <= 1\n        return str(dist_info_candidates[0]) if dist_info_candidates else None\n\n    if not LEGACY_EDITABLE:\n        # PEP660 hooks:\n        # build_editable\n        # get_requires_for_build_editable\n        # prepare_metadata_for_build_editable\n        def build_editable(\n            self,\n            wheel_directory: StrPath,\n            config_settings: _ConfigSettings = None,\n            metadata_directory: StrPath | None = None,\n        ):\n            # XXX can or should we hide our editable_wheel command normally?\n            info_dir = self._get_dist_info_dir(metadata_directory)\n            opts = [\"--dist-info-dir\", info_dir] if info_dir else []\n            cmd = [\"editable_wheel\", *opts, *self._editable_args(config_settings)]\n            with suppress_known_deprecation():\n                return self._build_with_temp_dir(\n                    cmd, \".whl\", wheel_directory, config_settings\n                )\n\n        def get_requires_for_build_editable(\n            self, config_settings: _ConfigSettings = None\n        ):\n            return self.get_requires_for_build_wheel(config_settings)\n\n        def prepare_metadata_for_build_editable(\n            self, metadata_directory: StrPath, config_settings: _ConfigSettings = None\n        ):\n            return self.prepare_metadata_for_build_wheel(\n                metadata_directory, config_settings\n            )\n\n\nclass _BuildMetaLegacyBackend(_BuildMetaBackend):\n    \"\"\"Compatibility backend for setuptools\n\n    This is a version of setuptools.build_meta that endeavors\n    to maintain backwards\n    compatibility with pre-PEP 517 modes of invocation. It\n    exists as a temporary\n    bridge between the old packaging mechanism and the new\n    packaging mechanism,\n    and will eventually be removed.\n    \"\"\"\n\n    def run_setup(self, setup_script: str = 'setup.py'):\n        # In order to maintain compatibility with scripts assuming that\n        # the setup.py script is in a directory on the PYTHONPATH, inject\n        # '' into sys.path. (pypa/setuptools#1642)\n        sys_path = list(sys.path)  # Save the original path\n\n        script_dir = os.path.dirname(os.path.abspath(setup_script))\n        if script_dir not in sys.path:\n            sys.path.insert(0, script_dir)\n\n        # Some setup.py scripts (e.g. in pygame and numpy) use sys.argv[0] to\n        # get the directory of the source code. They expect it to refer to the\n        # setup.py script.\n        sys_argv_0 = sys.argv[0]\n        sys.argv[0] = setup_script\n\n        try:\n            super().run_setup(setup_script=setup_script)\n        finally:\n            # While PEP 517 frontends should be calling each hook in a fresh\n            # subprocess according to the standard (and thus it should not be\n            # strictly necessary to restore the old sys.path), we'll restore\n            # the original path so that the path manipulation does not persist\n            # within the hook after run_setup is called.\n            sys.path[:] = sys_path\n            sys.argv[0] = sys_argv_0\n\n\nclass _IncompatibleBdistWheel(SetuptoolsDeprecationWarning):\n    _SUMMARY = \"wheel.bdist_wheel is deprecated, please import it from setuptools\"\n    _DETAILS = \"\"\"\n    Ensure that any custom bdist_wheel implementation is a subclass of\n    setuptools.command.bdist_wheel.bdist_wheel.\n    \"\"\"\n    _DUE_DATE = (2025, 10, 15)\n    # Initially introduced in 2024/10/15, but maybe too disruptive to be enforced?\n    _SEE_URL = \"https://github.com/pypa/wheel/pull/631\"\n\n\n# The primary backend\n_BACKEND = _BuildMetaBackend()\n\nget_requires_for_build_wheel = _BACKEND.get_requires_for_build_wheel\nget_requires_for_build_sdist = _BACKEND.get_requires_for_build_sdist\nprepare_metadata_for_build_wheel = _BACKEND.prepare_metadata_for_build_wheel\nbuild_wheel = _BACKEND.build_wheel\nbuild_sdist = _BACKEND.build_sdist\n\nif not LEGACY_EDITABLE:\n    get_requires_for_build_editable = _BACKEND.get_requires_for_build_editable\n    prepare_metadata_for_build_editable = _BACKEND.prepare_metadata_for_build_editable\n    build_editable = _BACKEND.build_editable\n\n\n# The legacy backend\n__legacy__ = _BuildMetaLegacyBackend()\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/command/__init__.py","size":803,"sha1":"8213b18a753a06d6f034c78c216e9d094838bd82","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"# mypy: disable_error_code=call-overload\n# pyright: reportCallIssue=false, reportArgumentType=false\n# Can't disable on the exact line because distutils doesn't exists on Python 3.12\n# and type-checkers aren't aware of distutils_hack,\n# causing distutils.command.bdist.bdist.format_commands to be Any.\n\nimport sys\n\nfrom distutils.command.bdist import bdist\n\nif 'egg' not in bdist.format_commands:\n    try:\n        # format_commands is a dict in vendored distutils\n        # It used to be a list in older (stdlib) distutils\n        # We support both for backwards compatibility\n        bdist.format_commands['egg'] = ('bdist_egg', \"Python .egg file\")\n    except TypeError:\n        bdist.format_command['egg'] = ('bdist_egg', \"Python .egg file\")\n        bdist.format_commands.append('egg')\n\ndel bdist, sys\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/command/_requirestxt.py","size":4228,"sha1":"b4afbac0838dd290e981704430db373552d1987a","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"Helper code used to generate ``requires.txt`` files in the egg-info directory.\n\nThe ``requires.txt`` file has an specific format:\n    - Environment markers need to be part of the section headers and\n      should not be part of the requirement spec itself.\n\nSee https://setuptools.pypa.io/en/latest/deprecated/python_eggs.html#requires-txt\n\"\"\"\n\nfrom __future__ import annotations\n\nimport io\nfrom collections import defaultdict\nfrom collections.abc import Mapping\nfrom itertools import filterfalse\nfrom typing import TypeVar\n\nfrom jaraco.text import yield_lines\nfrom packaging.requirements import Requirement\n\nfrom .. import _reqs\nfrom .._reqs import _StrOrIter\n\n# dict can work as an ordered set\n_T = TypeVar(\"_T\")\n_Ordered = dict[_T, None]\n\n\ndef _prepare(\n    install_requires: _StrOrIter, extras_require: Mapping[str, _StrOrIter]\n) -> tuple[list[str], dict[str, list[str]]]:\n    \"\"\"Given values for ``install_requires`` and ``extras_require``\n    create modified versions in a way that can be written in ``requires.txt``\n    \"\"\"\n    extras = _convert_extras_requirements(extras_require)\n    return _move_install_requirements_markers(install_requires, extras)\n\n\ndef _convert_extras_requirements(\n    extras_require: Mapping[str, _StrOrIter],\n) -> defaultdict[str, _Ordered[Requirement]]:\n    \"\"\"\n    Convert requirements in `extras_require` of the form\n    `\"extra\": [\"barbazquux; {marker}\"]` to\n    `\"extra:{marker}\": [\"barbazquux\"]`.\n    \"\"\"\n    output = defaultdict[str, _Ordered[Requirement]](dict)\n    for section, v in extras_require.items():\n        # Do not strip empty sections.\n        output[section]\n        for r in _reqs.parse(v):\n            output[section + _suffix_for(r)].setdefault(r)\n\n    return output\n\n\ndef _move_install_requirements_markers(\n    install_requires: _StrOrIter, extras_require: Mapping[str, _Ordered[Requirement]]\n) -> tuple[list[str], dict[str, list[str]]]:\n    \"\"\"\n    The ``requires.txt`` file has an specific format:\n        - Environment markers need to be part of the section headers and\n          should not be part of the requirement spec itself.\n\n    Move requirements in ``install_requires`` that are using environment\n    markers ``extras_require``.\n    \"\"\"\n\n    # divide the install_requires into two sets, simple ones still\n    # handled by install_requires and more complex ones handled by extras_require.\n\n    inst_reqs = list(_reqs.parse(install_requires))\n    simple_reqs = filter(_no_marker, inst_reqs)\n    complex_reqs = filterfalse(_no_marker, inst_reqs)\n    simple_install_requires = list(map(str, simple_reqs))\n\n    for r in complex_reqs:\n        extras_require[':' + str(r.marker)].setdefault(r)\n\n    expanded_extras = dict(\n        # list(dict.fromkeys(...))  ensures a list of unique strings\n        (k, list(dict.fromkeys(str(r) for r in map(_clean_req, v))))\n        for k, v in extras_require.items()\n    )\n\n    return simple_install_requires, expanded_extras\n\n\ndef _suffix_for(req):\n    \"\"\"Return the 'extras_require' suffix for a given requirement.\"\"\"\n    return ':' + str(req.marker) if req.marker else ''\n\n\ndef _clean_req(req):\n    \"\"\"Given a Requirement, remove environment markers and return it\"\"\"\n    r = Requirement(str(req))  # create a copy before modifying\n    r.marker = None\n    return r\n\n\ndef _no_marker(req):\n    return not req.marker\n\n\ndef _write_requirements(stream, reqs):\n    lines = yield_lines(reqs or ())\n\n    def append_cr(line):\n        return line + '\\n'\n\n    lines = map(append_cr, lines)\n    stream.writelines(lines)\n\n\ndef write_requirements(cmd, basename, filename):\n    dist = cmd.distribution\n    data = io.StringIO()\n    install_requires, extras_require = _prepare(\n        dist.install_requires or (), dist.extras_require or {}\n    )\n    _write_requirements(data, install_requires)\n    for extra in sorted(extras_require):\n        data.write('\\n[{extra}]\\n'.format(**vars()))\n        _write_requirements(data, extras_require[extra])\n    cmd.write_or_delete_file(\"requirements\", filename, data.getvalue())\n\n\ndef write_setup_requirements(cmd, basename, filename):\n    data = io.StringIO()\n    _write_requirements(data, cmd.distribution.setup_requires)\n    cmd.write_or_delete_file(\"setup-requirements\", filename, data.getvalue())\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/command/alias.py","size":2380,"sha1":"1c4f54f08c05fc721ad02ebdbf3014437260f864","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"from setuptools.command.setopt import config_file, edit_config, option_base\n\nfrom distutils.errors import DistutilsOptionError\n\n\ndef shquote(arg):\n    \"\"\"Quote an argument for later parsing by shlex.split()\"\"\"\n    for c in '\"', \"'\", \"\\\\\", \"#\":\n        if c in arg:\n            return repr(arg)\n    if arg.split() != [arg]:\n        return repr(arg)\n    return arg\n\n\nclass alias(option_base):\n    \"\"\"Define a shortcut that invokes one or more commands\"\"\"\n\n    description = \"define a shortcut to invoke one or more commands\"\n    command_consumes_arguments = True\n\n    user_options = [\n        ('remove', 'r', 'remove (unset) the alias'),\n    ] + option_base.user_options\n\n    boolean_options = option_base.boolean_options + ['remove']\n\n    def initialize_options(self):\n        option_base.initialize_options(self)\n        self.args = None\n        self.remove = None\n\n    def finalize_options(self) -> None:\n        option_base.finalize_options(self)\n        if self.remove and len(self.args) != 1:\n            raise DistutilsOptionError(\n                \"Must specify exactly one argument (the alias name) when using --remove\"\n            )\n\n    def run(self) -> None:\n        aliases = self.distribution.get_option_dict('aliases')\n\n        if not self.args:\n            print(\"Command Aliases\")\n            print(\"---------------\")\n            for alias in aliases:\n                print(\"setup.py alias\", format_alias(alias, aliases))\n            return\n\n        elif len(self.args) == 1:\n            (alias,) = self.args\n            if self.remove:\n                command = None\n            elif alias in aliases:\n                print(\"setup.py alias\", format_alias(alias, aliases))\n                return\n            else:\n                print(f\"No alias definition found for {alias!r}\")\n                return\n        else:\n            alias = self.args[0]\n            command = ' '.join(map(shquote, self.args[1:]))\n\n        edit_config(self.filename, {'aliases': {alias: command}}, self.dry_run)\n\n\ndef format_alias(name, aliases):\n    source, command = aliases[name]\n    if source == config_file('global'):\n        source = '--global-config '\n    elif source == config_file('user'):\n        source = '--user-config '\n    elif source == config_file('local'):\n        source = ''\n    else:\n        source = f'--filename={source!r}'\n    return source + name + ' ' + command\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/command/bdist_egg.py","size":16972,"sha1":"32e8446ee041c70705efdf7c4ea0f979524066ed","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"setuptools.command.bdist_egg\n\nBuild .egg distributions\"\"\"\n\nfrom __future__ import annotations\n\nimport marshal\nimport os\nimport re\nimport sys\nimport textwrap\nfrom sysconfig import get_path, get_python_version\nfrom types import CodeType\nfrom typing import TYPE_CHECKING, Literal\n\nfrom setuptools import Command\nfrom setuptools.extension import Library\n\nfrom .._path import StrPathT, ensure_directory\n\nfrom distutils import log\nfrom distutils.dir_util import mkpath, remove_tree\n\nif TYPE_CHECKING:\n    from typing_extensions import TypeAlias\n\n# Same as zipfile._ZipFileMode from typeshed\n_ZipFileMode: TypeAlias = Literal[\"r\", \"w\", \"x\", \"a\"]\n\n\ndef _get_purelib():\n    return get_path(\"purelib\")\n\n\ndef strip_module(filename):\n    if '.' in filename:\n        filename = os.path.splitext(filename)[0]\n    if filename.endswith('module'):\n        filename = filename[:-6]\n    return filename\n\n\ndef sorted_walk(dir):\n    \"\"\"Do os.walk in a reproducible way,\n    independent of indeterministic filesystem readdir order\n    \"\"\"\n    for base, dirs, files in os.walk(dir):\n        dirs.sort()\n        files.sort()\n        yield base, dirs, files\n\n\ndef write_stub(resource, pyfile) -> None:\n    _stub_template = textwrap.dedent(\n        \"\"\"\n        def __bootstrap__():\n            global __bootstrap__, __loader__, __file__\n            import sys, pkg_resources, importlib.util\n            __file__ = pkg_resources.resource_filename(__name__, %r)\n            __loader__ = None; del __bootstrap__, __loader__\n            spec = importlib.util.spec_from_file_location(__name__,__file__)\n            mod = importlib.util.module_from_spec(spec)\n            spec.loader.exec_module(mod)\n        __bootstrap__()\n        \"\"\"\n    ).lstrip()\n    with open(pyfile, 'w', encoding=\"utf-8\") as f:\n        f.write(_stub_template % resource)\n\n\nclass bdist_egg(Command):\n    description = 'create an \"egg\" distribution'\n\n    user_options = [\n        ('bdist-dir=', 'b', \"temporary directory for creating the distribution\"),\n        (\n            'plat-name=',\n            'p',\n            \"platform name to embed in generated filenames \"\n            \"(by default uses `pkg_resources.get_build_platform()`)\",\n        ),\n        ('exclude-source-files', None, \"remove all .py files from the generated egg\"),\n        (\n            'keep-temp',\n            'k',\n            \"keep the pseudo-installation tree around after \"\n            \"creating the distribution archive\",\n        ),\n        ('dist-dir=', 'd', \"directory to put final built distributions in\"),\n        ('skip-build', None, \"skip rebuilding everything (for testing/debugging)\"),\n    ]\n\n    boolean_options = ['keep-temp', 'skip-build', 'exclude-source-files']\n\n    def initialize_options(self):\n        self.bdist_dir = None\n        self.plat_name = None\n        self.keep_temp = False\n        self.dist_dir = None\n        self.skip_build = False\n        self.egg_output = None\n        self.exclude_source_files = None\n\n    def finalize_options(self) -> None:\n        ei_cmd = self.ei_cmd = self.get_finalized_command(\"egg_info\")\n        self.egg_info = ei_cmd.egg_info\n\n        if self.bdist_dir is None:\n            bdist_base = self.get_finalized_command('bdist').bdist_base\n            self.bdist_dir = os.path.join(bdist_base, 'egg')\n\n        if self.plat_name is None:\n            from pkg_resources import get_build_platform\n\n            self.plat_name = get_build_platform()\n\n        self.set_undefined_options('bdist', ('dist_dir', 'dist_dir'))\n\n        if self.egg_output is None:\n            # Compute filename of the output egg\n            basename = ei_cmd._get_egg_basename(\n                py_version=get_python_version(),\n                platform=self.distribution.has_ext_modules() and self.plat_name,\n            )\n\n            self.egg_output = os.path.join(self.dist_dir, basename + '.egg')\n\n    def do_install_data(self) -> None:\n        # Hack for packages that install data to install's --install-lib\n        self.get_finalized_command('install').install_lib = self.bdist_dir\n\n        site_packages = os.path.normcase(os.path.realpath(_get_purelib()))\n        old, self.distribution.data_files = self.distribution.data_files, []\n\n        for item in old:\n            if isinstance(item, tuple) and len(item) == 2:\n                if os.path.isabs(item[0]):\n                    realpath = os.path.realpath(item[0])\n                    normalized = os.path.normcase(realpath)\n                    if normalized == site_packages or normalized.startswith(\n                        site_packages + os.sep\n                    ):\n                        item = realpath[len(site_packages) + 1 :], item[1]\n                        # XXX else: raise ???\n            self.distribution.data_files.append(item)\n\n        try:\n            log.info(\"installing package data to %s\", self.bdist_dir)\n            self.call_command('install_data', force=False, root=None)\n        finally:\n            self.distribution.data_files = old\n\n    def get_outputs(self):\n        return [self.egg_output]\n\n    def call_command(self, cmdname, **kw):\n        \"\"\"Invoke reinitialized command `cmdname` with keyword args\"\"\"\n        for dirname in INSTALL_DIRECTORY_ATTRS:\n            kw.setdefault(dirname, self.bdist_dir)\n        kw.setdefault('skip_build', self.skip_build)\n        kw.setdefault('dry_run', self.dry_run)\n        cmd = self.reinitialize_command(cmdname, **kw)\n        self.run_command(cmdname)\n        return cmd\n\n    def run(self):  # noqa: C901  # is too complex (14)  # FIXME\n        # Generate metadata first\n        self.run_command(\"egg_info\")\n        # We run install_lib before install_data, because some data hacks\n        # pull their data path from the install_lib command.\n        log.info(\"installing library code to %s\", self.bdist_dir)\n        instcmd = self.get_finalized_command('install')\n        old_root = instcmd.root\n        instcmd.root = None\n        if self.distribution.has_c_libraries() and not self.skip_build:\n            self.run_command('build_clib')\n        cmd = self.call_command('install_lib', warn_dir=False)\n        instcmd.root = old_root\n\n        all_outputs, ext_outputs = self.get_ext_outputs()\n        self.stubs = []\n        to_compile = []\n        for p, ext_name in enumerate(ext_outputs):\n            filename, _ext = os.path.splitext(ext_name)\n            pyfile = os.path.join(self.bdist_dir, strip_module(filename) + '.py')\n            self.stubs.append(pyfile)\n            log.info(\"creating stub loader for %s\", ext_name)\n            if not self.dry_run:\n                write_stub(os.path.basename(ext_name), pyfile)\n            to_compile.append(pyfile)\n            ext_outputs[p] = ext_name.replace(os.sep, '/')\n\n        if to_compile:\n            cmd.byte_compile(to_compile)\n        if self.distribution.data_files:\n            self.do_install_data()\n\n        # Make the EGG-INFO directory\n        archive_root = self.bdist_dir\n        egg_info = os.path.join(archive_root, 'EGG-INFO')\n        self.mkpath(egg_info)\n        if self.distribution.scripts:\n            script_dir = os.path.join(egg_info, 'scripts')\n            log.info(\"installing scripts to %s\", script_dir)\n            self.call_command('install_scripts', install_dir=script_dir, no_ep=True)\n\n        self.copy_metadata_to(egg_info)\n        native_libs = os.path.join(egg_info, \"native_libs.txt\")\n        if all_outputs:\n            log.info(\"writing %s\", native_libs)\n            if not self.dry_run:\n                ensure_directory(native_libs)\n                with open(native_libs, 'wt', encoding=\"utf-8\") as libs_file:\n                    libs_file.write('\\n'.join(all_outputs))\n                    libs_file.write('\\n')\n        elif os.path.isfile(native_libs):\n            log.info(\"removing %s\", native_libs)\n            if not self.dry_run:\n                os.unlink(native_libs)\n\n        write_safety_flag(os.path.join(archive_root, 'EGG-INFO'), self.zip_safe())\n\n        if os.path.exists(os.path.join(self.egg_info, 'depends.txt')):\n            log.warn(\n                \"WARNING: 'depends.txt' will not be used by setuptools 0.6!\\n\"\n                \"Use the install_requires/extras_require setup() args instead.\"\n            )\n\n        if self.exclude_source_files:\n            self.zap_pyfiles()\n\n        # Make the archive\n        make_zipfile(\n            self.egg_output,\n            archive_root,\n            verbose=self.verbose,\n            dry_run=self.dry_run,\n            mode=self.gen_header(),\n        )\n        if not self.keep_temp:\n            remove_tree(self.bdist_dir, dry_run=self.dry_run)\n\n        # Add to 'Distribution.dist_files' so that the \"upload\" command works\n        getattr(self.distribution, 'dist_files', []).append((\n            'bdist_egg',\n            get_python_version(),\n            self.egg_output,\n        ))\n\n    def zap_pyfiles(self):\n        log.info(\"Removing .py files from temporary directory\")\n        for base, dirs, files in walk_egg(self.bdist_dir):\n            for name in files:\n                path = os.path.join(base, name)\n\n                if name.endswith('.py'):\n                    log.debug(\"Deleting %s\", path)\n                    os.unlink(path)\n\n                if base.endswith('__pycache__'):\n                    path_old = path\n\n                    pattern = r'(?P<name>.+)\\.(?P<magic>[^.]+)\\.pyc'\n                    m = re.match(pattern, name)\n                    path_new = os.path.join(base, os.pardir, m.group('name') + '.pyc')\n                    log.info(f\"Renaming file from [{path_old}] to [{path_new}]\")\n                    try:\n                        os.remove(path_new)\n                    except OSError:\n                        pass\n                    os.rename(path_old, path_new)\n\n    def zip_safe(self):\n        safe = getattr(self.distribution, 'zip_safe', None)\n        if safe is not None:\n            return safe\n        log.warn(\"zip_safe flag not set; analyzing archive contents...\")\n        return analyze_egg(self.bdist_dir, self.stubs)\n\n    def gen_header(self) -> Literal[\"w\"]:\n        return 'w'\n\n    def copy_metadata_to(self, target_dir) -> None:\n        \"Copy metadata (egg info) to the target_dir\"\n        # normalize the path (so that a forward-slash in egg_info will\n        # match using startswith below)\n        norm_egg_info = os.path.normpath(self.egg_info)\n        prefix = os.path.join(norm_egg_info, '')\n        for path in self.ei_cmd.filelist.files:\n            if path.startswith(prefix):\n                target = os.path.join(target_dir, path[len(prefix) :])\n                ensure_directory(target)\n                self.copy_file(path, target)\n\n    def get_ext_outputs(self):\n        \"\"\"Get a list of relative paths to C extensions in the output distro\"\"\"\n\n        all_outputs = []\n        ext_outputs = []\n\n        paths = {self.bdist_dir: ''}\n        for base, dirs, files in sorted_walk(self.bdist_dir):\n            all_outputs.extend(\n                paths[base] + filename\n                for filename in files\n                if os.path.splitext(filename)[1].lower() in NATIVE_EXTENSIONS\n            )\n            for filename in dirs:\n                paths[os.path.join(base, filename)] = paths[base] + filename + '/'\n\n        if self.distribution.has_ext_modules():\n            build_cmd = self.get_finalized_command('build_ext')\n            for ext in build_cmd.extensions:\n                if isinstance(ext, Library):\n                    continue\n                fullname = build_cmd.get_ext_fullname(ext.name)\n                filename = build_cmd.get_ext_filename(fullname)\n                if not os.path.basename(filename).startswith('dl-'):\n                    if os.path.exists(os.path.join(self.bdist_dir, filename)):\n                        ext_outputs.append(filename)\n\n        return all_outputs, ext_outputs\n\n\nNATIVE_EXTENSIONS: dict[str, None] = dict.fromkeys('.dll .so .dylib .pyd'.split())\n\n\ndef walk_egg(egg_dir):\n    \"\"\"Walk an unpacked egg's contents, skipping the metadata directory\"\"\"\n    walker = sorted_walk(egg_dir)\n    base, dirs, files = next(walker)\n    if 'EGG-INFO' in dirs:\n        dirs.remove('EGG-INFO')\n    yield base, dirs, files\n    yield from walker\n\n\ndef analyze_egg(egg_dir, stubs):\n    # check for existing flag in EGG-INFO\n    for flag, fn in safety_flags.items():\n        if os.path.exists(os.path.join(egg_dir, 'EGG-INFO', fn)):\n            return flag\n    if not can_scan():\n        return False\n    safe = True\n    for base, dirs, files in walk_egg(egg_dir):\n        for name in files:\n            if name.endswith('.py') or name.endswith('.pyw'):\n                continue\n            elif name.endswith('.pyc') or name.endswith('.pyo'):\n                # always scan, even if we already know we're not safe\n                safe = scan_module(egg_dir, base, name, stubs) and safe\n    return safe\n\n\ndef write_safety_flag(egg_dir, safe) -> None:\n    # Write or remove zip safety flag file(s)\n    for flag, fn in safety_flags.items():\n        fn = os.path.join(egg_dir, fn)\n        if os.path.exists(fn):\n            if safe is None or bool(safe) != flag:\n                os.unlink(fn)\n        elif safe is not None and bool(safe) == flag:\n            with open(fn, 'wt', encoding=\"utf-8\") as f:\n                f.write('\\n')\n\n\nsafety_flags = {\n    True: 'zip-safe',\n    False: 'not-zip-safe',\n}\n\n\ndef scan_module(egg_dir, base, name, stubs):\n    \"\"\"Check whether module possibly uses unsafe-for-zipfile stuff\"\"\"\n\n    filename = os.path.join(base, name)\n    if filename[:-1] in stubs:\n        return True  # Extension module\n    pkg = base[len(egg_dir) + 1 :].replace(os.sep, '.')\n    module = pkg + (pkg and '.' or '') + os.path.splitext(name)[0]\n    skip = 16  # skip magic & reserved? & date & file size\n    f = open(filename, 'rb')\n    f.read(skip)\n    code = marshal.load(f)\n    f.close()\n    safe = True\n    symbols = dict.fromkeys(iter_symbols(code))\n    for bad in ['__file__', '__path__']:\n        if bad in symbols:\n            log.warn(\"%s: module references %s\", module, bad)\n            safe = False\n    if 'inspect' in symbols:\n        for bad in [\n            'getsource',\n            'getabsfile',\n            'getfile',\n            'getsourcefile',\n            'getsourcelines',\n            'findsource',\n            'getcomments',\n            'getframeinfo',\n            'getinnerframes',\n            'getouterframes',\n            'stack',\n            'trace',\n        ]:\n            if bad in symbols:\n                log.warn(\"%s: module MAY be using inspect.%s\", module, bad)\n                safe = False\n    return safe\n\n\ndef iter_symbols(code):\n    \"\"\"Yield names and strings used by `code` and its nested code objects\"\"\"\n    yield from code.co_names\n    for const in code.co_consts:\n        if isinstance(const, str):\n            yield const\n        elif isinstance(const, CodeType):\n            yield from iter_symbols(const)\n\n\ndef can_scan() -> bool:\n    if not sys.platform.startswith('java') and sys.platform != 'cli':\n        # CPython, PyPy, etc.\n        return True\n    log.warn(\"Unable to analyze compiled code on this platform.\")\n    log.warn(\n        \"Please ask the author to include a 'zip_safe'\"\n        \" setting (either True or False) in the package's setup.py\"\n    )\n    return False\n\n\n# Attribute names of options for commands that might need to be convinced to\n# install to the egg build directory\n\nINSTALL_DIRECTORY_ATTRS = ['install_lib', 'install_dir', 'install_data', 'install_base']\n\n\ndef make_zipfile(\n    zip_filename: StrPathT,\n    base_dir,\n    verbose: bool = False,\n    dry_run: bool = False,\n    compress=True,\n    mode: _ZipFileMode = 'w',\n) -> StrPathT:\n    \"\"\"Create a zip file from all the files under 'base_dir'.  The output\n    zip file will be named 'base_dir' + \".zip\".  Uses either the \"zipfile\"\n    Python module (if available) or the InfoZIP \"zip\" utility (if installed\n    and found on the default search path).  If neither tool is available,\n    raises DistutilsExecError.  Returns the name of the output zip file.\n    \"\"\"\n    import zipfile\n\n    mkpath(os.path.dirname(zip_filename), dry_run=dry_run)  # type: ignore[arg-type] # python/mypy#18075\n    log.info(\"creating '%s' and adding '%s' to it\", zip_filename, base_dir)\n\n    def visit(z, dirname, names):\n        for name in names:\n            path = os.path.normpath(os.path.join(dirname, name))\n            if os.path.isfile(path):\n                p = path[len(base_dir) + 1 :]\n                if not dry_run:\n                    z.write(path, p)\n                log.debug(\"adding '%s'\", p)\n\n    compression = zipfile.ZIP_DEFLATED if compress else zipfile.ZIP_STORED\n    if not dry_run:\n        z = zipfile.ZipFile(zip_filename, mode, compression=compression)\n        for dirname, dirs, files in sorted_walk(base_dir):\n            visit(z, dirname, files)\n        z.close()\n    else:\n        for dirname, dirs, files in sorted_walk(base_dir):\n            visit(None, dirname, files)\n    return zip_filename\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/command/bdist_rpm.py","size":1435,"sha1":"3af505d8d29b3433a9de1c9b38f65b10607d572d","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"from ..dist import Distribution\nfrom ..warnings import SetuptoolsDeprecationWarning\n\nimport distutils.command.bdist_rpm as orig\n\n\nclass bdist_rpm(orig.bdist_rpm):\n    \"\"\"\n    Override the default bdist_rpm behavior to do the following:\n\n    1. Run egg_info to ensure the name and version are properly calculated.\n    2. Always run 'install' using --single-version-externally-managed to\n       disable eggs in RPM distributions.\n    \"\"\"\n\n    distribution: Distribution  # override distutils.dist.Distribution with setuptools.dist.Distribution\n\n    def run(self) -> None:\n        SetuptoolsDeprecationWarning.emit(\n            \"Deprecated command\",\n            \"\"\"\n            bdist_rpm is deprecated and will be removed in a future version.\n            Use bdist_wheel (wheel packages) instead.\n            \"\"\",\n            see_url=\"https://github.com/pypa/setuptools/issues/1988\",\n            due_date=(2023, 10, 30),  # Deprecation introduced in 22 Oct 2021.\n        )\n\n        # ensure distro name is up-to-date\n        self.run_command('egg_info')\n\n        orig.bdist_rpm.run(self)\n\n    def _make_spec_file(self):\n        spec = orig.bdist_rpm._make_spec_file(self)\n        return [\n            line.replace(\n                \"setup.py install \",\n                \"setup.py install --single-version-externally-managed \",\n            ).replace(\"%setup\", \"%setup -n %{name}-%{unmangled_version}\")\n            for line in spec\n        ]\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/command/bdist_wheel.py","size":22274,"sha1":"9317bb424e092a875651dae68face6cb7f37f818","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"\nCreate a wheel (.whl) distribution.\n\nA wheel is a built archive format.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nimport re\nimport shutil\nimport struct\nimport sys\nimport sysconfig\nimport warnings\nfrom collections.abc import Iterable, Sequence\nfrom email.generator import BytesGenerator\nfrom glob import iglob\nfrom typing import Literal, cast\nfrom zipfile import ZIP_DEFLATED, ZIP_STORED\n\nfrom packaging import tags, version as _packaging_version\nfrom wheel.wheelfile import WheelFile\n\nfrom .. import Command, __version__, _shutil\nfrom ..warnings import SetuptoolsDeprecationWarning\nfrom .egg_info import egg_info as egg_info_cls\n\nfrom distutils import log\n\n\ndef safe_name(name: str) -> str:\n    \"\"\"Convert an arbitrary string to a standard distribution name\n    Any runs of non-alphanumeric/. characters are replaced with a single '-'.\n    \"\"\"\n    return re.sub(\"[^A-Za-z0-9.]+\", \"-\", name)\n\n\ndef safe_version(version: str) -> str:\n    \"\"\"\n    Convert an arbitrary string to a standard version string\n    \"\"\"\n    try:\n        # normalize the version\n        return str(_packaging_version.Version(version))\n    except _packaging_version.InvalidVersion:\n        version = version.replace(\" \", \".\")\n        return re.sub(\"[^A-Za-z0-9.]+\", \"-\", version)\n\n\nsetuptools_major_version = int(__version__.split(\".\")[0])\n\nPY_LIMITED_API_PATTERN = r\"cp3\\d\"\n\n\ndef _is_32bit_interpreter() -> bool:\n    return struct.calcsize(\"P\") == 4\n\n\ndef python_tag() -> str:\n    return f\"py{sys.version_info.major}\"\n\n\ndef get_platform(archive_root: str | None) -> str:\n    \"\"\"Return our platform name 'win32', 'linux_x86_64'\"\"\"\n    result = sysconfig.get_platform()\n    if result.startswith(\"macosx\") and archive_root is not None:  # pragma: no cover\n        from wheel.macosx_libfile import calculate_macosx_platform_tag\n\n        result = calculate_macosx_platform_tag(archive_root, result)\n    elif _is_32bit_interpreter():\n        if result == \"linux-x86_64\":\n            # pip pull request #3497\n            result = \"linux-i686\"\n        elif result == \"linux-aarch64\":\n            # packaging pull request #234\n            # TODO armv8l, packaging pull request #690 => this did not land\n            # in pip/packaging yet\n            result = \"linux-armv7l\"\n\n    return result.replace(\"-\", \"_\")\n\n\ndef get_flag(\n    var: str, fallback: bool, expected: bool = True, warn: bool = True\n) -> bool:\n    \"\"\"Use a fallback value for determining SOABI flags if the needed config\n    var is unset or unavailable.\"\"\"\n    val = sysconfig.get_config_var(var)\n    if val is None:\n        if warn:\n            warnings.warn(\n                f\"Config variable '{var}' is unset, Python ABI tag may be incorrect\",\n                RuntimeWarning,\n                stacklevel=2,\n            )\n        return fallback\n    return val == expected\n\n\ndef get_abi_tag() -> str | None:\n    \"\"\"Return the ABI tag based on SOABI (if available) or emulate SOABI (PyPy2).\"\"\"\n    soabi: str = sysconfig.get_config_var(\"SOABI\")\n    impl = tags.interpreter_name()\n    if not soabi and impl in (\"cp\", \"pp\") and hasattr(sys, \"maxunicode\"):\n        d = \"\"\n        u = \"\"\n        if get_flag(\"Py_DEBUG\", hasattr(sys, \"gettotalrefcount\"), warn=(impl == \"cp\")):\n            d = \"d\"\n\n        abi = f\"{impl}{tags.interpreter_version()}{d}{u}\"\n    elif soabi and impl == \"cp\" and soabi.startswith(\"cpython\"):\n        # non-Windows\n        abi = \"cp\" + soabi.split(\"-\")[1]\n    elif soabi and impl == \"cp\" and soabi.startswith(\"cp\"):\n        # Windows\n        abi = soabi.split(\"-\")[0]\n        if hasattr(sys, \"gettotalrefcount\"):\n            # using debug build; append \"d\" flag\n            abi += \"d\"\n    elif soabi and impl == \"pp\":\n        # we want something like pypy36-pp73\n        abi = \"-\".join(soabi.split(\"-\")[:2])\n        abi = abi.replace(\".\", \"_\").replace(\"-\", \"_\")\n    elif soabi and impl == \"graalpy\":\n        abi = \"-\".join(soabi.split(\"-\")[:3])\n        abi = abi.replace(\".\", \"_\").replace(\"-\", \"_\")\n    elif soabi:\n        abi = soabi.replace(\".\", \"_\").replace(\"-\", \"_\")\n    else:\n        abi = None\n\n    return abi\n\n\ndef safer_name(name: str) -> str:\n    return safe_name(name).replace(\"-\", \"_\")\n\n\ndef safer_version(version: str) -> str:\n    return safe_version(version).replace(\"-\", \"_\")\n\n\nclass bdist_wheel(Command):\n    description = \"create a wheel distribution\"\n\n    supported_compressions = {\n        \"stored\": ZIP_STORED,\n        \"deflated\": ZIP_DEFLATED,\n    }\n\n    user_options = [\n        (\"bdist-dir=\", \"b\", \"temporary directory for creating the distribution\"),\n        (\n            \"plat-name=\",\n            \"p\",\n            \"platform name to embed in generated filenames \"\n            f\"[default: {get_platform(None)}]\",\n        ),\n        (\n            \"keep-temp\",\n            \"k\",\n            \"keep the pseudo-installation tree around after \"\n            \"creating the distribution archive\",\n        ),\n        (\"dist-dir=\", \"d\", \"directory to put final built distributions in\"),\n        (\"skip-build\", None, \"skip rebuilding everything (for testing/debugging)\"),\n        (\n            \"relative\",\n            None,\n            \"build the archive using relative paths [default: false]\",\n        ),\n        (\n            \"owner=\",\n            \"u\",\n            \"Owner name used when creating a tar file [default: current user]\",\n        ),\n        (\n            \"group=\",\n            \"g\",\n            \"Group name used when creating a tar file [default: current group]\",\n        ),\n        (\"universal\", None, \"*DEPRECATED* make a universal wheel [default: false]\"),\n        (\n            \"compression=\",\n            None,\n            f\"zipfile compression (one of: {', '.join(supported_compressions)}) [default: 'deflated']\",\n        ),\n        (\n            \"python-tag=\",\n            None,\n            f\"Python implementation compatibility tag [default: '{python_tag()}']\",\n        ),\n        (\n            \"build-number=\",\n            None,\n            \"Build number for this particular version. \"\n            \"As specified in PEP-0427, this must start with a digit. \"\n            \"[default: None]\",\n        ),\n        (\n            \"py-limited-api=\",\n            None,\n            \"Python tag (cp32|cp33|cpNN) for abi3 wheel tag [default: false]\",\n        ),\n        (\n            \"dist-info-dir=\",\n            None,\n            \"directory where a pre-generated dist-info can be found (e.g. as a \"\n            \"result of calling the PEP517 'prepare_metadata_for_build_wheel' \"\n            \"method)\",\n        ),\n    ]\n\n    boolean_options = [\"keep-temp\", \"skip-build\", \"relative\", \"universal\"]\n\n    def initialize_options(self) -> None:\n        self.bdist_dir: str | None = None\n        self.data_dir = \"\"\n        self.plat_name: str | None = None\n        self.plat_tag: str | None = None\n        self.format = \"zip\"\n        self.keep_temp = False\n        self.dist_dir: str | None = None\n        self.dist_info_dir = None\n        self.egginfo_dir: str | None = None\n        self.root_is_pure: bool | None = None\n        self.skip_build = False\n        self.relative = False\n        self.owner = None\n        self.group = None\n        self.universal = False\n        self.compression: str | int = \"deflated\"\n        self.python_tag = python_tag()\n        self.build_number: str | None = None\n        self.py_limited_api: str | Literal[False] = False\n        self.plat_name_supplied = False\n\n    def finalize_options(self) -> None:\n        if not self.bdist_dir:\n            bdist_base = self.get_finalized_command(\"bdist\").bdist_base\n            self.bdist_dir = os.path.join(bdist_base, \"wheel\")\n\n        if self.dist_info_dir is None:\n            egg_info = cast(egg_info_cls, self.distribution.get_command_obj(\"egg_info\"))\n            egg_info.ensure_finalized()  # needed for correct `wheel_dist_name`\n\n        self.data_dir = self.wheel_dist_name + \".data\"\n        self.plat_name_supplied = bool(self.plat_name)\n\n        need_options = (\"dist_dir\", \"plat_name\", \"skip_build\")\n\n        self.set_undefined_options(\"bdist\", *zip(need_options, need_options))\n\n        self.root_is_pure = not (\n            self.distribution.has_ext_modules() or self.distribution.has_c_libraries()\n        )\n\n        self._validate_py_limited_api()\n\n        # Support legacy [wheel] section for setting universal\n        wheel = self.distribution.get_option_dict(\"wheel\")\n        if \"universal\" in wheel:  # pragma: no cover\n            # please don't define this in your global configs\n            log.warn(\"The [wheel] section is deprecated. Use [bdist_wheel] instead.\")\n            val = wheel[\"universal\"][1].strip()\n            if val.lower() in (\"1\", \"true\", \"yes\"):\n                self.universal = True\n\n        if self.universal:\n            SetuptoolsDeprecationWarning.emit(\n                \"bdist_wheel.universal is deprecated\",\n                \"\"\"\n                With Python 2.7 end-of-life, support for building universal wheels\n                (i.e., wheels that support both Python 2 and Python 3)\n                is being obviated.\n                Please discontinue using this option, or if you still need it,\n                file an issue with pypa/setuptools describing your use case.\n                \"\"\",\n                due_date=(2025, 8, 30),  # Introduced in 2024-08-30\n            )\n\n        if self.build_number is not None and not self.build_number[:1].isdigit():\n            raise ValueError(\"Build tag (build-number) must start with a digit.\")\n\n    def _validate_py_limited_api(self) -> None:\n        if not self.py_limited_api:\n            return\n\n        if not re.match(PY_LIMITED_API_PATTERN, self.py_limited_api):\n            raise ValueError(f\"py-limited-api must match '{PY_LIMITED_API_PATTERN}'\")\n\n        if sysconfig.get_config_var(\"Py_GIL_DISABLED\"):\n            raise ValueError(\n                f\"`py_limited_api={self.py_limited_api!r}` not supported. \"\n                \"`Py_LIMITED_API` is currently incompatible with \"\n                f\"`Py_GIL_DISABLED` ({sys.abiflags=!r}). \"\n                \"See https://github.com/python/cpython/issues/111506.\"\n            )\n\n    @property\n    def wheel_dist_name(self) -> str:\n        \"\"\"Return distribution full name with - replaced with _\"\"\"\n        components = [\n            safer_name(self.distribution.get_name()),\n            safer_version(self.distribution.get_version()),\n        ]\n        if self.build_number:\n            components.append(self.build_number)\n        return \"-\".join(components)\n\n    def get_tag(self) -> tuple[str, str, str]:\n        # bdist sets self.plat_name if unset, we should only use it for purepy\n        # wheels if the user supplied it.\n        if self.plat_name_supplied and self.plat_name:\n            plat_name = self.plat_name\n        elif self.root_is_pure:\n            plat_name = \"any\"\n        else:\n            # macosx contains system version in platform name so need special handle\n            if self.plat_name and not self.plat_name.startswith(\"macosx\"):\n                plat_name = self.plat_name\n            else:\n                # on macosx always limit the platform name to comply with any\n                # c-extension modules in bdist_dir, since the user can specify\n                # a higher MACOSX_DEPLOYMENT_TARGET via tools like CMake\n\n                # on other platforms, and on macosx if there are no c-extension\n                # modules, use the default platform name.\n                plat_name = get_platform(self.bdist_dir)\n\n            if _is_32bit_interpreter():\n                if plat_name in (\"linux-x86_64\", \"linux_x86_64\"):\n                    plat_name = \"linux_i686\"\n                if plat_name in (\"linux-aarch64\", \"linux_aarch64\"):\n                    # TODO armv8l, packaging pull request #690 => this did not land\n                    # in pip/packaging yet\n                    plat_name = \"linux_armv7l\"\n\n        plat_name = (\n            plat_name.lower().replace(\"-\", \"_\").replace(\".\", \"_\").replace(\" \", \"_\")\n        )\n\n        if self.root_is_pure:\n            if self.universal:\n                impl = \"py2.py3\"\n            else:\n                impl = self.python_tag\n            tag = (impl, \"none\", plat_name)\n        else:\n            impl_name = tags.interpreter_name()\n            impl_ver = tags.interpreter_version()\n            impl = impl_name + impl_ver\n            # We don't work on CPython 3.1, 3.0.\n            if self.py_limited_api and (impl_name + impl_ver).startswith(\"cp3\"):\n                impl = self.py_limited_api\n                abi_tag = \"abi3\"\n            else:\n                abi_tag = str(get_abi_tag()).lower()\n            tag = (impl, abi_tag, plat_name)\n            # issue gh-374: allow overriding plat_name\n            supported_tags = [\n                (t.interpreter, t.abi, plat_name) for t in tags.sys_tags()\n            ]\n            assert tag in supported_tags, (\n                f\"would build wheel with unsupported tag {tag}\"\n            )\n        return tag\n\n    def run(self):\n        build_scripts = self.reinitialize_command(\"build_scripts\")\n        build_scripts.executable = \"python\"\n        build_scripts.force = True\n\n        build_ext = self.reinitialize_command(\"build_ext\")\n        build_ext.inplace = False\n\n        if not self.skip_build:\n            self.run_command(\"build\")\n\n        install = self.reinitialize_command(\"install\", reinit_subcommands=True)\n        install.root = self.bdist_dir\n        install.compile = False\n        install.skip_build = self.skip_build\n        install.warn_dir = False\n\n        # A wheel without setuptools scripts is more cross-platform.\n        # Use the (undocumented) `no_ep` option to setuptools'\n        # install_scripts command to avoid creating entry point scripts.\n        install_scripts = self.reinitialize_command(\"install_scripts\")\n        install_scripts.no_ep = True\n\n        # Use a custom scheme for the archive, because we have to decide\n        # at installation time which scheme to use.\n        for key in (\"headers\", \"scripts\", \"data\", \"purelib\", \"platlib\"):\n            setattr(install, \"install_\" + key, os.path.join(self.data_dir, key))\n\n        basedir_observed = \"\"\n\n        if os.name == \"nt\":\n            # win32 barfs if any of these are ''; could be '.'?\n            # (distutils.command.install:change_roots bug)\n            basedir_observed = os.path.normpath(os.path.join(self.data_dir, \"..\"))\n            self.install_libbase = self.install_lib = basedir_observed\n\n        setattr(\n            install,\n            \"install_purelib\" if self.root_is_pure else \"install_platlib\",\n            basedir_observed,\n        )\n\n        log.info(f\"installing to {self.bdist_dir}\")\n\n        self.run_command(\"install\")\n\n        impl_tag, abi_tag, plat_tag = self.get_tag()\n        archive_basename = f\"{self.wheel_dist_name}-{impl_tag}-{abi_tag}-{plat_tag}\"\n        if not self.relative:\n            archive_root = self.bdist_dir\n        else:\n            archive_root = os.path.join(\n                self.bdist_dir, self._ensure_relative(install.install_base)\n            )\n\n        self.set_undefined_options(\"install_egg_info\", (\"target\", \"egginfo_dir\"))\n        distinfo_dirname = (\n            f\"{safer_name(self.distribution.get_name())}-\"\n            f\"{safer_version(self.distribution.get_version())}.dist-info\"\n        )\n        distinfo_dir = os.path.join(self.bdist_dir, distinfo_dirname)\n        if self.dist_info_dir:\n            # Use the given dist-info directly.\n            log.debug(f\"reusing {self.dist_info_dir}\")\n            shutil.copytree(self.dist_info_dir, distinfo_dir)\n            # Egg info is still generated, so remove it now to avoid it getting\n            # copied into the wheel.\n            _shutil.rmtree(self.egginfo_dir)\n        else:\n            # Convert the generated egg-info into dist-info.\n            self.egg2dist(self.egginfo_dir, distinfo_dir)\n\n        self.write_wheelfile(distinfo_dir)\n\n        # Make the archive\n        if not os.path.exists(self.dist_dir):\n            os.makedirs(self.dist_dir)\n\n        wheel_path = os.path.join(self.dist_dir, archive_basename + \".whl\")\n        with WheelFile(wheel_path, \"w\", self._zip_compression()) as wf:\n            wf.write_files(archive_root)\n\n        # Add to 'Distribution.dist_files' so that the \"upload\" command works\n        getattr(self.distribution, \"dist_files\", []).append((\n            \"bdist_wheel\",\n            f\"{sys.version_info.major}.{sys.version_info.minor}\",\n            wheel_path,\n        ))\n\n        if not self.keep_temp:\n            log.info(f\"removing {self.bdist_dir}\")\n            if not self.dry_run:\n                _shutil.rmtree(self.bdist_dir)\n\n    def write_wheelfile(\n        self, wheelfile_base: str, generator: str = f\"setuptools ({__version__})\"\n    ) -> None:\n        from email.message import Message\n\n        msg = Message()\n        msg[\"Wheel-Version\"] = \"1.0\"  # of the spec\n        msg[\"Generator\"] = generator\n        msg[\"Root-Is-Purelib\"] = str(self.root_is_pure).lower()\n        if self.build_number is not None:\n            msg[\"Build\"] = self.build_number\n\n        # Doesn't work for bdist_wininst\n        impl_tag, abi_tag, plat_tag = self.get_tag()\n        for impl in impl_tag.split(\".\"):\n            for abi in abi_tag.split(\".\"):\n                for plat in plat_tag.split(\".\"):\n                    msg[\"Tag\"] = \"-\".join((impl, abi, plat))\n\n        wheelfile_path = os.path.join(wheelfile_base, \"WHEEL\")\n        log.info(f\"creating {wheelfile_path}\")\n        with open(wheelfile_path, \"wb\") as f:\n            BytesGenerator(f, maxheaderlen=0).flatten(msg)\n\n    def _ensure_relative(self, path: str) -> str:\n        # copied from dir_util, deleted\n        drive, path = os.path.splitdrive(path)\n        if path[0:1] == os.sep:\n            path = drive + path[1:]\n        return path\n\n    @property\n    def license_paths(self) -> Iterable[str]:\n        if setuptools_major_version >= 57:\n            # Setuptools has resolved any patterns to actual file names\n            return self.distribution.metadata.license_files or ()\n\n        files = set[str]()\n        metadata = self.distribution.get_option_dict(\"metadata\")\n        if setuptools_major_version >= 42:\n            # Setuptools recognizes the license_files option but does not do globbing\n            patterns = cast(Sequence[str], self.distribution.metadata.license_files)\n        else:\n            # Prior to those, wheel is entirely responsible for handling license files\n            if \"license_files\" in metadata:\n                patterns = metadata[\"license_files\"][1].split()\n            else:\n                patterns = ()\n\n        if \"license_file\" in metadata:\n            warnings.warn(\n                'The \"license_file\" option is deprecated. Use \"license_files\" instead.',\n                DeprecationWarning,\n                stacklevel=2,\n            )\n            files.add(metadata[\"license_file\"][1])\n\n        if not files and not patterns and not isinstance(patterns, list):\n            patterns = (\"LICEN[CS]E*\", \"COPYING*\", \"NOTICE*\", \"AUTHORS*\")\n\n        for pattern in patterns:\n            for path in iglob(pattern):\n                if path.endswith(\"~\"):\n                    log.debug(\n                        f'ignoring license file \"{path}\" as it looks like a backup'\n                    )\n                    continue\n\n                if path not in files and os.path.isfile(path):\n                    log.info(\n                        f'adding license file \"{path}\" (matched pattern \"{pattern}\")'\n                    )\n                    files.add(path)\n\n        return files\n\n    def egg2dist(self, egginfo_path: str, distinfo_path: str) -> None:\n        \"\"\"Convert an .egg-info directory into a .dist-info directory\"\"\"\n\n        def adios(p: str) -> None:\n            \"\"\"Appropriately delete directory, file or link.\"\"\"\n            if os.path.exists(p) and not os.path.islink(p) and os.path.isdir(p):\n                _shutil.rmtree(p)\n            elif os.path.exists(p):\n                os.unlink(p)\n\n        adios(distinfo_path)\n\n        if not os.path.exists(egginfo_path):\n            # There is no egg-info. This is probably because the egg-info\n            # file/directory is not named matching the distribution name used\n            # to name the archive file. Check for this case and report\n            # accordingly.\n            import glob\n\n            pat = os.path.join(os.path.dirname(egginfo_path), \"*.egg-info\")\n            possible = glob.glob(pat)\n            err = f\"Egg metadata expected at {egginfo_path} but not found\"\n            if possible:\n                alt = os.path.basename(possible[0])\n                err += f\" ({alt} found - possible misnamed archive file?)\"\n\n            raise ValueError(err)\n\n        # .egg-info is a directory\n        pkginfo_path = os.path.join(egginfo_path, \"PKG-INFO\")\n\n        # ignore common egg metadata that is useless to wheel\n        shutil.copytree(\n            egginfo_path,\n            distinfo_path,\n            ignore=lambda x, y: {\n                \"PKG-INFO\",\n                \"requires.txt\",\n                \"SOURCES.txt\",\n                \"not-zip-safe\",\n            },\n        )\n\n        # delete dependency_links if it is only whitespace\n        dependency_links_path = os.path.join(distinfo_path, \"dependency_links.txt\")\n        with open(dependency_links_path, encoding=\"utf-8\") as dependency_links_file:\n            dependency_links = dependency_links_file.read().strip()\n        if not dependency_links:\n            adios(dependency_links_path)\n\n        metadata_path = os.path.join(distinfo_path, \"METADATA\")\n        shutil.copy(pkginfo_path, metadata_path)\n\n        for license_path in self.license_paths:\n            filename = os.path.basename(license_path)\n            shutil.copy(license_path, os.path.join(distinfo_path, filename))\n\n        adios(egginfo_path)\n\n    def _zip_compression(self) -> int:\n        if (\n            isinstance(self.compression, int)\n            and self.compression in self.supported_compressions.values()\n        ):\n            return self.compression\n\n        compression = self.supported_compressions.get(str(self.compression))\n        if compression is not None:\n            return compression\n\n        raise ValueError(f\"Unsupported compression: {self.compression!r}\")\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/command/build.py","size":6052,"sha1":"c61a1f67c039fd854e3625a509401aa56524ed58","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"from __future__ import annotations\n\nfrom typing import Protocol\n\nfrom ..dist import Distribution\n\nfrom distutils.command.build import build as _build\n\n_ORIGINAL_SUBCOMMANDS = {\"build_py\", \"build_clib\", \"build_ext\", \"build_scripts\"}\n\n\nclass build(_build):\n    distribution: Distribution  # override distutils.dist.Distribution with setuptools.dist.Distribution\n\n    # copy to avoid sharing the object with parent class\n    sub_commands = _build.sub_commands[:]\n\n\nclass SubCommand(Protocol):\n    \"\"\"In order to support editable installations (see :pep:`660`) all\n    build subcommands **SHOULD** implement this protocol. They also **MUST** inherit\n    from ``setuptools.Command``.\n\n    When creating an :pep:`editable wheel <660>`, ``setuptools`` will try to evaluate\n    custom ``build`` subcommands using the following procedure:\n\n    1. ``setuptools`` will set the ``editable_mode`` attribute to ``True``\n    2. ``setuptools`` will execute the ``run()`` command.\n\n       .. important::\n          Subcommands **SHOULD** take advantage of ``editable_mode=True`` to adequate\n          its behaviour or perform optimisations.\n\n          For example, if a subcommand doesn't need to generate an extra file and\n          all it does is to copy a source file into the build directory,\n          ``run()`` **SHOULD** simply \"early return\".\n\n          Similarly, if the subcommand creates files that would be placed alongside\n          Python files in the final distribution, during an editable install\n          the command **SHOULD** generate these files \"in place\" (i.e. write them to\n          the original source directory, instead of using the build directory).\n          Note that ``get_output_mapping()`` should reflect that and include mappings\n          for \"in place\" builds accordingly.\n\n    3. ``setuptools`` use any knowledge it can derive from the return values of\n       ``get_outputs()`` and ``get_output_mapping()`` to create an editable wheel.\n       When relevant ``setuptools`` **MAY** attempt to use file links based on the value\n       of ``get_output_mapping()``. Alternatively, ``setuptools`` **MAY** attempt to use\n       :doc:`import hooks <python:reference/import>` to redirect any attempt to import\n       to the directory with the original source code and other files built in place.\n\n    Please note that custom sub-commands **SHOULD NOT** rely on ``run()`` being\n    executed (or not) to provide correct return values for ``get_outputs()``,\n    ``get_output_mapping()`` or ``get_source_files()``. The ``get_*`` methods should\n    work independently of ``run()``.\n    \"\"\"\n\n    editable_mode: bool = False\n    \"\"\"Boolean flag that will be set to ``True`` when setuptools is used for an\n    editable installation (see :pep:`660`).\n    Implementations **SHOULD** explicitly set the default value of this attribute to\n    ``False``.\n    When subcommands run, they can use this flag to perform optimizations or change\n    their behaviour accordingly.\n    \"\"\"\n\n    build_lib: str\n    \"\"\"String representing the directory where the build artifacts should be stored,\n    e.g. ``build/lib``.\n    For example, if a distribution wants to provide a Python module named ``pkg.mod``,\n    then a corresponding file should be written to ``{build_lib}/package/module.py``.\n    A way of thinking about this is that the files saved under ``build_lib``\n    would be eventually copied to one of the directories in :obj:`site.PREFIXES`\n    upon installation.\n\n    A command that produces platform-independent files (e.g. compiling text templates\n    into Python functions), **CAN** initialize ``build_lib`` by copying its value from\n    the ``build_py`` command. On the other hand, a command that produces\n    platform-specific files **CAN** initialize ``build_lib`` by copying its value from\n    the ``build_ext`` command. In general this is done inside the ``finalize_options``\n    method with the help of the ``set_undefined_options`` command::\n\n        def finalize_options(self):\n            self.set_undefined_options(\"build_py\", (\"build_lib\", \"build_lib\"))\n            ...\n    \"\"\"\n\n    def initialize_options(self) -> None:\n        \"\"\"(Required by the original :class:`setuptools.Command` interface)\"\"\"\n        ...\n\n    def finalize_options(self) -> None:\n        \"\"\"(Required by the original :class:`setuptools.Command` interface)\"\"\"\n        ...\n\n    def run(self) -> None:\n        \"\"\"(Required by the original :class:`setuptools.Command` interface)\"\"\"\n        ...\n\n    def get_source_files(self) -> list[str]:\n        \"\"\"\n        Return a list of all files that are used by the command to create the expected\n        outputs.\n        For example, if your build command transpiles Java files into Python, you should\n        list here all the Java files.\n        The primary purpose of this function is to help populating the ``sdist``\n        with all the files necessary to build the distribution.\n        All files should be strings relative to the project root directory.\n        \"\"\"\n        ...\n\n    def get_outputs(self) -> list[str]:\n        \"\"\"\n        Return a list of files intended for distribution as they would have been\n        produced by the build.\n        These files should be strings in the form of\n        ``\"{build_lib}/destination/file/path\"``.\n\n        .. note::\n           The return value of ``get_output()`` should include all files used as keys\n           in ``get_output_mapping()`` plus files that are generated during the build\n           and don't correspond to any source file already present in the project.\n        \"\"\"\n        ...\n\n    def get_output_mapping(self) -> dict[str, str]:\n        \"\"\"\n        Return a mapping between destination files as they would be produced by the\n        build (dict keys) into the respective existing (source) files (dict values).\n        Existing (source) files should be represented as strings relative to the project\n        root directory.\n        Destination files should be strings in the form of\n        ``\"{build_lib}/destination/file/path\"``.\n        \"\"\"\n        ...\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/command/build_clib.py","size":4528,"sha1":"43804fd9c5ee11b6540b672b8e66023795907ea6","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"from ..dist import Distribution\nfrom ..modified import newer_pairwise_group\n\nimport distutils.command.build_clib as orig\nfrom distutils import log\nfrom distutils.errors import DistutilsSetupError\n\n\nclass build_clib(orig.build_clib):\n    \"\"\"\n    Override the default build_clib behaviour to do the following:\n\n    1. Implement a rudimentary timestamp-based dependency system\n       so 'compile()' doesn't run every time.\n    2. Add more keys to the 'build_info' dictionary:\n        * obj_deps - specify dependencies for each object compiled.\n                     this should be a dictionary mapping a key\n                     with the source filename to a list of\n                     dependencies. Use an empty string for global\n                     dependencies.\n        * cflags   - specify a list of additional flags to pass to\n                     the compiler.\n    \"\"\"\n\n    distribution: Distribution  # override distutils.dist.Distribution with setuptools.dist.Distribution\n\n    def build_libraries(self, libraries) -> None:\n        for lib_name, build_info in libraries:\n            sources = build_info.get('sources')\n            if sources is None or not isinstance(sources, (list, tuple)):\n                raise DistutilsSetupError(\n                    f\"in 'libraries' option (library '{lib_name}'), \"\n                    \"'sources' must be present and must be \"\n                    \"a list of source filenames\"\n                )\n            sources = sorted(list(sources))\n\n            log.info(\"building '%s' library\", lib_name)\n\n            # Make sure everything is the correct type.\n            # obj_deps should be a dictionary of keys as sources\n            # and a list/tuple of files that are its dependencies.\n            obj_deps = build_info.get('obj_deps', dict())\n            if not isinstance(obj_deps, dict):\n                raise DistutilsSetupError(\n                    f\"in 'libraries' option (library '{lib_name}'), \"\n                    \"'obj_deps' must be a dictionary of \"\n                    \"type 'source: list'\"\n                )\n            dependencies = []\n\n            # Get the global dependencies that are specified by the '' key.\n            # These will go into every source's dependency list.\n            global_deps = obj_deps.get('', list())\n            if not isinstance(global_deps, (list, tuple)):\n                raise DistutilsSetupError(\n                    f\"in 'libraries' option (library '{lib_name}'), \"\n                    \"'obj_deps' must be a dictionary of \"\n                    \"type 'source: list'\"\n                )\n\n            # Build the list to be used by newer_pairwise_group\n            # each source will be auto-added to its dependencies.\n            for source in sources:\n                src_deps = [source]\n                src_deps.extend(global_deps)\n                extra_deps = obj_deps.get(source, list())\n                if not isinstance(extra_deps, (list, tuple)):\n                    raise DistutilsSetupError(\n                        f\"in 'libraries' option (library '{lib_name}'), \"\n                        \"'obj_deps' must be a dictionary of \"\n                        \"type 'source: list'\"\n                    )\n                src_deps.extend(extra_deps)\n                dependencies.append(src_deps)\n\n            expected_objects = self.compiler.object_filenames(\n                sources,\n                output_dir=self.build_temp,\n            )\n\n            if newer_pairwise_group(dependencies, expected_objects) != ([], []):\n                # First, compile the source code to object files in the library\n                # directory.  (This should probably change to putting object\n                # files in a temporary build directory.)\n                macros = build_info.get('macros')\n                include_dirs = build_info.get('include_dirs')\n                cflags = build_info.get('cflags')\n                self.compiler.compile(\n                    sources,\n                    output_dir=self.build_temp,\n                    macros=macros,\n                    include_dirs=include_dirs,\n                    extra_postargs=cflags,\n                    debug=self.debug,\n                )\n\n            # Now \"link\" the object files together into a static library.\n            # (On Unix at least, this isn't really linking -- it just\n            # builds an archive.  Whatever.)\n            self.compiler.create_static_lib(\n                expected_objects, lib_name, output_dir=self.build_clib, debug=self.debug\n            )\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/command/build_ext.py","size":18377,"sha1":"a0eb2581be6884b725be19094fad30991e065877","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"from __future__ import annotations\n\nimport itertools\nimport os\nimport sys\nfrom collections.abc import Iterator\nfrom importlib.machinery import EXTENSION_SUFFIXES\nfrom importlib.util import cache_from_source as _compiled_file_name\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING\n\nfrom setuptools.dist import Distribution\nfrom setuptools.errors import BaseError\nfrom setuptools.extension import Extension, Library\n\nfrom distutils import log\nfrom distutils.ccompiler import new_compiler\nfrom distutils.sysconfig import customize_compiler, get_config_var\n\nif TYPE_CHECKING:\n    # Cython not installed on CI tests, causing _build_ext to be `Any`\n    from distutils.command.build_ext import build_ext as _build_ext\nelse:\n    try:\n        # Attempt to use Cython for building extensions, if available\n        from Cython.Distutils.build_ext import build_ext as _build_ext\n\n        # Additionally, assert that the compiler module will load\n        # also. Ref #1229.\n        __import__('Cython.Compiler.Main')\n    except ImportError:\n        from distutils.command.build_ext import build_ext as _build_ext\n\n# make sure _config_vars is initialized\nget_config_var(\"LDSHARED\")\n# Not publicly exposed in typeshed distutils stubs, but this is done on purpose\n# See https://github.com/pypa/setuptools/pull/4228#issuecomment-1959856400\nfrom distutils.sysconfig import _config_vars as _CONFIG_VARS  # noqa: E402\n\n\ndef _customize_compiler_for_shlib(compiler):\n    if sys.platform == \"darwin\":\n        # building .dylib requires additional compiler flags on OSX; here we\n        # temporarily substitute the pyconfig.h variables so that distutils'\n        # 'customize_compiler' uses them before we build the shared libraries.\n        tmp = _CONFIG_VARS.copy()\n        try:\n            # XXX Help!  I don't have any idea whether these are right...\n            _CONFIG_VARS['LDSHARED'] = (\n                \"gcc -Wl,-x -dynamiclib -undefined dynamic_lookup\"\n            )\n            _CONFIG_VARS['CCSHARED'] = \" -dynamiclib\"\n            _CONFIG_VARS['SO'] = \".dylib\"\n            customize_compiler(compiler)\n        finally:\n            _CONFIG_VARS.clear()\n            _CONFIG_VARS.update(tmp)\n    else:\n        customize_compiler(compiler)\n\n\nhave_rtld = False\nuse_stubs = False\nlibtype = 'shared'\n\nif sys.platform == \"darwin\":\n    use_stubs = True\nelif os.name != 'nt':\n    try:\n        import dl  # type: ignore[import-not-found] # https://github.com/python/mypy/issues/13002\n\n        use_stubs = have_rtld = hasattr(dl, 'RTLD_NOW')\n    except ImportError:\n        pass\n\n\ndef if_dl(s):\n    return s if have_rtld else ''\n\n\ndef get_abi3_suffix():\n    \"\"\"Return the file extension for an abi3-compliant Extension()\"\"\"\n    for suffix in EXTENSION_SUFFIXES:\n        if '.abi3' in suffix:  # Unix\n            return suffix\n        elif suffix == '.pyd':  # Windows\n            return suffix\n    return None\n\n\nclass build_ext(_build_ext):\n    distribution: Distribution  # override distutils.dist.Distribution with setuptools.dist.Distribution\n    editable_mode = False\n    inplace = False\n\n    def run(self):\n        \"\"\"Build extensions in build directory, then copy if --inplace\"\"\"\n        old_inplace, self.inplace = self.inplace, False\n        _build_ext.run(self)\n        self.inplace = old_inplace\n        if old_inplace:\n            self.copy_extensions_to_source()\n\n    def _get_inplace_equivalent(self, build_py, ext: Extension) -> tuple[str, str]:\n        fullname = self.get_ext_fullname(ext.name)\n        filename = self.get_ext_filename(fullname)\n        modpath = fullname.split('.')\n        package = '.'.join(modpath[:-1])\n        package_dir = build_py.get_package_dir(package)\n        inplace_file = os.path.join(package_dir, os.path.basename(filename))\n        regular_file = os.path.join(self.build_lib, filename)\n        return (inplace_file, regular_file)\n\n    def copy_extensions_to_source(self) -> None:\n        build_py = self.get_finalized_command('build_py')\n        for ext in self.extensions:\n            inplace_file, regular_file = self._get_inplace_equivalent(build_py, ext)\n\n            # Always copy, even if source is older than destination, to ensure\n            # that the right extensions for the current Python/platform are\n            # used.\n            if os.path.exists(regular_file) or not ext.optional:\n                self.copy_file(regular_file, inplace_file, level=self.verbose)\n\n            if ext._needs_stub:\n                inplace_stub = self._get_equivalent_stub(ext, inplace_file)\n                self._write_stub_file(inplace_stub, ext, compile=True)\n                # Always compile stub and remove the original (leave the cache behind)\n                # (this behaviour was observed in previous iterations of the code)\n\n    def _get_equivalent_stub(self, ext: Extension, output_file: str) -> str:\n        dir_ = os.path.dirname(output_file)\n        _, _, name = ext.name.rpartition(\".\")\n        return f\"{os.path.join(dir_, name)}.py\"\n\n    def _get_output_mapping(self) -> Iterator[tuple[str, str]]:\n        if not self.inplace:\n            return\n\n        build_py = self.get_finalized_command('build_py')\n        opt = self.get_finalized_command('install_lib').optimize or \"\"\n\n        for ext in self.extensions:\n            inplace_file, regular_file = self._get_inplace_equivalent(build_py, ext)\n            yield (regular_file, inplace_file)\n\n            if ext._needs_stub:\n                # This version of `build_ext` always builds artifacts in another dir,\n                # when \"inplace=True\" is given it just copies them back.\n                # This is done in the `copy_extensions_to_source` function, which\n                # always compile stub files via `_compile_and_remove_stub`.\n                # At the end of the process, a `.pyc` stub file is created without the\n                # corresponding `.py`.\n\n                inplace_stub = self._get_equivalent_stub(ext, inplace_file)\n                regular_stub = self._get_equivalent_stub(ext, regular_file)\n                inplace_cache = _compiled_file_name(inplace_stub, optimization=opt)\n                output_cache = _compiled_file_name(regular_stub, optimization=opt)\n                yield (output_cache, inplace_cache)\n\n    def get_ext_filename(self, fullname: str) -> str:\n        so_ext = os.getenv('SETUPTOOLS_EXT_SUFFIX')\n        if so_ext:\n            filename = os.path.join(*fullname.split('.')) + so_ext\n        else:\n            filename = _build_ext.get_ext_filename(self, fullname)\n            ext_suffix = get_config_var('EXT_SUFFIX')\n            if not isinstance(ext_suffix, str):\n                raise OSError(\n                    \"Configuration variable EXT_SUFFIX not found for this platform \"\n                    \"and environment variable SETUPTOOLS_EXT_SUFFIX is missing\"\n                )\n            so_ext = ext_suffix\n\n        if fullname in self.ext_map:\n            ext = self.ext_map[fullname]\n            abi3_suffix = get_abi3_suffix()\n            if ext.py_limited_api and abi3_suffix:  # Use abi3\n                filename = filename[: -len(so_ext)] + abi3_suffix\n            if isinstance(ext, Library):\n                fn, ext = os.path.splitext(filename)\n                return self.shlib_compiler.library_filename(fn, libtype)\n            elif use_stubs and ext._links_to_dynamic:\n                d, fn = os.path.split(filename)\n                return os.path.join(d, 'dl-' + fn)\n        return filename\n\n    def initialize_options(self):\n        _build_ext.initialize_options(self)\n        self.shlib_compiler = None\n        self.shlibs = []\n        self.ext_map = {}\n        self.editable_mode = False\n\n    def finalize_options(self) -> None:\n        _build_ext.finalize_options(self)\n        self.extensions = self.extensions or []\n        self.check_extensions_list(self.extensions)\n        self.shlibs = [ext for ext in self.extensions if isinstance(ext, Library)]\n        if self.shlibs:\n            self.setup_shlib_compiler()\n        for ext in self.extensions:\n            ext._full_name = self.get_ext_fullname(ext.name)\n        for ext in self.extensions:\n            fullname = ext._full_name\n            self.ext_map[fullname] = ext\n\n            # distutils 3.1 will also ask for module names\n            # XXX what to do with conflicts?\n            self.ext_map[fullname.split('.')[-1]] = ext\n\n            ltd = self.shlibs and self.links_to_dynamic(ext) or False\n            ns = ltd and use_stubs and not isinstance(ext, Library)\n            ext._links_to_dynamic = ltd\n            ext._needs_stub = ns\n            filename = ext._file_name = self.get_ext_filename(fullname)\n            libdir = os.path.dirname(os.path.join(self.build_lib, filename))\n            if ltd and libdir not in ext.library_dirs:\n                ext.library_dirs.append(libdir)\n            if ltd and use_stubs and os.curdir not in ext.runtime_library_dirs:\n                ext.runtime_library_dirs.append(os.curdir)\n\n        if self.editable_mode:\n            self.inplace = True\n\n    def setup_shlib_compiler(self):\n        compiler = self.shlib_compiler = new_compiler(\n            compiler=self.compiler, dry_run=self.dry_run, force=self.force\n        )\n        _customize_compiler_for_shlib(compiler)\n\n        if self.include_dirs is not None:\n            compiler.set_include_dirs(self.include_dirs)\n        if self.define is not None:\n            # 'define' option is a list of (name,value) tuples\n            for name, value in self.define:\n                compiler.define_macro(name, value)\n        if self.undef is not None:\n            for macro in self.undef:\n                compiler.undefine_macro(macro)\n        if self.libraries is not None:\n            compiler.set_libraries(self.libraries)\n        if self.library_dirs is not None:\n            compiler.set_library_dirs(self.library_dirs)\n        if self.rpath is not None:\n            compiler.set_runtime_library_dirs(self.rpath)\n        if self.link_objects is not None:\n            compiler.set_link_objects(self.link_objects)\n\n        # hack so distutils' build_extension() builds a library instead\n        compiler.link_shared_object = link_shared_object.__get__(compiler)  # type: ignore[method-assign]\n\n    def get_export_symbols(self, ext):\n        if isinstance(ext, Library):\n            return ext.export_symbols\n        return _build_ext.get_export_symbols(self, ext)\n\n    def build_extension(self, ext) -> None:\n        ext._convert_pyx_sources_to_lang()\n        _compiler = self.compiler\n        try:\n            if isinstance(ext, Library):\n                self.compiler = self.shlib_compiler\n            _build_ext.build_extension(self, ext)\n            if ext._needs_stub:\n                build_lib = self.get_finalized_command('build_py').build_lib\n                self.write_stub(build_lib, ext)\n        finally:\n            self.compiler = _compiler\n\n    def links_to_dynamic(self, ext):\n        \"\"\"Return true if 'ext' links to a dynamic lib in the same package\"\"\"\n        # XXX this should check to ensure the lib is actually being built\n        # XXX as dynamic, and not just using a locally-found version or a\n        # XXX static-compiled version\n        libnames = dict.fromkeys([lib._full_name for lib in self.shlibs])\n        pkg = '.'.join(ext._full_name.split('.')[:-1] + [''])\n        return any(pkg + libname in libnames for libname in ext.libraries)\n\n    def get_source_files(self) -> list[str]:\n        return [*_build_ext.get_source_files(self), *self._get_internal_depends()]\n\n    def _get_internal_depends(self) -> Iterator[str]:\n        \"\"\"Yield ``ext.depends`` that are contained by the project directory\"\"\"\n        project_root = Path(self.distribution.src_root or os.curdir).resolve()\n        depends = (dep for ext in self.extensions for dep in ext.depends)\n\n        def skip(orig_path: str, reason: str) -> None:\n            log.info(\n                \"dependency %s won't be automatically \"\n                \"included in the manifest: the path %s\",\n                orig_path,\n                reason,\n            )\n\n        for dep in depends:\n            path = Path(dep)\n\n            if path.is_absolute():\n                skip(dep, \"must be relative\")\n                continue\n\n            if \"..\" in path.parts:\n                skip(dep, \"can't have `..` segments\")\n                continue\n\n            try:\n                resolved = (project_root / path).resolve(strict=True)\n            except OSError:\n                skip(dep, \"doesn't exist\")\n                continue\n\n            try:\n                resolved.relative_to(project_root)\n            except ValueError:\n                skip(dep, \"must be inside the project root\")\n                continue\n\n            yield path.as_posix()\n\n    def get_outputs(self) -> list[str]:\n        if self.inplace:\n            return list(self.get_output_mapping().keys())\n        return sorted(_build_ext.get_outputs(self) + self.__get_stubs_outputs())\n\n    def get_output_mapping(self) -> dict[str, str]:\n        \"\"\"See :class:`setuptools.commands.build.SubCommand`\"\"\"\n        mapping = self._get_output_mapping()\n        return dict(sorted(mapping, key=lambda x: x[0]))\n\n    def __get_stubs_outputs(self):\n        # assemble the base name for each extension that needs a stub\n        ns_ext_bases = (\n            os.path.join(self.build_lib, *ext._full_name.split('.'))\n            for ext in self.extensions\n            if ext._needs_stub\n        )\n        # pair each base with the extension\n        pairs = itertools.product(ns_ext_bases, self.__get_output_extensions())\n        return list(base + fnext for base, fnext in pairs)\n\n    def __get_output_extensions(self):\n        yield '.py'\n        yield '.pyc'\n        if self.get_finalized_command('build_py').optimize:\n            yield '.pyo'\n\n    def write_stub(self, output_dir, ext, compile=False) -> None:\n        stub_file = os.path.join(output_dir, *ext._full_name.split('.')) + '.py'\n        self._write_stub_file(stub_file, ext, compile)\n\n    def _write_stub_file(self, stub_file: str, ext: Extension, compile=False):\n        log.info(\"writing stub loader for %s to %s\", ext._full_name, stub_file)\n        if compile and os.path.exists(stub_file):\n            raise BaseError(stub_file + \" already exists! Please delete.\")\n        if not self.dry_run:\n            with open(stub_file, 'w', encoding=\"utf-8\") as f:\n                content = '\\n'.join([\n                    \"def __bootstrap__():\",\n                    \"   global __bootstrap__, __file__, __loader__\",\n                    \"   import sys, os, pkg_resources, importlib.util\" + if_dl(\", dl\"),\n                    \"   __file__ = pkg_resources.resource_filename\"\n                    f\"(__name__,{os.path.basename(ext._file_name)!r})\",\n                    \"   del __bootstrap__\",\n                    \"   if '__loader__' in globals():\",\n                    \"       del __loader__\",\n                    if_dl(\"   old_flags = sys.getdlopenflags()\"),\n                    \"   old_dir = os.getcwd()\",\n                    \"   try:\",\n                    \"     os.chdir(os.path.dirname(__file__))\",\n                    if_dl(\"     sys.setdlopenflags(dl.RTLD_NOW)\"),\n                    \"     spec = importlib.util.spec_from_file_location(\",\n                    \"                __name__, __file__)\",\n                    \"     mod = importlib.util.module_from_spec(spec)\",\n                    \"     spec.loader.exec_module(mod)\",\n                    \"   finally:\",\n                    if_dl(\"     sys.setdlopenflags(old_flags)\"),\n                    \"     os.chdir(old_dir)\",\n                    \"__bootstrap__()\",\n                    \"\",  # terminal \\n\n                ])\n                f.write(content)\n        if compile:\n            self._compile_and_remove_stub(stub_file)\n\n    def _compile_and_remove_stub(self, stub_file: str):\n        from distutils.util import byte_compile\n\n        byte_compile([stub_file], optimize=0, force=True, dry_run=self.dry_run)\n        optimize = self.get_finalized_command('install_lib').optimize\n        if optimize > 0:\n            byte_compile(\n                [stub_file],\n                optimize=optimize,\n                force=True,\n                dry_run=self.dry_run,\n            )\n        if os.path.exists(stub_file) and not self.dry_run:\n            os.unlink(stub_file)\n\n\nif use_stubs or os.name == 'nt':\n    # Build shared libraries\n    #\n    def link_shared_object(\n        self,\n        objects,\n        output_libname,\n        output_dir=None,\n        libraries=None,\n        library_dirs=None,\n        runtime_library_dirs=None,\n        export_symbols=None,\n        debug: bool = False,\n        extra_preargs=None,\n        extra_postargs=None,\n        build_temp=None,\n        target_lang=None,\n    ) -> None:\n        self.link(\n            self.SHARED_LIBRARY,\n            objects,\n            output_libname,\n            output_dir,\n            libraries,\n            library_dirs,\n            runtime_library_dirs,\n            export_symbols,\n            debug,\n            extra_preargs,\n            extra_postargs,\n            build_temp,\n            target_lang,\n        )\n\nelse:\n    # Build static libraries everywhere else\n    libtype = 'static'\n\n    def link_shared_object(\n        self,\n        objects,\n        output_libname,\n        output_dir=None,\n        libraries=None,\n        library_dirs=None,\n        runtime_library_dirs=None,\n        export_symbols=None,\n        debug: bool = False,\n        extra_preargs=None,\n        extra_postargs=None,\n        build_temp=None,\n        target_lang=None,\n    ) -> None:\n        # XXX we need to either disallow these attrs on Library instances,\n        # or warn/abort here if set, or something...\n        # libraries=None, library_dirs=None, runtime_library_dirs=None,\n        # export_symbols=None, extra_preargs=None, extra_postargs=None,\n        # build_temp=None\n\n        assert output_dir is None  # distutils build_ext doesn't pass this\n        output_dir, filename = os.path.split(output_libname)\n        basename, _ext = os.path.splitext(filename)\n        if self.library_filename(\"x\").startswith('lib'):\n            # strip 'lib' prefix; this is kludgy if some platform uses\n            # a different prefix\n            basename = basename[3:]\n\n        self.create_static_lib(objects, basename, output_dir, debug, target_lang)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/command/build_py.py","size":15539,"sha1":"9fcb468e824baccf16432d6b0de822f4b84e1b40","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"from __future__ import annotations\n\nimport fnmatch\nimport itertools\nimport os\nimport stat\nimport textwrap\nfrom collections.abc import Iterable, Iterator\nfrom functools import partial\nfrom glob import glob\nfrom pathlib import Path\n\nfrom more_itertools import unique_everseen\n\nfrom .._path import StrPath, StrPathT\nfrom ..dist import Distribution\nfrom ..warnings import SetuptoolsDeprecationWarning\n\nimport distutils.command.build_py as orig\nimport distutils.errors\nfrom distutils.util import convert_path\n\n_IMPLICIT_DATA_FILES = ('*.pyi', 'py.typed')\n\n\ndef make_writable(target) -> None:\n    os.chmod(target, os.stat(target).st_mode | stat.S_IWRITE)\n\n\nclass build_py(orig.build_py):\n    \"\"\"Enhanced 'build_py' command that includes data files with packages\n\n    The data files are specified via a 'package_data' argument to 'setup()'.\n    See 'setuptools.dist.Distribution' for more details.\n\n    Also, this version of the 'build_py' command allows you to specify both\n    'py_modules' and 'packages' in the same setup operation.\n    \"\"\"\n\n    distribution: Distribution  # override distutils.dist.Distribution with setuptools.dist.Distribution\n    editable_mode: bool = False\n    existing_egg_info_dir: StrPath | None = None  #: Private API, internal use only.\n\n    def finalize_options(self):\n        orig.build_py.finalize_options(self)\n        self.package_data = self.distribution.package_data\n        self.exclude_package_data = self.distribution.exclude_package_data or {}\n        if 'data_files' in self.__dict__:\n            del self.__dict__['data_files']\n\n    def copy_file(  # type: ignore[override] # No overload, no bytes support\n        self,\n        infile: StrPath,\n        outfile: StrPathT,\n        preserve_mode: bool = True,\n        preserve_times: bool = True,\n        link: str | None = None,\n        level: object = 1,\n    ) -> tuple[StrPathT | str, bool]:\n        # Overwrite base class to allow using links\n        if link:\n            infile = str(Path(infile).resolve())\n            outfile = str(Path(outfile).resolve())  # type: ignore[assignment] # Re-assigning a str when outfile is StrPath is ok\n        return super().copy_file(  # pyright: ignore[reportReturnType] # pypa/distutils#309\n            infile, outfile, preserve_mode, preserve_times, link, level\n        )\n\n    def run(self) -> None:\n        \"\"\"Build modules, packages, and copy data files to build directory\"\"\"\n        if not (self.py_modules or self.packages) or self.editable_mode:\n            return\n\n        if self.py_modules:\n            self.build_modules()\n\n        if self.packages:\n            self.build_packages()\n            self.build_package_data()\n\n        # Only compile actual .py files, using our base class' idea of what our\n        # output files are.\n        self.byte_compile(orig.build_py.get_outputs(self, include_bytecode=False))\n\n    def __getattr__(self, attr: str):\n        \"lazily compute data files\"\n        if attr == 'data_files':\n            self.data_files = self._get_data_files()\n            return self.data_files\n        return orig.build_py.__getattr__(self, attr)\n\n    def _get_data_files(self):\n        \"\"\"Generate list of '(package,src_dir,build_dir,filenames)' tuples\"\"\"\n        self.analyze_manifest()\n        return list(map(self._get_pkg_data_files, self.packages or ()))\n\n    def get_data_files_without_manifest(self):\n        \"\"\"\n        Generate list of ``(package,src_dir,build_dir,filenames)`` tuples,\n        but without triggering any attempt to analyze or build the manifest.\n        \"\"\"\n        # Prevent eventual errors from unset `manifest_files`\n        # (that would otherwise be set by `analyze_manifest`)\n        self.__dict__.setdefault('manifest_files', {})\n        return list(map(self._get_pkg_data_files, self.packages or ()))\n\n    def _get_pkg_data_files(self, package):\n        # Locate package source directory\n        src_dir = self.get_package_dir(package)\n\n        # Compute package build directory\n        build_dir = os.path.join(*([self.build_lib] + package.split('.')))\n\n        # Strip directory from globbed filenames\n        filenames = [\n            os.path.relpath(file, src_dir)\n            for file in self.find_data_files(package, src_dir)\n        ]\n        return package, src_dir, build_dir, filenames\n\n    def find_data_files(self, package, src_dir):\n        \"\"\"Return filenames for package's data files in 'src_dir'\"\"\"\n        patterns = self._get_platform_patterns(\n            self.package_data,\n            package,\n            src_dir,\n            extra_patterns=_IMPLICIT_DATA_FILES,\n        )\n        globs_expanded = map(partial(glob, recursive=True), patterns)\n        # flatten the expanded globs into an iterable of matches\n        globs_matches = itertools.chain.from_iterable(globs_expanded)\n        glob_files = filter(os.path.isfile, globs_matches)\n        files = itertools.chain(\n            self.manifest_files.get(package, []),\n            glob_files,\n        )\n        return self.exclude_data_files(package, src_dir, files)\n\n    def get_outputs(self, include_bytecode: bool = True) -> list[str]:  # type: ignore[override] # Using a real boolean instead of 0|1\n        \"\"\"See :class:`setuptools.commands.build.SubCommand`\"\"\"\n        if self.editable_mode:\n            return list(self.get_output_mapping().keys())\n        return super().get_outputs(include_bytecode)\n\n    def get_output_mapping(self) -> dict[str, str]:\n        \"\"\"See :class:`setuptools.commands.build.SubCommand`\"\"\"\n        mapping = itertools.chain(\n            self._get_package_data_output_mapping(),\n            self._get_module_mapping(),\n        )\n        return dict(sorted(mapping, key=lambda x: x[0]))\n\n    def _get_module_mapping(self) -> Iterator[tuple[str, str]]:\n        \"\"\"Iterate over all modules producing (dest, src) pairs.\"\"\"\n        for package, module, module_file in self.find_all_modules():\n            package = package.split('.')\n            filename = self.get_module_outfile(self.build_lib, package, module)\n            yield (filename, module_file)\n\n    def _get_package_data_output_mapping(self) -> Iterator[tuple[str, str]]:\n        \"\"\"Iterate over package data producing (dest, src) pairs.\"\"\"\n        for package, src_dir, build_dir, filenames in self.data_files:\n            for filename in filenames:\n                target = os.path.join(build_dir, filename)\n                srcfile = os.path.join(src_dir, filename)\n                yield (target, srcfile)\n\n    def build_package_data(self) -> None:\n        \"\"\"Copy data files into build directory\"\"\"\n        for target, srcfile in self._get_package_data_output_mapping():\n            self.mkpath(os.path.dirname(target))\n            _outf, _copied = self.copy_file(srcfile, target)\n            make_writable(target)\n\n    def analyze_manifest(self) -> None:\n        self.manifest_files: dict[str, list[str]] = {}\n        if not self.distribution.include_package_data:\n            return\n        src_dirs: dict[str, str] = {}\n        for package in self.packages or ():\n            # Locate package source directory\n            src_dirs[assert_relative(self.get_package_dir(package))] = package\n\n        if (\n            self.existing_egg_info_dir\n            and Path(self.existing_egg_info_dir, \"SOURCES.txt\").exists()\n        ):\n            egg_info_dir = self.existing_egg_info_dir\n            manifest = Path(egg_info_dir, \"SOURCES.txt\")\n            files = manifest.read_text(encoding=\"utf-8\").splitlines()\n        else:\n            self.run_command('egg_info')\n            ei_cmd = self.get_finalized_command('egg_info')\n            egg_info_dir = ei_cmd.egg_info\n            files = ei_cmd.filelist.files\n\n        check = _IncludePackageDataAbuse()\n        for path in self._filter_build_files(files, egg_info_dir):\n            d, f = os.path.split(assert_relative(path))\n            prev = None\n            oldf = f\n            while d and d != prev and d not in src_dirs:\n                prev = d\n                d, df = os.path.split(d)\n                f = os.path.join(df, f)\n            if d in src_dirs:\n                if f == oldf:\n                    if check.is_module(f):\n                        continue  # it's a module, not data\n                else:\n                    importable = check.importable_subpackage(src_dirs[d], f)\n                    if importable:\n                        check.warn(importable)\n                self.manifest_files.setdefault(src_dirs[d], []).append(path)\n\n    def _filter_build_files(\n        self, files: Iterable[str], egg_info: StrPath\n    ) -> Iterator[str]:\n        \"\"\"\n        ``build_meta`` may try to create egg_info outside of the project directory,\n        and this can be problematic for certain plugins (reported in issue #3500).\n\n        Extensions might also include between their sources files created on the\n        ``build_lib`` and ``build_temp`` directories.\n\n        This function should filter this case of invalid files out.\n        \"\"\"\n        build = self.get_finalized_command(\"build\")\n        build_dirs = (egg_info, self.build_lib, build.build_temp, build.build_base)\n        norm_dirs = [os.path.normpath(p) for p in build_dirs if p]\n\n        for file in files:\n            norm_path = os.path.normpath(file)\n            if not os.path.isabs(file) or all(d not in norm_path for d in norm_dirs):\n                yield file\n\n    def get_data_files(self) -> None:\n        pass  # Lazily compute data files in _get_data_files() function.\n\n    def check_package(self, package, package_dir):\n        \"\"\"Check namespace packages' __init__ for declare_namespace\"\"\"\n        try:\n            return self.packages_checked[package]\n        except KeyError:\n            pass\n\n        init_py = orig.build_py.check_package(self, package, package_dir)\n        self.packages_checked[package] = init_py\n\n        if not init_py or not self.distribution.namespace_packages:\n            return init_py\n\n        for pkg in self.distribution.namespace_packages:\n            if pkg == package or pkg.startswith(package + '.'):\n                break\n        else:\n            return init_py\n\n        with open(init_py, 'rb') as f:\n            contents = f.read()\n        if b'declare_namespace' not in contents:\n            raise distutils.errors.DistutilsError(\n                f\"Namespace package problem: {package} is a namespace package, but \"\n                \"its\\n__init__.py does not call declare_namespace()! Please \"\n                'fix it.\\n(See the setuptools manual under '\n                '\"Namespace Packages\" for details.)\\n\"'\n            )\n        return init_py\n\n    def initialize_options(self):\n        self.packages_checked = {}\n        orig.build_py.initialize_options(self)\n        self.editable_mode = False\n        self.existing_egg_info_dir = None\n\n    def get_package_dir(self, package):\n        res = orig.build_py.get_package_dir(self, package)\n        if self.distribution.src_root is not None:\n            return os.path.join(self.distribution.src_root, res)\n        return res\n\n    def exclude_data_files(self, package, src_dir, files):\n        \"\"\"Filter filenames for package's data files in 'src_dir'\"\"\"\n        files = list(files)\n        patterns = self._get_platform_patterns(\n            self.exclude_package_data,\n            package,\n            src_dir,\n        )\n        match_groups = (fnmatch.filter(files, pattern) for pattern in patterns)\n        # flatten the groups of matches into an iterable of matches\n        matches = itertools.chain.from_iterable(match_groups)\n        bad = set(matches)\n        keepers = (fn for fn in files if fn not in bad)\n        # ditch dupes\n        return list(unique_everseen(keepers))\n\n    @staticmethod\n    def _get_platform_patterns(spec, package, src_dir, extra_patterns=()):\n        \"\"\"\n        yield platform-specific path patterns (suitable for glob\n        or fn_match) from a glob-based spec (such as\n        self.package_data or self.exclude_package_data)\n        matching package in src_dir.\n        \"\"\"\n        raw_patterns = itertools.chain(\n            extra_patterns,\n            spec.get('', []),\n            spec.get(package, []),\n        )\n        return (\n            # Each pattern has to be converted to a platform-specific path\n            os.path.join(src_dir, convert_path(pattern))\n            for pattern in raw_patterns\n        )\n\n\ndef assert_relative(path):\n    if not os.path.isabs(path):\n        return path\n    from distutils.errors import DistutilsSetupError\n\n    msg = (\n        textwrap.dedent(\n            \"\"\"\n        Error: setup script specifies an absolute path:\n\n            %s\n\n        setup() arguments must *always* be /-separated paths relative to the\n        setup.py directory, *never* absolute paths.\n        \"\"\"\n        ).lstrip()\n        % path\n    )\n    raise DistutilsSetupError(msg)\n\n\nclass _IncludePackageDataAbuse:\n    \"\"\"Inform users that package or module is included as 'data file'\"\"\"\n\n    class _Warning(SetuptoolsDeprecationWarning):\n        _SUMMARY = \"\"\"\n        Package {importable!r} is absent from the `packages` configuration.\n        \"\"\"\n\n        _DETAILS = \"\"\"\n        ############################\n        # Package would be ignored #\n        ############################\n        Python recognizes {importable!r} as an importable package[^1],\n        but it is absent from setuptools' `packages` configuration.\n\n        This leads to an ambiguous overall configuration. If you want to distribute this\n        package, please make sure that {importable!r} is explicitly added\n        to the `packages` configuration field.\n\n        Alternatively, you can also rely on setuptools' discovery methods\n        (for example by using `find_namespace_packages(...)`/`find_namespace:`\n        instead of `find_packages(...)`/`find:`).\n\n        You can read more about \"package discovery\" on setuptools documentation page:\n\n        - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n\n        If you don't want {importable!r} to be distributed and are\n        already explicitly excluding {importable!r} via\n        `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n        you can try to use `exclude_package_data`, or `include-package-data=False` in\n        combination with a more fine grained `package-data` configuration.\n\n        You can read more about \"package data files\" on setuptools documentation page:\n\n        - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n\n\n        [^1]: For Python, any directory (with suitable naming) can be imported,\n              even if it does not contain any `.py` files.\n              On the other hand, currently there is no concept of package data\n              directory, all directories are treated like packages.\n        \"\"\"\n        # _DUE_DATE: still not defined as this is particularly controversial.\n        # Warning initially introduced in May 2022. See issue #3340 for discussion.\n\n    def __init__(self):\n        self._already_warned = set()\n\n    def is_module(self, file):\n        return file.endswith(\".py\") and file[: -len(\".py\")].isidentifier()\n\n    def importable_subpackage(self, parent, file):\n        pkg = Path(file).parent\n        parts = list(itertools.takewhile(str.isidentifier, pkg.parts))\n        if parts:\n            return \".\".join([parent, *parts])\n        return None\n\n    def warn(self, importable):\n        if importable not in self._already_warned:\n            self._Warning.emit(importable=importable)\n            self._already_warned.add(importable)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/command/develop.py","size":6886,"sha1":"9569b4b2084f0f00104fe2f625115f9b8ae44ab5","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import glob\nimport os\n\nimport setuptools\nfrom setuptools import _normalization, _path, namespaces\nfrom setuptools.command.easy_install import easy_install\n\nfrom ..unicode_utils import _read_utf8_with_fallback\n\nfrom distutils import log\nfrom distutils.errors import DistutilsOptionError\nfrom distutils.util import convert_path\n\n\nclass develop(namespaces.DevelopInstaller, easy_install):\n    \"\"\"Set up package for development\"\"\"\n\n    description = \"install package in 'development mode'\"\n\n    user_options = easy_install.user_options + [\n        (\"uninstall\", \"u\", \"Uninstall this source package\"),\n        (\"egg-path=\", None, \"Set the path to be used in the .egg-link file\"),\n    ]\n\n    boolean_options = easy_install.boolean_options + ['uninstall']\n\n    command_consumes_arguments = False  # override base\n\n    def run(self):\n        if self.uninstall:\n            self.multi_version = True\n            self.uninstall_link()\n            self.uninstall_namespaces()\n        else:\n            self.install_for_development()\n        self.warn_deprecated_options()\n\n    def initialize_options(self):\n        self.uninstall = None\n        self.egg_path = None\n        easy_install.initialize_options(self)\n        self.setup_path = None\n        self.always_copy_from = '.'  # always copy eggs installed in curdir\n\n    def finalize_options(self) -> None:\n        import pkg_resources\n\n        ei = self.get_finalized_command(\"egg_info\")\n        self.args = [ei.egg_name]\n\n        easy_install.finalize_options(self)\n        self.expand_basedirs()\n        self.expand_dirs()\n        # pick up setup-dir .egg files only: no .egg-info\n        self.package_index.scan(glob.glob('*.egg'))\n\n        egg_link_fn = (\n            _normalization.filename_component_broken(ei.egg_name) + '.egg-link'\n        )\n        self.egg_link = os.path.join(self.install_dir, egg_link_fn)\n        self.egg_base = ei.egg_base\n        if self.egg_path is None:\n            self.egg_path = os.path.abspath(ei.egg_base)\n\n        target = _path.normpath(self.egg_base)\n        egg_path = _path.normpath(os.path.join(self.install_dir, self.egg_path))\n        if egg_path != target:\n            raise DistutilsOptionError(\n                \"--egg-path must be a relative path from the install\"\n                \" directory to \" + target\n            )\n\n        # Make a distribution for the package's source\n        self.dist = pkg_resources.Distribution(\n            target,\n            pkg_resources.PathMetadata(target, os.path.abspath(ei.egg_info)),\n            project_name=ei.egg_name,\n        )\n\n        self.setup_path = self._resolve_setup_path(\n            self.egg_base,\n            self.install_dir,\n            self.egg_path,\n        )\n\n    @staticmethod\n    def _resolve_setup_path(egg_base, install_dir, egg_path):\n        \"\"\"\n        Generate a path from egg_base back to '.' where the\n        setup script resides and ensure that path points to the\n        setup path from $install_dir/$egg_path.\n        \"\"\"\n        path_to_setup = egg_base.replace(os.sep, '/').rstrip('/')\n        if path_to_setup != os.curdir:\n            path_to_setup = '../' * (path_to_setup.count('/') + 1)\n        resolved = _path.normpath(os.path.join(install_dir, egg_path, path_to_setup))\n        curdir = _path.normpath(os.curdir)\n        if resolved != curdir:\n            raise DistutilsOptionError(\n                \"Can't get a consistent path to setup script from\"\n                \" installation directory\",\n                resolved,\n                curdir,\n            )\n        return path_to_setup\n\n    def install_for_development(self) -> None:\n        self.run_command('egg_info')\n\n        # Build extensions in-place\n        self.reinitialize_command('build_ext', inplace=True)\n        self.run_command('build_ext')\n\n        if setuptools.bootstrap_install_from:\n            self.easy_install(setuptools.bootstrap_install_from)\n            setuptools.bootstrap_install_from = None\n\n        self.install_namespaces()\n\n        # create an .egg-link in the installation dir, pointing to our egg\n        log.info(\"Creating %s (link to %s)\", self.egg_link, self.egg_base)\n        if not self.dry_run:\n            with open(self.egg_link, \"w\", encoding=\"utf-8\") as f:\n                f.write(self.egg_path + \"\\n\" + self.setup_path)\n        # postprocess the installed distro, fixing up .pth, installing scripts,\n        # and handling requirements\n        self.process_distribution(None, self.dist, not self.no_deps)\n\n    def uninstall_link(self) -> None:\n        if os.path.exists(self.egg_link):\n            log.info(\"Removing %s (link to %s)\", self.egg_link, self.egg_base)\n\n            contents = [\n                line.rstrip()\n                for line in _read_utf8_with_fallback(self.egg_link).splitlines()\n            ]\n\n            if contents not in ([self.egg_path], [self.egg_path, self.setup_path]):\n                log.warn(\"Link points to %s: uninstall aborted\", contents)\n                return\n            if not self.dry_run:\n                os.unlink(self.egg_link)\n        if not self.dry_run:\n            self.update_pth(self.dist)  # remove any .pth link to us\n        if self.distribution.scripts:\n            # XXX should also check for entry point scripts!\n            log.warn(\"Note: you must uninstall or replace scripts manually!\")\n\n    def install_egg_scripts(self, dist):\n        if dist is not self.dist:\n            # Installing a dependency, so fall back to normal behavior\n            return easy_install.install_egg_scripts(self, dist)\n\n        # create wrapper scripts in the script dir, pointing to dist.scripts\n\n        # new-style...\n        self.install_wrapper_scripts(dist)\n\n        # ...and old-style\n        for script_name in self.distribution.scripts or []:\n            script_path = os.path.abspath(convert_path(script_name))\n            script_name = os.path.basename(script_path)\n            script_text = _read_utf8_with_fallback(script_path)\n            self.install_script(dist, script_name, script_text, script_path)\n\n        return None\n\n    def install_wrapper_scripts(self, dist):\n        dist = VersionlessRequirement(dist)\n        return easy_install.install_wrapper_scripts(self, dist)\n\n\nclass VersionlessRequirement:\n    \"\"\"\n    Adapt a pkg_resources.Distribution to simply return the project\n    name as the 'requirement' so that scripts will work across\n    multiple versions.\n\n    >>> from pkg_resources import Distribution\n    >>> dist = Distribution(project_name='foo', version='1.0')\n    >>> str(dist.as_requirement())\n    'foo==1.0'\n    >>> adapted_dist = VersionlessRequirement(dist)\n    >>> str(adapted_dist.as_requirement())\n    'foo'\n    \"\"\"\n\n    def __init__(self, dist) -> None:\n        self.__dist = dist\n\n    def __getattr__(self, name: str):\n        return getattr(self.__dist, name)\n\n    def as_requirement(self):\n        return self.project_name\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/command/dist_info.py","size":3450,"sha1":"84a67b7fbf008fa04a39b83670c104b931715e47","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"\nCreate a dist_info directory\nAs defined in the wheel specification\n\"\"\"\n\nimport os\nimport shutil\nfrom contextlib import contextmanager\nfrom pathlib import Path\nfrom typing import cast\n\nfrom .. import _normalization\nfrom .._shutil import rmdir as _rm\nfrom .egg_info import egg_info as egg_info_cls\n\nfrom distutils import log\nfrom distutils.core import Command\n\n\nclass dist_info(Command):\n    \"\"\"\n    This command is private and reserved for internal use of setuptools,\n    users should rely on ``setuptools.build_meta`` APIs.\n    \"\"\"\n\n    description = \"DO NOT CALL DIRECTLY, INTERNAL ONLY: create .dist-info directory\"\n\n    user_options = [\n        (\n            'output-dir=',\n            'o',\n            \"directory inside of which the .dist-info will be\"\n            \"created [default: top of the source tree]\",\n        ),\n        ('tag-date', 'd', \"Add date stamp (e.g. 20050528) to version number\"),\n        ('tag-build=', 'b', \"Specify explicit tag to add to version number\"),\n        ('no-date', 'D', \"Don't include date stamp [default]\"),\n        ('keep-egg-info', None, \"*TRANSITIONAL* will be removed in the future\"),\n    ]\n\n    boolean_options = ['tag-date', 'keep-egg-info']\n    negative_opt = {'no-date': 'tag-date'}\n\n    def initialize_options(self):\n        self.output_dir = None\n        self.name = None\n        self.dist_info_dir = None\n        self.tag_date = None\n        self.tag_build = None\n        self.keep_egg_info = False\n\n    def finalize_options(self) -> None:\n        dist = self.distribution\n        project_dir = dist.src_root or os.curdir\n        self.output_dir = Path(self.output_dir or project_dir)\n\n        egg_info = cast(egg_info_cls, self.reinitialize_command(\"egg_info\"))\n        egg_info.egg_base = str(self.output_dir)\n\n        if self.tag_date:\n            egg_info.tag_date = self.tag_date\n        else:\n            self.tag_date = egg_info.tag_date\n\n        if self.tag_build:\n            egg_info.tag_build = self.tag_build\n        else:\n            self.tag_build = egg_info.tag_build\n\n        egg_info.finalize_options()\n        self.egg_info = egg_info\n\n        name = _normalization.safer_name(dist.get_name())\n        version = _normalization.safer_best_effort_version(dist.get_version())\n        self.name = f\"{name}-{version}\"\n        self.dist_info_dir = os.path.join(self.output_dir, f\"{self.name}.dist-info\")\n\n    @contextmanager\n    def _maybe_bkp_dir(self, dir_path: str, requires_bkp: bool):\n        if requires_bkp:\n            bkp_name = f\"{dir_path}.__bkp__\"\n            _rm(bkp_name, ignore_errors=True)\n            shutil.copytree(dir_path, bkp_name, dirs_exist_ok=True, symlinks=True)\n            try:\n                yield\n            finally:\n                _rm(dir_path, ignore_errors=True)\n                shutil.move(bkp_name, dir_path)\n        else:\n            yield\n\n    def run(self) -> None:\n        self.output_dir.mkdir(parents=True, exist_ok=True)\n        self.egg_info.run()\n        egg_info_dir = self.egg_info.egg_info\n        assert os.path.isdir(egg_info_dir), \".egg-info dir should have been created\"\n\n        log.info(f\"creating '{os.path.abspath(self.dist_info_dir)}'\")\n        bdist_wheel = self.get_finalized_command('bdist_wheel')\n\n        # TODO: if bdist_wheel if merged into setuptools, just add \"keep_egg_info\" there\n        with self._maybe_bkp_dir(egg_info_dir, self.keep_egg_info):\n            bdist_wheel.egg2dist(egg_info_dir, self.dist_info_dir)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/command/easy_install.py","size":87870,"sha1":"9d1ea04d080284c83f364c147df8155b0152c271","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"\nEasy Install\n------------\n\nA tool for doing automatic download/extract/build of distutils-based Python\npackages.  For detailed documentation, see the accompanying EasyInstall.txt\nfile, or visit the `EasyInstall home page`__.\n\n__ https://setuptools.pypa.io/en/latest/deprecated/easy_install.html\n\n\"\"\"\n\nfrom __future__ import annotations\n\nimport configparser\nimport contextlib\nimport io\nimport os\nimport random\nimport re\nimport shlex\nimport shutil\nimport site\nimport stat\nimport struct\nimport subprocess\nimport sys\nimport sysconfig\nimport tempfile\nimport textwrap\nimport warnings\nimport zipfile\nimport zipimport\nfrom collections.abc import Iterable\nfrom glob import glob\nfrom sysconfig import get_path\nfrom typing import TYPE_CHECKING, NoReturn, TypedDict\n\nfrom jaraco.text import yield_lines\n\nimport pkg_resources\nfrom pkg_resources import (\n    DEVELOP_DIST,\n    Distribution,\n    DistributionNotFound,\n    EggMetadata,\n    Environment,\n    PathMetadata,\n    Requirement,\n    VersionConflict,\n    WorkingSet,\n    find_distributions,\n    get_distribution,\n    normalize_path,\n    resource_string,\n)\nfrom setuptools import Command\nfrom setuptools.archive_util import unpack_archive\nfrom setuptools.command import bdist_egg, egg_info, setopt\nfrom setuptools.package_index import URL_SCHEME, PackageIndex, parse_requirement_arg\nfrom setuptools.sandbox import run_setup\nfrom setuptools.warnings import SetuptoolsDeprecationWarning, SetuptoolsWarning\nfrom setuptools.wheel import Wheel\n\nfrom .._path import ensure_directory\nfrom .._shutil import attempt_chmod_verbose as chmod, rmtree as _rmtree\nfrom ..compat import py39, py312\n\nfrom distutils import dir_util, log\nfrom distutils.command import install\nfrom distutils.command.build_scripts import first_line_re\nfrom distutils.errors import (\n    DistutilsArgError,\n    DistutilsError,\n    DistutilsOptionError,\n    DistutilsPlatformError,\n)\nfrom distutils.util import convert_path, get_platform, subst_vars\n\nif TYPE_CHECKING:\n    from typing_extensions import Self\n\n# Turn on PEP440Warnings\nwarnings.filterwarnings(\"default\", category=pkg_resources.PEP440Warning)\n\n__all__ = [\n    'easy_install',\n    'PthDistributions',\n    'extract_wininst_cfg',\n    'get_exe_prefixes',\n]\n\n\ndef is_64bit():\n    return struct.calcsize(\"P\") == 8\n\n\ndef _to_bytes(s):\n    return s.encode('utf8')\n\n\ndef isascii(s):\n    try:\n        s.encode('ascii')\n    except UnicodeError:\n        return False\n    return True\n\n\ndef _one_liner(text):\n    return textwrap.dedent(text).strip().replace('\\n', '; ')\n\n\nclass easy_install(Command):\n    \"\"\"Manage a download/build/install process\"\"\"\n\n    description = \"Find/get/install Python packages\"\n    command_consumes_arguments = True\n\n    user_options = [\n        ('prefix=', None, \"installation prefix\"),\n        (\"zip-ok\", \"z\", \"install package as a zipfile\"),\n        (\"multi-version\", \"m\", \"make apps have to require() a version\"),\n        (\"upgrade\", \"U\", \"force upgrade (searches PyPI for latest versions)\"),\n        (\"install-dir=\", \"d\", \"install package to DIR\"),\n        (\"script-dir=\", \"s\", \"install scripts to DIR\"),\n        (\"exclude-scripts\", \"x\", \"Don't install scripts\"),\n        (\"always-copy\", \"a\", \"Copy all needed packages to install dir\"),\n        (\"index-url=\", \"i\", \"base URL of Python Package Index\"),\n        (\"find-links=\", \"f\", \"additional URL(s) to search for packages\"),\n        (\"build-directory=\", \"b\", \"download/extract/build in DIR; keep the results\"),\n        (\n            'optimize=',\n            'O',\n            'also compile with optimization: -O1 for \"python -O\", '\n            '-O2 for \"python -OO\", and -O0 to disable [default: -O0]',\n        ),\n        ('record=', None, \"filename in which to record list of installed files\"),\n        ('always-unzip', 'Z', \"don't install as a zipfile, no matter what\"),\n        ('site-dirs=', 'S', \"list of directories where .pth files work\"),\n        ('editable', 'e', \"Install specified packages in editable form\"),\n        ('no-deps', 'N', \"don't install dependencies\"),\n        ('allow-hosts=', 'H', \"pattern(s) that hostnames must match\"),\n        ('local-snapshots-ok', 'l', \"allow building eggs from local checkouts\"),\n        ('version', None, \"print version information and exit\"),\n        (\n            'no-find-links',\n            None,\n            \"Don't load find-links defined in packages being installed\",\n        ),\n        ('user', None, f\"install in user site-package '{site.USER_SITE}'\"),\n    ]\n    boolean_options = [\n        'zip-ok',\n        'multi-version',\n        'exclude-scripts',\n        'upgrade',\n        'always-copy',\n        'editable',\n        'no-deps',\n        'local-snapshots-ok',\n        'version',\n        'user',\n    ]\n\n    negative_opt = {'always-unzip': 'zip-ok'}\n    create_index = PackageIndex\n\n    def initialize_options(self):\n        EasyInstallDeprecationWarning.emit()\n\n        # the --user option seems to be an opt-in one,\n        # so the default should be False.\n        self.user = False\n        self.zip_ok = self.local_snapshots_ok = None\n        self.install_dir = self.script_dir = self.exclude_scripts = None\n        self.index_url = None\n        self.find_links = None\n        self.build_directory = None\n        self.args = None\n        self.optimize = self.record = None\n        self.upgrade = self.always_copy = self.multi_version = None\n        self.editable = self.no_deps = self.allow_hosts = None\n        self.root = self.prefix = self.no_report = None\n        self.version = None\n        self.install_purelib = None  # for pure module distributions\n        self.install_platlib = None  # non-pure (dists w/ extensions)\n        self.install_headers = None  # for C/C++ headers\n        self.install_lib = None  # set to either purelib or platlib\n        self.install_scripts = None\n        self.install_data = None\n        self.install_base = None\n        self.install_platbase = None\n        self.install_userbase = site.USER_BASE\n        self.install_usersite = site.USER_SITE\n        self.no_find_links = None\n\n        # Options not specifiable via command line\n        self.package_index = None\n        self.pth_file = self.always_copy_from = None\n        self.site_dirs = None\n        self.installed_projects = {}\n        # Always read easy_install options, even if we are subclassed, or have\n        # an independent instance created.  This ensures that defaults will\n        # always come from the standard configuration file(s)' \"easy_install\"\n        # section, even if this is a \"develop\" or \"install\" command, or some\n        # other embedding.\n        self._dry_run = None\n        self.verbose = self.distribution.verbose\n        self.distribution._set_command_options(\n            self, self.distribution.get_option_dict('easy_install')\n        )\n\n    def delete_blockers(self, blockers) -> None:\n        extant_blockers = (\n            filename\n            for filename in blockers\n            if os.path.exists(filename) or os.path.islink(filename)\n        )\n        list(map(self._delete_path, extant_blockers))\n\n    def _delete_path(self, path):\n        log.info(\"Deleting %s\", path)\n        if self.dry_run:\n            return\n\n        is_tree = os.path.isdir(path) and not os.path.islink(path)\n        remover = _rmtree if is_tree else os.unlink\n        remover(path)\n\n    @staticmethod\n    def _render_version():\n        \"\"\"\n        Render the Setuptools version and installation details, then exit.\n        \"\"\"\n        ver = f'{sys.version_info.major}.{sys.version_info.minor}'\n        dist = get_distribution('setuptools')\n        print(f'setuptools {dist.version} from {dist.location} (Python {ver})')\n        raise SystemExit\n\n    def finalize_options(self) -> None:  # noqa: C901  # is too complex (25)  # FIXME\n        self.version and self._render_version()\n\n        py_version = sys.version.split()[0]\n\n        self.config_vars = dict(sysconfig.get_config_vars())\n\n        self.config_vars.update({\n            'dist_name': self.distribution.get_name(),\n            'dist_version': self.distribution.get_version(),\n            'dist_fullname': self.distribution.get_fullname(),\n            'py_version': py_version,\n            'py_version_short': f'{sys.version_info.major}.{sys.version_info.minor}',\n            'py_version_nodot': f'{sys.version_info.major}{sys.version_info.minor}',\n            'sys_prefix': self.config_vars['prefix'],\n            'sys_exec_prefix': self.config_vars['exec_prefix'],\n            # Only POSIX systems have abiflags\n            'abiflags': getattr(sys, 'abiflags', ''),\n            # Only python 3.9+ has platlibdir\n            'platlibdir': getattr(sys, 'platlibdir', 'lib'),\n        })\n        with contextlib.suppress(AttributeError):\n            # only for distutils outside stdlib\n            self.config_vars.update({\n                'implementation_lower': install._get_implementation().lower(),\n                'implementation': install._get_implementation(),\n            })\n\n        # pypa/distutils#113 Python 3.9 compat\n        self.config_vars.setdefault(\n            'py_version_nodot_plat',\n            getattr(sys, 'windir', '').replace('.', ''),\n        )\n\n        self.config_vars['userbase'] = self.install_userbase\n        self.config_vars['usersite'] = self.install_usersite\n        if self.user and not site.ENABLE_USER_SITE:\n            log.warn(\"WARNING: The user site-packages directory is disabled.\")\n\n        self._fix_install_dir_for_user_site()\n\n        self.expand_basedirs()\n        self.expand_dirs()\n\n        self._expand(\n            'install_dir',\n            'script_dir',\n            'build_directory',\n            'site_dirs',\n        )\n        # If a non-default installation directory was specified, default the\n        # script directory to match it.\n        if self.script_dir is None:\n            self.script_dir = self.install_dir\n\n        if self.no_find_links is None:\n            self.no_find_links = False\n\n        # Let install_dir get set by install_lib command, which in turn\n        # gets its info from the install command, and takes into account\n        # --prefix and --home and all that other crud.\n        self.set_undefined_options('install_lib', ('install_dir', 'install_dir'))\n        # Likewise, set default script_dir from 'install_scripts.install_dir'\n        self.set_undefined_options('install_scripts', ('install_dir', 'script_dir'))\n\n        if self.user and self.install_purelib:\n            self.install_dir = self.install_purelib\n            self.script_dir = self.install_scripts\n        # default --record from the install command\n        self.set_undefined_options('install', ('record', 'record'))\n        self.all_site_dirs = get_site_dirs()\n        self.all_site_dirs.extend(self._process_site_dirs(self.site_dirs))\n\n        if not self.editable:\n            self.check_site_dir()\n        default_index = os.getenv(\"__EASYINSTALL_INDEX\", \"https://pypi.org/simple/\")\n        # ^ Private API for testing purposes only\n        self.index_url = self.index_url or default_index\n        self.shadow_path = self.all_site_dirs[:]\n        for path_item in self.install_dir, normalize_path(self.script_dir):\n            if path_item not in self.shadow_path:\n                self.shadow_path.insert(0, path_item)\n\n        if self.allow_hosts is not None:\n            hosts = [s.strip() for s in self.allow_hosts.split(',')]\n        else:\n            hosts = ['*']\n        if self.package_index is None:\n            self.package_index = self.create_index(\n                self.index_url,\n                search_path=self.shadow_path,\n                hosts=hosts,\n            )\n        self.local_index = Environment(self.shadow_path + sys.path)\n\n        if self.find_links is not None:\n            if isinstance(self.find_links, str):\n                self.find_links = self.find_links.split()\n        else:\n            self.find_links = []\n        if self.local_snapshots_ok:\n            self.package_index.scan_egg_links(self.shadow_path + sys.path)\n        if not self.no_find_links:\n            self.package_index.add_find_links(self.find_links)\n        self.set_undefined_options('install_lib', ('optimize', 'optimize'))\n        self.optimize = self._validate_optimize(self.optimize)\n\n        if self.editable and not self.build_directory:\n            raise DistutilsArgError(\n                \"Must specify a build directory (-b) when using --editable\"\n            )\n        if not self.args:\n            raise DistutilsArgError(\n                \"No urls, filenames, or requirements specified (see --help)\"\n            )\n\n        self.outputs: list[str] = []\n\n    @staticmethod\n    def _process_site_dirs(site_dirs):\n        if site_dirs is None:\n            return\n\n        normpath = map(normalize_path, sys.path)\n        site_dirs = [os.path.expanduser(s.strip()) for s in site_dirs.split(',')]\n        for d in site_dirs:\n            if not os.path.isdir(d):\n                log.warn(\"%s (in --site-dirs) does not exist\", d)\n            elif normalize_path(d) not in normpath:\n                raise DistutilsOptionError(d + \" (in --site-dirs) is not on sys.path\")\n            else:\n                yield normalize_path(d)\n\n    @staticmethod\n    def _validate_optimize(value):\n        try:\n            value = int(value)\n            if value not in range(3):\n                raise ValueError\n        except ValueError as e:\n            raise DistutilsOptionError(\"--optimize must be 0, 1, or 2\") from e\n\n        return value\n\n    def _fix_install_dir_for_user_site(self):\n        \"\"\"\n        Fix the install_dir if \"--user\" was used.\n        \"\"\"\n        if not self.user:\n            return\n\n        self.create_home_path()\n        if self.install_userbase is None:\n            msg = \"User base directory is not specified\"\n            raise DistutilsPlatformError(msg)\n        self.install_base = self.install_platbase = self.install_userbase\n        scheme_name = f'{os.name}_user'\n        self.select_scheme(scheme_name)\n\n    def _expand_attrs(self, attrs):\n        for attr in attrs:\n            val = getattr(self, attr)\n            if val is not None:\n                if os.name == 'posix' or os.name == 'nt':\n                    val = os.path.expanduser(val)\n                val = subst_vars(val, self.config_vars)\n                setattr(self, attr, val)\n\n    def expand_basedirs(self) -> None:\n        \"\"\"Calls `os.path.expanduser` on install_base, install_platbase and\n        root.\"\"\"\n        self._expand_attrs(['install_base', 'install_platbase', 'root'])\n\n    def expand_dirs(self) -> None:\n        \"\"\"Calls `os.path.expanduser` on install dirs.\"\"\"\n        dirs = [\n            'install_purelib',\n            'install_platlib',\n            'install_lib',\n            'install_headers',\n            'install_scripts',\n            'install_data',\n        ]\n        self._expand_attrs(dirs)\n\n    def run(self, show_deprecation: bool = True) -> None:\n        if show_deprecation:\n            self.announce(\n                \"WARNING: The easy_install command is deprecated \"\n                \"and will be removed in a future version.\",\n                log.WARN,\n            )\n        if self.verbose != self.distribution.verbose:\n            log.set_verbosity(self.verbose)\n        try:\n            for spec in self.args:\n                self.easy_install(spec, not self.no_deps)\n            if self.record:\n                outputs = self.outputs\n                if self.root:  # strip any package prefix\n                    root_len = len(self.root)\n                    for counter in range(len(outputs)):\n                        outputs[counter] = outputs[counter][root_len:]\n                from distutils import file_util\n\n                self.execute(\n                    file_util.write_file,\n                    (self.record, outputs),\n                    f\"writing list of installed files to '{self.record}'\",\n                )\n            self.warn_deprecated_options()\n        finally:\n            log.set_verbosity(self.distribution.verbose)\n\n    def pseudo_tempname(self):\n        \"\"\"Return a pseudo-tempname base in the install directory.\n        This code is intentionally naive; if a malicious party can write to\n        the target directory you're already in deep doodoo.\n        \"\"\"\n        try:\n            pid = os.getpid()\n        except Exception:\n            pid = random.randint(0, sys.maxsize)\n        return os.path.join(self.install_dir, f\"test-easy-install-{pid}\")\n\n    def warn_deprecated_options(self) -> None:\n        pass\n\n    def check_site_dir(self) -> None:  # is too complex (12)  # FIXME\n        \"\"\"Verify that self.install_dir is .pth-capable dir, if needed\"\"\"\n\n        instdir = normalize_path(self.install_dir)\n        pth_file = os.path.join(instdir, 'easy-install.pth')\n\n        if not os.path.exists(instdir):\n            try:\n                os.makedirs(instdir)\n            except OSError:\n                self.cant_write_to_target()\n\n        # Is it a configured, PYTHONPATH, implicit, or explicit site dir?\n        is_site_dir = instdir in self.all_site_dirs\n\n        if not is_site_dir and not self.multi_version:\n            # No?  Then directly test whether it does .pth file processing\n            is_site_dir = self.check_pth_processing()\n        else:\n            # make sure we can write to target dir\n            testfile = self.pseudo_tempname() + '.write-test'\n            test_exists = os.path.exists(testfile)\n            try:\n                if test_exists:\n                    os.unlink(testfile)\n                open(testfile, 'wb').close()\n                os.unlink(testfile)\n            except OSError:\n                self.cant_write_to_target()\n\n        if not is_site_dir and not self.multi_version:\n            # Can't install non-multi to non-site dir with easy_install\n            pythonpath = os.environ.get('PYTHONPATH', '')\n            log.warn(self.__no_default_msg, self.install_dir, pythonpath)\n\n        if is_site_dir:\n            if self.pth_file is None:\n                self.pth_file = PthDistributions(pth_file, self.all_site_dirs)\n        else:\n            self.pth_file = None\n\n        if self.multi_version and not os.path.exists(pth_file):\n            self.pth_file = None  # don't create a .pth file\n        self.install_dir = instdir\n\n    __cant_write_msg = textwrap.dedent(\n        \"\"\"\n        can't create or remove files in install directory\n\n        The following error occurred while trying to add or remove files in the\n        installation directory:\n\n            %s\n\n        The installation directory you specified (via --install-dir, --prefix, or\n        the distutils default setting) was:\n\n            %s\n        \"\"\"\n    ).lstrip()\n\n    __not_exists_id = textwrap.dedent(\n        \"\"\"\n        This directory does not currently exist.  Please create it and try again, or\n        choose a different installation directory (using the -d or --install-dir\n        option).\n        \"\"\"\n    ).lstrip()\n\n    __access_msg = textwrap.dedent(\n        \"\"\"\n        Perhaps your account does not have write access to this directory?  If the\n        installation directory is a system-owned directory, you may need to sign in\n        as the administrator or \"root\" account.  If you do not have administrative\n        access to this machine, you may wish to choose a different installation\n        directory, preferably one that is listed in your PYTHONPATH environment\n        variable.\n\n        For information on other options, you may wish to consult the\n        documentation at:\n\n          https://setuptools.pypa.io/en/latest/deprecated/easy_install.html\n\n        Please make the appropriate changes for your system and try again.\n        \"\"\"\n    ).lstrip()\n\n    def cant_write_to_target(self) -> NoReturn:\n        msg = self.__cant_write_msg % (\n            sys.exc_info()[1],\n            self.install_dir,\n        )\n\n        if not os.path.exists(self.install_dir):\n            msg += '\\n' + self.__not_exists_id\n        else:\n            msg += '\\n' + self.__access_msg\n        raise DistutilsError(msg)\n\n    def check_pth_processing(self):  # noqa: C901\n        \"\"\"Empirically verify whether .pth files are supported in inst. dir\"\"\"\n        instdir = self.install_dir\n        log.info(\"Checking .pth file support in %s\", instdir)\n        pth_file = self.pseudo_tempname() + \".pth\"\n        ok_file = pth_file + '.ok'\n        ok_exists = os.path.exists(ok_file)\n        tmpl = (\n            _one_liner(\n                \"\"\"\n            import os\n            f = open({ok_file!r}, 'w', encoding=\"utf-8\")\n            f.write('OK')\n            f.close()\n            \"\"\"\n            )\n            + '\\n'\n        )\n        try:\n            if ok_exists:\n                os.unlink(ok_file)\n            dirname = os.path.dirname(ok_file)\n            os.makedirs(dirname, exist_ok=True)\n            f = open(pth_file, 'w', encoding=py312.PTH_ENCODING)\n            # ^-- Python<3.13 require encoding=\"locale\" instead of \"utf-8\",\n            #     see python/cpython#77102.\n        except OSError:\n            self.cant_write_to_target()\n        else:\n            try:\n                f.write(tmpl.format(**locals()))\n                f.close()\n                f = None\n                executable = sys.executable\n                if os.name == 'nt':\n                    dirname, basename = os.path.split(executable)\n                    alt = os.path.join(dirname, 'pythonw.exe')\n                    use_alt = basename.lower() == 'python.exe' and os.path.exists(alt)\n                    if use_alt:\n                        # use pythonw.exe to avoid opening a console window\n                        executable = alt\n\n                from distutils.spawn import spawn\n\n                spawn([executable, '-E', '-c', 'pass'], 0)\n\n                if os.path.exists(ok_file):\n                    log.info(\"TEST PASSED: %s appears to support .pth files\", instdir)\n                    return True\n            finally:\n                if f:\n                    f.close()\n                if os.path.exists(ok_file):\n                    os.unlink(ok_file)\n                if os.path.exists(pth_file):\n                    os.unlink(pth_file)\n        if not self.multi_version:\n            log.warn(\"TEST FAILED: %s does NOT support .pth files\", instdir)\n        return False\n\n    def install_egg_scripts(self, dist) -> None:\n        \"\"\"Write all the scripts for `dist`, unless scripts are excluded\"\"\"\n        if not self.exclude_scripts and dist.metadata_isdir('scripts'):\n            for script_name in dist.metadata_listdir('scripts'):\n                if dist.metadata_isdir('scripts/' + script_name):\n                    # The \"script\" is a directory, likely a Python 3\n                    # __pycache__ directory, so skip it.\n                    continue\n                self.install_script(\n                    dist, script_name, dist.get_metadata('scripts/' + script_name)\n                )\n        self.install_wrapper_scripts(dist)\n\n    def add_output(self, path) -> None:\n        if os.path.isdir(path):\n            for base, dirs, files in os.walk(path):\n                for filename in files:\n                    self.outputs.append(os.path.join(base, filename))\n        else:\n            self.outputs.append(path)\n\n    def not_editable(self, spec) -> None:\n        if self.editable:\n            raise DistutilsArgError(\n                f\"Invalid argument {spec!r}: you can't use filenames or URLs \"\n                \"with --editable (except via the --find-links option).\"\n            )\n\n    def check_editable(self, spec) -> None:\n        if not self.editable:\n            return\n\n        if os.path.exists(os.path.join(self.build_directory, spec.key)):\n            raise DistutilsArgError(\n                f\"{spec.key!r} already exists in {self.build_directory}; can't do a checkout there\"\n            )\n\n    @contextlib.contextmanager\n    def _tmpdir(self):\n        tmpdir = tempfile.mkdtemp(prefix=\"easy_install-\")\n        try:\n            # cast to str as workaround for #709 and #710 and #712\n            yield str(tmpdir)\n        finally:\n            os.path.exists(tmpdir) and _rmtree(tmpdir)\n\n    def easy_install(self, spec, deps: bool = False) -> Distribution | None:\n        with self._tmpdir() as tmpdir:\n            if not isinstance(spec, Requirement):\n                if URL_SCHEME(spec):\n                    # It's a url, download it to tmpdir and process\n                    self.not_editable(spec)\n                    dl = self.package_index.download(spec, tmpdir)\n                    return self.install_item(None, dl, tmpdir, deps, True)\n\n                elif os.path.exists(spec):\n                    # Existing file or directory, just process it directly\n                    self.not_editable(spec)\n                    return self.install_item(None, spec, tmpdir, deps, True)\n                else:\n                    spec = parse_requirement_arg(spec)\n\n            self.check_editable(spec)\n            dist = self.package_index.fetch_distribution(\n                spec,\n                tmpdir,\n                self.upgrade,\n                self.editable,\n                not self.always_copy,\n                self.local_index,\n            )\n            if dist is None:\n                msg = f\"Could not find suitable distribution for {spec!r}\"\n                if self.always_copy:\n                    msg += \" (--always-copy skips system and development eggs)\"\n                raise DistutilsError(msg)\n            elif dist.precedence == DEVELOP_DIST:\n                # .egg-info dists don't need installing, just process deps\n                self.process_distribution(spec, dist, deps, \"Using\")\n                return dist\n            else:\n                return self.install_item(spec, dist.location, tmpdir, deps)\n\n    def install_item(\n        self, spec, download, tmpdir, deps, install_needed: bool = False\n    ) -> Distribution | None:\n        # Installation is also needed if file in tmpdir or is not an egg\n        install_needed = install_needed or bool(self.always_copy)\n        install_needed = install_needed or os.path.dirname(download) == tmpdir\n        install_needed = install_needed or not download.endswith('.egg')\n        install_needed = install_needed or (\n            self.always_copy_from is not None\n            and os.path.dirname(normalize_path(download))\n            == normalize_path(self.always_copy_from)\n        )\n\n        if spec and not install_needed:\n            # at this point, we know it's a local .egg, we just don't know if\n            # it's already installed.\n            for dist in self.local_index[spec.project_name]:\n                if dist.location == download:\n                    break\n            else:\n                install_needed = True  # it's not in the local index\n\n        log.info(\"Processing %s\", os.path.basename(download))\n\n        if install_needed:\n            dists = self.install_eggs(spec, download, tmpdir)\n            for dist in dists:\n                self.process_distribution(spec, dist, deps)\n        else:\n            dists = [self.egg_distribution(download)]\n            self.process_distribution(spec, dists[0], deps, \"Using\")\n\n        if spec is not None:\n            for dist in dists:\n                if dist in spec:\n                    return dist\n        return None\n\n    def select_scheme(self, name):\n        try:\n            install._select_scheme(self, name)\n        except AttributeError:\n            # stdlib distutils\n            install.install.select_scheme(self, name.replace('posix', 'unix'))\n\n    # FIXME: 'easy_install.process_distribution' is too complex (12)\n    def process_distribution(  # noqa: C901\n        self,\n        requirement,\n        dist,\n        deps: bool = True,\n        *info,\n    ) -> None:\n        self.update_pth(dist)\n        self.package_index.add(dist)\n        if dist in self.local_index[dist.key]:\n            self.local_index.remove(dist)\n        self.local_index.add(dist)\n        self.install_egg_scripts(dist)\n        self.installed_projects[dist.key] = dist\n        log.info(self.installation_report(requirement, dist, *info))\n        if dist.has_metadata('dependency_links.txt') and not self.no_find_links:\n            self.package_index.add_find_links(\n                dist.get_metadata_lines('dependency_links.txt')\n            )\n        if not deps and not self.always_copy:\n            return\n        elif requirement is not None and dist.key != requirement.key:\n            log.warn(\"Skipping dependencies for %s\", dist)\n            return  # XXX this is not the distribution we were looking for\n        elif requirement is None or dist not in requirement:\n            # if we wound up with a different version, resolve what we've got\n            distreq = dist.as_requirement()\n            requirement = Requirement(str(distreq))\n        log.info(\"Processing dependencies for %s\", requirement)\n        try:\n            distros = WorkingSet([]).resolve(\n                [requirement], self.local_index, self.easy_install\n            )\n        except DistributionNotFound as e:\n            raise DistutilsError(str(e)) from e\n        except VersionConflict as e:\n            raise DistutilsError(e.report()) from e\n        if self.always_copy or self.always_copy_from:\n            # Force all the relevant distros to be copied or activated\n            for dist in distros:\n                if dist.key not in self.installed_projects:\n                    self.easy_install(dist.as_requirement())\n        log.info(\"Finished processing dependencies for %s\", requirement)\n\n    def should_unzip(self, dist) -> bool:\n        if self.zip_ok is not None:\n            return not self.zip_ok\n        if dist.has_metadata('not-zip-safe'):\n            return True\n        if not dist.has_metadata('zip-safe'):\n            return True\n        return False\n\n    def maybe_move(self, spec, dist_filename, setup_base):\n        dst = os.path.join(self.build_directory, spec.key)\n        if os.path.exists(dst):\n            msg = \"%r already exists in %s; build directory %s will not be kept\"\n            log.warn(msg, spec.key, self.build_directory, setup_base)\n            return setup_base\n        if os.path.isdir(dist_filename):\n            setup_base = dist_filename\n        else:\n            if os.path.dirname(dist_filename) == setup_base:\n                os.unlink(dist_filename)  # get it out of the tmp dir\n            contents = os.listdir(setup_base)\n            if len(contents) == 1:\n                dist_filename = os.path.join(setup_base, contents[0])\n                if os.path.isdir(dist_filename):\n                    # if the only thing there is a directory, move it instead\n                    setup_base = dist_filename\n        ensure_directory(dst)\n        shutil.move(setup_base, dst)\n        return dst\n\n    def install_wrapper_scripts(self, dist) -> None:\n        if self.exclude_scripts:\n            return\n        for args in ScriptWriter.best().get_args(dist):\n            self.write_script(*args)\n\n    def install_script(self, dist, script_name, script_text, dev_path=None) -> None:\n        \"\"\"Generate a legacy script wrapper and install it\"\"\"\n        spec = str(dist.as_requirement())\n        is_script = is_python_script(script_text, script_name)\n\n        if is_script:\n            body = self._load_template(dev_path) % locals()\n            script_text = ScriptWriter.get_header(script_text) + body\n        self.write_script(script_name, _to_bytes(script_text), 'b')\n\n    @staticmethod\n    def _load_template(dev_path):\n        \"\"\"\n        There are a couple of template scripts in the package. This\n        function loads one of them and prepares it for use.\n        \"\"\"\n        # See https://github.com/pypa/setuptools/issues/134 for info\n        # on script file naming and downstream issues with SVR4\n        name = 'script.tmpl'\n        if dev_path:\n            name = name.replace('.tmpl', ' (dev).tmpl')\n\n        raw_bytes = resource_string('setuptools', name)\n        return raw_bytes.decode('utf-8')\n\n    def write_script(self, script_name, contents, mode: str = \"t\", blockers=()) -> None:\n        \"\"\"Write an executable file to the scripts directory\"\"\"\n        self.delete_blockers(  # clean up old .py/.pyw w/o a script\n            [os.path.join(self.script_dir, x) for x in blockers]\n        )\n        log.info(\"Installing %s script to %s\", script_name, self.script_dir)\n        target = os.path.join(self.script_dir, script_name)\n        self.add_output(target)\n\n        if self.dry_run:\n            return\n\n        mask = current_umask()\n        ensure_directory(target)\n        if os.path.exists(target):\n            os.unlink(target)\n\n        encoding = None if \"b\" in mode else \"utf-8\"\n        with open(target, \"w\" + mode, encoding=encoding) as f:\n            f.write(contents)\n        chmod(target, 0o777 - mask)\n\n    def install_eggs(self, spec, dist_filename, tmpdir) -> list[Distribution]:\n        # .egg dirs or files are already built, so just return them\n        installer_map = {\n            '.egg': self.install_egg,\n            '.exe': self.install_exe,\n            '.whl': self.install_wheel,\n        }\n        try:\n            install_dist = installer_map[dist_filename.lower()[-4:]]\n        except KeyError:\n            pass\n        else:\n            return [install_dist(dist_filename, tmpdir)]\n\n        # Anything else, try to extract and build\n        setup_base = tmpdir\n        if os.path.isfile(dist_filename) and not dist_filename.endswith('.py'):\n            unpack_archive(dist_filename, tmpdir, self.unpack_progress)\n        elif os.path.isdir(dist_filename):\n            setup_base = os.path.abspath(dist_filename)\n\n        if (\n            setup_base.startswith(tmpdir)  # something we downloaded\n            and self.build_directory\n            and spec is not None\n        ):\n            setup_base = self.maybe_move(spec, dist_filename, setup_base)\n\n        # Find the setup.py file\n        setup_script = os.path.join(setup_base, 'setup.py')\n\n        if not os.path.exists(setup_script):\n            setups = glob(os.path.join(setup_base, '*', 'setup.py'))\n            if not setups:\n                raise DistutilsError(\n                    f\"Couldn't find a setup script in {os.path.abspath(dist_filename)}\"\n                )\n            if len(setups) > 1:\n                raise DistutilsError(\n                    f\"Multiple setup scripts in {os.path.abspath(dist_filename)}\"\n                )\n            setup_script = setups[0]\n\n        # Now run it, and return the result\n        if self.editable:\n            log.info(self.report_editable(spec, setup_script))\n            return []\n        else:\n            return self.build_and_install(setup_script, setup_base)\n\n    def egg_distribution(self, egg_path):\n        if os.path.isdir(egg_path):\n            metadata = PathMetadata(egg_path, os.path.join(egg_path, 'EGG-INFO'))\n        else:\n            metadata = EggMetadata(zipimport.zipimporter(egg_path))\n        return Distribution.from_filename(egg_path, metadata=metadata)\n\n    # FIXME: 'easy_install.install_egg' is too complex (11)\n    def install_egg(self, egg_path, tmpdir):\n        destination = os.path.join(\n            self.install_dir,\n            os.path.basename(egg_path),\n        )\n        destination = os.path.abspath(destination)\n        if not self.dry_run:\n            ensure_directory(destination)\n\n        dist = self.egg_distribution(egg_path)\n        if not (\n            os.path.exists(destination) and os.path.samefile(egg_path, destination)\n        ):\n            if os.path.isdir(destination) and not os.path.islink(destination):\n                dir_util.remove_tree(destination, dry_run=self.dry_run)\n            elif os.path.exists(destination):\n                self.execute(\n                    os.unlink,\n                    (destination,),\n                    \"Removing \" + destination,\n                )\n            try:\n                new_dist_is_zipped = False\n                if os.path.isdir(egg_path):\n                    if egg_path.startswith(tmpdir):\n                        f, m = shutil.move, \"Moving\"\n                    else:\n                        f, m = shutil.copytree, \"Copying\"\n                elif self.should_unzip(dist):\n                    self.mkpath(destination)\n                    f, m = self.unpack_and_compile, \"Extracting\"\n                else:\n                    new_dist_is_zipped = True\n                    if egg_path.startswith(tmpdir):\n                        f, m = shutil.move, \"Moving\"\n                    else:\n                        f, m = shutil.copy2, \"Copying\"\n                self.execute(\n                    f,\n                    (egg_path, destination),\n                    (m + \" %s to %s\")\n                    % (os.path.basename(egg_path), os.path.dirname(destination)),\n                )\n                update_dist_caches(\n                    destination,\n                    fix_zipimporter_caches=new_dist_is_zipped,\n                )\n            except Exception:\n                update_dist_caches(destination, fix_zipimporter_caches=False)\n                raise\n\n        self.add_output(destination)\n        return self.egg_distribution(destination)\n\n    def install_exe(self, dist_filename, tmpdir):\n        # See if it's valid, get data\n        cfg = extract_wininst_cfg(dist_filename)\n        if cfg is None:\n            raise DistutilsError(\n                f\"{dist_filename} is not a valid distutils Windows .exe\"\n            )\n        # Create a dummy distribution object until we build the real distro\n        dist = Distribution(\n            None,\n            project_name=cfg.get('metadata', 'name'),\n            version=cfg.get('metadata', 'version'),\n            platform=get_platform(),\n        )\n\n        # Convert the .exe to an unpacked egg\n        egg_path = os.path.join(tmpdir, dist.egg_name() + '.egg')\n        dist.location = egg_path\n        egg_tmp = egg_path + '.tmp'\n        _egg_info = os.path.join(egg_tmp, 'EGG-INFO')\n        pkg_inf = os.path.join(_egg_info, 'PKG-INFO')\n        ensure_directory(pkg_inf)  # make sure EGG-INFO dir exists\n        dist._provider = PathMetadata(egg_tmp, _egg_info)  # XXX\n        self.exe_to_egg(dist_filename, egg_tmp)\n\n        # Write EGG-INFO/PKG-INFO\n        if not os.path.exists(pkg_inf):\n            with open(pkg_inf, 'w', encoding=\"utf-8\") as f:\n                f.write('Metadata-Version: 1.0\\n')\n                for k, v in cfg.items('metadata'):\n                    if k != 'target_version':\n                        k = k.replace('_', '-').title()\n                        f.write(f'{k}: {v}\\n')\n        script_dir = os.path.join(_egg_info, 'scripts')\n        # delete entry-point scripts to avoid duping\n        self.delete_blockers([\n            os.path.join(script_dir, args[0]) for args in ScriptWriter.get_args(dist)\n        ])\n        # Build .egg file from tmpdir\n        bdist_egg.make_zipfile(\n            egg_path,\n            egg_tmp,\n            verbose=self.verbose,\n            dry_run=self.dry_run,\n        )\n        # install the .egg\n        return self.install_egg(egg_path, tmpdir)\n\n    # FIXME: 'easy_install.exe_to_egg' is too complex (12)\n    def exe_to_egg(self, dist_filename, egg_tmp) -> None:  # noqa: C901\n        \"\"\"Extract a bdist_wininst to the directories an egg would use\"\"\"\n        # Check for .pth file and set up prefix translations\n        prefixes = get_exe_prefixes(dist_filename)\n        to_compile = []\n        native_libs = []\n        top_level = set()\n\n        def process(src, dst):\n            s = src.lower()\n            for old, new in prefixes:\n                if s.startswith(old):\n                    src = new + src[len(old) :]\n                    parts = src.split('/')\n                    dst = os.path.join(egg_tmp, *parts)\n                    dl = dst.lower()\n                    if dl.endswith('.pyd') or dl.endswith('.dll'):\n                        parts[-1] = bdist_egg.strip_module(parts[-1])\n                        top_level.add([os.path.splitext(parts[0])[0]])\n                        native_libs.append(src)\n                    elif dl.endswith('.py') and old != 'SCRIPTS/':\n                        top_level.add([os.path.splitext(parts[0])[0]])\n                        to_compile.append(dst)\n                    return dst\n            if not src.endswith('.pth'):\n                log.warn(\"WARNING: can't process %s\", src)\n            return None\n\n        # extract, tracking .pyd/.dll->native_libs and .py -> to_compile\n        unpack_archive(dist_filename, egg_tmp, process)\n        stubs = []\n        for res in native_libs:\n            if res.lower().endswith('.pyd'):  # create stubs for .pyd's\n                parts = res.split('/')\n                resource = parts[-1]\n                parts[-1] = bdist_egg.strip_module(parts[-1]) + '.py'\n                pyfile = os.path.join(egg_tmp, *parts)\n                to_compile.append(pyfile)\n                stubs.append(pyfile)\n                bdist_egg.write_stub(resource, pyfile)\n        self.byte_compile(to_compile)  # compile .py's\n        bdist_egg.write_safety_flag(\n            os.path.join(egg_tmp, 'EGG-INFO'), bdist_egg.analyze_egg(egg_tmp, stubs)\n        )  # write zip-safety flag\n\n        for name in 'top_level', 'native_libs':\n            if locals()[name]:\n                txt = os.path.join(egg_tmp, 'EGG-INFO', name + '.txt')\n                if not os.path.exists(txt):\n                    with open(txt, 'w', encoding=\"utf-8\") as f:\n                        f.write('\\n'.join(locals()[name]) + '\\n')\n\n    def install_wheel(self, wheel_path, tmpdir):\n        wheel = Wheel(wheel_path)\n        assert wheel.is_compatible()\n        destination = os.path.join(self.install_dir, wheel.egg_name())\n        destination = os.path.abspath(destination)\n        if not self.dry_run:\n            ensure_directory(destination)\n        if os.path.isdir(destination) and not os.path.islink(destination):\n            dir_util.remove_tree(destination, dry_run=self.dry_run)\n        elif os.path.exists(destination):\n            self.execute(\n                os.unlink,\n                (destination,),\n                \"Removing \" + destination,\n            )\n        try:\n            self.execute(\n                wheel.install_as_egg,\n                (destination,),\n                (\n                    f\"Installing {os.path.basename(wheel_path)} to {os.path.dirname(destination)}\"\n                ),\n            )\n        finally:\n            update_dist_caches(destination, fix_zipimporter_caches=False)\n        self.add_output(destination)\n        return self.egg_distribution(destination)\n\n    __mv_warning = textwrap.dedent(\n        \"\"\"\n        Because this distribution was installed --multi-version, before you can\n        import modules from this package in an application, you will need to\n        'import pkg_resources' and then use a 'require()' call similar to one of\n        these examples, in order to select the desired version:\n\n            pkg_resources.require(\"%(name)s\")  # latest installed version\n            pkg_resources.require(\"%(name)s==%(version)s\")  # this exact version\n            pkg_resources.require(\"%(name)s>=%(version)s\")  # this version or higher\n        \"\"\"\n    ).lstrip()\n\n    __id_warning = textwrap.dedent(\n        \"\"\"\n        Note also that the installation directory must be on sys.path at runtime for\n        this to work.  (e.g. by being the application's script directory, by being on\n        PYTHONPATH, or by being added to sys.path by your code.)\n        \"\"\"\n    )\n\n    def installation_report(self, req, dist, what: str = \"Installed\") -> str:\n        \"\"\"Helpful installation message for display to package users\"\"\"\n        msg = \"\\n%(what)s %(eggloc)s%(extras)s\"\n        if self.multi_version and not self.no_report:\n            msg += '\\n' + self.__mv_warning\n            if self.install_dir not in map(normalize_path, sys.path):\n                msg += '\\n' + self.__id_warning\n\n        eggloc = dist.location\n        name = dist.project_name\n        version = dist.version\n        extras = ''  # TODO: self.report_extras(req, dist)\n        return msg % locals()\n\n    __editable_msg = textwrap.dedent(\n        \"\"\"\n        Extracted editable version of %(spec)s to %(dirname)s\n\n        If it uses setuptools in its setup script, you can activate it in\n        \"development\" mode by going to that directory and running::\n\n            %(python)s setup.py develop\n\n        See the setuptools documentation for the \"develop\" command for more info.\n        \"\"\"\n    ).lstrip()\n\n    def report_editable(self, spec, setup_script):\n        dirname = os.path.dirname(setup_script)\n        python = sys.executable\n        return '\\n' + self.__editable_msg % locals()\n\n    def run_setup(self, setup_script, setup_base, args) -> None:\n        sys.modules.setdefault('distutils.command.bdist_egg', bdist_egg)\n        sys.modules.setdefault('distutils.command.egg_info', egg_info)\n\n        args = list(args)\n        if self.verbose > 2:\n            v = 'v' * (self.verbose - 1)\n            args.insert(0, '-' + v)\n        elif self.verbose < 2:\n            args.insert(0, '-q')\n        if self.dry_run:\n            args.insert(0, '-n')\n        log.info(\"Running %s %s\", setup_script[len(setup_base) + 1 :], ' '.join(args))\n        try:\n            run_setup(setup_script, args)\n        except SystemExit as v:\n            raise DistutilsError(f\"Setup script exited with {v.args[0]}\") from v\n\n    def build_and_install(self, setup_script, setup_base):\n        args = ['bdist_egg', '--dist-dir']\n\n        dist_dir = tempfile.mkdtemp(\n            prefix='egg-dist-tmp-', dir=os.path.dirname(setup_script)\n        )\n        try:\n            self._set_fetcher_options(os.path.dirname(setup_script))\n            args.append(dist_dir)\n\n            self.run_setup(setup_script, setup_base, args)\n            all_eggs = Environment([dist_dir])\n            eggs = [\n                self.install_egg(dist.location, setup_base)\n                for key in all_eggs\n                for dist in all_eggs[key]\n            ]\n            if not eggs and not self.dry_run:\n                log.warn(\"No eggs found in %s (setup script problem?)\", dist_dir)\n            return eggs\n        finally:\n            _rmtree(dist_dir)\n            log.set_verbosity(self.verbose)  # restore our log verbosity\n\n    def _set_fetcher_options(self, base):\n        \"\"\"\n        When easy_install is about to run bdist_egg on a source dist, that\n        source dist might have 'setup_requires' directives, requiring\n        additional fetching. Ensure the fetcher options given to easy_install\n        are available to that command as well.\n        \"\"\"\n        # find the fetch options from easy_install and write them out\n        # to the setup.cfg file.\n        ei_opts = self.distribution.get_option_dict('easy_install').copy()\n        fetch_directives = (\n            'find_links',\n            'site_dirs',\n            'index_url',\n            'optimize',\n            'allow_hosts',\n        )\n        fetch_options = {}\n        for key, val in ei_opts.items():\n            if key not in fetch_directives:\n                continue\n            fetch_options[key] = val[1]\n        # create a settings dictionary suitable for `edit_config`\n        settings = dict(easy_install=fetch_options)\n        cfg_filename = os.path.join(base, 'setup.cfg')\n        setopt.edit_config(cfg_filename, settings)\n\n    def update_pth(self, dist) -> None:  # noqa: C901  # is too complex (11)  # FIXME\n        if self.pth_file is None:\n            return\n\n        for d in self.pth_file[dist.key]:  # drop old entries\n            if not self.multi_version and d.location == dist.location:\n                continue\n\n            log.info(\"Removing %s from easy-install.pth file\", d)\n            self.pth_file.remove(d)\n            if d.location in self.shadow_path:\n                self.shadow_path.remove(d.location)\n\n        if not self.multi_version:\n            if dist.location in self.pth_file.paths:\n                log.info(\n                    \"%s is already the active version in easy-install.pth\",\n                    dist,\n                )\n            else:\n                log.info(\"Adding %s to easy-install.pth file\", dist)\n                self.pth_file.add(dist)  # add new entry\n                if dist.location not in self.shadow_path:\n                    self.shadow_path.append(dist.location)\n\n        if self.dry_run:\n            return\n\n        self.pth_file.save()\n\n        if dist.key != 'setuptools':\n            return\n\n        # Ensure that setuptools itself never becomes unavailable!\n        # XXX should this check for latest version?\n        filename = os.path.join(self.install_dir, 'setuptools.pth')\n        if os.path.islink(filename):\n            os.unlink(filename)\n\n        with open(filename, 'wt', encoding=py312.PTH_ENCODING) as f:\n            # ^-- Python<3.13 require encoding=\"locale\" instead of \"utf-8\",\n            #     see python/cpython#77102.\n            f.write(self.pth_file.make_relative(dist.location) + '\\n')\n\n    def unpack_progress(self, src, dst):\n        # Progress filter for unpacking\n        log.debug(\"Unpacking %s to %s\", src, dst)\n        return dst  # only unpack-and-compile skips files for dry run\n\n    def unpack_and_compile(self, egg_path, destination) -> None:\n        to_compile = []\n        to_chmod = []\n\n        def pf(src, dst):\n            if dst.endswith('.py') and not src.startswith('EGG-INFO/'):\n                to_compile.append(dst)\n            elif dst.endswith('.dll') or dst.endswith('.so'):\n                to_chmod.append(dst)\n            self.unpack_progress(src, dst)\n            return not self.dry_run and dst or None\n\n        unpack_archive(egg_path, destination, pf)\n        self.byte_compile(to_compile)\n        if not self.dry_run:\n            for f in to_chmod:\n                mode = ((os.stat(f)[stat.ST_MODE]) | 0o555) & 0o7755\n                chmod(f, mode)\n\n    def byte_compile(self, to_compile) -> None:\n        if sys.dont_write_bytecode:\n            return\n\n        from distutils.util import byte_compile\n\n        try:\n            # try to make the byte compile messages quieter\n            log.set_verbosity(self.verbose - 1)\n\n            byte_compile(to_compile, optimize=0, force=True, dry_run=self.dry_run)\n            if self.optimize:\n                byte_compile(\n                    to_compile,\n                    optimize=self.optimize,\n                    force=True,\n                    dry_run=self.dry_run,\n                )\n        finally:\n            log.set_verbosity(self.verbose)  # restore original verbosity\n\n    __no_default_msg = textwrap.dedent(\n        \"\"\"\n        bad install directory or PYTHONPATH\n\n        You are attempting to install a package to a directory that is not\n        on PYTHONPATH and which Python does not read \".pth\" files from.  The\n        installation directory you specified (via --install-dir, --prefix, or\n        the distutils default setting) was:\n\n            %s\n\n        and your PYTHONPATH environment variable currently contains:\n\n            %r\n\n        Here are some of your options for correcting the problem:\n\n        * You can choose a different installation directory, i.e., one that is\n          on PYTHONPATH or supports .pth files\n\n        * You can add the installation directory to the PYTHONPATH environment\n          variable.  (It must then also be on PYTHONPATH whenever you run\n          Python and want to use the package(s) you are installing.)\n\n        * You can set up the installation directory to support \".pth\" files by\n          using one of the approaches described here:\n\n          https://setuptools.pypa.io/en/latest/deprecated/easy_install.html#custom-installation-locations\n\n\n        Please make the appropriate changes for your system and try again.\n        \"\"\"\n    ).strip()\n\n    def create_home_path(self) -> None:\n        \"\"\"Create directories under ~.\"\"\"\n        if not self.user:\n            return\n        home = convert_path(os.path.expanduser(\"~\"))\n        for path in only_strs(self.config_vars.values()):\n            if path.startswith(home) and not os.path.isdir(path):\n                self.debug_print(f\"os.makedirs('{path}', 0o700)\")\n                os.makedirs(path, 0o700)\n\n    INSTALL_SCHEMES = dict(\n        posix=dict(\n            install_dir='$base/lib/python$py_version_short/site-packages',\n            script_dir='$base/bin',\n        ),\n    )\n\n    DEFAULT_SCHEME = dict(\n        install_dir='$base/Lib/site-packages',\n        script_dir='$base/Scripts',\n    )\n\n    def _expand(self, *attrs):\n        config_vars = self.get_finalized_command('install').config_vars\n\n        if self.prefix:\n            # Set default install_dir/scripts from --prefix\n            config_vars = dict(config_vars)\n            config_vars['base'] = self.prefix\n            scheme = self.INSTALL_SCHEMES.get(os.name, self.DEFAULT_SCHEME)\n            for attr, val in scheme.items():\n                if getattr(self, attr, None) is None:\n                    setattr(self, attr, val)\n\n        from distutils.util import subst_vars\n\n        for attr in attrs:\n            val = getattr(self, attr)\n            if val is not None:\n                val = subst_vars(val, config_vars)\n                if os.name == 'posix':\n                    val = os.path.expanduser(val)\n                setattr(self, attr, val)\n\n\ndef _pythonpath():\n    items = os.environ.get('PYTHONPATH', '').split(os.pathsep)\n    return filter(None, items)\n\n\ndef get_site_dirs():\n    \"\"\"\n    Return a list of 'site' dirs\n    \"\"\"\n\n    sitedirs = []\n\n    # start with PYTHONPATH\n    sitedirs.extend(_pythonpath())\n\n    prefixes = [sys.prefix]\n    if sys.exec_prefix != sys.prefix:\n        prefixes.append(sys.exec_prefix)\n    for prefix in prefixes:\n        if not prefix:\n            continue\n\n        if sys.platform in ('os2emx', 'riscos'):\n            sitedirs.append(os.path.join(prefix, \"Lib\", \"site-packages\"))\n        elif os.sep == '/':\n            sitedirs.extend([\n                os.path.join(\n                    prefix,\n                    \"lib\",\n                    f\"python{sys.version_info.major}.{sys.version_info.minor}\",\n                    \"site-packages\",\n                ),\n                os.path.join(prefix, \"lib\", \"site-python\"),\n            ])\n        else:\n            sitedirs.extend([\n                prefix,\n                os.path.join(prefix, \"lib\", \"site-packages\"),\n            ])\n        if sys.platform != 'darwin':\n            continue\n\n        # for framework builds *only* we add the standard Apple\n        # locations. Currently only per-user, but /Library and\n        # /Network/Library could be added too\n        if 'Python.framework' not in prefix:\n            continue\n\n        home = os.environ.get('HOME')\n        if not home:\n            continue\n\n        home_sp = os.path.join(\n            home,\n            'Library',\n            'Python',\n            f'{sys.version_info.major}.{sys.version_info.minor}',\n            'site-packages',\n        )\n        sitedirs.append(home_sp)\n    lib_paths = get_path('purelib'), get_path('platlib')\n\n    sitedirs.extend(s for s in lib_paths if s not in sitedirs)\n\n    if site.ENABLE_USER_SITE:\n        sitedirs.append(site.USER_SITE)\n\n    with contextlib.suppress(AttributeError):\n        sitedirs.extend(site.getsitepackages())\n\n    return list(map(normalize_path, sitedirs))\n\n\ndef expand_paths(inputs):  # noqa: C901  # is too complex (11)  # FIXME\n    \"\"\"Yield sys.path directories that might contain \"old-style\" packages\"\"\"\n\n    seen = set()\n\n    for dirname in inputs:\n        dirname = normalize_path(dirname)\n        if dirname in seen:\n            continue\n\n        seen.add(dirname)\n        if not os.path.isdir(dirname):\n            continue\n\n        files = os.listdir(dirname)\n        yield dirname, files\n\n        for name in files:\n            if not name.endswith('.pth'):\n                # We only care about the .pth files\n                continue\n            if name in ('easy-install.pth', 'setuptools.pth'):\n                # Ignore .pth files that we control\n                continue\n\n            # Read the .pth file\n            content = _read_pth(os.path.join(dirname, name))\n            lines = list(yield_lines(content))\n\n            # Yield existing non-dupe, non-import directory lines from it\n            for line in lines:\n                if line.startswith(\"import\"):\n                    continue\n\n                line = normalize_path(line.rstrip())\n                if line in seen:\n                    continue\n\n                seen.add(line)\n                if not os.path.isdir(line):\n                    continue\n\n                yield line, os.listdir(line)\n\n\ndef extract_wininst_cfg(dist_filename):\n    \"\"\"Extract configuration data from a bdist_wininst .exe\n\n    Returns a configparser.RawConfigParser, or None\n    \"\"\"\n    f = open(dist_filename, 'rb')\n    try:\n        endrec = zipfile._EndRecData(f)\n        if endrec is None:\n            return None\n\n        prepended = (endrec[9] - endrec[5]) - endrec[6]\n        if prepended < 12:  # no wininst data here\n            return None\n        f.seek(prepended - 12)\n\n        tag, cfglen, _bmlen = struct.unpack(\"<iii\", f.read(12))\n        if tag not in (0x1234567A, 0x1234567B):\n            return None  # not a valid tag\n\n        f.seek(prepended - (12 + cfglen))\n        init = {'version': '', 'target_version': ''}\n        cfg = configparser.RawConfigParser(init)\n        try:\n            part = f.read(cfglen)\n            # Read up to the first null byte.\n            config = part.split(b'\\0', 1)[0]\n            # Now the config is in bytes, but for RawConfigParser, it should\n            #  be text, so decode it.\n            config = config.decode(sys.getfilesystemencoding())\n            cfg.read_file(io.StringIO(config))\n        except configparser.Error:\n            return None\n        if not cfg.has_section('metadata') or not cfg.has_section('Setup'):\n            return None\n        return cfg\n\n    finally:\n        f.close()\n\n\ndef get_exe_prefixes(exe_filename):\n    \"\"\"Get exe->egg path translations for a given .exe file\"\"\"\n\n    prefixes = [\n        ('PURELIB/', ''),\n        ('PLATLIB/pywin32_system32', ''),\n        ('PLATLIB/', ''),\n        ('SCRIPTS/', 'EGG-INFO/scripts/'),\n        ('DATA/lib/site-packages', ''),\n    ]\n    z = zipfile.ZipFile(exe_filename)\n    try:\n        for info in z.infolist():\n            name = info.filename\n            parts = name.split('/')\n            if len(parts) == 3 and parts[2] == 'PKG-INFO':\n                if parts[1].endswith('.egg-info'):\n                    prefixes.insert(0, ('/'.join(parts[:2]), 'EGG-INFO/'))\n                    break\n            if len(parts) != 2 or not name.endswith('.pth'):\n                continue\n            if name.endswith('-nspkg.pth'):\n                continue\n            if parts[0].upper() in ('PURELIB', 'PLATLIB'):\n                contents = z.read(name).decode()\n                for pth in yield_lines(contents):\n                    pth = pth.strip().replace('\\\\', '/')\n                    if not pth.startswith('import'):\n                        prefixes.append(((f'{parts[0]}/{pth}/'), ''))\n    finally:\n        z.close()\n    prefixes = [(x.lower(), y) for x, y in prefixes]\n    prefixes.sort()\n    prefixes.reverse()\n    return prefixes\n\n\nclass PthDistributions(Environment):\n    \"\"\"A .pth file with Distribution paths in it\"\"\"\n\n    def __init__(self, filename, sitedirs=()) -> None:\n        self.filename = filename\n        self.sitedirs = list(map(normalize_path, sitedirs))\n        self.basedir = normalize_path(os.path.dirname(self.filename))\n        self.paths, self.dirty = self._load()\n        # keep a copy if someone manually updates the paths attribute on the instance\n        self._init_paths = self.paths[:]\n        super().__init__([], None, None)\n        for path in yield_lines(self.paths):\n            list(map(self.add, find_distributions(path, True)))\n\n    def _load_raw(self):\n        paths = []\n        dirty = saw_import = False\n        seen = set(self.sitedirs)\n        content = _read_pth(self.filename)\n        for line in content.splitlines():\n            path = line.rstrip()\n            # still keep imports and empty/commented lines for formatting\n            paths.append(path)\n            if line.startswith(('import ', 'from ')):\n                saw_import = True\n                continue\n            stripped_path = path.strip()\n            if not stripped_path or stripped_path.startswith('#'):\n                continue\n            # skip non-existent paths, in case somebody deleted a package\n            # manually, and duplicate paths as well\n            normalized_path = normalize_path(os.path.join(self.basedir, path))\n            if normalized_path in seen or not os.path.exists(normalized_path):\n                log.debug(\"cleaned up dirty or duplicated %r\", path)\n                dirty = True\n                paths.pop()\n                continue\n            seen.add(normalized_path)\n        # remove any trailing empty/blank line\n        while paths and not paths[-1].strip():\n            paths.pop()\n            dirty = True\n        return paths, dirty or (paths and saw_import)\n\n    def _load(self):\n        if os.path.isfile(self.filename):\n            return self._load_raw()\n        return [], False\n\n    def save(self) -> None:\n        \"\"\"Write changed .pth file back to disk\"\"\"\n        # first reload the file\n        last_paths, last_dirty = self._load()\n        # and check that there are no difference with what we have.\n        # there can be difference if someone else has written to the file\n        # since we first loaded it.\n        # we don't want to lose the eventual new paths added since then.\n        for path in last_paths[:]:\n            if path not in self.paths:\n                self.paths.append(path)\n                log.info(\"detected new path %r\", path)\n                last_dirty = True\n            else:\n                last_paths.remove(path)\n        # also, re-check that all paths are still valid before saving them\n        for path in self.paths[:]:\n            if path not in last_paths and not path.startswith((\n                'import ',\n                'from ',\n                '#',\n            )):\n                absolute_path = os.path.join(self.basedir, path)\n                if not os.path.exists(absolute_path):\n                    self.paths.remove(path)\n                    log.info(\"removing now non-existent path %r\", path)\n                    last_dirty = True\n\n        self.dirty |= last_dirty or self.paths != self._init_paths\n        if not self.dirty:\n            return\n\n        rel_paths = list(map(self.make_relative, self.paths))\n        if rel_paths:\n            log.debug(\"Saving %s\", self.filename)\n            lines = self._wrap_lines(rel_paths)\n            data = '\\n'.join(lines) + '\\n'\n            if os.path.islink(self.filename):\n                os.unlink(self.filename)\n            with open(self.filename, 'wt', encoding=py312.PTH_ENCODING) as f:\n                # ^-- Python<3.13 require encoding=\"locale\" instead of \"utf-8\",\n                #     see python/cpython#77102.\n                f.write(data)\n        elif os.path.exists(self.filename):\n            log.debug(\"Deleting empty %s\", self.filename)\n            os.unlink(self.filename)\n\n        self.dirty = False\n        self._init_paths[:] = self.paths[:]\n\n    @staticmethod\n    def _wrap_lines(lines):\n        return lines\n\n    def add(self, dist) -> None:\n        \"\"\"Add `dist` to the distribution map\"\"\"\n        new_path = dist.location not in self.paths and (\n            dist.location not in self.sitedirs\n            or\n            # account for '.' being in PYTHONPATH\n            dist.location == os.getcwd()\n        )\n        if new_path:\n            self.paths.append(dist.location)\n            self.dirty = True\n        super().add(dist)\n\n    def remove(self, dist) -> None:\n        \"\"\"Remove `dist` from the distribution map\"\"\"\n        while dist.location in self.paths:\n            self.paths.remove(dist.location)\n            self.dirty = True\n        super().remove(dist)\n\n    def make_relative(self, path):\n        npath, last = os.path.split(normalize_path(path))\n        baselen = len(self.basedir)\n        parts = [last]\n        sep = os.altsep == '/' and '/' or os.sep\n        while len(npath) >= baselen:\n            if npath == self.basedir:\n                parts.append(os.curdir)\n                parts.reverse()\n                return sep.join(parts)\n            npath, last = os.path.split(npath)\n            parts.append(last)\n        else:\n            return path\n\n\nclass RewritePthDistributions(PthDistributions):\n    @classmethod\n    def _wrap_lines(cls, lines):\n        yield cls.prelude\n        yield from lines\n        yield cls.postlude\n\n    prelude = _one_liner(\n        \"\"\"\n        import sys\n        sys.__plen = len(sys.path)\n        \"\"\"\n    )\n    postlude = _one_liner(\n        \"\"\"\n        import sys\n        new = sys.path[sys.__plen:]\n        del sys.path[sys.__plen:]\n        p = getattr(sys, '__egginsert', 0)\n        sys.path[p:p] = new\n        sys.__egginsert = p + len(new)\n        \"\"\"\n    )\n\n\nif os.environ.get('SETUPTOOLS_SYS_PATH_TECHNIQUE', 'raw') == 'rewrite':\n    PthDistributions = RewritePthDistributions  # type: ignore[misc]  # Overwriting type\n\n\ndef _first_line_re():\n    \"\"\"\n    Return a regular expression based on first_line_re suitable for matching\n    strings.\n    \"\"\"\n    if isinstance(first_line_re.pattern, str):\n        return first_line_re\n\n    # first_line_re in Python >=3.1.4 and >=3.2.1 is a bytes pattern.\n    return re.compile(first_line_re.pattern.decode())\n\n\ndef update_dist_caches(dist_path, fix_zipimporter_caches):\n    \"\"\"\n    Fix any globally cached `dist_path` related data\n\n    `dist_path` should be a path of a newly installed egg distribution (zipped\n    or unzipped).\n\n    sys.path_importer_cache contains finder objects that have been cached when\n    importing data from the original distribution. Any such finders need to be\n    cleared since the replacement distribution might be packaged differently,\n    e.g. a zipped egg distribution might get replaced with an unzipped egg\n    folder or vice versa. Having the old finders cached may then cause Python\n    to attempt loading modules from the replacement distribution using an\n    incorrect loader.\n\n    zipimport.zipimporter objects are Python loaders charged with importing\n    data packaged inside zip archives. If stale loaders referencing the\n    original distribution, are left behind, they can fail to load modules from\n    the replacement distribution. E.g. if an old zipimport.zipimporter instance\n    is used to load data from a new zipped egg archive, it may cause the\n    operation to attempt to locate the requested data in the wrong location -\n    one indicated by the original distribution's zip archive directory\n    information. Such an operation may then fail outright, e.g. report having\n    read a 'bad local file header', or even worse, it may fail silently &\n    return invalid data.\n\n    zipimport._zip_directory_cache contains cached zip archive directory\n    information for all existing zipimport.zipimporter instances and all such\n    instances connected to the same archive share the same cached directory\n    information.\n\n    If asked, and the underlying Python implementation allows it, we can fix\n    all existing zipimport.zipimporter instances instead of having to track\n    them down and remove them one by one, by updating their shared cached zip\n    archive directory information. This, of course, assumes that the\n    replacement distribution is packaged as a zipped egg.\n\n    If not asked to fix existing zipimport.zipimporter instances, we still do\n    our best to clear any remaining zipimport.zipimporter related cached data\n    that might somehow later get used when attempting to load data from the new\n    distribution and thus cause such load operations to fail. Note that when\n    tracking down such remaining stale data, we can not catch every conceivable\n    usage from here, and we clear only those that we know of and have found to\n    cause problems if left alive. Any remaining caches should be updated by\n    whomever is in charge of maintaining them, i.e. they should be ready to\n    handle us replacing their zip archives with new distributions at runtime.\n\n    \"\"\"\n    # There are several other known sources of stale zipimport.zipimporter\n    # instances that we do not clear here, but might if ever given a reason to\n    # do so:\n    # * Global setuptools pkg_resources.working_set (a.k.a. 'master working\n    # set') may contain distributions which may in turn contain their\n    #   zipimport.zipimporter loaders.\n    # * Several zipimport.zipimporter loaders held by local variables further\n    #   up the function call stack when running the setuptools installation.\n    # * Already loaded modules may have their __loader__ attribute set to the\n    #   exact loader instance used when importing them. Python 3.4 docs state\n    #   that this information is intended mostly for introspection and so is\n    #   not expected to cause us problems.\n    normalized_path = normalize_path(dist_path)\n    _uncache(normalized_path, sys.path_importer_cache)\n    if fix_zipimporter_caches:\n        _replace_zip_directory_cache_data(normalized_path)\n    else:\n        # Here, even though we do not want to fix existing and now stale\n        # zipimporter cache information, we still want to remove it. Related to\n        # Python's zip archive directory information cache, we clear each of\n        # its stale entries in two phases:\n        #   1. Clear the entry so attempting to access zip archive information\n        #      via any existing stale zipimport.zipimporter instances fails.\n        #   2. Remove the entry from the cache so any newly constructed\n        #      zipimport.zipimporter instances do not end up using old stale\n        #      zip archive directory information.\n        # This whole stale data removal step does not seem strictly necessary,\n        # but has been left in because it was done before we started replacing\n        # the zip archive directory information cache content if possible, and\n        # there are no relevant unit tests that we can depend on to tell us if\n        # this is really needed.\n        _remove_and_clear_zip_directory_cache_data(normalized_path)\n\n\ndef _collect_zipimporter_cache_entries(normalized_path, cache):\n    \"\"\"\n    Return zipimporter cache entry keys related to a given normalized path.\n\n    Alternative path spellings (e.g. those using different character case or\n    those using alternative path separators) related to the same path are\n    included. Any sub-path entries are included as well, i.e. those\n    corresponding to zip archives embedded in other zip archives.\n\n    \"\"\"\n    result = []\n    prefix_len = len(normalized_path)\n    for p in cache:\n        np = normalize_path(p)\n        if np.startswith(normalized_path) and np[prefix_len : prefix_len + 1] in (\n            os.sep,\n            '',\n        ):\n            result.append(p)\n    return result\n\n\ndef _update_zipimporter_cache(normalized_path, cache, updater=None):\n    \"\"\"\n    Update zipimporter cache data for a given normalized path.\n\n    Any sub-path entries are processed as well, i.e. those corresponding to zip\n    archives embedded in other zip archives.\n\n    Given updater is a callable taking a cache entry key and the original entry\n    (after already removing the entry from the cache), and expected to update\n    the entry and possibly return a new one to be inserted in its place.\n    Returning None indicates that the entry should not be replaced with a new\n    one. If no updater is given, the cache entries are simply removed without\n    any additional processing, the same as if the updater simply returned None.\n\n    \"\"\"\n    for p in _collect_zipimporter_cache_entries(normalized_path, cache):\n        # N.B. pypy's custom zipimport._zip_directory_cache implementation does\n        # not support the complete dict interface:\n        # * Does not support item assignment, thus not allowing this function\n        #    to be used only for removing existing cache entries.\n        #  * Does not support the dict.pop() method, forcing us to use the\n        #    get/del patterns instead. For more detailed information see the\n        #    following links:\n        #      https://github.com/pypa/setuptools/issues/202#issuecomment-202913420\n        #      https://foss.heptapod.net/pypy/pypy/-/blob/144c4e65cb6accb8e592f3a7584ea38265d1873c/pypy/module/zipimport/interp_zipimport.py\n        old_entry = cache[p]\n        del cache[p]\n        new_entry = updater and updater(p, old_entry)\n        if new_entry is not None:\n            cache[p] = new_entry\n\n\ndef _uncache(normalized_path, cache):\n    _update_zipimporter_cache(normalized_path, cache)\n\n\ndef _remove_and_clear_zip_directory_cache_data(normalized_path):\n    def clear_and_remove_cached_zip_archive_directory_data(path, old_entry):\n        old_entry.clear()\n\n    _update_zipimporter_cache(\n        normalized_path,\n        zipimport._zip_directory_cache,\n        updater=clear_and_remove_cached_zip_archive_directory_data,\n    )\n\n\n# PyPy Python implementation does not allow directly writing to the\n# zipimport._zip_directory_cache and so prevents us from attempting to correct\n# its content. The best we can do there is clear the problematic cache content\n# and have PyPy repopulate it as needed. The downside is that if there are any\n# stale zipimport.zipimporter instances laying around, attempting to use them\n# will fail due to not having its zip archive directory information available\n# instead of being automatically corrected to use the new correct zip archive\n# directory information.\nif '__pypy__' in sys.builtin_module_names:\n    _replace_zip_directory_cache_data = _remove_and_clear_zip_directory_cache_data\nelse:\n\n    def _replace_zip_directory_cache_data(normalized_path):\n        def replace_cached_zip_archive_directory_data(path, old_entry):\n            # N.B. In theory, we could load the zip directory information just\n            # once for all updated path spellings, and then copy it locally and\n            # update its contained path strings to contain the correct\n            # spelling, but that seems like a way too invasive move (this cache\n            # structure is not officially documented anywhere and could in\n            # theory change with new Python releases) for no significant\n            # benefit.\n            old_entry.clear()\n            zipimport.zipimporter(path)\n            old_entry.update(zipimport._zip_directory_cache[path])\n            return old_entry\n\n        _update_zipimporter_cache(\n            normalized_path,\n            zipimport._zip_directory_cache,\n            updater=replace_cached_zip_archive_directory_data,\n        )\n\n\ndef is_python(text, filename='<string>'):\n    \"Is this string a valid Python script?\"\n    try:\n        compile(text, filename, 'exec')\n    except (SyntaxError, TypeError):\n        return False\n    else:\n        return True\n\n\ndef is_sh(executable):\n    \"\"\"Determine if the specified executable is a .sh (contains a #! line)\"\"\"\n    try:\n        with open(executable, encoding='latin-1') as fp:\n            magic = fp.read(2)\n    except OSError:\n        return executable\n    return magic == '#!'\n\n\ndef nt_quote_arg(arg):\n    \"\"\"Quote a command line argument according to Windows parsing rules\"\"\"\n    return subprocess.list2cmdline([arg])\n\n\ndef is_python_script(script_text, filename):\n    \"\"\"Is this text, as a whole, a Python script? (as opposed to shell/bat/etc.\"\"\"\n    if filename.endswith('.py') or filename.endswith('.pyw'):\n        return True  # extension says it's Python\n    if is_python(script_text, filename):\n        return True  # it's syntactically valid Python\n    if script_text.startswith('#!'):\n        # It begins with a '#!' line, so check if 'python' is in it somewhere\n        return 'python' in script_text.splitlines()[0].lower()\n\n    return False  # Not any Python I can recognize\n\n\nclass _SplitArgs(TypedDict, total=False):\n    comments: bool\n    posix: bool\n\n\nclass CommandSpec(list):\n    \"\"\"\n    A command spec for a #! header, specified as a list of arguments akin to\n    those passed to Popen.\n    \"\"\"\n\n    options: list[str] = []\n    split_args = _SplitArgs()\n\n    @classmethod\n    def best(cls):\n        \"\"\"\n        Choose the best CommandSpec class based on environmental conditions.\n        \"\"\"\n        return cls\n\n    @classmethod\n    def _sys_executable(cls):\n        _default = os.path.normpath(sys.executable)\n        return os.environ.get('__PYVENV_LAUNCHER__', _default)\n\n    @classmethod\n    def from_param(cls, param: Self | str | Iterable[str] | None) -> Self:\n        \"\"\"\n        Construct a CommandSpec from a parameter to build_scripts, which may\n        be None.\n        \"\"\"\n        if isinstance(param, cls):\n            return param\n        if isinstance(param, str):\n            return cls.from_string(param)\n        if isinstance(param, Iterable):\n            return cls(param)\n        if param is None:\n            return cls.from_environment()\n        raise TypeError(f\"Argument has an unsupported type {type(param)}\")\n\n    @classmethod\n    def from_environment(cls):\n        return cls([cls._sys_executable()])\n\n    @classmethod\n    def from_string(cls, string: str) -> Self:\n        \"\"\"\n        Construct a command spec from a simple string representing a command\n        line parseable by shlex.split.\n        \"\"\"\n        items = shlex.split(string, **cls.split_args)\n        return cls(items)\n\n    def install_options(self, script_text: str):\n        self.options = shlex.split(self._extract_options(script_text))\n        cmdline = subprocess.list2cmdline(self)\n        if not isascii(cmdline):\n            self.options[:0] = ['-x']\n\n    @staticmethod\n    def _extract_options(orig_script):\n        \"\"\"\n        Extract any options from the first line of the script.\n        \"\"\"\n        first = (orig_script + '\\n').splitlines()[0]\n        match = _first_line_re().match(first)\n        options = match.group(1) or '' if match else ''\n        return options.strip()\n\n    def as_header(self):\n        return self._render(self + list(self.options))\n\n    @staticmethod\n    def _strip_quotes(item):\n        _QUOTES = '\"\\''\n        for q in _QUOTES:\n            if item.startswith(q) and item.endswith(q):\n                return item[1:-1]\n        return item\n\n    @staticmethod\n    def _render(items):\n        cmdline = subprocess.list2cmdline(\n            CommandSpec._strip_quotes(item.strip()) for item in items\n        )\n        return '#!' + cmdline + '\\n'\n\n\n# For pbr compat; will be removed in a future version.\nsys_executable = CommandSpec._sys_executable()\n\n\nclass WindowsCommandSpec(CommandSpec):\n    split_args = _SplitArgs(posix=False)\n\n\nclass ScriptWriter:\n    \"\"\"\n    Encapsulates behavior around writing entry point scripts for console and\n    gui apps.\n    \"\"\"\n\n    template = textwrap.dedent(\n        r\"\"\"\n        # EASY-INSTALL-ENTRY-SCRIPT: %(spec)r,%(group)r,%(name)r\n        import re\n        import sys\n\n        # for compatibility with easy_install; see #2198\n        __requires__ = %(spec)r\n\n        try:\n            from importlib.metadata import distribution\n        except ImportError:\n            try:\n                from importlib_metadata import distribution\n            except ImportError:\n                from pkg_resources import load_entry_point\n\n\n        def importlib_load_entry_point(spec, group, name):\n            dist_name, _, _ = spec.partition('==')\n            matches = (\n                entry_point\n                for entry_point in distribution(dist_name).entry_points\n                if entry_point.group == group and entry_point.name == name\n            )\n            return next(matches).load()\n\n\n        globals().setdefault('load_entry_point', importlib_load_entry_point)\n\n\n        if __name__ == '__main__':\n            sys.argv[0] = re.sub(r'(-script\\.pyw?|\\.exe)?$', '', sys.argv[0])\n            sys.exit(load_entry_point(%(spec)r, %(group)r, %(name)r)())\n        \"\"\"\n    ).lstrip()\n\n    command_spec_class = CommandSpec\n\n    @classmethod\n    def get_args(cls, dist, header=None):\n        \"\"\"\n        Yield write_script() argument tuples for a distribution's\n        console_scripts and gui_scripts entry points.\n        \"\"\"\n        if header is None:\n            header = cls.get_header()\n        spec = str(dist.as_requirement())\n        for type_ in 'console', 'gui':\n            group = type_ + '_scripts'\n            for name in dist.get_entry_map(group).keys():\n                cls._ensure_safe_name(name)\n                script_text = cls.template % locals()\n                args = cls._get_script_args(type_, name, header, script_text)\n                yield from args\n\n    @staticmethod\n    def _ensure_safe_name(name):\n        \"\"\"\n        Prevent paths in *_scripts entry point names.\n        \"\"\"\n        has_path_sep = re.search(r'[\\\\/]', name)\n        if has_path_sep:\n            raise ValueError(\"Path separators not allowed in script names\")\n\n    @classmethod\n    def best(cls):\n        \"\"\"\n        Select the best ScriptWriter for this environment.\n        \"\"\"\n        if sys.platform == 'win32' or (os.name == 'java' and os._name == 'nt'):\n            return WindowsScriptWriter.best()\n        else:\n            return cls\n\n    @classmethod\n    def _get_script_args(cls, type_, name, header, script_text):\n        # Simply write the stub with no extension.\n        yield (name, header + script_text)\n\n    @classmethod\n    def get_header(\n        cls,\n        script_text: str = \"\",\n        executable: str | CommandSpec | Iterable[str] | None = None,\n    ) -> str:\n        \"\"\"Create a #! line, getting options (if any) from script_text\"\"\"\n        cmd = cls.command_spec_class.best().from_param(executable)\n        cmd.install_options(script_text)\n        return cmd.as_header()\n\n\nclass WindowsScriptWriter(ScriptWriter):\n    command_spec_class = WindowsCommandSpec\n\n    @classmethod\n    def best(cls):\n        \"\"\"\n        Select the best ScriptWriter suitable for Windows\n        \"\"\"\n        writer_lookup = dict(\n            executable=WindowsExecutableLauncherWriter,\n            natural=cls,\n        )\n        # for compatibility, use the executable launcher by default\n        launcher = os.environ.get('SETUPTOOLS_LAUNCHER', 'executable')\n        return writer_lookup[launcher]\n\n    @classmethod\n    def _get_script_args(cls, type_, name, header, script_text):\n        \"For Windows, add a .py extension\"\n        ext = dict(console='.pya', gui='.pyw')[type_]\n        if ext not in os.environ['PATHEXT'].lower().split(';'):\n            msg = (\n                \"{ext} not listed in PATHEXT; scripts will not be \"\n                \"recognized as executables.\"\n            ).format(**locals())\n            SetuptoolsWarning.emit(msg)\n        old = ['.pya', '.py', '-script.py', '.pyc', '.pyo', '.pyw', '.exe']\n        old.remove(ext)\n        header = cls._adjust_header(type_, header)\n        blockers = [name + x for x in old]\n        yield name + ext, header + script_text, 't', blockers\n\n    @classmethod\n    def _adjust_header(cls, type_, orig_header):\n        \"\"\"\n        Make sure 'pythonw' is used for gui and 'python' is used for\n        console (regardless of what sys.executable is).\n        \"\"\"\n        pattern = 'pythonw.exe'\n        repl = 'python.exe'\n        if type_ == 'gui':\n            pattern, repl = repl, pattern\n        pattern_ob = re.compile(re.escape(pattern), re.IGNORECASE)\n        new_header = pattern_ob.sub(string=orig_header, repl=repl)\n        return new_header if cls._use_header(new_header) else orig_header\n\n    @staticmethod\n    def _use_header(new_header):\n        \"\"\"\n        Should _adjust_header use the replaced header?\n\n        On non-windows systems, always use. On\n        Windows systems, only use the replaced header if it resolves\n        to an executable on the system.\n        \"\"\"\n        clean_header = new_header[2:-1].strip('\"')\n        return sys.platform != 'win32' or shutil.which(clean_header)\n\n\nclass WindowsExecutableLauncherWriter(WindowsScriptWriter):\n    @classmethod\n    def _get_script_args(cls, type_, name, header, script_text):\n        \"\"\"\n        For Windows, add a .py extension and an .exe launcher\n        \"\"\"\n        if type_ == 'gui':\n            launcher_type = 'gui'\n            ext = '-script.pyw'\n            old = ['.pyw']\n        else:\n            launcher_type = 'cli'\n            ext = '-script.py'\n            old = ['.py', '.pyc', '.pyo']\n        hdr = cls._adjust_header(type_, header)\n        blockers = [name + x for x in old]\n        yield (name + ext, hdr + script_text, 't', blockers)\n        yield (\n            name + '.exe',\n            get_win_launcher(launcher_type),\n            'b',  # write in binary mode\n        )\n        if not is_64bit():\n            # install a manifest for the launcher to prevent Windows\n            # from detecting it as an installer (which it will for\n            #  launchers like easy_install.exe). Consider only\n            #  adding a manifest for launchers detected as installers.\n            #  See Distribute #143 for details.\n            m_name = name + '.exe.manifest'\n            yield (m_name, load_launcher_manifest(name), 't')\n\n\ndef get_win_launcher(type):\n    \"\"\"\n    Load the Windows launcher (executable) suitable for launching a script.\n\n    `type` should be either 'cli' or 'gui'\n\n    Returns the executable as a byte string.\n    \"\"\"\n    launcher_fn = f'{type}.exe'\n    if is_64bit():\n        if get_platform() == \"win-arm64\":\n            launcher_fn = launcher_fn.replace(\".\", \"-arm64.\")\n        else:\n            launcher_fn = launcher_fn.replace(\".\", \"-64.\")\n    else:\n        launcher_fn = launcher_fn.replace(\".\", \"-32.\")\n    return resource_string('setuptools', launcher_fn)\n\n\ndef load_launcher_manifest(name):\n    manifest = pkg_resources.resource_string(__name__, 'launcher manifest.xml')\n    return manifest.decode('utf-8') % vars()\n\n\ndef current_umask():\n    tmp = os.umask(0o022)\n    os.umask(tmp)\n    return tmp\n\n\ndef only_strs(values):\n    \"\"\"\n    Exclude non-str values. Ref #3063.\n    \"\"\"\n    return filter(lambda val: isinstance(val, str), values)\n\n\ndef _read_pth(fullname: str) -> str:\n    # Python<3.13 require encoding=\"locale\" instead of \"utf-8\", see python/cpython#77102\n    # In the case old versions of setuptools are producing `pth` files with\n    # different encodings that might be problematic... So we fallback to \"locale\".\n\n    try:\n        with open(fullname, encoding=py312.PTH_ENCODING) as f:\n            return f.read()\n    except UnicodeDecodeError:  # pragma: no cover\n        # This error may only happen for Python >= 3.13\n        # TODO: Possible deprecation warnings to be added in the future:\n        #       ``.pth file {fullname!r} is not UTF-8.``\n        #       Your environment contain {fullname!r} that cannot be read as UTF-8.\n        #       This is likely to have been produced with an old version of setuptools.\n        #       Please be mindful that this is deprecated and in the future, non-utf8\n        #       .pth files may cause setuptools to fail.\n        with open(fullname, encoding=py39.LOCALE_ENCODING) as f:\n            return f.read()\n\n\nclass EasyInstallDeprecationWarning(SetuptoolsDeprecationWarning):\n    _SUMMARY = \"easy_install command is deprecated.\"\n    _DETAILS = \"\"\"\n    Please avoid running ``setup.py`` and ``easy_install``.\n    Instead, use pypa/build, pypa/installer or other\n    standards-based tools.\n    \"\"\"\n    _SEE_URL = \"https://github.com/pypa/setuptools/issues/917\"\n    # _DUE_DATE not defined yet\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/command/editable_wheel.py","size":35626,"sha1":"c8f9a0a554ca58d6c3ec405d69feac3332c50251","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"\nCreate a wheel that, when installed, will make the source package 'editable'\n(add it to the interpreter's path, including metadata) per PEP 660. Replaces\n'setup.py develop'.\n\n.. note::\n   One of the mechanisms briefly mentioned in PEP 660 to implement editable installs is\n   to create a separated directory inside ``build`` and use a .pth file to point to that\n   directory. In the context of this file such directory is referred as\n   *auxiliary build directory* or ``auxiliary_dir``.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport io\nimport logging\nimport os\nimport shutil\nimport traceback\nfrom collections.abc import Iterable, Iterator, Mapping\nfrom contextlib import suppress\nfrom enum import Enum\nfrom inspect import cleandoc\nfrom itertools import chain, starmap\nfrom pathlib import Path\nfrom tempfile import TemporaryDirectory\nfrom types import TracebackType\nfrom typing import TYPE_CHECKING, Protocol, TypeVar, cast\n\nfrom .. import Command, _normalization, _path, _shutil, errors, namespaces\nfrom .._path import StrPath\nfrom ..compat import py312\nfrom ..discovery import find_package_path\nfrom ..dist import Distribution\nfrom ..warnings import InformationOnly, SetuptoolsDeprecationWarning, SetuptoolsWarning\nfrom .build import build as build_cls\nfrom .build_py import build_py as build_py_cls\nfrom .dist_info import dist_info as dist_info_cls\nfrom .egg_info import egg_info as egg_info_cls\nfrom .install import install as install_cls\nfrom .install_scripts import install_scripts as install_scripts_cls\n\nif TYPE_CHECKING:\n    from typing_extensions import Self\n\n    from .._vendor.wheel.wheelfile import WheelFile\n\n_P = TypeVar(\"_P\", bound=StrPath)\n_logger = logging.getLogger(__name__)\n\n\nclass _EditableMode(Enum):\n    \"\"\"\n    Possible editable installation modes:\n    `lenient` (new files automatically added to the package - DEFAULT);\n    `strict` (requires a new installation when files are added/removed); or\n    `compat` (attempts to emulate `python setup.py develop` - DEPRECATED).\n    \"\"\"\n\n    STRICT = \"strict\"\n    LENIENT = \"lenient\"\n    COMPAT = \"compat\"  # TODO: Remove `compat` after Dec/2022.\n\n    @classmethod\n    def convert(cls, mode: str | None) -> _EditableMode:\n        if not mode:\n            return _EditableMode.LENIENT  # default\n\n        _mode = mode.upper()\n        if _mode not in _EditableMode.__members__:\n            raise errors.OptionError(f\"Invalid editable mode: {mode!r}. Try: 'strict'.\")\n\n        if _mode == \"COMPAT\":\n            SetuptoolsDeprecationWarning.emit(\n                \"Compat editable installs\",\n                \"\"\"\n                The 'compat' editable mode is transitional and will be removed\n                in future versions of `setuptools`.\n                Please adapt your code accordingly to use either the 'strict' or the\n                'lenient' modes.\n                \"\"\",\n                see_docs=\"userguide/development_mode.html\",\n                # TODO: define due_date\n                # There is a series of shortcomings with the available editable install\n                # methods, and they are very controversial. This is something that still\n                # needs work.\n                # Moreover, `pip` is still hiding this warning, so users are not aware.\n            )\n\n        return _EditableMode[_mode]\n\n\n_STRICT_WARNING = \"\"\"\nNew or renamed files may not be automatically picked up without a new installation.\n\"\"\"\n\n_LENIENT_WARNING = \"\"\"\nOptions like `package-data`, `include/exclude-package-data` or\n`packages.find.exclude/include` may have no effect.\n\"\"\"\n\n\nclass editable_wheel(Command):\n    \"\"\"Build 'editable' wheel for development.\n    This command is private and reserved for internal use of setuptools,\n    users should rely on ``setuptools.build_meta`` APIs.\n    \"\"\"\n\n    description = \"DO NOT CALL DIRECTLY, INTERNAL ONLY: create PEP 660 editable wheel\"\n\n    user_options = [\n        (\"dist-dir=\", \"d\", \"directory to put final built distributions in\"),\n        (\"dist-info-dir=\", \"I\", \"path to a pre-build .dist-info directory\"),\n        (\"mode=\", None, cleandoc(_EditableMode.__doc__ or \"\")),\n    ]\n\n    def initialize_options(self):\n        self.dist_dir = None\n        self.dist_info_dir = None\n        self.project_dir = None\n        self.mode = None\n\n    def finalize_options(self) -> None:\n        dist = self.distribution\n        self.project_dir = dist.src_root or os.curdir\n        self.package_dir = dist.package_dir or {}\n        self.dist_dir = Path(self.dist_dir or os.path.join(self.project_dir, \"dist\"))\n\n    def run(self) -> None:\n        try:\n            self.dist_dir.mkdir(exist_ok=True)\n            self._ensure_dist_info()\n\n            # Add missing dist_info files\n            self.reinitialize_command(\"bdist_wheel\")\n            bdist_wheel = self.get_finalized_command(\"bdist_wheel\")\n            bdist_wheel.write_wheelfile(self.dist_info_dir)\n\n            self._create_wheel_file(bdist_wheel)\n        except Exception:\n            traceback.print_exc()\n            project = self.distribution.name or self.distribution.get_name()\n            _DebuggingTips.emit(project=project)\n            raise\n\n    def _ensure_dist_info(self):\n        if self.dist_info_dir is None:\n            dist_info = cast(dist_info_cls, self.reinitialize_command(\"dist_info\"))\n            dist_info.output_dir = self.dist_dir\n            dist_info.ensure_finalized()\n            dist_info.run()\n            self.dist_info_dir = dist_info.dist_info_dir\n        else:\n            assert str(self.dist_info_dir).endswith(\".dist-info\")\n            assert Path(self.dist_info_dir, \"METADATA\").exists()\n\n    def _install_namespaces(self, installation_dir, pth_prefix):\n        # XXX: Only required to support the deprecated namespace practice\n        dist = self.distribution\n        if not dist.namespace_packages:\n            return\n\n        src_root = Path(self.project_dir, self.package_dir.get(\"\", \".\")).resolve()\n        installer = _NamespaceInstaller(dist, installation_dir, pth_prefix, src_root)\n        installer.install_namespaces()\n\n    def _find_egg_info_dir(self) -> str | None:\n        parent_dir = Path(self.dist_info_dir).parent if self.dist_info_dir else Path()\n        candidates = map(str, parent_dir.glob(\"*.egg-info\"))\n        return next(candidates, None)\n\n    def _configure_build(\n        self, name: str, unpacked_wheel: StrPath, build_lib: StrPath, tmp_dir: StrPath\n    ):\n        \"\"\"Configure commands to behave in the following ways:\n\n        - Build commands can write to ``build_lib`` if they really want to...\n          (but this folder is expected to be ignored and modules are expected to live\n          in the project directory...)\n        - Binary extensions should be built in-place (editable_mode = True)\n        - Data/header/script files are not part of the \"editable\" specification\n          so they are written directly to the unpacked_wheel directory.\n        \"\"\"\n        # Non-editable files (data, headers, scripts) are written directly to the\n        # unpacked_wheel\n\n        dist = self.distribution\n        wheel = str(unpacked_wheel)\n        build_lib = str(build_lib)\n        data = str(Path(unpacked_wheel, f\"{name}.data\", \"data\"))\n        headers = str(Path(unpacked_wheel, f\"{name}.data\", \"headers\"))\n        scripts = str(Path(unpacked_wheel, f\"{name}.data\", \"scripts\"))\n\n        # egg-info may be generated again to create a manifest (used for package data)\n        egg_info = cast(\n            egg_info_cls, dist.reinitialize_command(\"egg_info\", reinit_subcommands=True)\n        )\n        egg_info.egg_base = str(tmp_dir)\n        egg_info.ignore_egg_info_in_manifest = True\n\n        build = cast(\n            build_cls, dist.reinitialize_command(\"build\", reinit_subcommands=True)\n        )\n        install = cast(\n            install_cls, dist.reinitialize_command(\"install\", reinit_subcommands=True)\n        )\n\n        build.build_platlib = build.build_purelib = build.build_lib = build_lib\n        install.install_purelib = install.install_platlib = install.install_lib = wheel\n        install.install_scripts = build.build_scripts = scripts\n        install.install_headers = headers\n        install.install_data = data\n\n        install_scripts = cast(\n            install_scripts_cls, dist.get_command_obj(\"install_scripts\")\n        )\n        install_scripts.no_ep = True\n\n        build.build_temp = str(tmp_dir)\n\n        build_py = cast(build_py_cls, dist.get_command_obj(\"build_py\"))\n        build_py.compile = False\n        build_py.existing_egg_info_dir = self._find_egg_info_dir()\n\n        self._set_editable_mode()\n\n        build.ensure_finalized()\n        install.ensure_finalized()\n\n    def _set_editable_mode(self):\n        \"\"\"Set the ``editable_mode`` flag in the build sub-commands\"\"\"\n        dist = self.distribution\n        build = dist.get_command_obj(\"build\")\n        for cmd_name in build.get_sub_commands():\n            cmd = dist.get_command_obj(cmd_name)\n            if hasattr(cmd, \"editable_mode\"):\n                cmd.editable_mode = True\n            elif hasattr(cmd, \"inplace\"):\n                cmd.inplace = True  # backward compatibility with distutils\n\n    def _collect_build_outputs(self) -> tuple[list[str], dict[str, str]]:\n        files: list[str] = []\n        mapping: dict[str, str] = {}\n        build = self.get_finalized_command(\"build\")\n\n        for cmd_name in build.get_sub_commands():\n            cmd = self.get_finalized_command(cmd_name)\n            if hasattr(cmd, \"get_outputs\"):\n                files.extend(cmd.get_outputs() or [])\n            if hasattr(cmd, \"get_output_mapping\"):\n                mapping.update(cmd.get_output_mapping() or {})\n\n        return files, mapping\n\n    def _run_build_commands(\n        self,\n        dist_name: str,\n        unpacked_wheel: StrPath,\n        build_lib: StrPath,\n        tmp_dir: StrPath,\n    ) -> tuple[list[str], dict[str, str]]:\n        self._configure_build(dist_name, unpacked_wheel, build_lib, tmp_dir)\n        self._run_build_subcommands()\n        files, mapping = self._collect_build_outputs()\n        self._run_install(\"headers\")\n        self._run_install(\"scripts\")\n        self._run_install(\"data\")\n        return files, mapping\n\n    def _run_build_subcommands(self) -> None:\n        \"\"\"\n        Issue #3501 indicates that some plugins/customizations might rely on:\n\n        1. ``build_py`` not running\n        2. ``build_py`` always copying files to ``build_lib``\n\n        However both these assumptions may be false in editable_wheel.\n        This method implements a temporary workaround to support the ecosystem\n        while the implementations catch up.\n        \"\"\"\n        # TODO: Once plugins/customisations had the chance to catch up, replace\n        #       `self._run_build_subcommands()` with `self.run_command(\"build\")`.\n        #       Also remove _safely_run, TestCustomBuildPy. Suggested date: Aug/2023.\n        build = self.get_finalized_command(\"build\")\n        for name in build.get_sub_commands():\n            cmd = self.get_finalized_command(name)\n            if name == \"build_py\" and type(cmd) is not build_py_cls:\n                self._safely_run(name)\n            else:\n                self.run_command(name)\n\n    def _safely_run(self, cmd_name: str):\n        try:\n            return self.run_command(cmd_name)\n        except Exception:\n            SetuptoolsDeprecationWarning.emit(\n                \"Customization incompatible with editable install\",\n                f\"\"\"\n                {traceback.format_exc()}\n\n                If you are seeing this warning it is very likely that a setuptools\n                plugin or customization overrides the `{cmd_name}` command, without\n                taking into consideration how editable installs run build steps\n                starting from setuptools v64.0.0.\n\n                Plugin authors and developers relying on custom build steps are\n                encouraged to update their `{cmd_name}` implementation considering the\n                information about editable installs in\n                https://setuptools.pypa.io/en/latest/userguide/extension.html.\n\n                For the time being `setuptools` will silence this error and ignore\n                the faulty command, but this behaviour will change in future versions.\n                \"\"\",\n                # TODO: define due_date\n                # There is a series of shortcomings with the available editable install\n                # methods, and they are very controversial. This is something that still\n                # needs work.\n            )\n\n    def _create_wheel_file(self, bdist_wheel):\n        from wheel.wheelfile import WheelFile\n\n        dist_info = self.get_finalized_command(\"dist_info\")\n        dist_name = dist_info.name\n        tag = \"-\".join(bdist_wheel.get_tag())\n        build_tag = \"0.editable\"  # According to PEP 427 needs to start with digit\n        archive_name = f\"{dist_name}-{build_tag}-{tag}.whl\"\n        wheel_path = Path(self.dist_dir, archive_name)\n        if wheel_path.exists():\n            wheel_path.unlink()\n\n        unpacked_wheel = TemporaryDirectory(suffix=archive_name)\n        build_lib = TemporaryDirectory(suffix=\".build-lib\")\n        build_tmp = TemporaryDirectory(suffix=\".build-temp\")\n\n        with unpacked_wheel as unpacked, build_lib as lib, build_tmp as tmp:\n            unpacked_dist_info = Path(unpacked, Path(self.dist_info_dir).name)\n            shutil.copytree(self.dist_info_dir, unpacked_dist_info)\n            self._install_namespaces(unpacked, dist_name)\n            files, mapping = self._run_build_commands(dist_name, unpacked, lib, tmp)\n            strategy = self._select_strategy(dist_name, tag, lib)\n            with strategy, WheelFile(wheel_path, \"w\") as wheel_obj:\n                strategy(wheel_obj, files, mapping)\n                wheel_obj.write_files(unpacked)\n\n        return wheel_path\n\n    def _run_install(self, category: str):\n        has_category = getattr(self.distribution, f\"has_{category}\", None)\n        if has_category and has_category():\n            _logger.info(f\"Installing {category} as non editable\")\n            self.run_command(f\"install_{category}\")\n\n    def _select_strategy(\n        self,\n        name: str,\n        tag: str,\n        build_lib: StrPath,\n    ) -> EditableStrategy:\n        \"\"\"Decides which strategy to use to implement an editable installation.\"\"\"\n        build_name = f\"__editable__.{name}-{tag}\"\n        project_dir = Path(self.project_dir)\n        mode = _EditableMode.convert(self.mode)\n\n        if mode is _EditableMode.STRICT:\n            auxiliary_dir = _empty_dir(Path(self.project_dir, \"build\", build_name))\n            return _LinkTree(self.distribution, name, auxiliary_dir, build_lib)\n\n        packages = _find_packages(self.distribution)\n        has_simple_layout = _simple_layout(packages, self.package_dir, project_dir)\n        is_compat_mode = mode is _EditableMode.COMPAT\n        if set(self.package_dir) == {\"\"} and has_simple_layout or is_compat_mode:\n            # src-layout(ish) is relatively safe for a simple pth file\n            src_dir = self.package_dir.get(\"\", \".\")\n            return _StaticPth(self.distribution, name, [Path(project_dir, src_dir)])\n\n        # Use a MetaPathFinder to avoid adding accidental top-level packages/modules\n        return _TopLevelFinder(self.distribution, name)\n\n\nclass EditableStrategy(Protocol):\n    def __call__(\n        self, wheel: WheelFile, files: list[str], mapping: Mapping[str, str]\n    ) -> object: ...\n    def __enter__(self) -> Self: ...\n    def __exit__(\n        self,\n        _exc_type: type[BaseException] | None,\n        _exc_value: BaseException | None,\n        _traceback: TracebackType | None,\n    ) -> object: ...\n\n\nclass _StaticPth:\n    def __init__(self, dist: Distribution, name: str, path_entries: list[Path]) -> None:\n        self.dist = dist\n        self.name = name\n        self.path_entries = path_entries\n\n    def __call__(self, wheel: WheelFile, files: list[str], mapping: Mapping[str, str]):\n        entries = \"\\n\".join(str(p.resolve()) for p in self.path_entries)\n        contents = _encode_pth(f\"{entries}\\n\")\n        wheel.writestr(f\"__editable__.{self.name}.pth\", contents)\n\n    def __enter__(self) -> Self:\n        msg = f\"\"\"\n        Editable install will be performed using .pth file to extend `sys.path` with:\n        {list(map(os.fspath, self.path_entries))!r}\n        \"\"\"\n        _logger.warning(msg + _LENIENT_WARNING)\n        return self\n\n    def __exit__(\n        self,\n        _exc_type: object,\n        _exc_value: object,\n        _traceback: object,\n    ) -> None:\n        pass\n\n\nclass _LinkTree(_StaticPth):\n    \"\"\"\n    Creates a ``.pth`` file that points to a link tree in the ``auxiliary_dir``.\n\n    This strategy will only link files (not dirs), so it can be implemented in\n    any OS, even if that means using hardlinks instead of symlinks.\n\n    By collocating ``auxiliary_dir`` and the original source code, limitations\n    with hardlinks should be avoided.\n    \"\"\"\n\n    def __init__(\n        self,\n        dist: Distribution,\n        name: str,\n        auxiliary_dir: StrPath,\n        build_lib: StrPath,\n    ) -> None:\n        self.auxiliary_dir = Path(auxiliary_dir)\n        self.build_lib = Path(build_lib).resolve()\n        self._file = dist.get_command_obj(\"build_py\").copy_file\n        super().__init__(dist, name, [self.auxiliary_dir])\n\n    def __call__(self, wheel: WheelFile, files: list[str], mapping: Mapping[str, str]):\n        self._create_links(files, mapping)\n        super().__call__(wheel, files, mapping)\n\n    def _normalize_output(self, file: str) -> str | None:\n        # Files relative to build_lib will be normalized to None\n        with suppress(ValueError):\n            path = Path(file).resolve().relative_to(self.build_lib)\n            return str(path).replace(os.sep, '/')\n        return None\n\n    def _create_file(self, relative_output: str, src_file: str, link=None):\n        dest = self.auxiliary_dir / relative_output\n        if not dest.parent.is_dir():\n            dest.parent.mkdir(parents=True)\n        self._file(src_file, dest, link=link)\n\n    def _create_links(self, outputs, output_mapping: Mapping[str, str]):\n        self.auxiliary_dir.mkdir(parents=True, exist_ok=True)\n        link_type = \"sym\" if _can_symlink_files(self.auxiliary_dir) else \"hard\"\n        normalised = ((self._normalize_output(k), v) for k, v in output_mapping.items())\n        # remove files that are not relative to build_lib\n        mappings = {k: v for k, v in normalised if k is not None}\n\n        for output in outputs:\n            relative = self._normalize_output(output)\n            if relative and relative not in mappings:\n                self._create_file(relative, output)\n\n        for relative, src in mappings.items():\n            self._create_file(relative, src, link=link_type)\n\n    def __enter__(self) -> Self:\n        msg = \"Strict editable install will be performed using a link tree.\\n\"\n        _logger.warning(msg + _STRICT_WARNING)\n        return self\n\n    def __exit__(\n        self,\n        _exc_type: object,\n        _exc_value: object,\n        _traceback: object,\n    ) -> None:\n        msg = f\"\"\"\\n\n        Strict editable installation performed using the auxiliary directory:\n            {self.auxiliary_dir}\n\n        Please be careful to not remove this directory, otherwise you might not be able\n        to import/use your package.\n        \"\"\"\n        InformationOnly.emit(\"Editable installation.\", msg)\n\n\nclass _TopLevelFinder:\n    def __init__(self, dist: Distribution, name: str) -> None:\n        self.dist = dist\n        self.name = name\n\n    def template_vars(self) -> tuple[str, str, dict[str, str], dict[str, list[str]]]:\n        src_root = self.dist.src_root or os.curdir\n        top_level = chain(_find_packages(self.dist), _find_top_level_modules(self.dist))\n        package_dir = self.dist.package_dir or {}\n        roots = _find_package_roots(top_level, package_dir, src_root)\n\n        namespaces_ = dict(\n            chain(\n                _find_namespaces(self.dist.packages or [], roots),\n                ((ns, []) for ns in _find_virtual_namespaces(roots)),\n            )\n        )\n\n        legacy_namespaces = {\n            pkg: find_package_path(pkg, roots, self.dist.src_root or \"\")\n            for pkg in self.dist.namespace_packages or []\n        }\n\n        mapping = {**roots, **legacy_namespaces}\n        # ^-- We need to explicitly add the legacy_namespaces to the mapping to be\n        #     able to import their modules even if another package sharing the same\n        #     namespace is installed in a conventional (non-editable) way.\n\n        name = f\"__editable__.{self.name}.finder\"\n        finder = _normalization.safe_identifier(name)\n        return finder, name, mapping, namespaces_\n\n    def get_implementation(self) -> Iterator[tuple[str, bytes]]:\n        finder, name, mapping, namespaces_ = self.template_vars()\n\n        content = bytes(_finder_template(name, mapping, namespaces_), \"utf-8\")\n        yield (f\"{finder}.py\", content)\n\n        content = _encode_pth(f\"import {finder}; {finder}.install()\")\n        yield (f\"__editable__.{self.name}.pth\", content)\n\n    def __call__(self, wheel: WheelFile, files: list[str], mapping: Mapping[str, str]):\n        for file, content in self.get_implementation():\n            wheel.writestr(file, content)\n\n    def __enter__(self) -> Self:\n        msg = \"Editable install will be performed using a meta path finder.\\n\"\n        _logger.warning(msg + _LENIENT_WARNING)\n        return self\n\n    def __exit__(\n        self,\n        _exc_type: object,\n        _exc_value: object,\n        _traceback: object,\n    ) -> None:\n        msg = \"\"\"\\n\n        Please be careful with folders in your working directory with the same\n        name as your package as they may take precedence during imports.\n        \"\"\"\n        InformationOnly.emit(\"Editable installation.\", msg)\n\n\ndef _encode_pth(content: str) -> bytes:\n    \"\"\"\n    Prior to Python 3.13 (see https://github.com/python/cpython/issues/77102),\n    .pth files are always read with 'locale' encoding, the recommendation\n    from the cpython core developers is to write them as ``open(path, \"w\")``\n    and ignore warnings (see python/cpython#77102, pypa/setuptools#3937).\n    This function tries to simulate this behaviour without having to create an\n    actual file, in a way that supports a range of active Python versions.\n    (There seems to be some variety in the way different version of Python handle\n    ``encoding=None``, not all of them use ``locale.getpreferredencoding(False)``\n    or ``locale.getencoding()``).\n    \"\"\"\n    with io.BytesIO() as buffer:\n        wrapper = io.TextIOWrapper(buffer, encoding=py312.PTH_ENCODING)\n        # TODO: Python 3.13 replace the whole function with `bytes(content, \"utf-8\")`\n        wrapper.write(content)\n        wrapper.flush()\n        buffer.seek(0)\n        return buffer.read()\n\n\ndef _can_symlink_files(base_dir: Path) -> bool:\n    with TemporaryDirectory(dir=str(base_dir.resolve())) as tmp:\n        path1, path2 = Path(tmp, \"file1.txt\"), Path(tmp, \"file2.txt\")\n        path1.write_text(\"file1\", encoding=\"utf-8\")\n        with suppress(AttributeError, NotImplementedError, OSError):\n            os.symlink(path1, path2)\n            if path2.is_symlink() and path2.read_text(encoding=\"utf-8\") == \"file1\":\n                return True\n\n        try:\n            os.link(path1, path2)  # Ensure hard links can be created\n        except Exception as ex:\n            msg = (\n                \"File system does not seem to support either symlinks or hard links. \"\n                \"Strict editable installs require one of them to be supported.\"\n            )\n            raise LinksNotSupported(msg) from ex\n        return False\n\n\ndef _simple_layout(\n    packages: Iterable[str], package_dir: dict[str, str], project_dir: StrPath\n) -> bool:\n    \"\"\"Return ``True`` if:\n    - all packages are contained by the same parent directory, **and**\n    - all packages become importable if the parent directory is added to ``sys.path``.\n\n    >>> _simple_layout(['a'], {\"\": \"src\"}, \"/tmp/myproj\")\n    True\n    >>> _simple_layout(['a', 'a.b'], {\"\": \"src\"}, \"/tmp/myproj\")\n    True\n    >>> _simple_layout(['a', 'a.b'], {}, \"/tmp/myproj\")\n    True\n    >>> _simple_layout(['a', 'a.a1', 'a.a1.a2', 'b'], {\"\": \"src\"}, \"/tmp/myproj\")\n    True\n    >>> _simple_layout(['a', 'a.a1', 'a.a1.a2', 'b'], {\"a\": \"a\", \"b\": \"b\"}, \".\")\n    True\n    >>> _simple_layout(['a', 'a.a1', 'a.a1.a2', 'b'], {\"a\": \"_a\", \"b\": \"_b\"}, \".\")\n    False\n    >>> _simple_layout(['a', 'a.a1', 'a.a1.a2', 'b'], {\"a\": \"_a\"}, \"/tmp/myproj\")\n    False\n    >>> _simple_layout(['a', 'a.a1', 'a.a1.a2', 'b'], {\"a.a1.a2\": \"_a2\"}, \".\")\n    False\n    >>> _simple_layout(['a', 'a.b'], {\"\": \"src\", \"a.b\": \"_ab\"}, \"/tmp/myproj\")\n    False\n    >>> # Special cases, no packages yet:\n    >>> _simple_layout([], {\"\": \"src\"}, \"/tmp/myproj\")\n    True\n    >>> _simple_layout([], {\"a\": \"_a\", \"\": \"src\"}, \"/tmp/myproj\")\n    False\n    \"\"\"\n    layout = {pkg: find_package_path(pkg, package_dir, project_dir) for pkg in packages}\n    if not layout:\n        return set(package_dir) in ({}, {\"\"})\n    parent = os.path.commonpath(starmap(_parent_path, layout.items()))\n    return all(\n        _path.same_path(Path(parent, *key.split('.')), value)\n        for key, value in layout.items()\n    )\n\n\ndef _parent_path(pkg, pkg_path):\n    \"\"\"Infer the parent path containing a package, that if added to ``sys.path`` would\n    allow importing that package.\n    When ``pkg`` is directly mapped into a directory with a different name, return its\n    own path.\n    >>> _parent_path(\"a\", \"src/a\")\n    'src'\n    >>> _parent_path(\"b\", \"src/c\")\n    'src/c'\n    \"\"\"\n    parent = pkg_path[: -len(pkg)] if pkg_path.endswith(pkg) else pkg_path\n    return parent.rstrip(\"/\" + os.sep)\n\n\ndef _find_packages(dist: Distribution) -> Iterator[str]:\n    yield from iter(dist.packages or [])\n\n    py_modules = dist.py_modules or []\n    nested_modules = [mod for mod in py_modules if \".\" in mod]\n    if dist.ext_package:\n        yield dist.ext_package\n    else:\n        ext_modules = dist.ext_modules or []\n        nested_modules += [x.name for x in ext_modules if \".\" in x.name]\n\n    for module in nested_modules:\n        package, _, _ = module.rpartition(\".\")\n        yield package\n\n\ndef _find_top_level_modules(dist: Distribution) -> Iterator[str]:\n    py_modules = dist.py_modules or []\n    yield from (mod for mod in py_modules if \".\" not in mod)\n\n    if not dist.ext_package:\n        ext_modules = dist.ext_modules or []\n        yield from (x.name for x in ext_modules if \".\" not in x.name)\n\n\ndef _find_package_roots(\n    packages: Iterable[str],\n    package_dir: Mapping[str, str],\n    src_root: StrPath,\n) -> dict[str, str]:\n    pkg_roots: dict[str, str] = {\n        pkg: _absolute_root(find_package_path(pkg, package_dir, src_root))\n        for pkg in sorted(packages)\n    }\n\n    return _remove_nested(pkg_roots)\n\n\ndef _absolute_root(path: StrPath) -> str:\n    \"\"\"Works for packages and top-level modules\"\"\"\n    path_ = Path(path)\n    parent = path_.parent\n\n    if path_.exists():\n        return str(path_.resolve())\n    else:\n        return str(parent.resolve() / path_.name)\n\n\ndef _find_virtual_namespaces(pkg_roots: dict[str, str]) -> Iterator[str]:\n    \"\"\"By carefully designing ``package_dir``, it is possible to implement the logical\n    structure of PEP 420 in a package without the corresponding directories.\n\n    Moreover a parent package can be purposefully/accidentally skipped in the discovery\n    phase (e.g. ``find_packages(include=[\"mypkg.*\"])``, when ``mypkg.foo`` is included\n    by ``mypkg`` itself is not).\n    We consider this case to also be a virtual namespace (ignoring the original\n    directory) to emulate a non-editable installation.\n\n    This function will try to find these kinds of namespaces.\n    \"\"\"\n    for pkg in pkg_roots:\n        if \".\" not in pkg:\n            continue\n        parts = pkg.split(\".\")\n        for i in range(len(parts) - 1, 0, -1):\n            partial_name = \".\".join(parts[:i])\n            path = Path(find_package_path(partial_name, pkg_roots, \"\"))\n            if not path.exists() or partial_name not in pkg_roots:\n                # partial_name not in pkg_roots ==> purposefully/accidentally skipped\n                yield partial_name\n\n\ndef _find_namespaces(\n    packages: list[str], pkg_roots: dict[str, str]\n) -> Iterator[tuple[str, list[str]]]:\n    for pkg in packages:\n        path = find_package_path(pkg, pkg_roots, \"\")\n        if Path(path).exists() and not Path(path, \"__init__.py\").exists():\n            yield (pkg, [path])\n\n\ndef _remove_nested(pkg_roots: dict[str, str]) -> dict[str, str]:\n    output = dict(pkg_roots.copy())\n\n    for pkg, path in reversed(list(pkg_roots.items())):\n        if any(\n            pkg != other and _is_nested(pkg, path, other, other_path)\n            for other, other_path in pkg_roots.items()\n        ):\n            output.pop(pkg)\n\n    return output\n\n\ndef _is_nested(pkg: str, pkg_path: str, parent: str, parent_path: str) -> bool:\n    \"\"\"\n    Return ``True`` if ``pkg`` is nested inside ``parent`` both logically and in the\n    file system.\n    >>> _is_nested(\"a.b\", \"path/a/b\", \"a\", \"path/a\")\n    True\n    >>> _is_nested(\"a.b\", \"path/a/b\", \"a\", \"otherpath/a\")\n    False\n    >>> _is_nested(\"a.b\", \"path/a/b\", \"c\", \"path/c\")\n    False\n    >>> _is_nested(\"a.a\", \"path/a/a\", \"a\", \"path/a\")\n    True\n    >>> _is_nested(\"b.a\", \"path/b/a\", \"a\", \"path/a\")\n    False\n    \"\"\"\n    norm_pkg_path = _path.normpath(pkg_path)\n    rest = pkg.replace(parent, \"\", 1).strip(\".\").split(\".\")\n    return pkg.startswith(parent) and norm_pkg_path == _path.normpath(\n        Path(parent_path, *rest)\n    )\n\n\ndef _empty_dir(dir_: _P) -> _P:\n    \"\"\"Create a directory ensured to be empty. Existing files may be removed.\"\"\"\n    _shutil.rmtree(dir_, ignore_errors=True)\n    os.makedirs(dir_)\n    return dir_\n\n\nclass _NamespaceInstaller(namespaces.Installer):\n    def __init__(self, distribution, installation_dir, editable_name, src_root) -> None:\n        self.distribution = distribution\n        self.src_root = src_root\n        self.installation_dir = installation_dir\n        self.editable_name = editable_name\n        self.outputs: list[str] = []\n        self.dry_run = False\n\n    def _get_nspkg_file(self):\n        \"\"\"Installation target.\"\"\"\n        return os.path.join(self.installation_dir, self.editable_name + self.nspkg_ext)\n\n    def _get_root(self):\n        \"\"\"Where the modules/packages should be loaded from.\"\"\"\n        return repr(str(self.src_root))\n\n\n_FINDER_TEMPLATE = \"\"\"\\\nfrom __future__ import annotations\nimport sys\nfrom importlib.machinery import ModuleSpec, PathFinder\nfrom importlib.machinery import all_suffixes as module_suffixes\nfrom importlib.util import spec_from_file_location\nfrom itertools import chain\nfrom pathlib import Path\n\nMAPPING: dict[str, str] = {mapping!r}\nNAMESPACES: dict[str, list[str]] = {namespaces!r}\nPATH_PLACEHOLDER = {name!r} + \".__path_hook__\"\n\n\nclass _EditableFinder:  # MetaPathFinder\n    @classmethod\n    def find_spec(cls, fullname: str, path=None, target=None) -> ModuleSpec | None:  # type: ignore\n        # Top-level packages and modules (we know these exist in the FS)\n        if fullname in MAPPING:\n            pkg_path = MAPPING[fullname]\n            return cls._find_spec(fullname, Path(pkg_path))\n\n        # Handle immediate children modules (required for namespaces to work)\n        # To avoid problems with case sensitivity in the file system we delegate\n        # to the importlib.machinery implementation.\n        parent, _, child = fullname.rpartition(\".\")\n        if parent and parent in MAPPING:\n            return PathFinder.find_spec(fullname, path=[MAPPING[parent]])\n\n        # Other levels of nesting should be handled automatically by importlib\n        # using the parent path.\n        return None\n\n    @classmethod\n    def _find_spec(cls, fullname: str, candidate_path: Path) -> ModuleSpec | None:\n        init = candidate_path / \"__init__.py\"\n        candidates = (candidate_path.with_suffix(x) for x in module_suffixes())\n        for candidate in chain([init], candidates):\n            if candidate.exists():\n                return spec_from_file_location(fullname, candidate)\n        return None\n\n\nclass _EditableNamespaceFinder:  # PathEntryFinder\n    @classmethod\n    def _path_hook(cls, path) -> type[_EditableNamespaceFinder]:\n        if path == PATH_PLACEHOLDER:\n            return cls\n        raise ImportError\n\n    @classmethod\n    def _paths(cls, fullname: str) -> list[str]:\n        paths = NAMESPACES[fullname]\n        if not paths and fullname in MAPPING:\n            paths = [MAPPING[fullname]]\n        # Always add placeholder, for 2 reasons:\n        # 1. __path__ cannot be empty for the spec to be considered namespace.\n        # 2. In the case of nested namespaces, we need to force\n        #    import machinery to query _EditableNamespaceFinder again.\n        return [*paths, PATH_PLACEHOLDER]\n\n    @classmethod\n    def find_spec(cls, fullname: str, target=None) -> ModuleSpec | None:  # type: ignore\n        if fullname in NAMESPACES:\n            spec = ModuleSpec(fullname, None, is_package=True)\n            spec.submodule_search_locations = cls._paths(fullname)\n            return spec\n        return None\n\n    @classmethod\n    def find_module(cls, _fullname) -> None:\n        return None\n\n\ndef install():\n    if not any(finder == _EditableFinder for finder in sys.meta_path):\n        sys.meta_path.append(_EditableFinder)\n\n    if not NAMESPACES:\n        return\n\n    if not any(hook == _EditableNamespaceFinder._path_hook for hook in sys.path_hooks):\n        # PathEntryFinder is needed to create NamespaceSpec without private APIS\n        sys.path_hooks.append(_EditableNamespaceFinder._path_hook)\n    if PATH_PLACEHOLDER not in sys.path:\n        sys.path.append(PATH_PLACEHOLDER)  # Used just to trigger the path hook\n\"\"\"\n\n\ndef _finder_template(\n    name: str, mapping: Mapping[str, str], namespaces: dict[str, list[str]]\n) -> str:\n    \"\"\"Create a string containing the code for the``MetaPathFinder`` and\n    ``PathEntryFinder``.\n    \"\"\"\n    mapping = dict(sorted(mapping.items(), key=lambda p: p[0]))\n    return _FINDER_TEMPLATE.format(name=name, mapping=mapping, namespaces=namespaces)\n\n\nclass LinksNotSupported(errors.FileError):\n    \"\"\"File system does not seem to support either symlinks or hard links.\"\"\"\n\n\nclass _DebuggingTips(SetuptoolsWarning):\n    _SUMMARY = \"Problem in editable installation.\"\n    _DETAILS = \"\"\"\n    An error happened while installing `{project}` in editable mode.\n\n    The following steps are recommended to help debug this problem:\n\n    - Try to install the project normally, without using the editable mode.\n      Does the error still persist?\n      (If it does, try fixing the problem before attempting the editable mode).\n    - If you are using binary extensions, make sure you have all OS-level\n      dependencies installed (e.g. compilers, toolchains, binary libraries, ...).\n    - Try the latest version of setuptools (maybe the error was already fixed).\n    - If you (or your project dependencies) are using any setuptools extension\n      or customization, make sure they support the editable mode.\n\n    After following the steps above, if the problem still persists and\n    you think this is related to how setuptools handles editable installations,\n    please submit a reproducible example\n    (see https://stackoverflow.com/help/minimal-reproducible-example) to:\n\n        https://github.com/pypa/setuptools/issues\n    \"\"\"\n    _SEE_DOCS = \"userguide/development_mode.html\"\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/command/egg_info.py","size":25982,"sha1":"8404d278e5264395458619eeee952e11c28b9bc1","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"setuptools.command.egg_info\n\nCreate a distribution's .egg-info directory and contents\"\"\"\n\nimport functools\nimport os\nimport re\nimport sys\nimport time\nfrom collections.abc import Callable\n\nimport packaging\nimport packaging.requirements\nimport packaging.version\n\nimport setuptools.unicode_utils as unicode_utils\nfrom setuptools import Command\nfrom setuptools.command import bdist_egg\nfrom setuptools.command.sdist import sdist, walk_revctrl\nfrom setuptools.command.setopt import edit_config\nfrom setuptools.glob import glob\n\nfrom .. import _entry_points, _normalization\nfrom .._importlib import metadata\nfrom ..warnings import SetuptoolsDeprecationWarning\nfrom . import _requirestxt\n\nimport distutils.errors\nimport distutils.filelist\nfrom distutils import log\nfrom distutils.errors import DistutilsInternalError\nfrom distutils.filelist import FileList as _FileList\nfrom distutils.util import convert_path\n\nPY_MAJOR = f'{sys.version_info.major}.{sys.version_info.minor}'\n\n\ndef translate_pattern(glob):  # noqa: C901  # is too complex (14)  # FIXME\n    \"\"\"\n    Translate a file path glob like '*.txt' in to a regular expression.\n    This differs from fnmatch.translate which allows wildcards to match\n    directory separators. It also knows about '**/' which matches any number of\n    directories.\n    \"\"\"\n    pat = ''\n\n    # This will split on '/' within [character classes]. This is deliberate.\n    chunks = glob.split(os.path.sep)\n\n    sep = re.escape(os.sep)\n    valid_char = f'[^{sep}]'\n\n    for c, chunk in enumerate(chunks):\n        last_chunk = c == len(chunks) - 1\n\n        # Chunks that are a literal ** are globstars. They match anything.\n        if chunk == '**':\n            if last_chunk:\n                # Match anything if this is the last component\n                pat += '.*'\n            else:\n                # Match '(name/)*'\n                pat += f'(?:{valid_char}+{sep})*'\n            continue  # Break here as the whole path component has been handled\n\n        # Find any special characters in the remainder\n        i = 0\n        chunk_len = len(chunk)\n        while i < chunk_len:\n            char = chunk[i]\n            if char == '*':\n                # Match any number of name characters\n                pat += valid_char + '*'\n            elif char == '?':\n                # Match a name character\n                pat += valid_char\n            elif char == '[':\n                # Character class\n                inner_i = i + 1\n                # Skip initial !/] chars\n                if inner_i < chunk_len and chunk[inner_i] == '!':\n                    inner_i = inner_i + 1\n                if inner_i < chunk_len and chunk[inner_i] == ']':\n                    inner_i = inner_i + 1\n\n                # Loop till the closing ] is found\n                while inner_i < chunk_len and chunk[inner_i] != ']':\n                    inner_i = inner_i + 1\n\n                if inner_i >= chunk_len:\n                    # Got to the end of the string without finding a closing ]\n                    # Do not treat this as a matching group, but as a literal [\n                    pat += re.escape(char)\n                else:\n                    # Grab the insides of the [brackets]\n                    inner = chunk[i + 1 : inner_i]\n                    char_class = ''\n\n                    # Class negation\n                    if inner[0] == '!':\n                        char_class = '^'\n                        inner = inner[1:]\n\n                    char_class += re.escape(inner)\n                    pat += f'[{char_class}]'\n\n                    # Skip to the end ]\n                    i = inner_i\n            else:\n                pat += re.escape(char)\n            i += 1\n\n        # Join each chunk with the dir separator\n        if not last_chunk:\n            pat += sep\n\n    pat += r'\\Z'\n    return re.compile(pat, flags=re.MULTILINE | re.DOTALL)\n\n\nclass InfoCommon:\n    tag_build = None\n    tag_date = None\n\n    @property\n    def name(self):\n        return _normalization.safe_name(self.distribution.get_name())\n\n    def tagged_version(self):\n        tagged = self._maybe_tag(self.distribution.get_version())\n        return _normalization.safe_version(tagged)\n\n    def _maybe_tag(self, version):\n        \"\"\"\n        egg_info may be called more than once for a distribution,\n        in which case the version string already contains all tags.\n        \"\"\"\n        return (\n            version\n            if self.vtags and self._already_tagged(version)\n            else version + self.vtags\n        )\n\n    def _already_tagged(self, version: str) -> bool:\n        # Depending on their format, tags may change with version normalization.\n        # So in addition the regular tags, we have to search for the normalized ones.\n        return version.endswith(self.vtags) or version.endswith(self._safe_tags())\n\n    def _safe_tags(self) -> str:\n        # To implement this we can rely on `safe_version` pretending to be version 0\n        # followed by tags. Then we simply discard the starting 0 (fake version number)\n        try:\n            return _normalization.safe_version(f\"0{self.vtags}\")[1:]\n        except packaging.version.InvalidVersion:\n            return _normalization.safe_name(self.vtags.replace(' ', '.'))\n\n    def tags(self) -> str:\n        version = ''\n        if self.tag_build:\n            version += self.tag_build\n        if self.tag_date:\n            version += time.strftime(\"%Y%m%d\")\n        return version\n\n    vtags = property(tags)\n\n\nclass egg_info(InfoCommon, Command):\n    description = \"create a distribution's .egg-info directory\"\n\n    user_options = [\n        (\n            'egg-base=',\n            'e',\n            \"directory containing .egg-info directories\"\n            \" [default: top of the source tree]\",\n        ),\n        ('tag-date', 'd', \"Add date stamp (e.g. 20050528) to version number\"),\n        ('tag-build=', 'b', \"Specify explicit tag to add to version number\"),\n        ('no-date', 'D', \"Don't include date stamp [default]\"),\n    ]\n\n    boolean_options = ['tag-date']\n    negative_opt = {\n        'no-date': 'tag-date',\n    }\n\n    def initialize_options(self):\n        self.egg_base = None\n        self.egg_name = None\n        self.egg_info = None\n        self.egg_version = None\n        self.ignore_egg_info_in_manifest = False\n\n    ####################################\n    # allow the 'tag_svn_revision' to be detected and\n    # set, supporting sdists built on older Setuptools.\n    @property\n    def tag_svn_revision(self) -> None:\n        pass\n\n    @tag_svn_revision.setter\n    def tag_svn_revision(self, value):\n        pass\n\n    ####################################\n\n    def save_version_info(self, filename) -> None:\n        \"\"\"\n        Materialize the value of date into the\n        build tag. Install build keys in a deterministic order\n        to avoid arbitrary reordering on subsequent builds.\n        \"\"\"\n        # follow the order these keys would have been added\n        # when PYTHONHASHSEED=0\n        egg_info = dict(tag_build=self.tags(), tag_date=0)\n        edit_config(filename, dict(egg_info=egg_info))\n\n    def finalize_options(self) -> None:\n        # Note: we need to capture the current value returned\n        # by `self.tagged_version()`, so we can later update\n        # `self.distribution.metadata.version` without\n        # repercussions.\n        self.egg_name = self.name\n        self.egg_version = self.tagged_version()\n        parsed_version = packaging.version.Version(self.egg_version)\n\n        try:\n            is_version = isinstance(parsed_version, packaging.version.Version)\n            spec = \"%s==%s\" if is_version else \"%s===%s\"\n            packaging.requirements.Requirement(spec % (self.egg_name, self.egg_version))\n        except ValueError as e:\n            raise distutils.errors.DistutilsOptionError(\n                f\"Invalid distribution name or version syntax: {self.egg_name}-{self.egg_version}\"\n            ) from e\n\n        if self.egg_base is None:\n            dirs = self.distribution.package_dir\n            self.egg_base = (dirs or {}).get('', os.curdir)\n\n        self.ensure_dirname('egg_base')\n        self.egg_info = _normalization.filename_component(self.egg_name) + '.egg-info'\n        if self.egg_base != os.curdir:\n            self.egg_info = os.path.join(self.egg_base, self.egg_info)\n\n        # Set package version for the benefit of dumber commands\n        # (e.g. sdist, bdist_wininst, etc.)\n        #\n        self.distribution.metadata.version = self.egg_version\n\n    def _get_egg_basename(self, py_version=PY_MAJOR, platform=None):\n        \"\"\"Compute filename of the output egg. Private API.\"\"\"\n        return _egg_basename(self.egg_name, self.egg_version, py_version, platform)\n\n    def write_or_delete_file(self, what, filename, data, force: bool = False) -> None:\n        \"\"\"Write `data` to `filename` or delete if empty\n\n        If `data` is non-empty, this routine is the same as ``write_file()``.\n        If `data` is empty but not ``None``, this is the same as calling\n        ``delete_file(filename)`.  If `data` is ``None``, then this is a no-op\n        unless `filename` exists, in which case a warning is issued about the\n        orphaned file (if `force` is false), or deleted (if `force` is true).\n        \"\"\"\n        if data:\n            self.write_file(what, filename, data)\n        elif os.path.exists(filename):\n            if data is None and not force:\n                log.warn(\"%s not set in setup(), but %s exists\", what, filename)\n                return\n            else:\n                self.delete_file(filename)\n\n    def write_file(self, what, filename, data) -> None:\n        \"\"\"Write `data` to `filename` (if not a dry run) after announcing it\n\n        `what` is used in a log message to identify what is being written\n        to the file.\n        \"\"\"\n        log.info(\"writing %s to %s\", what, filename)\n        data = data.encode(\"utf-8\")\n        if not self.dry_run:\n            f = open(filename, 'wb')\n            f.write(data)\n            f.close()\n\n    def delete_file(self, filename) -> None:\n        \"\"\"Delete `filename` (if not a dry run) after announcing it\"\"\"\n        log.info(\"deleting %s\", filename)\n        if not self.dry_run:\n            os.unlink(filename)\n\n    def run(self) -> None:\n        # Pre-load to avoid iterating over entry-points while an empty .egg-info\n        # exists in sys.path. See pypa/pyproject-hooks#206\n        writers = list(metadata.entry_points(group='egg_info.writers'))\n\n        self.mkpath(self.egg_info)\n        try:\n            os.utime(self.egg_info, None)\n        except OSError as e:\n            msg = f\"Cannot update time stamp of directory '{self.egg_info}'\"\n            raise distutils.errors.DistutilsFileError(msg) from e\n        for ep in writers:\n            writer = ep.load()\n            writer(self, ep.name, os.path.join(self.egg_info, ep.name))\n\n        # Get rid of native_libs.txt if it was put there by older bdist_egg\n        nl = os.path.join(self.egg_info, \"native_libs.txt\")\n        if os.path.exists(nl):\n            self.delete_file(nl)\n\n        self.find_sources()\n\n    def find_sources(self) -> None:\n        \"\"\"Generate SOURCES.txt manifest file\"\"\"\n        manifest_filename = os.path.join(self.egg_info, \"SOURCES.txt\")\n        mm = manifest_maker(self.distribution)\n        mm.ignore_egg_info_dir = self.ignore_egg_info_in_manifest\n        mm.manifest = manifest_filename\n        mm.run()\n        self.filelist = mm.filelist\n\n\nclass FileList(_FileList):\n    # Implementations of the various MANIFEST.in commands\n\n    def __init__(\n        self, warn=None, debug_print=None, ignore_egg_info_dir: bool = False\n    ) -> None:\n        super().__init__(warn, debug_print)\n        self.ignore_egg_info_dir = ignore_egg_info_dir\n\n    def process_template_line(self, line) -> None:\n        # Parse the line: split it up, make sure the right number of words\n        # is there, and return the relevant words.  'action' is always\n        # defined: it's the first word of the line.  Which of the other\n        # three are defined depends on the action; it'll be either\n        # patterns, (dir and patterns), or (dir_pattern).\n        (action, patterns, dir, dir_pattern) = self._parse_template_line(line)\n\n        action_map: dict[str, Callable] = {\n            'include': self.include,\n            'exclude': self.exclude,\n            'global-include': self.global_include,\n            'global-exclude': self.global_exclude,\n            'recursive-include': functools.partial(\n                self.recursive_include,\n                dir,\n            ),\n            'recursive-exclude': functools.partial(\n                self.recursive_exclude,\n                dir,\n            ),\n            'graft': self.graft,\n            'prune': self.prune,\n        }\n        log_map = {\n            'include': \"warning: no files found matching '%s'\",\n            'exclude': (\"warning: no previously-included files found matching '%s'\"),\n            'global-include': (\n                \"warning: no files found matching '%s' anywhere in distribution\"\n            ),\n            'global-exclude': (\n                \"warning: no previously-included files matching \"\n                \"'%s' found anywhere in distribution\"\n            ),\n            'recursive-include': (\n                \"warning: no files found matching '%s' under directory '%s'\"\n            ),\n            'recursive-exclude': (\n                \"warning: no previously-included files matching \"\n                \"'%s' found under directory '%s'\"\n            ),\n            'graft': \"warning: no directories found matching '%s'\",\n            'prune': \"no previously-included directories found matching '%s'\",\n        }\n\n        try:\n            process_action = action_map[action]\n        except KeyError:\n            msg = f\"Invalid MANIFEST.in: unknown action {action!r} in {line!r}\"\n            raise DistutilsInternalError(msg) from None\n\n        # OK, now we know that the action is valid and we have the\n        # right number of words on the line for that action -- so we\n        # can proceed with minimal error-checking.\n\n        action_is_recursive = action.startswith('recursive-')\n        if action in {'graft', 'prune'}:\n            patterns = [dir_pattern]\n        extra_log_args = (dir,) if action_is_recursive else ()\n        log_tmpl = log_map[action]\n\n        self.debug_print(\n            ' '.join(\n                [action] + ([dir] if action_is_recursive else []) + patterns,\n            )\n        )\n        for pattern in patterns:\n            if not process_action(pattern):\n                log.warn(log_tmpl, pattern, *extra_log_args)\n\n    def _remove_files(self, predicate):\n        \"\"\"\n        Remove all files from the file list that match the predicate.\n        Return True if any matching files were removed\n        \"\"\"\n        found = False\n        for i in range(len(self.files) - 1, -1, -1):\n            if predicate(self.files[i]):\n                self.debug_print(\" removing \" + self.files[i])\n                del self.files[i]\n                found = True\n        return found\n\n    def include(self, pattern):\n        \"\"\"Include files that match 'pattern'.\"\"\"\n        found = [f for f in glob(pattern) if not os.path.isdir(f)]\n        self.extend(found)\n        return bool(found)\n\n    def exclude(self, pattern):\n        \"\"\"Exclude files that match 'pattern'.\"\"\"\n        match = translate_pattern(pattern)\n        return self._remove_files(match.match)\n\n    def recursive_include(self, dir, pattern):\n        \"\"\"\n        Include all files anywhere in 'dir/' that match the pattern.\n        \"\"\"\n        full_pattern = os.path.join(dir, '**', pattern)\n        found = [f for f in glob(full_pattern, recursive=True) if not os.path.isdir(f)]\n        self.extend(found)\n        return bool(found)\n\n    def recursive_exclude(self, dir, pattern):\n        \"\"\"\n        Exclude any file anywhere in 'dir/' that match the pattern.\n        \"\"\"\n        match = translate_pattern(os.path.join(dir, '**', pattern))\n        return self._remove_files(match.match)\n\n    def graft(self, dir):\n        \"\"\"Include all files from 'dir/'.\"\"\"\n        found = [\n            item\n            for match_dir in glob(dir)\n            for item in distutils.filelist.findall(match_dir)\n        ]\n        self.extend(found)\n        return bool(found)\n\n    def prune(self, dir):\n        \"\"\"Filter out files from 'dir/'.\"\"\"\n        match = translate_pattern(os.path.join(dir, '**'))\n        return self._remove_files(match.match)\n\n    def global_include(self, pattern):\n        \"\"\"\n        Include all files anywhere in the current directory that match the\n        pattern. This is very inefficient on large file trees.\n        \"\"\"\n        if self.allfiles is None:\n            self.findall()\n        match = translate_pattern(os.path.join('**', pattern))\n        found = [f for f in self.allfiles if match.match(f)]\n        self.extend(found)\n        return bool(found)\n\n    def global_exclude(self, pattern):\n        \"\"\"\n        Exclude all files anywhere that match the pattern.\n        \"\"\"\n        match = translate_pattern(os.path.join('**', pattern))\n        return self._remove_files(match.match)\n\n    def append(self, item) -> None:\n        if item.endswith('\\r'):  # Fix older sdists built on Windows\n            item = item[:-1]\n        path = convert_path(item)\n\n        if self._safe_path(path):\n            self.files.append(path)\n\n    def extend(self, paths) -> None:\n        self.files.extend(filter(self._safe_path, paths))\n\n    def _repair(self):\n        \"\"\"\n        Replace self.files with only safe paths\n\n        Because some owners of FileList manipulate the underlying\n        ``files`` attribute directly, this method must be called to\n        repair those paths.\n        \"\"\"\n        self.files = list(filter(self._safe_path, self.files))\n\n    def _safe_path(self, path):\n        enc_warn = \"'%s' not %s encodable -- skipping\"\n\n        # To avoid accidental trans-codings errors, first to unicode\n        u_path = unicode_utils.filesys_decode(path)\n        if u_path is None:\n            log.warn(f\"'{path}' in unexpected encoding -- skipping\")\n            return False\n\n        # Must ensure utf-8 encodability\n        utf8_path = unicode_utils.try_encode(u_path, \"utf-8\")\n        if utf8_path is None:\n            log.warn(enc_warn, path, 'utf-8')\n            return False\n\n        try:\n            # ignore egg-info paths\n            is_egg_info = \".egg-info\" in u_path or b\".egg-info\" in utf8_path\n            if self.ignore_egg_info_dir and is_egg_info:\n                return False\n            # accept is either way checks out\n            if os.path.exists(u_path) or os.path.exists(utf8_path):\n                return True\n        # this will catch any encode errors decoding u_path\n        except UnicodeEncodeError:\n            log.warn(enc_warn, path, sys.getfilesystemencoding())\n\n\nclass manifest_maker(sdist):\n    template = \"MANIFEST.in\"\n\n    def initialize_options(self) -> None:\n        self.use_defaults = True\n        self.prune = True\n        self.manifest_only = True\n        self.force_manifest = True\n        self.ignore_egg_info_dir = False\n\n    def finalize_options(self) -> None:\n        pass\n\n    def run(self) -> None:\n        self.filelist = FileList(ignore_egg_info_dir=self.ignore_egg_info_dir)\n        if not os.path.exists(self.manifest):\n            self.write_manifest()  # it must exist so it'll get in the list\n        self.add_defaults()\n        if os.path.exists(self.template):\n            self.read_template()\n        self.add_license_files()\n        self._add_referenced_files()\n        self.prune_file_list()\n        self.filelist.sort()\n        self.filelist.remove_duplicates()\n        self.write_manifest()\n\n    def _manifest_normalize(self, path):\n        path = unicode_utils.filesys_decode(path)\n        return path.replace(os.sep, '/')\n\n    def write_manifest(self) -> None:\n        \"\"\"\n        Write the file list in 'self.filelist' to the manifest file\n        named by 'self.manifest'.\n        \"\"\"\n        self.filelist._repair()\n\n        # Now _repairs should encodability, but not unicode\n        files = [self._manifest_normalize(f) for f in self.filelist.files]\n        msg = f\"writing manifest file '{self.manifest}'\"\n        self.execute(write_file, (self.manifest, files), msg)\n\n    def warn(self, msg) -> None:\n        if not self._should_suppress_warning(msg):\n            sdist.warn(self, msg)\n\n    @staticmethod\n    def _should_suppress_warning(msg):\n        \"\"\"\n        suppress missing-file warnings from sdist\n        \"\"\"\n        return re.match(r\"standard file .*not found\", msg)\n\n    def add_defaults(self) -> None:\n        sdist.add_defaults(self)\n        self.filelist.append(self.template)\n        self.filelist.append(self.manifest)\n        rcfiles = list(walk_revctrl())\n        if rcfiles:\n            self.filelist.extend(rcfiles)\n        elif os.path.exists(self.manifest):\n            self.read_manifest()\n\n        if os.path.exists(\"setup.py\"):\n            # setup.py should be included by default, even if it's not\n            # the script called to create the sdist\n            self.filelist.append(\"setup.py\")\n\n        ei_cmd = self.get_finalized_command('egg_info')\n        self.filelist.graft(ei_cmd.egg_info)\n\n    def add_license_files(self) -> None:\n        license_files = self.distribution.metadata.license_files or []\n        for lf in license_files:\n            log.info(\"adding license file '%s'\", lf)\n        self.filelist.extend(license_files)\n\n    def _add_referenced_files(self):\n        \"\"\"Add files referenced by the config (e.g. `file:` directive) to filelist\"\"\"\n        referenced = getattr(self.distribution, '_referenced_files', [])\n        # ^-- fallback if dist comes from distutils or is a custom class\n        for rf in referenced:\n            log.debug(\"adding file referenced by config '%s'\", rf)\n        self.filelist.extend(referenced)\n\n    def _safe_data_files(self, build_py):\n        \"\"\"\n        The parent class implementation of this method\n        (``sdist``) will try to include data files, which\n        might cause recursion problems when\n        ``include_package_data=True``.\n\n        Therefore, avoid triggering any attempt of\n        analyzing/building the manifest again.\n        \"\"\"\n        if hasattr(build_py, 'get_data_files_without_manifest'):\n            return build_py.get_data_files_without_manifest()\n\n        SetuptoolsDeprecationWarning.emit(\n            \"`build_py` command does not inherit from setuptools' `build_py`.\",\n            \"\"\"\n            Custom 'build_py' does not implement 'get_data_files_without_manifest'.\n            Please extend command classes from setuptools instead of distutils.\n            \"\"\",\n            see_url=\"https://peps.python.org/pep-0632/\",\n            # due_date not defined yet, old projects might still do it?\n        )\n        return build_py.get_data_files()\n\n\ndef write_file(filename, contents) -> None:\n    \"\"\"Create a file with the specified name and write 'contents' (a\n    sequence of strings without line terminators) to it.\n    \"\"\"\n    contents = \"\\n\".join(contents)\n\n    # assuming the contents has been vetted for utf-8 encoding\n    contents = contents.encode(\"utf-8\")\n\n    with open(filename, \"wb\") as f:  # always write POSIX-style manifest\n        f.write(contents)\n\n\ndef write_pkg_info(cmd, basename, filename) -> None:\n    log.info(\"writing %s\", filename)\n    if not cmd.dry_run:\n        metadata = cmd.distribution.metadata\n        metadata.version, oldver = cmd.egg_version, metadata.version\n        metadata.name, oldname = cmd.egg_name, metadata.name\n\n        try:\n            # write unescaped data to PKG-INFO, so older pkg_resources\n            # can still parse it\n            metadata.write_pkg_info(cmd.egg_info)\n        finally:\n            metadata.name, metadata.version = oldname, oldver\n\n        safe = getattr(cmd.distribution, 'zip_safe', None)\n\n        bdist_egg.write_safety_flag(cmd.egg_info, safe)\n\n\ndef warn_depends_obsolete(cmd, basename, filename) -> None:\n    \"\"\"\n    Unused: left to avoid errors when updating (from source) from <= 67.8.\n    Old installations have a .dist-info directory with the entry-point\n    ``depends.txt = setuptools.command.egg_info:warn_depends_obsolete``.\n    This may trigger errors when running the first egg_info in build_meta.\n    TODO: Remove this function in a version sufficiently > 68.\n    \"\"\"\n\n\n# Export API used in entry_points\nwrite_requirements = _requirestxt.write_requirements\nwrite_setup_requirements = _requirestxt.write_setup_requirements\n\n\ndef write_toplevel_names(cmd, basename, filename) -> None:\n    pkgs = dict.fromkeys([\n        k.split('.', 1)[0] for k in cmd.distribution.iter_distribution_names()\n    ])\n    cmd.write_file(\"top-level names\", filename, '\\n'.join(sorted(pkgs)) + '\\n')\n\n\ndef overwrite_arg(cmd, basename, filename) -> None:\n    write_arg(cmd, basename, filename, True)\n\n\ndef write_arg(cmd, basename, filename, force: bool = False) -> None:\n    argname = os.path.splitext(basename)[0]\n    value = getattr(cmd.distribution, argname, None)\n    if value is not None:\n        value = '\\n'.join(value) + '\\n'\n    cmd.write_or_delete_file(argname, filename, value, force)\n\n\ndef write_entries(cmd, basename, filename) -> None:\n    eps = _entry_points.load(cmd.distribution.entry_points)\n    defn = _entry_points.render(eps)\n    cmd.write_or_delete_file('entry points', filename, defn, True)\n\n\ndef _egg_basename(egg_name, egg_version, py_version=None, platform=None):\n    \"\"\"Compute filename of the output egg. Private API.\"\"\"\n    name = _normalization.filename_component(egg_name)\n    version = _normalization.filename_component(egg_version)\n    egg = f\"{name}-{version}-py{py_version or PY_MAJOR}\"\n    if platform:\n        egg += f\"-{platform}\"\n    return egg\n\n\nclass EggInfoDeprecationWarning(SetuptoolsDeprecationWarning):\n    \"\"\"Deprecated behavior warning for EggInfo, bypassing suppression.\"\"\"\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/command/install.py","size":7045,"sha1":"4b75cdd8cea574295f8ab42e00e56f6b380cf279","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"from __future__ import annotations\n\nimport glob\nimport inspect\nimport platform\nfrom collections.abc import Callable\nfrom typing import TYPE_CHECKING, Any, ClassVar, cast\n\nimport setuptools\n\nfrom ..dist import Distribution\nfrom ..warnings import SetuptoolsDeprecationWarning, SetuptoolsWarning\nfrom .bdist_egg import bdist_egg as bdist_egg_cls\n\nimport distutils.command.install as orig\nfrom distutils.errors import DistutilsArgError\n\nif TYPE_CHECKING:\n    # This is only used for a type-cast, don't import at runtime or it'll cause deprecation warnings\n    from .easy_install import easy_install as easy_install_cls\nelse:\n    easy_install_cls = None\n\n\ndef __getattr__(name: str):  # pragma: no cover\n    if name == \"_install\":\n        SetuptoolsDeprecationWarning.emit(\n            \"`setuptools.command._install` was an internal implementation detail \"\n            + \"that was left in for numpy<1.9 support.\",\n            due_date=(2025, 5, 2),  # Originally added on 2024-11-01\n        )\n        return orig.install\n    raise AttributeError(f\"module {__name__!r} has no attribute {name!r}\")\n\n\nclass install(orig.install):\n    \"\"\"Use easy_install to install the package, w/dependencies\"\"\"\n\n    distribution: Distribution  # override distutils.dist.Distribution with setuptools.dist.Distribution\n\n    user_options = orig.install.user_options + [\n        ('old-and-unmanageable', None, \"Try not to use this!\"),\n        (\n            'single-version-externally-managed',\n            None,\n            \"used by system package builders to create 'flat' eggs\",\n        ),\n    ]\n    boolean_options = orig.install.boolean_options + [\n        'old-and-unmanageable',\n        'single-version-externally-managed',\n    ]\n    # Type the same as distutils.command.install.install.sub_commands\n    # Must keep the second tuple item potentially None due to invariance\n    new_commands: ClassVar[list[tuple[str, Callable[[Any], bool] | None]]] = [\n        ('install_egg_info', lambda self: True),\n        ('install_scripts', lambda self: True),\n    ]\n    _nc = dict(new_commands)\n\n    def initialize_options(self):\n        SetuptoolsDeprecationWarning.emit(\n            \"setup.py install is deprecated.\",\n            \"\"\"\n            Please avoid running ``setup.py`` directly.\n            Instead, use pypa/build, pypa/installer or other\n            standards-based tools.\n            \"\"\",\n            see_url=\"https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html\",\n            # TODO: Document how to bootstrap setuptools without install\n            #       (e.g. by unziping the wheel file)\n            #       and then add a due_date to this warning.\n        )\n\n        super().initialize_options()\n        self.old_and_unmanageable = None\n        self.single_version_externally_managed = None\n\n    def finalize_options(self) -> None:\n        super().finalize_options()\n        if self.root:\n            self.single_version_externally_managed = True\n        elif self.single_version_externally_managed:\n            if not self.root and not self.record:\n                raise DistutilsArgError(\n                    \"You must specify --record or --root when building system packages\"\n                )\n\n    def handle_extra_path(self):\n        if self.root or self.single_version_externally_managed:\n            # explicit backward-compatibility mode, allow extra_path to work\n            return orig.install.handle_extra_path(self)\n\n        # Ignore extra_path when installing an egg (or being run by another\n        # command without --root or --single-version-externally-managed\n        self.path_file = None\n        self.extra_dirs = ''\n        return None\n\n    def run(self):\n        # Explicit request for old-style install?  Just do it\n        if self.old_and_unmanageable or self.single_version_externally_managed:\n            return super().run()\n\n        if not self._called_from_setup(inspect.currentframe()):\n            # Run in backward-compatibility mode to support bdist_* commands.\n            super().run()\n        else:\n            self.do_egg_install()\n\n        return None\n\n    @staticmethod\n    def _called_from_setup(run_frame):\n        \"\"\"\n        Attempt to detect whether run() was called from setup() or by another\n        command.  If called by setup(), the parent caller will be the\n        'run_command' method in 'distutils.dist', and *its* caller will be\n        the 'run_commands' method.  If called any other way, the\n        immediate caller *might* be 'run_command', but it won't have been\n        called by 'run_commands'. Return True in that case or if a call stack\n        is unavailable. Return False otherwise.\n        \"\"\"\n        if run_frame is None:\n            msg = \"Call stack not available. bdist_* commands may fail.\"\n            SetuptoolsWarning.emit(msg)\n            if platform.python_implementation() == 'IronPython':\n                msg = \"For best results, pass -X:Frames to enable call stack.\"\n                SetuptoolsWarning.emit(msg)\n            return True\n\n        frames = inspect.getouterframes(run_frame)\n        for frame in frames[2:4]:\n            (caller,) = frame[:1]\n            info = inspect.getframeinfo(caller)\n            caller_module = caller.f_globals.get('__name__', '')\n\n            if caller_module == \"setuptools.dist\" and info.function == \"run_command\":\n                # Starting from v61.0.0 setuptools overwrites dist.run_command\n                continue\n\n            return caller_module == 'distutils.dist' and info.function == 'run_commands'\n\n        return False\n\n    def do_egg_install(self) -> None:\n        easy_install = self.distribution.get_command_class('easy_install')\n\n        cmd = cast(\n            # We'd want to cast easy_install as type[easy_install_cls] but a bug in\n            # mypy makes it think easy_install() returns a Command on Python 3.12+\n            # https://github.com/python/mypy/issues/18088\n            easy_install_cls,\n            easy_install(  # type: ignore[call-arg]\n                self.distribution,\n                args=\"x\",\n                root=self.root,\n                record=self.record,\n            ),\n        )\n        cmd.ensure_finalized()  # finalize before bdist_egg munges install cmd\n        cmd.always_copy_from = '.'  # make sure local-dir eggs get installed\n\n        # pick up setup-dir .egg files only: no .egg-info\n        cmd.package_index.scan(glob.glob('*.egg'))\n\n        self.run_command('bdist_egg')\n        bdist_egg = cast(bdist_egg_cls, self.distribution.get_command_obj('bdist_egg'))\n        args = [bdist_egg.egg_output]\n\n        if setuptools.bootstrap_install_from:\n            # Bootstrap self-installation of setuptools\n            args.insert(0, setuptools.bootstrap_install_from)\n\n        cmd.args = args\n        cmd.run(show_deprecation=False)\n        setuptools.bootstrap_install_from = None\n\n\n# XXX Python 3.1 doesn't see _nc if this is inside the class\ninstall.sub_commands = [\n    cmd for cmd in orig.install.sub_commands if cmd[0] not in install._nc\n] + install.new_commands\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/command/install_egg_info.py","size":2075,"sha1":"0cd4f9aabcdfb501b4db614d2b9a1b922155ebbe","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import os\n\nfrom setuptools import Command, namespaces\nfrom setuptools.archive_util import unpack_archive\n\nfrom .._path import ensure_directory\n\nfrom distutils import dir_util, log\n\n\nclass install_egg_info(namespaces.Installer, Command):\n    \"\"\"Install an .egg-info directory for the package\"\"\"\n\n    description = \"Install an .egg-info directory for the package\"\n\n    user_options = [\n        ('install-dir=', 'd', \"directory to install to\"),\n    ]\n\n    def initialize_options(self):\n        self.install_dir = None\n\n    def finalize_options(self) -> None:\n        self.set_undefined_options('install_lib', ('install_dir', 'install_dir'))\n        ei_cmd = self.get_finalized_command(\"egg_info\")\n        basename = f\"{ei_cmd._get_egg_basename()}.egg-info\"\n        self.source = ei_cmd.egg_info\n        self.target = os.path.join(self.install_dir, basename)\n        self.outputs: list[str] = []\n\n    def run(self) -> None:\n        self.run_command('egg_info')\n        if os.path.isdir(self.target) and not os.path.islink(self.target):\n            dir_util.remove_tree(self.target, dry_run=self.dry_run)\n        elif os.path.exists(self.target):\n            self.execute(os.unlink, (self.target,), \"Removing \" + self.target)\n        if not self.dry_run:\n            ensure_directory(self.target)\n        self.execute(self.copytree, (), f\"Copying {self.source} to {self.target}\")\n        self.install_namespaces()\n\n    def get_outputs(self):\n        return self.outputs\n\n    def copytree(self) -> None:\n        # Copy the .egg-info tree to site-packages\n        def skimmer(src, dst):\n            # filter out source-control directories; note that 'src' is always\n            # a '/'-separated path, regardless of platform.  'dst' is a\n            # platform-specific path.\n            for skip in '.svn/', 'CVS/':\n                if src.startswith(skip) or '/' + skip in src:\n                    return None\n            self.outputs.append(dst)\n            log.debug(\"Copying %s to %s\", src, dst)\n            return dst\n\n        unpack_archive(self.source, self.target, skimmer)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/command/install_lib.py","size":4319,"sha1":"1f7d8b28599a1d797dbc947436e25d98eeb9d6f7","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"from __future__ import annotations\n\nimport os\nimport sys\nfrom itertools import product, starmap\n\nfrom .._path import StrPath\nfrom ..dist import Distribution\n\nimport distutils.command.install_lib as orig\n\n\nclass install_lib(orig.install_lib):\n    \"\"\"Don't add compiled flags to filenames of non-Python files\"\"\"\n\n    distribution: Distribution  # override distutils.dist.Distribution with setuptools.dist.Distribution\n\n    def run(self) -> None:\n        self.build()\n        outfiles = self.install()\n        if outfiles is not None:\n            # always compile, in case we have any extension stubs to deal with\n            self.byte_compile(outfiles)\n\n    def get_exclusions(self):\n        \"\"\"\n        Return a collections.Sized collections.Container of paths to be\n        excluded for single_version_externally_managed installations.\n        \"\"\"\n        all_packages = (\n            pkg\n            for ns_pkg in self._get_SVEM_NSPs()\n            for pkg in self._all_packages(ns_pkg)\n        )\n\n        excl_specs = product(all_packages, self._gen_exclusion_paths())\n        return set(starmap(self._exclude_pkg_path, excl_specs))\n\n    def _exclude_pkg_path(self, pkg, exclusion_path):\n        \"\"\"\n        Given a package name and exclusion path within that package,\n        compute the full exclusion path.\n        \"\"\"\n        parts = pkg.split('.') + [exclusion_path]\n        return os.path.join(self.install_dir, *parts)\n\n    @staticmethod\n    def _all_packages(pkg_name):\n        \"\"\"\n        >>> list(install_lib._all_packages('foo.bar.baz'))\n        ['foo.bar.baz', 'foo.bar', 'foo']\n        \"\"\"\n        while pkg_name:\n            yield pkg_name\n            pkg_name, _sep, _child = pkg_name.rpartition('.')\n\n    def _get_SVEM_NSPs(self):\n        \"\"\"\n        Get namespace packages (list) but only for\n        single_version_externally_managed installations and empty otherwise.\n        \"\"\"\n        # TODO: is it necessary to short-circuit here? i.e. what's the cost\n        # if get_finalized_command is called even when namespace_packages is\n        # False?\n        if not self.distribution.namespace_packages:\n            return []\n\n        install_cmd = self.get_finalized_command('install')\n        svem = install_cmd.single_version_externally_managed\n\n        return self.distribution.namespace_packages if svem else []\n\n    @staticmethod\n    def _gen_exclusion_paths():\n        \"\"\"\n        Generate file paths to be excluded for namespace packages (bytecode\n        cache files).\n        \"\"\"\n        # always exclude the package module itself\n        yield '__init__.py'\n\n        yield '__init__.pyc'\n        yield '__init__.pyo'\n\n        if not hasattr(sys, 'implementation'):\n            return\n\n        base = os.path.join('__pycache__', '__init__.' + sys.implementation.cache_tag)\n        yield base + '.pyc'\n        yield base + '.pyo'\n        yield base + '.opt-1.pyc'\n        yield base + '.opt-2.pyc'\n\n    def copy_tree(\n        self,\n        infile: StrPath,\n        outfile: str,\n        # override: Using actual booleans\n        preserve_mode: bool = True,  # type: ignore[override]\n        preserve_times: bool = True,  # type: ignore[override]\n        preserve_symlinks: bool = False,  # type: ignore[override]\n        level: object = 1,\n    ) -> list[str]:\n        assert preserve_mode\n        assert preserve_times\n        assert not preserve_symlinks\n        exclude = self.get_exclusions()\n\n        if not exclude:\n            return orig.install_lib.copy_tree(self, infile, outfile)\n\n        # Exclude namespace package __init__.py* files from the output\n\n        from setuptools.archive_util import unpack_directory\n\n        from distutils import log\n\n        outfiles: list[str] = []\n\n        def pf(src: str, dst: str):\n            if dst in exclude:\n                log.warn(\"Skipping installation of %s (namespace package)\", dst)\n                return False\n\n            log.info(\"copying %s -> %s\", src, os.path.dirname(dst))\n            outfiles.append(dst)\n            return dst\n\n        unpack_directory(infile, outfile, pf)\n        return outfiles\n\n    def get_outputs(self):\n        outputs = orig.install_lib.get_outputs(self)\n        exclude = self.get_exclusions()\n        if exclude:\n            return [f for f in outputs if f not in exclude]\n        return outputs\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/command/install_scripts.py","size":2637,"sha1":"90dd33703bf0644d483048efcec20c4c1d71ddfd","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"from __future__ import annotations\n\nimport os\nimport sys\n\nfrom .._path import ensure_directory\nfrom ..dist import Distribution\n\nimport distutils.command.install_scripts as orig\nfrom distutils import log\n\n\nclass install_scripts(orig.install_scripts):\n    \"\"\"Do normal script install, plus any egg_info wrapper scripts\"\"\"\n\n    distribution: Distribution  # override distutils.dist.Distribution with setuptools.dist.Distribution\n\n    def initialize_options(self) -> None:\n        orig.install_scripts.initialize_options(self)\n        self.no_ep = False\n\n    def run(self) -> None:\n        self.run_command(\"egg_info\")\n        if self.distribution.scripts:\n            orig.install_scripts.run(self)  # run first to set up self.outfiles\n        else:\n            self.outfiles: list[str] = []\n        if self.no_ep:\n            # don't install entry point scripts into .egg file!\n            return\n        self._install_ep_scripts()\n\n    def _install_ep_scripts(self):\n        # Delay import side-effects\n        from pkg_resources import Distribution, PathMetadata\n\n        from . import easy_install as ei\n\n        ei_cmd = self.get_finalized_command(\"egg_info\")\n        dist = Distribution(\n            ei_cmd.egg_base,\n            PathMetadata(ei_cmd.egg_base, ei_cmd.egg_info),\n            ei_cmd.egg_name,\n            ei_cmd.egg_version,\n        )\n        bs_cmd = self.get_finalized_command('build_scripts')\n        exec_param = getattr(bs_cmd, 'executable', None)\n        writer = ei.ScriptWriter\n        if exec_param == sys.executable:\n            # In case the path to the Python executable contains a space, wrap\n            # it so it's not split up.\n            exec_param = [exec_param]\n        # resolve the writer to the environment\n        writer = writer.best()\n        cmd = writer.command_spec_class.best().from_param(exec_param)\n        for args in writer.get_args(dist, cmd.as_header()):\n            self.write_script(*args)\n\n    def write_script(self, script_name, contents, mode: str = \"t\", *ignored) -> None:\n        \"\"\"Write an executable file to the scripts directory\"\"\"\n        from setuptools.command.easy_install import chmod, current_umask\n\n        log.info(\"Installing %s script to %s\", script_name, self.install_dir)\n        target = os.path.join(self.install_dir, script_name)\n        self.outfiles.append(target)\n\n        encoding = None if \"b\" in mode else \"utf-8\"\n        mask = current_umask()\n        if not self.dry_run:\n            ensure_directory(target)\n            with open(target, \"w\" + mode, encoding=encoding) as f:\n                f.write(contents)\n            chmod(target, 0o777 - mask)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/command/rotate.py","size":2187,"sha1":"46d61ca2b97a7bfe39f4f4aaabe0ba382a7c1c3b","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"from __future__ import annotations\n\nimport os\nfrom typing import ClassVar\n\nfrom .. import Command, _shutil\n\nfrom distutils import log\nfrom distutils.errors import DistutilsOptionError\nfrom distutils.util import convert_path\n\n\nclass rotate(Command):\n    \"\"\"Delete older distributions\"\"\"\n\n    description = \"delete older distributions, keeping N newest files\"\n    user_options = [\n        ('match=', 'm', \"patterns to match (required)\"),\n        ('dist-dir=', 'd', \"directory where the distributions are\"),\n        ('keep=', 'k', \"number of matching distributions to keep\"),\n    ]\n\n    boolean_options: ClassVar[list[str]] = []\n\n    def initialize_options(self):\n        self.match = None\n        self.dist_dir = None\n        self.keep = None\n\n    def finalize_options(self) -> None:\n        if self.match is None:\n            raise DistutilsOptionError(\n                \"Must specify one or more (comma-separated) match patterns \"\n                \"(e.g. '.zip' or '.egg')\"\n            )\n        if self.keep is None:\n            raise DistutilsOptionError(\"Must specify number of files to keep\")\n        try:\n            self.keep = int(self.keep)\n        except ValueError as e:\n            raise DistutilsOptionError(\"--keep must be an integer\") from e\n        if isinstance(self.match, str):\n            self.match = [convert_path(p.strip()) for p in self.match.split(',')]\n        self.set_undefined_options('bdist', ('dist_dir', 'dist_dir'))\n\n    def run(self) -> None:\n        self.run_command(\"egg_info\")\n        from glob import glob\n\n        for pattern in self.match:\n            pattern = self.distribution.get_name() + '*' + pattern\n            files = glob(os.path.join(self.dist_dir, pattern))\n            files = [(os.path.getmtime(f), f) for f in files]\n            files.sort()\n            files.reverse()\n\n            log.info(\"%d file(s) matching %s\", len(files), pattern)\n            files = files[self.keep :]\n            for t, f in files:\n                log.info(\"Deleting %s\", f)\n                if not self.dry_run:\n                    if os.path.isdir(f):\n                        _shutil.rmtree(f)\n                    else:\n                        os.unlink(f)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/command/saveopts.py","size":692,"sha1":"d7c6f17cb0c156ee541138b4c6ae647e3811b1d0","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"from setuptools.command.setopt import edit_config, option_base\n\n\nclass saveopts(option_base):\n    \"\"\"Save command-line options to a file\"\"\"\n\n    description = \"save supplied options to setup.cfg or other config file\"\n\n    def run(self) -> None:\n        dist = self.distribution\n        settings: dict[str, dict[str, str]] = {}\n\n        for cmd in dist.command_options:\n            if cmd == 'saveopts':\n                continue  # don't save our own options!\n\n            for opt, (src, val) in dist.get_option_dict(cmd).items():\n                if src == \"command line\":\n                    settings.setdefault(cmd, {})[opt] = val\n\n        edit_config(self.filename, settings, self.dry_run)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/command/sdist.py","size":7374,"sha1":"0774185d008f57cd9538043074c2689bedcceec3","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"from __future__ import annotations\n\nimport contextlib\nimport os\nimport re\nfrom itertools import chain\nfrom typing import ClassVar\n\nfrom .._importlib import metadata\nfrom ..dist import Distribution\nfrom .build import _ORIGINAL_SUBCOMMANDS\n\nimport distutils.command.sdist as orig\nfrom distutils import log\n\n_default_revctrl = list\n\n\ndef walk_revctrl(dirname=''):\n    \"\"\"Find all files under revision control\"\"\"\n    for ep in metadata.entry_points(group='setuptools.file_finders'):\n        yield from ep.load()(dirname)\n\n\nclass sdist(orig.sdist):\n    \"\"\"Smart sdist that finds anything supported by revision control\"\"\"\n\n    user_options = [\n        ('formats=', None, \"formats for source distribution (comma-separated list)\"),\n        (\n            'keep-temp',\n            'k',\n            \"keep the distribution tree around after creating \" + \"archive file(s)\",\n        ),\n        (\n            'dist-dir=',\n            'd',\n            \"directory to put the source distribution archive(s) in [default: dist]\",\n        ),\n        (\n            'owner=',\n            'u',\n            \"Owner name used when creating a tar file [default: current user]\",\n        ),\n        (\n            'group=',\n            'g',\n            \"Group name used when creating a tar file [default: current group]\",\n        ),\n    ]\n\n    distribution: Distribution  # override distutils.dist.Distribution with setuptools.dist.Distribution\n    negative_opt: ClassVar[dict[str, str]] = {}\n\n    README_EXTENSIONS = ['', '.rst', '.txt', '.md']\n    READMES = tuple(f'README{ext}' for ext in README_EXTENSIONS)\n\n    def run(self) -> None:\n        self.run_command('egg_info')\n        ei_cmd = self.get_finalized_command('egg_info')\n        self.filelist = ei_cmd.filelist\n        self.filelist.append(os.path.join(ei_cmd.egg_info, 'SOURCES.txt'))\n        self.check_readme()\n\n        # Run sub commands\n        for cmd_name in self.get_sub_commands():\n            self.run_command(cmd_name)\n\n        self.make_distribution()\n\n        dist_files = getattr(self.distribution, 'dist_files', [])\n        for file in self.archive_files:\n            data = ('sdist', '', file)\n            if data not in dist_files:\n                dist_files.append(data)\n\n    def initialize_options(self) -> None:\n        orig.sdist.initialize_options(self)\n\n    def make_distribution(self) -> None:\n        \"\"\"\n        Workaround for #516\n        \"\"\"\n        with self._remove_os_link():\n            orig.sdist.make_distribution(self)\n\n    @staticmethod\n    @contextlib.contextmanager\n    def _remove_os_link():\n        \"\"\"\n        In a context, remove and restore os.link if it exists\n        \"\"\"\n\n        class NoValue:\n            pass\n\n        orig_val = getattr(os, 'link', NoValue)\n        try:\n            del os.link\n        except Exception:\n            pass\n        try:\n            yield\n        finally:\n            if orig_val is not NoValue:\n                os.link = orig_val\n\n    def add_defaults(self) -> None:\n        super().add_defaults()\n        self._add_defaults_build_sub_commands()\n\n    def _add_defaults_optional(self):\n        super()._add_defaults_optional()\n        if os.path.isfile('pyproject.toml'):\n            self.filelist.append('pyproject.toml')\n\n    def _add_defaults_python(self):\n        \"\"\"getting python files\"\"\"\n        if self.distribution.has_pure_modules():\n            build_py = self.get_finalized_command('build_py')\n            self.filelist.extend(build_py.get_source_files())\n            self._add_data_files(self._safe_data_files(build_py))\n\n    def _add_defaults_build_sub_commands(self):\n        build = self.get_finalized_command(\"build\")\n        missing_cmds = set(build.get_sub_commands()) - _ORIGINAL_SUBCOMMANDS\n        # ^-- the original built-in sub-commands are already handled by default.\n        cmds = (self.get_finalized_command(c) for c in missing_cmds)\n        files = (c.get_source_files() for c in cmds if hasattr(c, \"get_source_files\"))\n        self.filelist.extend(chain.from_iterable(files))\n\n    def _safe_data_files(self, build_py):\n        \"\"\"\n        Since the ``sdist`` class is also used to compute the MANIFEST\n        (via :obj:`setuptools.command.egg_info.manifest_maker`),\n        there might be recursion problems when trying to obtain the list of\n        data_files and ``include_package_data=True`` (which in turn depends on\n        the files included in the MANIFEST).\n\n        To avoid that, ``manifest_maker`` should be able to overwrite this\n        method and avoid recursive attempts to build/analyze the MANIFEST.\n        \"\"\"\n        return build_py.data_files\n\n    def _add_data_files(self, data_files):\n        \"\"\"\n        Add data files as found in build_py.data_files.\n        \"\"\"\n        self.filelist.extend(\n            os.path.join(src_dir, name)\n            for _, src_dir, _, filenames in data_files\n            for name in filenames\n        )\n\n    def _add_defaults_data_files(self):\n        try:\n            super()._add_defaults_data_files()\n        except TypeError:\n            log.warn(\"data_files contains unexpected objects\")\n\n    def prune_file_list(self) -> None:\n        super().prune_file_list()\n        # Prevent accidental inclusion of test-related cache dirs at the project root\n        sep = re.escape(os.sep)\n        self.filelist.exclude_pattern(r\"^(\\.tox|\\.nox|\\.venv)\" + sep, is_regex=True)\n\n    def check_readme(self) -> None:\n        for f in self.READMES:\n            if os.path.exists(f):\n                return\n        else:\n            self.warn(\n                \"standard file not found: should have one of \" + ', '.join(self.READMES)\n            )\n\n    def make_release_tree(self, base_dir, files) -> None:\n        orig.sdist.make_release_tree(self, base_dir, files)\n\n        # Save any egg_info command line options used to create this sdist\n        dest = os.path.join(base_dir, 'setup.cfg')\n        if hasattr(os, 'link') and os.path.exists(dest):\n            # unlink and re-copy, since it might be hard-linked, and\n            # we don't want to change the source version\n            os.unlink(dest)\n            self.copy_file('setup.cfg', dest)\n\n        self.get_finalized_command('egg_info').save_version_info(dest)\n\n    def _manifest_is_not_generated(self):\n        # check for special comment used in 2.7.1 and higher\n        if not os.path.isfile(self.manifest):\n            return False\n\n        with open(self.manifest, 'rb') as fp:\n            first_line = fp.readline()\n        return first_line != b'# file GENERATED by distutils, do NOT edit\\n'\n\n    def read_manifest(self):\n        \"\"\"Read the manifest file (named by 'self.manifest') and use it to\n        fill in 'self.filelist', the list of files to include in the source\n        distribution.\n        \"\"\"\n        log.info(\"reading manifest file '%s'\", self.manifest)\n        manifest = open(self.manifest, 'rb')\n        for bytes_line in manifest:\n            # The manifest must contain UTF-8. See #303.\n            try:\n                line = bytes_line.decode('UTF-8')\n            except UnicodeDecodeError:\n                log.warn(f\"{line!r} not UTF-8 decodable -- skipping\")\n                continue\n            # ignore comments and blank lines\n            line = line.strip()\n            if line.startswith('#') or not line:\n                continue\n            self.filelist.append(line)\n        manifest.close()\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/command/setopt.py","size":5100,"sha1":"8c04322c34762bc49e7a9702678fc0380dc73c18","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import configparser\nimport os\n\nfrom .. import Command\nfrom ..unicode_utils import _cfg_read_utf8_with_fallback\n\nimport distutils\nfrom distutils import log\nfrom distutils.errors import DistutilsOptionError\nfrom distutils.util import convert_path\n\n__all__ = ['config_file', 'edit_config', 'option_base', 'setopt']\n\n\ndef config_file(kind=\"local\"):\n    \"\"\"Get the filename of the distutils, local, global, or per-user config\n\n    `kind` must be one of \"local\", \"global\", or \"user\"\n    \"\"\"\n    if kind == 'local':\n        return 'setup.cfg'\n    if kind == 'global':\n        return os.path.join(os.path.dirname(distutils.__file__), 'distutils.cfg')\n    if kind == 'user':\n        dot = os.name == 'posix' and '.' or ''\n        return os.path.expanduser(convert_path(f\"~/{dot}pydistutils.cfg\"))\n    raise ValueError(\"config_file() type must be 'local', 'global', or 'user'\", kind)\n\n\ndef edit_config(filename, settings, dry_run=False):\n    \"\"\"Edit a configuration file to include `settings`\n\n    `settings` is a dictionary of dictionaries or ``None`` values, keyed by\n    command/section name.  A ``None`` value means to delete the entire section,\n    while a dictionary lists settings to be changed or deleted in that section.\n    A setting of ``None`` means to delete that setting.\n    \"\"\"\n    log.debug(\"Reading configuration from %s\", filename)\n    opts = configparser.RawConfigParser()\n    opts.optionxform = lambda optionstr: optionstr  # type: ignore[method-assign] # overriding method\n    _cfg_read_utf8_with_fallback(opts, filename)\n\n    for section, options in settings.items():\n        if options is None:\n            log.info(\"Deleting section [%s] from %s\", section, filename)\n            opts.remove_section(section)\n        else:\n            if not opts.has_section(section):\n                log.debug(\"Adding new section [%s] to %s\", section, filename)\n                opts.add_section(section)\n            for option, value in options.items():\n                if value is None:\n                    log.debug(\"Deleting %s.%s from %s\", section, option, filename)\n                    opts.remove_option(section, option)\n                    if not opts.options(section):\n                        log.info(\n                            \"Deleting empty [%s] section from %s\", section, filename\n                        )\n                        opts.remove_section(section)\n                else:\n                    log.debug(\n                        \"Setting %s.%s to %r in %s\", section, option, value, filename\n                    )\n                    opts.set(section, option, value)\n\n    log.info(\"Writing %s\", filename)\n    if not dry_run:\n        with open(filename, 'w', encoding=\"utf-8\") as f:\n            opts.write(f)\n\n\nclass option_base(Command):\n    \"\"\"Abstract base class for commands that mess with config files\"\"\"\n\n    user_options = [\n        ('global-config', 'g', \"save options to the site-wide distutils.cfg file\"),\n        ('user-config', 'u', \"save options to the current user's pydistutils.cfg file\"),\n        ('filename=', 'f', \"configuration file to use (default=setup.cfg)\"),\n    ]\n\n    boolean_options = [\n        'global-config',\n        'user-config',\n    ]\n\n    def initialize_options(self):\n        self.global_config = None\n        self.user_config = None\n        self.filename = None\n\n    def finalize_options(self):\n        filenames = []\n        if self.global_config:\n            filenames.append(config_file('global'))\n        if self.user_config:\n            filenames.append(config_file('user'))\n        if self.filename is not None:\n            filenames.append(self.filename)\n        if not filenames:\n            filenames.append(config_file('local'))\n        if len(filenames) > 1:\n            raise DistutilsOptionError(\n                \"Must specify only one configuration file option\", filenames\n            )\n        (self.filename,) = filenames\n\n\nclass setopt(option_base):\n    \"\"\"Save command-line options to a file\"\"\"\n\n    description = \"set an option in setup.cfg or another config file\"\n\n    user_options = [\n        ('command=', 'c', 'command to set an option for'),\n        ('option=', 'o', 'option to set'),\n        ('set-value=', 's', 'value of the option'),\n        ('remove', 'r', 'remove (unset) the value'),\n    ] + option_base.user_options\n\n    boolean_options = option_base.boolean_options + ['remove']\n\n    def initialize_options(self):\n        option_base.initialize_options(self)\n        self.command = None\n        self.option = None\n        self.set_value = None\n        self.remove = None\n\n    def finalize_options(self) -> None:\n        option_base.finalize_options(self)\n        if self.command is None or self.option is None:\n            raise DistutilsOptionError(\"Must specify --command *and* --option\")\n        if self.set_value is None and not self.remove:\n            raise DistutilsOptionError(\"Must specify --set-value or --remove\")\n\n    def run(self) -> None:\n        edit_config(\n            self.filename,\n            {self.command: {self.option.replace('-', '_'): self.set_value}},\n            self.dry_run,\n        )\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/command/test.py","size":1343,"sha1":"eaa86dec5f953b4712d3918aaf961ac23a0ef60b","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"from __future__ import annotations\n\nfrom setuptools import Command\nfrom setuptools.warnings import SetuptoolsDeprecationWarning\n\n\n# Would restrict to Literal[\"test\"], but mypy doesn't support it: https://github.com/python/mypy/issues/8203\ndef __getattr__(name: str) -> type[_test]:\n    if name == 'test':\n        SetuptoolsDeprecationWarning.emit(\n            \"The test command is disabled and references to it are deprecated.\",\n            \"Please remove any references to `setuptools.command.test` in all \"\n            \"supported versions of the affected package.\",\n            due_date=(2024, 11, 15),\n            stacklevel=2,\n        )\n        return _test\n    raise AttributeError(name)\n\n\nclass _test(Command):\n    \"\"\"\n    Stub to warn when test command is referenced or used.\n    \"\"\"\n\n    description = \"stub for old test command (do not use)\"\n\n    user_options = [\n        ('test-module=', 'm', \"Run 'test_suite' in specified module\"),\n        (\n            'test-suite=',\n            's',\n            \"Run single test, case or suite (e.g. 'module.test_suite')\",\n        ),\n        ('test-runner=', 'r', \"Test runner to use\"),\n    ]\n\n    def initialize_options(self):\n        pass\n\n    def finalize_options(self):\n        pass\n\n    def run(self):\n        raise RuntimeError(\"Support for the test command was removed in Setuptools 72\")\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/compat/__init__.py","size":0,"sha1":"da39a3ee5e6b4b0d3255bfef95601890afd80709","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":""},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/compat/py310.py","size":141,"sha1":"4a1454e82c83c16b75199979e2d9bd38bb66c33a","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import sys\n\n__all__ = ['tomllib']\n\n\nif sys.version_info >= (3, 11):\n    import tomllib\nelse:  # pragma: no cover\n    import tomli as tomllib\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/compat/py311.py","size":790,"sha1":"244136282823b7a34bdb2bf8b97edc18af6f7eff","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"from __future__ import annotations\n\nimport shutil\nimport sys\nfrom typing import TYPE_CHECKING, Any, Callable\n\nif TYPE_CHECKING:\n    from _typeshed import ExcInfo, StrOrBytesPath\n    from typing_extensions import TypeAlias\n\n# Same as shutil._OnExcCallback from typeshed\n_OnExcCallback: TypeAlias = Callable[[Callable[..., Any], str, BaseException], object]\n\n\ndef shutil_rmtree(\n    path: StrOrBytesPath,\n    ignore_errors: bool = False,\n    onexc: _OnExcCallback | None = None,\n) -> None:\n    if sys.version_info >= (3, 12):\n        return shutil.rmtree(path, ignore_errors, onexc=onexc)\n\n    def _handler(fn: Callable[..., Any], path: str, excinfo: ExcInfo) -> None:\n        if onexc:\n            onexc(fn, path, excinfo[1])\n\n    return shutil.rmtree(path, ignore_errors, onerror=_handler)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/compat/py312.py","size":366,"sha1":"4559703d03ca04175de0668c81eab4a675ef801d","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"from __future__ import annotations\n\nimport sys\n\nif sys.version_info >= (3, 12, 4):\n    # Python 3.13 should support `.pth` files encoded in UTF-8\n    # See discussion in https://github.com/python/cpython/issues/77102\n    PTH_ENCODING: str | None = \"utf-8\"\nelse:\n    from .py39 import LOCALE_ENCODING\n\n    # PTH_ENCODING = \"locale\"\n    PTH_ENCODING = LOCALE_ENCODING\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/compat/py39.py","size":493,"sha1":"fbb495d6bc552623ffa5d7d8c0dd3a1e27893663","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import sys\n\n# Explicitly use the ``\"locale\"`` encoding in versions that support it,\n# otherwise just rely on the implicit handling of ``encoding=None``.\n# Since all platforms that support ``EncodingWarning`` also support\n# ``encoding=\"locale\"``, this can be used to suppress the warning.\n# However, please try to use UTF-8 when possible\n# (.pth files are the notorious exception: python/cpython#77102, pypa/setuptools#3937).\nLOCALE_ENCODING = \"locale\" if sys.version_info >= (3, 10) else None\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/config/__init__.py","size":1499,"sha1":"3ef31c49bca97d76e890dd3173f8d1b053585482","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"For backward compatibility, expose main functions from\n``setuptools.config.setupcfg``\n\"\"\"\n\nfrom functools import wraps\nfrom typing import Callable, TypeVar, cast\n\nfrom ..warnings import SetuptoolsDeprecationWarning\nfrom . import setupcfg\n\nFn = TypeVar(\"Fn\", bound=Callable)\n\n__all__ = ('parse_configuration', 'read_configuration')\n\n\ndef _deprecation_notice(fn: Fn) -> Fn:\n    @wraps(fn)\n    def _wrapper(*args, **kwargs):\n        SetuptoolsDeprecationWarning.emit(\n            \"Deprecated API usage.\",\n            f\"\"\"\n            As setuptools moves its configuration towards `pyproject.toml`,\n            `{__name__}.{fn.__name__}` became deprecated.\n\n            For the time being, you can use the `{setupcfg.__name__}` module\n            to access a backward compatible API, but this module is provisional\n            and might be removed in the future.\n\n            To read project metadata, consider using\n            ``build.util.project_wheel_metadata`` (https://pypi.org/project/build/).\n            For simple scenarios, you can also try parsing the file directly\n            with the help of ``configparser``.\n            \"\"\",\n            # due_date not defined yet, because the community still heavily relies on it\n            # Warning introduced in 24 Mar 2022\n        )\n        return fn(*args, **kwargs)\n\n    return cast(Fn, _wrapper)\n\n\nread_configuration = _deprecation_notice(setupcfg.read_configuration)\nparse_configuration = _deprecation_notice(setupcfg.parse_configuration)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/config/_apply_pyprojecttoml.py","size":16988,"sha1":"a3292e27d21e43e5ee3f99d61f67e65bb71c444e","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"Translation layer between pyproject config and setuptools distribution and\nmetadata objects.\n\nThe distribution and metadata objects are modeled after (an old version of)\ncore metadata, therefore configs in the format specified for ``pyproject.toml``\nneed to be processed before being applied.\n\n**PRIVATE MODULE**: API reserved for setuptools internal usage only.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nimport os\nfrom collections.abc import Mapping\nfrom email.headerregistry import Address\nfrom functools import partial, reduce\nfrom inspect import cleandoc\nfrom itertools import chain\nfrom types import MappingProxyType\nfrom typing import TYPE_CHECKING, Any, Callable, TypeVar, Union\n\nfrom .. import _static\nfrom .._path import StrPath\nfrom ..errors import RemovedConfigError\nfrom ..extension import Extension\nfrom ..warnings import SetuptoolsWarning\n\nif TYPE_CHECKING:\n    from typing_extensions import TypeAlias\n\n    from setuptools._importlib import metadata\n    from setuptools.dist import Distribution\n\n    from distutils.dist import _OptionsList  # Comes from typeshed\n\n\nEMPTY: Mapping = MappingProxyType({})  # Immutable dict-like\n_ProjectReadmeValue: TypeAlias = Union[str, dict[str, str]]\n_Correspondence: TypeAlias = Callable[[\"Distribution\", Any, Union[StrPath, None]], None]\n_T = TypeVar(\"_T\")\n\n_logger = logging.getLogger(__name__)\n\n\ndef apply(dist: Distribution, config: dict, filename: StrPath) -> Distribution:\n    \"\"\"Apply configuration dict read with :func:`read_configuration`\"\"\"\n\n    if not config:\n        return dist  # short-circuit unrelated pyproject.toml file\n\n    root_dir = os.path.dirname(filename) or \".\"\n\n    _apply_project_table(dist, config, root_dir)\n    _apply_tool_table(dist, config, filename)\n\n    current_directory = os.getcwd()\n    os.chdir(root_dir)\n    try:\n        dist._finalize_requires()\n        dist._finalize_license_files()\n    finally:\n        os.chdir(current_directory)\n\n    return dist\n\n\ndef _apply_project_table(dist: Distribution, config: dict, root_dir: StrPath):\n    orig_config = config.get(\"project\", {})\n    if not orig_config:\n        return  # short-circuit\n\n    project_table = {k: _static.attempt_conversion(v) for k, v in orig_config.items()}\n    _handle_missing_dynamic(dist, project_table)\n    _unify_entry_points(project_table)\n\n    for field, value in project_table.items():\n        norm_key = json_compatible_key(field)\n        corresp = PYPROJECT_CORRESPONDENCE.get(norm_key, norm_key)\n        if callable(corresp):\n            corresp(dist, value, root_dir)\n        else:\n            _set_config(dist, corresp, value)\n\n\ndef _apply_tool_table(dist: Distribution, config: dict, filename: StrPath):\n    tool_table = config.get(\"tool\", {}).get(\"setuptools\", {})\n    if not tool_table:\n        return  # short-circuit\n\n    for field, value in tool_table.items():\n        norm_key = json_compatible_key(field)\n\n        if norm_key in TOOL_TABLE_REMOVALS:\n            suggestion = cleandoc(TOOL_TABLE_REMOVALS[norm_key])\n            msg = f\"\"\"\n            The parameter `tool.setuptools.{field}` was long deprecated\n            and has been removed from `pyproject.toml`.\n            \"\"\"\n            raise RemovedConfigError(\"\\n\".join([cleandoc(msg), suggestion]))\n\n        norm_key = TOOL_TABLE_RENAMES.get(norm_key, norm_key)\n        corresp = TOOL_TABLE_CORRESPONDENCE.get(norm_key, norm_key)\n        if callable(corresp):\n            corresp(dist, value)\n        else:\n            _set_config(dist, corresp, value)\n\n    _copy_command_options(config, dist, filename)\n\n\ndef _handle_missing_dynamic(dist: Distribution, project_table: dict):\n    \"\"\"Be temporarily forgiving with ``dynamic`` fields not listed in ``dynamic``\"\"\"\n    dynamic = set(project_table.get(\"dynamic\", []))\n    for field, getter in _PREVIOUSLY_DEFINED.items():\n        if not (field in project_table or field in dynamic):\n            value = getter(dist)\n            if value:\n                _MissingDynamic.emit(field=field, value=value)\n                project_table[field] = _RESET_PREVIOUSLY_DEFINED.get(field)\n\n\ndef json_compatible_key(key: str) -> str:\n    \"\"\"As defined in :pep:`566#json-compatible-metadata`\"\"\"\n    return key.lower().replace(\"-\", \"_\")\n\n\ndef _set_config(dist: Distribution, field: str, value: Any):\n    val = _PREPROCESS.get(field, _noop)(dist, value)\n    setter = getattr(dist.metadata, f\"set_{field}\", None)\n    if setter:\n        setter(val)\n    elif hasattr(dist.metadata, field) or field in SETUPTOOLS_PATCHES:\n        setattr(dist.metadata, field, val)\n    else:\n        setattr(dist, field, val)\n\n\n_CONTENT_TYPES = {\n    \".md\": \"text/markdown\",\n    \".rst\": \"text/x-rst\",\n    \".txt\": \"text/plain\",\n}\n\n\ndef _guess_content_type(file: str) -> str | None:\n    _, ext = os.path.splitext(file.lower())\n    if not ext:\n        return None\n\n    if ext in _CONTENT_TYPES:\n        return _static.Str(_CONTENT_TYPES[ext])\n\n    valid = \", \".join(f\"{k} ({v})\" for k, v in _CONTENT_TYPES.items())\n    msg = f\"only the following file extensions are recognized: {valid}.\"\n    raise ValueError(f\"Undefined content type for {file}, {msg}\")\n\n\ndef _long_description(\n    dist: Distribution, val: _ProjectReadmeValue, root_dir: StrPath | None\n):\n    from setuptools.config import expand\n\n    file: str | tuple[()]\n    if isinstance(val, str):\n        file = val\n        text = expand.read_files(file, root_dir)\n        ctype = _guess_content_type(file)\n    else:\n        file = val.get(\"file\") or ()\n        text = val.get(\"text\") or expand.read_files(file, root_dir)\n        ctype = val[\"content-type\"]\n\n    # XXX: Is it completely safe to assume static?\n    _set_config(dist, \"long_description\", _static.Str(text))\n\n    if ctype:\n        _set_config(dist, \"long_description_content_type\", _static.Str(ctype))\n\n    if file:\n        dist._referenced_files.add(file)\n\n\ndef _license(dist: Distribution, val: dict, root_dir: StrPath | None):\n    from setuptools.config import expand\n\n    if \"file\" in val:\n        # XXX: Is it completely safe to assume static?\n        value = expand.read_files([val[\"file\"]], root_dir)\n        _set_config(dist, \"license\", _static.Str(value))\n        dist._referenced_files.add(val[\"file\"])\n    else:\n        _set_config(dist, \"license\", _static.Str(val[\"text\"]))\n\n\ndef _people(dist: Distribution, val: list[dict], _root_dir: StrPath | None, kind: str):\n    field = []\n    email_field = []\n    for person in val:\n        if \"name\" not in person:\n            email_field.append(person[\"email\"])\n        elif \"email\" not in person:\n            field.append(person[\"name\"])\n        else:\n            addr = Address(display_name=person[\"name\"], addr_spec=person[\"email\"])\n            email_field.append(str(addr))\n\n    if field:\n        _set_config(dist, kind, _static.Str(\", \".join(field)))\n    if email_field:\n        _set_config(dist, f\"{kind}_email\", _static.Str(\", \".join(email_field)))\n\n\ndef _project_urls(dist: Distribution, val: dict, _root_dir: StrPath | None):\n    _set_config(dist, \"project_urls\", val)\n\n\ndef _python_requires(dist: Distribution, val: str, _root_dir: StrPath | None):\n    _set_config(dist, \"python_requires\", _static.SpecifierSet(val))\n\n\ndef _dependencies(dist: Distribution, val: list, _root_dir: StrPath | None):\n    if getattr(dist, \"install_requires\", []):\n        msg = \"`install_requires` overwritten in `pyproject.toml` (dependencies)\"\n        SetuptoolsWarning.emit(msg)\n    dist.install_requires = val\n\n\ndef _optional_dependencies(dist: Distribution, val: dict, _root_dir: StrPath | None):\n    if getattr(dist, \"extras_require\", None):\n        msg = \"`extras_require` overwritten in `pyproject.toml` (optional-dependencies)\"\n        SetuptoolsWarning.emit(msg)\n    dist.extras_require = val\n\n\ndef _ext_modules(dist: Distribution, val: list[dict]) -> list[Extension]:\n    existing = dist.ext_modules or []\n    args = ({k.replace(\"-\", \"_\"): v for k, v in x.items()} for x in val)\n    new = [Extension(**kw) for kw in args]\n    return [*existing, *new]\n\n\ndef _noop(_dist: Distribution, val: _T) -> _T:\n    return val\n\n\ndef _identity(val: _T) -> _T:\n    return val\n\n\ndef _unify_entry_points(project_table: dict):\n    project = project_table\n    given = project.pop(\"entry-points\", project.pop(\"entry_points\", {}))\n    entry_points = dict(given)  # Avoid problems with static\n    renaming = {\"scripts\": \"console_scripts\", \"gui_scripts\": \"gui_scripts\"}\n    for key, value in list(project.items()):  # eager to allow modifications\n        norm_key = json_compatible_key(key)\n        if norm_key in renaming:\n            # Don't skip even if value is empty (reason: reset missing `dynamic`)\n            entry_points[renaming[norm_key]] = project.pop(key)\n\n    if entry_points:\n        project[\"entry-points\"] = {\n            name: [f\"{k} = {v}\" for k, v in group.items()]\n            for name, group in entry_points.items()\n            if group  # now we can skip empty groups\n        }\n        # Sometimes this will set `project[\"entry-points\"] = {}`, and that is\n        # intentional (for resetting configurations that are missing `dynamic`).\n\n\ndef _copy_command_options(pyproject: dict, dist: Distribution, filename: StrPath):\n    tool_table = pyproject.get(\"tool\", {})\n    cmdclass = tool_table.get(\"setuptools\", {}).get(\"cmdclass\", {})\n    valid_options = _valid_command_options(cmdclass)\n\n    cmd_opts = dist.command_options\n    for cmd, config in pyproject.get(\"tool\", {}).get(\"distutils\", {}).items():\n        cmd = json_compatible_key(cmd)\n        valid = valid_options.get(cmd, set())\n        cmd_opts.setdefault(cmd, {})\n        for key, value in config.items():\n            key = json_compatible_key(key)\n            cmd_opts[cmd][key] = (str(filename), value)\n            if key not in valid:\n                # To avoid removing options that are specified dynamically we\n                # just log a warn...\n                _logger.warning(f\"Command option {cmd}.{key} is not defined\")\n\n\ndef _valid_command_options(cmdclass: Mapping = EMPTY) -> dict[str, set[str]]:\n    from setuptools.dist import Distribution\n\n    from .._importlib import metadata\n\n    valid_options = {\"global\": _normalise_cmd_options(Distribution.global_options)}\n\n    unloaded_entry_points = metadata.entry_points(group='distutils.commands')\n    loaded_entry_points = (_load_ep(ep) for ep in unloaded_entry_points)\n    entry_points = (ep for ep in loaded_entry_points if ep)\n    for cmd, cmd_class in chain(entry_points, cmdclass.items()):\n        opts = valid_options.get(cmd, set())\n        opts = opts | _normalise_cmd_options(getattr(cmd_class, \"user_options\", []))\n        valid_options[cmd] = opts\n\n    return valid_options\n\n\ndef _load_ep(ep: metadata.EntryPoint) -> tuple[str, type] | None:\n    if ep.value.startswith(\"wheel.bdist_wheel\"):\n        # Ignore deprecated entrypoint from wheel and avoid warning pypa/wheel#631\n        # TODO: remove check when `bdist_wheel` has been fully removed from pypa/wheel\n        return None\n\n    # Ignore all the errors\n    try:\n        return (ep.name, ep.load())\n    except Exception as ex:\n        msg = f\"{ex.__class__.__name__} while trying to load entry-point {ep.name}\"\n        _logger.warning(f\"{msg}: {ex}\")\n        return None\n\n\ndef _normalise_cmd_option_key(name: str) -> str:\n    return json_compatible_key(name).strip(\"_=\")\n\n\ndef _normalise_cmd_options(desc: _OptionsList) -> set[str]:\n    return {_normalise_cmd_option_key(fancy_option[0]) for fancy_option in desc}\n\n\ndef _get_previous_entrypoints(dist: Distribution) -> dict[str, list]:\n    ignore = (\"console_scripts\", \"gui_scripts\")\n    value = getattr(dist, \"entry_points\", None) or {}\n    return {k: v for k, v in value.items() if k not in ignore}\n\n\ndef _get_previous_scripts(dist: Distribution) -> list | None:\n    value = getattr(dist, \"entry_points\", None) or {}\n    return value.get(\"console_scripts\")\n\n\ndef _get_previous_gui_scripts(dist: Distribution) -> list | None:\n    value = getattr(dist, \"entry_points\", None) or {}\n    return value.get(\"gui_scripts\")\n\n\ndef _set_static_list_metadata(attr: str, dist: Distribution, val: list) -> None:\n    \"\"\"Apply distutils metadata validation but preserve \"static\" behaviour\"\"\"\n    meta = dist.metadata\n    setter, getter = getattr(meta, f\"set_{attr}\"), getattr(meta, f\"get_{attr}\")\n    setter(val)\n    setattr(meta, attr, _static.List(getter()))\n\n\ndef _attrgetter(attr):\n    \"\"\"\n    Similar to ``operator.attrgetter`` but returns None if ``attr`` is not found\n    >>> from types import SimpleNamespace\n    >>> obj = SimpleNamespace(a=42, b=SimpleNamespace(c=13))\n    >>> _attrgetter(\"a\")(obj)\n    42\n    >>> _attrgetter(\"b.c\")(obj)\n    13\n    >>> _attrgetter(\"d\")(obj) is None\n    True\n    \"\"\"\n    return partial(reduce, lambda acc, x: getattr(acc, x, None), attr.split(\".\"))\n\n\ndef _some_attrgetter(*items):\n    \"\"\"\n    Return the first \"truth-y\" attribute or None\n    >>> from types import SimpleNamespace\n    >>> obj = SimpleNamespace(a=42, b=SimpleNamespace(c=13))\n    >>> _some_attrgetter(\"d\", \"a\", \"b.c\")(obj)\n    42\n    >>> _some_attrgetter(\"d\", \"e\", \"b.c\", \"a\")(obj)\n    13\n    >>> _some_attrgetter(\"d\", \"e\", \"f\")(obj) is None\n    True\n    \"\"\"\n\n    def _acessor(obj):\n        values = (_attrgetter(i)(obj) for i in items)\n        return next((i for i in values if i is not None), None)\n\n    return _acessor\n\n\nPYPROJECT_CORRESPONDENCE: dict[str, _Correspondence] = {\n    \"readme\": _long_description,\n    \"license\": _license,\n    \"authors\": partial(_people, kind=\"author\"),\n    \"maintainers\": partial(_people, kind=\"maintainer\"),\n    \"urls\": _project_urls,\n    \"dependencies\": _dependencies,\n    \"optional_dependencies\": _optional_dependencies,\n    \"requires_python\": _python_requires,\n}\n\nTOOL_TABLE_RENAMES = {\"script_files\": \"scripts\"}\nTOOL_TABLE_REMOVALS = {\n    \"namespace_packages\": \"\"\"\n        Please migrate to implicit native namespaces instead.\n        See https://packaging.python.org/en/latest/guides/packaging-namespace-packages/.\n        \"\"\",\n}\nTOOL_TABLE_CORRESPONDENCE = {\n    # Fields with corresponding core metadata need to be marked as static:\n    \"obsoletes\": partial(_set_static_list_metadata, \"obsoletes\"),\n    \"provides\": partial(_set_static_list_metadata, \"provides\"),\n    \"platforms\": partial(_set_static_list_metadata, \"platforms\"),\n}\n\nSETUPTOOLS_PATCHES = {\n    \"long_description_content_type\",\n    \"project_urls\",\n    \"provides_extras\",\n    \"license_file\",\n    \"license_files\",\n}\n\n_PREPROCESS = {\n    \"ext_modules\": _ext_modules,\n}\n\n_PREVIOUSLY_DEFINED = {\n    \"name\": _attrgetter(\"metadata.name\"),\n    \"version\": _attrgetter(\"metadata.version\"),\n    \"description\": _attrgetter(\"metadata.description\"),\n    \"readme\": _attrgetter(\"metadata.long_description\"),\n    \"requires-python\": _some_attrgetter(\"python_requires\", \"metadata.python_requires\"),\n    \"license\": _attrgetter(\"metadata.license\"),\n    \"authors\": _some_attrgetter(\"metadata.author\", \"metadata.author_email\"),\n    \"maintainers\": _some_attrgetter(\"metadata.maintainer\", \"metadata.maintainer_email\"),\n    \"keywords\": _attrgetter(\"metadata.keywords\"),\n    \"classifiers\": _attrgetter(\"metadata.classifiers\"),\n    \"urls\": _attrgetter(\"metadata.project_urls\"),\n    \"entry-points\": _get_previous_entrypoints,\n    \"scripts\": _get_previous_scripts,\n    \"gui-scripts\": _get_previous_gui_scripts,\n    \"dependencies\": _attrgetter(\"install_requires\"),\n    \"optional-dependencies\": _attrgetter(\"extras_require\"),\n}\n\n\n_RESET_PREVIOUSLY_DEFINED: dict = {\n    # Fix improper setting: given in `setup.py`, but not listed in `dynamic`\n    # dict: pyproject name => value to which reset\n    \"license\": _static.EMPTY_DICT,\n    \"authors\": _static.EMPTY_LIST,\n    \"maintainers\": _static.EMPTY_LIST,\n    \"keywords\": _static.EMPTY_LIST,\n    \"classifiers\": _static.EMPTY_LIST,\n    \"urls\": _static.EMPTY_DICT,\n    \"entry-points\": _static.EMPTY_DICT,\n    \"scripts\": _static.EMPTY_DICT,\n    \"gui-scripts\": _static.EMPTY_DICT,\n    \"dependencies\": _static.EMPTY_LIST,\n    \"optional-dependencies\": _static.EMPTY_DICT,\n}\n\n\nclass _MissingDynamic(SetuptoolsWarning):\n    _SUMMARY = \"`{field}` defined outside of `pyproject.toml` is ignored.\"\n\n    _DETAILS = \"\"\"\n    The following seems to be defined outside of `pyproject.toml`:\n\n    `{field} = {value!r}`\n\n    According to the spec (see the link below), however, setuptools CANNOT\n    consider this value unless `{field}` is listed as `dynamic`.\n\n    https://packaging.python.org/en/latest/specifications/pyproject-toml/#declaring-project-metadata-the-project-table\n\n    To prevent this problem, you can list `{field}` under `dynamic` or alternatively\n    remove the `[project]` table from your file and rely entirely on other means of\n    configuration.\n    \"\"\"\n    # TODO: Consider removing this check in the future?\n    #       There is a trade-off here between improving \"debug-ability\" and the cost\n    #       of running/testing/maintaining these unnecessary checks...\n\n    @classmethod\n    def details(cls, field: str, value: Any) -> str:\n        return cls._DETAILS.format(field=field, value=value)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/config/_validate_pyproject/__init__.py","size":1042,"sha1":"2968fa782b1a389e59d516290cfbb3f5c43e3fa6","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"from functools import reduce\nfrom typing import Any, Callable, Dict\n\nfrom . import formats\nfrom .error_reporting import detailed_errors, ValidationError\nfrom .extra_validations import EXTRA_VALIDATIONS\nfrom .fastjsonschema_exceptions import JsonSchemaException, JsonSchemaValueException\nfrom .fastjsonschema_validations import validate as _validate\n\n__all__ = [\n    \"validate\",\n    \"FORMAT_FUNCTIONS\",\n    \"EXTRA_VALIDATIONS\",\n    \"ValidationError\",\n    \"JsonSchemaException\",\n    \"JsonSchemaValueException\",\n]\n\n\nFORMAT_FUNCTIONS: Dict[str, Callable[[str], bool]] = {\n    fn.__name__.replace(\"_\", \"-\"): fn\n    for fn in formats.__dict__.values()\n    if callable(fn) and not fn.__name__.startswith(\"_\")\n}\n\n\ndef validate(data: Any) -> bool:\n    \"\"\"Validate the given ``data`` object using JSON Schema\n    This function raises ``ValidationError`` if ``data`` is invalid.\n    \"\"\"\n    with detailed_errors():\n        _validate(data, custom_formats=FORMAT_FUNCTIONS)\n        reduce(lambda acc, fn: fn(acc), EXTRA_VALIDATIONS, data)\n    return True\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/config/_validate_pyproject/error_reporting.py","size":11813,"sha1":"6fa14ec0479feda49049f4eeeb2e741172e363b4","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import io\nimport json\nimport logging\nimport os\nimport re\nimport typing\nfrom contextlib import contextmanager\nfrom textwrap import indent, wrap\nfrom typing import Any, Dict, Generator, Iterator, List, Optional, Sequence, Union\n\nfrom .fastjsonschema_exceptions import JsonSchemaValueException\n\nif typing.TYPE_CHECKING:\n    import sys\n\n    if sys.version_info < (3, 11):\n        from typing_extensions import Self\n    else:\n        from typing import Self\n\n_logger = logging.getLogger(__name__)\n\n_MESSAGE_REPLACEMENTS = {\n    \"must be named by propertyName definition\": \"keys must be named by\",\n    \"one of contains definition\": \"at least one item that matches\",\n    \" same as const definition:\": \"\",\n    \"only specified items\": \"only items matching the definition\",\n}\n\n_SKIP_DETAILS = (\n    \"must not be empty\",\n    \"is always invalid\",\n    \"must not be there\",\n)\n\n_NEED_DETAILS = {\"anyOf\", \"oneOf\", \"allOf\", \"contains\", \"propertyNames\", \"not\", \"items\"}\n\n_CAMEL_CASE_SPLITTER = re.compile(r\"\\W+|([A-Z][^A-Z\\W]*)\")\n_IDENTIFIER = re.compile(r\"^[\\w_]+$\", re.I)\n\n_TOML_JARGON = {\n    \"object\": \"table\",\n    \"property\": \"key\",\n    \"properties\": \"keys\",\n    \"property names\": \"keys\",\n}\n\n_FORMATS_HELP = \"\"\"\nFor more details about `format` see\nhttps://validate-pyproject.readthedocs.io/en/latest/api/validate_pyproject.formats.html\n\"\"\"\n\n\nclass ValidationError(JsonSchemaValueException):\n    \"\"\"Report violations of a given JSON schema.\n\n    This class extends :exc:`~fastjsonschema.JsonSchemaValueException`\n    by adding the following properties:\n\n    - ``summary``: an improved version of the ``JsonSchemaValueException`` error message\n      with only the necessary information)\n\n    - ``details``: more contextual information about the error like the failing schema\n      itself and the value that violates the schema.\n\n    Depending on the level of the verbosity of the ``logging`` configuration\n    the exception message will be only ``summary`` (default) or a combination of\n    ``summary`` and ``details`` (when the logging level is set to :obj:`logging.DEBUG`).\n    \"\"\"\n\n    summary = \"\"\n    details = \"\"\n    _original_message = \"\"\n\n    @classmethod\n    def _from_jsonschema(cls, ex: JsonSchemaValueException) -> \"Self\":\n        formatter = _ErrorFormatting(ex)\n        obj = cls(str(formatter), ex.value, formatter.name, ex.definition, ex.rule)\n        debug_code = os.getenv(\"JSONSCHEMA_DEBUG_CODE_GENERATION\", \"false\").lower()\n        if debug_code != \"false\":  # pragma: no cover\n            obj.__cause__, obj.__traceback__ = ex.__cause__, ex.__traceback__\n        obj._original_message = ex.message\n        obj.summary = formatter.summary\n        obj.details = formatter.details\n        return obj\n\n\n@contextmanager\ndef detailed_errors() -> Generator[None, None, None]:\n    try:\n        yield\n    except JsonSchemaValueException as ex:\n        raise ValidationError._from_jsonschema(ex) from None\n\n\nclass _ErrorFormatting:\n    def __init__(self, ex: JsonSchemaValueException):\n        self.ex = ex\n        self.name = f\"`{self._simplify_name(ex.name)}`\"\n        self._original_message: str = self.ex.message.replace(ex.name, self.name)\n        self._summary = \"\"\n        self._details = \"\"\n\n    def __str__(self) -> str:\n        if _logger.getEffectiveLevel() <= logging.DEBUG and self.details:\n            return f\"{self.summary}\\n\\n{self.details}\"\n\n        return self.summary\n\n    @property\n    def summary(self) -> str:\n        if not self._summary:\n            self._summary = self._expand_summary()\n\n        return self._summary\n\n    @property\n    def details(self) -> str:\n        if not self._details:\n            self._details = self._expand_details()\n\n        return self._details\n\n    @staticmethod\n    def _simplify_name(name: str) -> str:\n        x = len(\"data.\")\n        return name[x:] if name.startswith(\"data.\") else name\n\n    def _expand_summary(self) -> str:\n        msg = self._original_message\n\n        for bad, repl in _MESSAGE_REPLACEMENTS.items():\n            msg = msg.replace(bad, repl)\n\n        if any(substring in msg for substring in _SKIP_DETAILS):\n            return msg\n\n        schema = self.ex.rule_definition\n        if self.ex.rule in _NEED_DETAILS and schema:\n            summary = _SummaryWriter(_TOML_JARGON)\n            return f\"{msg}:\\n\\n{indent(summary(schema), '    ')}\"\n\n        return msg\n\n    def _expand_details(self) -> str:\n        optional = []\n        definition = self.ex.definition or {}\n        desc_lines = definition.pop(\"$$description\", [])\n        desc = definition.pop(\"description\", None) or \" \".join(desc_lines)\n        if desc:\n            description = \"\\n\".join(\n                wrap(\n                    desc,\n                    width=80,\n                    initial_indent=\"    \",\n                    subsequent_indent=\"    \",\n                    break_long_words=False,\n                )\n            )\n            optional.append(f\"DESCRIPTION:\\n{description}\")\n        schema = json.dumps(definition, indent=4)\n        value = json.dumps(self.ex.value, indent=4)\n        defaults = [\n            f\"GIVEN VALUE:\\n{indent(value, '    ')}\",\n            f\"OFFENDING RULE: {self.ex.rule!r}\",\n            f\"DEFINITION:\\n{indent(schema, '    ')}\",\n        ]\n        msg = \"\\n\\n\".join(optional + defaults)\n        epilog = f\"\\n{_FORMATS_HELP}\" if \"format\" in msg.lower() else \"\"\n        return msg + epilog\n\n\nclass _SummaryWriter:\n    _IGNORE = frozenset((\"description\", \"default\", \"title\", \"examples\"))\n\n    def __init__(self, jargon: Optional[Dict[str, str]] = None):\n        self.jargon: Dict[str, str] = jargon or {}\n        # Clarify confusing terms\n        self._terms = {\n            \"anyOf\": \"at least one of the following\",\n            \"oneOf\": \"exactly one of the following\",\n            \"allOf\": \"all of the following\",\n            \"not\": \"(*NOT* the following)\",\n            \"prefixItems\": f\"{self._jargon('items')} (in order)\",\n            \"items\": \"items\",\n            \"contains\": \"contains at least one of\",\n            \"propertyNames\": (\n                f\"non-predefined acceptable {self._jargon('property names')}\"\n            ),\n            \"patternProperties\": f\"{self._jargon('properties')} named via pattern\",\n            \"const\": \"predefined value\",\n            \"enum\": \"one of\",\n        }\n        # Attributes that indicate that the definition is easy and can be done\n        # inline (e.g. string and number)\n        self._guess_inline_defs = [\n            \"enum\",\n            \"const\",\n            \"maxLength\",\n            \"minLength\",\n            \"pattern\",\n            \"format\",\n            \"minimum\",\n            \"maximum\",\n            \"exclusiveMinimum\",\n            \"exclusiveMaximum\",\n            \"multipleOf\",\n        ]\n\n    def _jargon(self, term: Union[str, List[str]]) -> Union[str, List[str]]:\n        if isinstance(term, list):\n            return [self.jargon.get(t, t) for t in term]\n        return self.jargon.get(term, term)\n\n    def __call__(\n        self,\n        schema: Union[dict, List[dict]],\n        prefix: str = \"\",\n        *,\n        _path: Sequence[str] = (),\n    ) -> str:\n        if isinstance(schema, list):\n            return self._handle_list(schema, prefix, _path)\n\n        filtered = self._filter_unecessary(schema, _path)\n        simple = self._handle_simple_dict(filtered, _path)\n        if simple:\n            return f\"{prefix}{simple}\"\n\n        child_prefix = self._child_prefix(prefix, \"  \")\n        item_prefix = self._child_prefix(prefix, \"- \")\n        indent = len(prefix) * \" \"\n        with io.StringIO() as buffer:\n            for i, (key, value) in enumerate(filtered.items()):\n                child_path = [*_path, key]\n                line_prefix = prefix if i == 0 else indent\n                buffer.write(f\"{line_prefix}{self._label(child_path)}:\")\n                # ^  just the first item should receive the complete prefix\n                if isinstance(value, dict):\n                    filtered = self._filter_unecessary(value, child_path)\n                    simple = self._handle_simple_dict(filtered, child_path)\n                    buffer.write(\n                        f\" {simple}\"\n                        if simple\n                        else f\"\\n{self(value, child_prefix, _path=child_path)}\"\n                    )\n                elif isinstance(value, list) and (\n                    key != \"type\" or self._is_property(child_path)\n                ):\n                    children = self._handle_list(value, item_prefix, child_path)\n                    sep = \" \" if children.startswith(\"[\") else \"\\n\"\n                    buffer.write(f\"{sep}{children}\")\n                else:\n                    buffer.write(f\" {self._value(value, child_path)}\\n\")\n            return buffer.getvalue()\n\n    def _is_unecessary(self, path: Sequence[str]) -> bool:\n        if self._is_property(path) or not path:  # empty path => instruction @ root\n            return False\n        key = path[-1]\n        return any(key.startswith(k) for k in \"$_\") or key in self._IGNORE\n\n    def _filter_unecessary(\n        self, schema: Dict[str, Any], path: Sequence[str]\n    ) -> Dict[str, Any]:\n        return {\n            key: value\n            for key, value in schema.items()\n            if not self._is_unecessary([*path, key])\n        }\n\n    def _handle_simple_dict(self, value: dict, path: Sequence[str]) -> Optional[str]:\n        inline = any(p in value for p in self._guess_inline_defs)\n        simple = not any(isinstance(v, (list, dict)) for v in value.values())\n        if inline or simple:\n            return f\"{{{', '.join(self._inline_attrs(value, path))}}}\\n\"\n        return None\n\n    def _handle_list(\n        self, schemas: list, prefix: str = \"\", path: Sequence[str] = ()\n    ) -> str:\n        if self._is_unecessary(path):\n            return \"\"\n\n        repr_ = repr(schemas)\n        if all(not isinstance(e, (dict, list)) for e in schemas) and len(repr_) < 60:\n            return f\"{repr_}\\n\"\n\n        item_prefix = self._child_prefix(prefix, \"- \")\n        return \"\".join(\n            self(v, item_prefix, _path=[*path, f\"[{i}]\"]) for i, v in enumerate(schemas)\n        )\n\n    def _is_property(self, path: Sequence[str]) -> bool:\n        \"\"\"Check if the given path can correspond to an arbitrarily named property\"\"\"\n        counter = 0\n        for key in path[-2::-1]:\n            if key not in {\"properties\", \"patternProperties\"}:\n                break\n            counter += 1\n\n        # If the counter if even, the path correspond to a JSON Schema keyword\n        # otherwise it can be any arbitrary string naming a property\n        return counter % 2 == 1\n\n    def _label(self, path: Sequence[str]) -> str:\n        *parents, key = path\n        if not self._is_property(path):\n            norm_key = _separate_terms(key)\n            return self._terms.get(key) or \" \".join(self._jargon(norm_key))\n\n        if parents[-1] == \"patternProperties\":\n            return f\"(regex {key!r})\"\n        return repr(key)  # property name\n\n    def _value(self, value: Any, path: Sequence[str]) -> str:\n        if path[-1] == \"type\" and not self._is_property(path):\n            type_ = self._jargon(value)\n            return f\"[{', '.join(type_)}]\" if isinstance(type_, list) else type_\n        return repr(value)\n\n    def _inline_attrs(self, schema: dict, path: Sequence[str]) -> Iterator[str]:\n        for key, value in schema.items():\n            child_path = [*path, key]\n            yield f\"{self._label(child_path)}: {self._value(value, child_path)}\"\n\n    def _child_prefix(self, parent_prefix: str, child_prefix: str) -> str:\n        return len(parent_prefix) * \" \" + child_prefix\n\n\ndef _separate_terms(word: str) -> List[str]:\n    \"\"\"\n    >>> _separate_terms(\"FooBar-foo\")\n    ['foo', 'bar', 'foo']\n    \"\"\"\n    return [w.lower() for w in _CAMEL_CASE_SPLITTER.split(word) if w]\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/config/_validate_pyproject/extra_validations.py","size":1625,"sha1":"06313b499a958b5e91b2b2ba7952ae881dd9a1f4","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"The purpose of this module is implement PEP 621 validations that are\ndifficult to express as a JSON Schema (or that are not supported by the current\nJSON Schema library).\n\"\"\"\n\nfrom inspect import cleandoc\nfrom typing import Mapping, TypeVar\n\nfrom .error_reporting import ValidationError\n\nT = TypeVar(\"T\", bound=Mapping)\n\n\nclass RedefiningStaticFieldAsDynamic(ValidationError):\n    _DESC = \"\"\"According to PEP 621:\n\n    Build back-ends MUST raise an error if the metadata specifies a field\n    statically as well as being listed in dynamic.\n    \"\"\"\n    __doc__ = _DESC\n    _URL = (\n        \"https://packaging.python.org/en/latest/specifications/\"\n        \"pyproject-toml/#dynamic\"\n    )\n\n\ndef validate_project_dynamic(pyproject: T) -> T:\n    project_table = pyproject.get(\"project\", {})\n    dynamic = project_table.get(\"dynamic\", [])\n\n    for field in dynamic:\n        if field in project_table:\n            raise RedefiningStaticFieldAsDynamic(\n                message=f\"You cannot provide a value for `project.{field}` and \"\n                \"list it under `project.dynamic` at the same time\",\n                value={\n                    field: project_table[field],\n                    \"...\": \" # ...\",\n                    \"dynamic\": dynamic,\n                },\n                name=f\"data.project.{field}\",\n                definition={\n                    \"description\": cleandoc(RedefiningStaticFieldAsDynamic._DESC),\n                    \"see\": RedefiningStaticFieldAsDynamic._URL,\n                },\n                rule=\"PEP 621\",\n            )\n\n    return pyproject\n\n\nEXTRA_VALIDATIONS = (validate_project_dynamic,)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/config/_validate_pyproject/fastjsonschema_exceptions.py","size":1612,"sha1":"6da52c7842a74add50946876b62ee3c5d0cc87b5","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import re\n\n\nSPLIT_RE = re.compile(r'[\\.\\[\\]]+')\n\n\nclass JsonSchemaException(ValueError):\n    \"\"\"\n    Base exception of ``fastjsonschema`` library.\n    \"\"\"\n\n\nclass JsonSchemaValueException(JsonSchemaException):\n    \"\"\"\n    Exception raised by validation function. Available properties:\n\n     * ``message`` containing human-readable information what is wrong (e.g. ``data.property[index] must be smaller than or equal to 42``),\n     * invalid ``value`` (e.g. ``60``),\n     * ``name`` of a path in the data structure (e.g. ``data.property[index]``),\n     * ``path`` as an array in the data structure (e.g. ``['data', 'property', 'index']``),\n     * the whole ``definition`` which the ``value`` has to fulfil (e.g. ``{'type': 'number', 'maximum': 42}``),\n     * ``rule`` which the ``value`` is breaking (e.g. ``maximum``)\n     * and ``rule_definition`` (e.g. ``42``).\n\n    .. versionchanged:: 2.14.0\n        Added all extra properties.\n    \"\"\"\n\n    def __init__(self, message, value=None, name=None, definition=None, rule=None):\n        super().__init__(message)\n        self.message = message\n        self.value = value\n        self.name = name\n        self.definition = definition\n        self.rule = rule\n\n    @property\n    def path(self):\n        return [item for item in SPLIT_RE.split(self.name) if item != '']\n\n    @property\n    def rule_definition(self):\n        if not self.rule or not self.definition:\n            return None\n        return self.definition.get(self.rule)\n\n\nclass JsonSchemaDefinitionException(JsonSchemaException):\n    \"\"\"\n    Exception raised by generator of validation function.\n    \"\"\"\n"}]}