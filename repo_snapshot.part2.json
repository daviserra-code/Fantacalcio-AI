{"generated_at":"2025-08-17T20:04:18.057896Z","root":"/home/runner/workspace","git":{"head":"7b7135163eb4227ef98e4c2d7b4ab78ea306bd73","branch":"main","status":" M corrections.db\n?? app_changes.json\n?? export_changes.py\n"},"filters":{"git_range":null,"since":null,"include_ext":[".cfg",".css",".env",".htm",".html",".ini",".jinja",".jinja2",".js",".json",".md",".py",".toml",".ts",".yaml",".yml"],"exclude_dirs":[".git",".ipynb_checkpoints",".mypy_cache",".pytest_cache",".pythonlibs",".venv","__pycache__","cache","chroma_db","data/exports","node_modules","venv"],"exclude_globs":["*.bmp","*.db","*.feather","*.gif","*.gz","*.ico","*.jpeg","*.jpg","*.jsonl","*.lock","*.log","*.parquet","*.png","*.sqlite","*.sqlite3","*.tar","*.webp","*.zip"],"max_file_bytes":400000},"summary":{"file_count":876,"total_bytes":9265289},"files":[{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/resolution/resolvelib/candidates.py","size":20001,"sha1":"07bdf49e2635587cea3927e0b91d433316fbb00f","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import logging\nimport sys\nfrom typing import TYPE_CHECKING, Any, FrozenSet, Iterable, Optional, Tuple, Union, cast\n\nfrom pip._vendor.packaging.requirements import InvalidRequirement\nfrom pip._vendor.packaging.utils import NormalizedName, canonicalize_name\nfrom pip._vendor.packaging.version import Version\n\nfrom pip._internal.exceptions import (\n    HashError,\n    InstallationSubprocessError,\n    InvalidInstalledPackage,\n    MetadataInconsistent,\n    MetadataInvalid,\n)\nfrom pip._internal.metadata import BaseDistribution\nfrom pip._internal.models.link import Link, links_equivalent\nfrom pip._internal.models.wheel import Wheel\nfrom pip._internal.req.constructors import (\n    install_req_from_editable,\n    install_req_from_line,\n)\nfrom pip._internal.req.req_install import InstallRequirement\nfrom pip._internal.utils.direct_url_helpers import direct_url_from_link\nfrom pip._internal.utils.misc import normalize_version_info\n\nfrom .base import Candidate, Requirement, format_name\n\nif TYPE_CHECKING:\n    from .factory import Factory\n\nlogger = logging.getLogger(__name__)\n\nBaseCandidate = Union[\n    \"AlreadyInstalledCandidate\",\n    \"EditableCandidate\",\n    \"LinkCandidate\",\n]\n\n# Avoid conflicting with the PyPI package \"Python\".\nREQUIRES_PYTHON_IDENTIFIER = cast(NormalizedName, \"<Python from Requires-Python>\")\n\n\ndef as_base_candidate(candidate: Candidate) -> Optional[BaseCandidate]:\n    \"\"\"The runtime version of BaseCandidate.\"\"\"\n    base_candidate_classes = (\n        AlreadyInstalledCandidate,\n        EditableCandidate,\n        LinkCandidate,\n    )\n    if isinstance(candidate, base_candidate_classes):\n        return candidate\n    return None\n\n\ndef make_install_req_from_link(\n    link: Link, template: InstallRequirement\n) -> InstallRequirement:\n    assert not template.editable, \"template is editable\"\n    if template.req:\n        line = str(template.req)\n    else:\n        line = link.url\n    ireq = install_req_from_line(\n        line,\n        user_supplied=template.user_supplied,\n        comes_from=template.comes_from,\n        use_pep517=template.use_pep517,\n        isolated=template.isolated,\n        constraint=template.constraint,\n        global_options=template.global_options,\n        hash_options=template.hash_options,\n        config_settings=template.config_settings,\n    )\n    ireq.original_link = template.original_link\n    ireq.link = link\n    ireq.extras = template.extras\n    return ireq\n\n\ndef make_install_req_from_editable(\n    link: Link, template: InstallRequirement\n) -> InstallRequirement:\n    assert template.editable, \"template not editable\"\n    ireq = install_req_from_editable(\n        link.url,\n        user_supplied=template.user_supplied,\n        comes_from=template.comes_from,\n        use_pep517=template.use_pep517,\n        isolated=template.isolated,\n        constraint=template.constraint,\n        permit_editable_wheels=template.permit_editable_wheels,\n        global_options=template.global_options,\n        hash_options=template.hash_options,\n        config_settings=template.config_settings,\n    )\n    ireq.extras = template.extras\n    return ireq\n\n\ndef _make_install_req_from_dist(\n    dist: BaseDistribution, template: InstallRequirement\n) -> InstallRequirement:\n    if template.req:\n        line = str(template.req)\n    elif template.link:\n        line = f\"{dist.canonical_name} @ {template.link.url}\"\n    else:\n        line = f\"{dist.canonical_name}=={dist.version}\"\n    ireq = install_req_from_line(\n        line,\n        user_supplied=template.user_supplied,\n        comes_from=template.comes_from,\n        use_pep517=template.use_pep517,\n        isolated=template.isolated,\n        constraint=template.constraint,\n        global_options=template.global_options,\n        hash_options=template.hash_options,\n        config_settings=template.config_settings,\n    )\n    ireq.satisfied_by = dist\n    return ireq\n\n\nclass _InstallRequirementBackedCandidate(Candidate):\n    \"\"\"A candidate backed by an ``InstallRequirement``.\n\n    This represents a package request with the target not being already\n    in the environment, and needs to be fetched and installed. The backing\n    ``InstallRequirement`` is responsible for most of the leg work; this\n    class exposes appropriate information to the resolver.\n\n    :param link: The link passed to the ``InstallRequirement``. The backing\n        ``InstallRequirement`` will use this link to fetch the distribution.\n    :param source_link: The link this candidate \"originates\" from. This is\n        different from ``link`` when the link is found in the wheel cache.\n        ``link`` would point to the wheel cache, while this points to the\n        found remote link (e.g. from pypi.org).\n    \"\"\"\n\n    dist: BaseDistribution\n    is_installed = False\n\n    def __init__(\n        self,\n        link: Link,\n        source_link: Link,\n        ireq: InstallRequirement,\n        factory: \"Factory\",\n        name: Optional[NormalizedName] = None,\n        version: Optional[Version] = None,\n    ) -> None:\n        self._link = link\n        self._source_link = source_link\n        self._factory = factory\n        self._ireq = ireq\n        self._name = name\n        self._version = version\n        self.dist = self._prepare()\n        self._hash: Optional[int] = None\n\n    def __str__(self) -> str:\n        return f\"{self.name} {self.version}\"\n\n    def __repr__(self) -> str:\n        return f\"{self.__class__.__name__}({str(self._link)!r})\"\n\n    def __hash__(self) -> int:\n        if self._hash is not None:\n            return self._hash\n\n        self._hash = hash((self.__class__, self._link))\n        return self._hash\n\n    def __eq__(self, other: Any) -> bool:\n        if isinstance(other, self.__class__):\n            return links_equivalent(self._link, other._link)\n        return False\n\n    @property\n    def source_link(self) -> Optional[Link]:\n        return self._source_link\n\n    @property\n    def project_name(self) -> NormalizedName:\n        \"\"\"The normalised name of the project the candidate refers to\"\"\"\n        if self._name is None:\n            self._name = self.dist.canonical_name\n        return self._name\n\n    @property\n    def name(self) -> str:\n        return self.project_name\n\n    @property\n    def version(self) -> Version:\n        if self._version is None:\n            self._version = self.dist.version\n        return self._version\n\n    def format_for_error(self) -> str:\n        return (\n            f\"{self.name} {self.version} \"\n            f\"(from {self._link.file_path if self._link.is_file else self._link})\"\n        )\n\n    def _prepare_distribution(self) -> BaseDistribution:\n        raise NotImplementedError(\"Override in subclass\")\n\n    def _check_metadata_consistency(self, dist: BaseDistribution) -> None:\n        \"\"\"Check for consistency of project name and version of dist.\"\"\"\n        if self._name is not None and self._name != dist.canonical_name:\n            raise MetadataInconsistent(\n                self._ireq,\n                \"name\",\n                self._name,\n                dist.canonical_name,\n            )\n        if self._version is not None and self._version != dist.version:\n            raise MetadataInconsistent(\n                self._ireq,\n                \"version\",\n                str(self._version),\n                str(dist.version),\n            )\n        # check dependencies are valid\n        # TODO performance: this means we iterate the dependencies at least twice,\n        # we may want to cache parsed Requires-Dist\n        try:\n            list(dist.iter_dependencies(list(dist.iter_provided_extras())))\n        except InvalidRequirement as e:\n            raise MetadataInvalid(self._ireq, str(e))\n\n    def _prepare(self) -> BaseDistribution:\n        try:\n            dist = self._prepare_distribution()\n        except HashError as e:\n            # Provide HashError the underlying ireq that caused it. This\n            # provides context for the resulting error message to show the\n            # offending line to the user.\n            e.req = self._ireq\n            raise\n        except InstallationSubprocessError as exc:\n            # The output has been presented already, so don't duplicate it.\n            exc.context = \"See above for output.\"\n            raise\n\n        self._check_metadata_consistency(dist)\n        return dist\n\n    def iter_dependencies(self, with_requires: bool) -> Iterable[Optional[Requirement]]:\n        requires = self.dist.iter_dependencies() if with_requires else ()\n        for r in requires:\n            yield from self._factory.make_requirements_from_spec(str(r), self._ireq)\n        yield self._factory.make_requires_python_requirement(self.dist.requires_python)\n\n    def get_install_requirement(self) -> Optional[InstallRequirement]:\n        return self._ireq\n\n\nclass LinkCandidate(_InstallRequirementBackedCandidate):\n    is_editable = False\n\n    def __init__(\n        self,\n        link: Link,\n        template: InstallRequirement,\n        factory: \"Factory\",\n        name: Optional[NormalizedName] = None,\n        version: Optional[Version] = None,\n    ) -> None:\n        source_link = link\n        cache_entry = factory.get_wheel_cache_entry(source_link, name)\n        if cache_entry is not None:\n            logger.debug(\"Using cached wheel link: %s\", cache_entry.link)\n            link = cache_entry.link\n        ireq = make_install_req_from_link(link, template)\n        assert ireq.link == link\n        if ireq.link.is_wheel and not ireq.link.is_file:\n            wheel = Wheel(ireq.link.filename)\n            wheel_name = canonicalize_name(wheel.name)\n            assert name == wheel_name, f\"{name!r} != {wheel_name!r} for wheel\"\n            # Version may not be present for PEP 508 direct URLs\n            if version is not None:\n                wheel_version = Version(wheel.version)\n                assert (\n                    version == wheel_version\n                ), f\"{version!r} != {wheel_version!r} for wheel {name}\"\n\n        if cache_entry is not None:\n            assert ireq.link.is_wheel\n            assert ireq.link.is_file\n            if cache_entry.persistent and template.link is template.original_link:\n                ireq.cached_wheel_source_link = source_link\n            if cache_entry.origin is not None:\n                ireq.download_info = cache_entry.origin\n            else:\n                # Legacy cache entry that does not have origin.json.\n                # download_info may miss the archive_info.hashes field.\n                ireq.download_info = direct_url_from_link(\n                    source_link, link_is_in_wheel_cache=cache_entry.persistent\n                )\n\n        super().__init__(\n            link=link,\n            source_link=source_link,\n            ireq=ireq,\n            factory=factory,\n            name=name,\n            version=version,\n        )\n\n    def _prepare_distribution(self) -> BaseDistribution:\n        preparer = self._factory.preparer\n        return preparer.prepare_linked_requirement(self._ireq, parallel_builds=True)\n\n\nclass EditableCandidate(_InstallRequirementBackedCandidate):\n    is_editable = True\n\n    def __init__(\n        self,\n        link: Link,\n        template: InstallRequirement,\n        factory: \"Factory\",\n        name: Optional[NormalizedName] = None,\n        version: Optional[Version] = None,\n    ) -> None:\n        super().__init__(\n            link=link,\n            source_link=link,\n            ireq=make_install_req_from_editable(link, template),\n            factory=factory,\n            name=name,\n            version=version,\n        )\n\n    def _prepare_distribution(self) -> BaseDistribution:\n        return self._factory.preparer.prepare_editable_requirement(self._ireq)\n\n\nclass AlreadyInstalledCandidate(Candidate):\n    is_installed = True\n    source_link = None\n\n    def __init__(\n        self,\n        dist: BaseDistribution,\n        template: InstallRequirement,\n        factory: \"Factory\",\n    ) -> None:\n        self.dist = dist\n        self._ireq = _make_install_req_from_dist(dist, template)\n        self._factory = factory\n        self._version = None\n\n        # This is just logging some messages, so we can do it eagerly.\n        # The returned dist would be exactly the same as self.dist because we\n        # set satisfied_by in _make_install_req_from_dist.\n        # TODO: Supply reason based on force_reinstall and upgrade_strategy.\n        skip_reason = \"already satisfied\"\n        factory.preparer.prepare_installed_requirement(self._ireq, skip_reason)\n\n    def __str__(self) -> str:\n        return str(self.dist)\n\n    def __repr__(self) -> str:\n        return f\"{self.__class__.__name__}({self.dist!r})\"\n\n    def __eq__(self, other: object) -> bool:\n        if not isinstance(other, AlreadyInstalledCandidate):\n            return NotImplemented\n        return self.name == other.name and self.version == other.version\n\n    def __hash__(self) -> int:\n        return hash((self.name, self.version))\n\n    @property\n    def project_name(self) -> NormalizedName:\n        return self.dist.canonical_name\n\n    @property\n    def name(self) -> str:\n        return self.project_name\n\n    @property\n    def version(self) -> Version:\n        if self._version is None:\n            self._version = self.dist.version\n        return self._version\n\n    @property\n    def is_editable(self) -> bool:\n        return self.dist.editable\n\n    def format_for_error(self) -> str:\n        return f\"{self.name} {self.version} (Installed)\"\n\n    def iter_dependencies(self, with_requires: bool) -> Iterable[Optional[Requirement]]:\n        if not with_requires:\n            return\n\n        try:\n            for r in self.dist.iter_dependencies():\n                yield from self._factory.make_requirements_from_spec(str(r), self._ireq)\n        except InvalidRequirement as exc:\n            raise InvalidInstalledPackage(dist=self.dist, invalid_exc=exc) from None\n\n    def get_install_requirement(self) -> Optional[InstallRequirement]:\n        return None\n\n\nclass ExtrasCandidate(Candidate):\n    \"\"\"A candidate that has 'extras', indicating additional dependencies.\n\n    Requirements can be for a project with dependencies, something like\n    foo[extra].  The extras don't affect the project/version being installed\n    directly, but indicate that we need additional dependencies. We model that\n    by having an artificial ExtrasCandidate that wraps the \"base\" candidate.\n\n    The ExtrasCandidate differs from the base in the following ways:\n\n    1. It has a unique name, of the form foo[extra]. This causes the resolver\n       to treat it as a separate node in the dependency graph.\n    2. When we're getting the candidate's dependencies,\n       a) We specify that we want the extra dependencies as well.\n       b) We add a dependency on the base candidate.\n          See below for why this is needed.\n    3. We return None for the underlying InstallRequirement, as the base\n       candidate will provide it, and we don't want to end up with duplicates.\n\n    The dependency on the base candidate is needed so that the resolver can't\n    decide that it should recommend foo[extra1] version 1.0 and foo[extra2]\n    version 2.0. Having those candidates depend on foo=1.0 and foo=2.0\n    respectively forces the resolver to recognise that this is a conflict.\n    \"\"\"\n\n    def __init__(\n        self,\n        base: BaseCandidate,\n        extras: FrozenSet[str],\n        *,\n        comes_from: Optional[InstallRequirement] = None,\n    ) -> None:\n        \"\"\"\n        :param comes_from: the InstallRequirement that led to this candidate if it\n            differs from the base's InstallRequirement. This will often be the\n            case in the sense that this candidate's requirement has the extras\n            while the base's does not. Unlike the InstallRequirement backed\n            candidates, this requirement is used solely for reporting purposes,\n            it does not do any leg work.\n        \"\"\"\n        self.base = base\n        self.extras = frozenset(canonicalize_name(e) for e in extras)\n        self._comes_from = comes_from if comes_from is not None else self.base._ireq\n\n    def __str__(self) -> str:\n        name, rest = str(self.base).split(\" \", 1)\n        return \"{}[{}] {}\".format(name, \",\".join(self.extras), rest)\n\n    def __repr__(self) -> str:\n        return f\"{self.__class__.__name__}(base={self.base!r}, extras={self.extras!r})\"\n\n    def __hash__(self) -> int:\n        return hash((self.base, self.extras))\n\n    def __eq__(self, other: Any) -> bool:\n        if isinstance(other, self.__class__):\n            return self.base == other.base and self.extras == other.extras\n        return False\n\n    @property\n    def project_name(self) -> NormalizedName:\n        return self.base.project_name\n\n    @property\n    def name(self) -> str:\n        \"\"\"The normalised name of the project the candidate refers to\"\"\"\n        return format_name(self.base.project_name, self.extras)\n\n    @property\n    def version(self) -> Version:\n        return self.base.version\n\n    def format_for_error(self) -> str:\n        return \"{} [{}]\".format(\n            self.base.format_for_error(), \", \".join(sorted(self.extras))\n        )\n\n    @property\n    def is_installed(self) -> bool:\n        return self.base.is_installed\n\n    @property\n    def is_editable(self) -> bool:\n        return self.base.is_editable\n\n    @property\n    def source_link(self) -> Optional[Link]:\n        return self.base.source_link\n\n    def iter_dependencies(self, with_requires: bool) -> Iterable[Optional[Requirement]]:\n        factory = self.base._factory\n\n        # Add a dependency on the exact base\n        # (See note 2b in the class docstring)\n        yield factory.make_requirement_from_candidate(self.base)\n        if not with_requires:\n            return\n\n        # The user may have specified extras that the candidate doesn't\n        # support. We ignore any unsupported extras here.\n        valid_extras = self.extras.intersection(self.base.dist.iter_provided_extras())\n        invalid_extras = self.extras.difference(self.base.dist.iter_provided_extras())\n        for extra in sorted(invalid_extras):\n            logger.warning(\n                \"%s %s does not provide the extra '%s'\",\n                self.base.name,\n                self.version,\n                extra,\n            )\n\n        for r in self.base.dist.iter_dependencies(valid_extras):\n            yield from factory.make_requirements_from_spec(\n                str(r),\n                self._comes_from,\n                valid_extras,\n            )\n\n    def get_install_requirement(self) -> Optional[InstallRequirement]:\n        # We don't return anything here, because we always\n        # depend on the base candidate, and we'll get the\n        # install requirement from that.\n        return None\n\n\nclass RequiresPythonCandidate(Candidate):\n    is_installed = False\n    source_link = None\n\n    def __init__(self, py_version_info: Optional[Tuple[int, ...]]) -> None:\n        if py_version_info is not None:\n            version_info = normalize_version_info(py_version_info)\n        else:\n            version_info = sys.version_info[:3]\n        self._version = Version(\".\".join(str(c) for c in version_info))\n\n    # We don't need to implement __eq__() and __ne__() since there is always\n    # only one RequiresPythonCandidate in a resolution, i.e. the host Python.\n    # The built-in object.__eq__() and object.__ne__() do exactly what we want.\n\n    def __str__(self) -> str:\n        return f\"Python {self._version}\"\n\n    @property\n    def project_name(self) -> NormalizedName:\n        return REQUIRES_PYTHON_IDENTIFIER\n\n    @property\n    def name(self) -> str:\n        return REQUIRES_PYTHON_IDENTIFIER\n\n    @property\n    def version(self) -> Version:\n        return self._version\n\n    def format_for_error(self) -> str:\n        return f\"Python {self.version}\"\n\n    def iter_dependencies(self, with_requires: bool) -> Iterable[Optional[Requirement]]:\n        return ()\n\n    def get_install_requirement(self) -> Optional[InstallRequirement]:\n        return None\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/resolution/resolvelib/factory.py","size":32659,"sha1":"4051e723d72441c4e4565f58beb8485931da48e8","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import contextlib\nimport functools\nimport logging\nfrom typing import (\n    TYPE_CHECKING,\n    Callable,\n    Dict,\n    FrozenSet,\n    Iterable,\n    Iterator,\n    List,\n    Mapping,\n    NamedTuple,\n    Optional,\n    Protocol,\n    Sequence,\n    Set,\n    Tuple,\n    TypeVar,\n    cast,\n)\n\nfrom pip._vendor.packaging.requirements import InvalidRequirement\nfrom pip._vendor.packaging.specifiers import SpecifierSet\nfrom pip._vendor.packaging.utils import NormalizedName, canonicalize_name\nfrom pip._vendor.packaging.version import InvalidVersion, Version\nfrom pip._vendor.resolvelib import ResolutionImpossible\n\nfrom pip._internal.cache import CacheEntry, WheelCache\nfrom pip._internal.exceptions import (\n    DistributionNotFound,\n    InstallationError,\n    InvalidInstalledPackage,\n    MetadataInconsistent,\n    MetadataInvalid,\n    UnsupportedPythonVersion,\n    UnsupportedWheel,\n)\nfrom pip._internal.index.package_finder import PackageFinder\nfrom pip._internal.metadata import BaseDistribution, get_default_environment\nfrom pip._internal.models.link import Link\nfrom pip._internal.models.wheel import Wheel\nfrom pip._internal.operations.prepare import RequirementPreparer\nfrom pip._internal.req.constructors import (\n    install_req_drop_extras,\n    install_req_from_link_and_ireq,\n)\nfrom pip._internal.req.req_install import (\n    InstallRequirement,\n    check_invalid_constraint_type,\n)\nfrom pip._internal.resolution.base import InstallRequirementProvider\nfrom pip._internal.utils.compatibility_tags import get_supported\nfrom pip._internal.utils.hashes import Hashes\nfrom pip._internal.utils.packaging import get_requirement\nfrom pip._internal.utils.virtualenv import running_under_virtualenv\n\nfrom .base import Candidate, Constraint, Requirement\nfrom .candidates import (\n    AlreadyInstalledCandidate,\n    BaseCandidate,\n    EditableCandidate,\n    ExtrasCandidate,\n    LinkCandidate,\n    RequiresPythonCandidate,\n    as_base_candidate,\n)\nfrom .found_candidates import FoundCandidates, IndexCandidateInfo\nfrom .requirements import (\n    ExplicitRequirement,\n    RequiresPythonRequirement,\n    SpecifierRequirement,\n    SpecifierWithoutExtrasRequirement,\n    UnsatisfiableRequirement,\n)\n\nif TYPE_CHECKING:\n\n    class ConflictCause(Protocol):\n        requirement: RequiresPythonRequirement\n        parent: Candidate\n\n\nlogger = logging.getLogger(__name__)\n\nC = TypeVar(\"C\")\nCache = Dict[Link, C]\n\n\nclass CollectedRootRequirements(NamedTuple):\n    requirements: List[Requirement]\n    constraints: Dict[str, Constraint]\n    user_requested: Dict[str, int]\n\n\nclass Factory:\n    def __init__(\n        self,\n        finder: PackageFinder,\n        preparer: RequirementPreparer,\n        make_install_req: InstallRequirementProvider,\n        wheel_cache: Optional[WheelCache],\n        use_user_site: bool,\n        force_reinstall: bool,\n        ignore_installed: bool,\n        ignore_requires_python: bool,\n        py_version_info: Optional[Tuple[int, ...]] = None,\n    ) -> None:\n        self._finder = finder\n        self.preparer = preparer\n        self._wheel_cache = wheel_cache\n        self._python_candidate = RequiresPythonCandidate(py_version_info)\n        self._make_install_req_from_spec = make_install_req\n        self._use_user_site = use_user_site\n        self._force_reinstall = force_reinstall\n        self._ignore_requires_python = ignore_requires_python\n\n        self._build_failures: Cache[InstallationError] = {}\n        self._link_candidate_cache: Cache[LinkCandidate] = {}\n        self._editable_candidate_cache: Cache[EditableCandidate] = {}\n        self._installed_candidate_cache: Dict[str, AlreadyInstalledCandidate] = {}\n        self._extras_candidate_cache: Dict[\n            Tuple[int, FrozenSet[NormalizedName]], ExtrasCandidate\n        ] = {}\n        self._supported_tags_cache = get_supported()\n\n        if not ignore_installed:\n            env = get_default_environment()\n            self._installed_dists = {\n                dist.canonical_name: dist\n                for dist in env.iter_installed_distributions(local_only=False)\n            }\n        else:\n            self._installed_dists = {}\n\n    @property\n    def force_reinstall(self) -> bool:\n        return self._force_reinstall\n\n    def _fail_if_link_is_unsupported_wheel(self, link: Link) -> None:\n        if not link.is_wheel:\n            return\n        wheel = Wheel(link.filename)\n        if wheel.supported(self._finder.target_python.get_unsorted_tags()):\n            return\n        msg = f\"{link.filename} is not a supported wheel on this platform.\"\n        raise UnsupportedWheel(msg)\n\n    def _make_extras_candidate(\n        self,\n        base: BaseCandidate,\n        extras: FrozenSet[str],\n        *,\n        comes_from: Optional[InstallRequirement] = None,\n    ) -> ExtrasCandidate:\n        cache_key = (id(base), frozenset(canonicalize_name(e) for e in extras))\n        try:\n            candidate = self._extras_candidate_cache[cache_key]\n        except KeyError:\n            candidate = ExtrasCandidate(base, extras, comes_from=comes_from)\n            self._extras_candidate_cache[cache_key] = candidate\n        return candidate\n\n    def _make_candidate_from_dist(\n        self,\n        dist: BaseDistribution,\n        extras: FrozenSet[str],\n        template: InstallRequirement,\n    ) -> Candidate:\n        try:\n            base = self._installed_candidate_cache[dist.canonical_name]\n        except KeyError:\n            base = AlreadyInstalledCandidate(dist, template, factory=self)\n            self._installed_candidate_cache[dist.canonical_name] = base\n        if not extras:\n            return base\n        return self._make_extras_candidate(base, extras, comes_from=template)\n\n    def _make_candidate_from_link(\n        self,\n        link: Link,\n        extras: FrozenSet[str],\n        template: InstallRequirement,\n        name: Optional[NormalizedName],\n        version: Optional[Version],\n    ) -> Optional[Candidate]:\n        base: Optional[BaseCandidate] = self._make_base_candidate_from_link(\n            link, template, name, version\n        )\n        if not extras or base is None:\n            return base\n        return self._make_extras_candidate(base, extras, comes_from=template)\n\n    def _make_base_candidate_from_link(\n        self,\n        link: Link,\n        template: InstallRequirement,\n        name: Optional[NormalizedName],\n        version: Optional[Version],\n    ) -> Optional[BaseCandidate]:\n        # TODO: Check already installed candidate, and use it if the link and\n        # editable flag match.\n\n        if link in self._build_failures:\n            # We already tried this candidate before, and it does not build.\n            # Don't bother trying again.\n            return None\n\n        if template.editable:\n            if link not in self._editable_candidate_cache:\n                try:\n                    self._editable_candidate_cache[link] = EditableCandidate(\n                        link,\n                        template,\n                        factory=self,\n                        name=name,\n                        version=version,\n                    )\n                except (MetadataInconsistent, MetadataInvalid) as e:\n                    logger.info(\n                        \"Discarding [blue underline]%s[/]: [yellow]%s[reset]\",\n                        link,\n                        e,\n                        extra={\"markup\": True},\n                    )\n                    self._build_failures[link] = e\n                    return None\n\n            return self._editable_candidate_cache[link]\n        else:\n            if link not in self._link_candidate_cache:\n                try:\n                    self._link_candidate_cache[link] = LinkCandidate(\n                        link,\n                        template,\n                        factory=self,\n                        name=name,\n                        version=version,\n                    )\n                except MetadataInconsistent as e:\n                    logger.info(\n                        \"Discarding [blue underline]%s[/]: [yellow]%s[reset]\",\n                        link,\n                        e,\n                        extra={\"markup\": True},\n                    )\n                    self._build_failures[link] = e\n                    return None\n            return self._link_candidate_cache[link]\n\n    def _iter_found_candidates(\n        self,\n        ireqs: Sequence[InstallRequirement],\n        specifier: SpecifierSet,\n        hashes: Hashes,\n        prefers_installed: bool,\n        incompatible_ids: Set[int],\n    ) -> Iterable[Candidate]:\n        if not ireqs:\n            return ()\n\n        # The InstallRequirement implementation requires us to give it a\n        # \"template\". Here we just choose the first requirement to represent\n        # all of them.\n        # Hopefully the Project model can correct this mismatch in the future.\n        template = ireqs[0]\n        assert template.req, \"Candidates found on index must be PEP 508\"\n        name = canonicalize_name(template.req.name)\n\n        extras: FrozenSet[str] = frozenset()\n        for ireq in ireqs:\n            assert ireq.req, \"Candidates found on index must be PEP 508\"\n            specifier &= ireq.req.specifier\n            hashes &= ireq.hashes(trust_internet=False)\n            extras |= frozenset(ireq.extras)\n\n        def _get_installed_candidate() -> Optional[Candidate]:\n            \"\"\"Get the candidate for the currently-installed version.\"\"\"\n            # If --force-reinstall is set, we want the version from the index\n            # instead, so we \"pretend\" there is nothing installed.\n            if self._force_reinstall:\n                return None\n            try:\n                installed_dist = self._installed_dists[name]\n            except KeyError:\n                return None\n\n            try:\n                # Don't use the installed distribution if its version\n                # does not fit the current dependency graph.\n                if not specifier.contains(installed_dist.version, prereleases=True):\n                    return None\n            except InvalidVersion as e:\n                raise InvalidInstalledPackage(dist=installed_dist, invalid_exc=e)\n\n            candidate = self._make_candidate_from_dist(\n                dist=installed_dist,\n                extras=extras,\n                template=template,\n            )\n            # The candidate is a known incompatibility. Don't use it.\n            if id(candidate) in incompatible_ids:\n                return None\n            return candidate\n\n        def iter_index_candidate_infos() -> Iterator[IndexCandidateInfo]:\n            result = self._finder.find_best_candidate(\n                project_name=name,\n                specifier=specifier,\n                hashes=hashes,\n            )\n            icans = result.applicable_candidates\n\n            # PEP 592: Yanked releases are ignored unless the specifier\n            # explicitly pins a version (via '==' or '===') that can be\n            # solely satisfied by a yanked release.\n            all_yanked = all(ican.link.is_yanked for ican in icans)\n\n            def is_pinned(specifier: SpecifierSet) -> bool:\n                for sp in specifier:\n                    if sp.operator == \"===\":\n                        return True\n                    if sp.operator != \"==\":\n                        continue\n                    if sp.version.endswith(\".*\"):\n                        continue\n                    return True\n                return False\n\n            pinned = is_pinned(specifier)\n\n            # PackageFinder returns earlier versions first, so we reverse.\n            for ican in reversed(icans):\n                if not (all_yanked and pinned) and ican.link.is_yanked:\n                    continue\n                func = functools.partial(\n                    self._make_candidate_from_link,\n                    link=ican.link,\n                    extras=extras,\n                    template=template,\n                    name=name,\n                    version=ican.version,\n                )\n                yield ican.version, func\n\n        return FoundCandidates(\n            iter_index_candidate_infos,\n            _get_installed_candidate(),\n            prefers_installed,\n            incompatible_ids,\n        )\n\n    def _iter_explicit_candidates_from_base(\n        self,\n        base_requirements: Iterable[Requirement],\n        extras: FrozenSet[str],\n    ) -> Iterator[Candidate]:\n        \"\"\"Produce explicit candidates from the base given an extra-ed package.\n\n        :param base_requirements: Requirements known to the resolver. The\n            requirements are guaranteed to not have extras.\n        :param extras: The extras to inject into the explicit requirements'\n            candidates.\n        \"\"\"\n        for req in base_requirements:\n            lookup_cand, _ = req.get_candidate_lookup()\n            if lookup_cand is None:  # Not explicit.\n                continue\n            # We've stripped extras from the identifier, and should always\n            # get a BaseCandidate here, unless there's a bug elsewhere.\n            base_cand = as_base_candidate(lookup_cand)\n            assert base_cand is not None, \"no extras here\"\n            yield self._make_extras_candidate(base_cand, extras)\n\n    def _iter_candidates_from_constraints(\n        self,\n        identifier: str,\n        constraint: Constraint,\n        template: InstallRequirement,\n    ) -> Iterator[Candidate]:\n        \"\"\"Produce explicit candidates from constraints.\n\n        This creates \"fake\" InstallRequirement objects that are basically clones\n        of what \"should\" be the template, but with original_link set to link.\n        \"\"\"\n        for link in constraint.links:\n            self._fail_if_link_is_unsupported_wheel(link)\n            candidate = self._make_base_candidate_from_link(\n                link,\n                template=install_req_from_link_and_ireq(link, template),\n                name=canonicalize_name(identifier),\n                version=None,\n            )\n            if candidate:\n                yield candidate\n\n    def find_candidates(\n        self,\n        identifier: str,\n        requirements: Mapping[str, Iterable[Requirement]],\n        incompatibilities: Mapping[str, Iterator[Candidate]],\n        constraint: Constraint,\n        prefers_installed: bool,\n        is_satisfied_by: Callable[[Requirement, Candidate], bool],\n    ) -> Iterable[Candidate]:\n        # Collect basic lookup information from the requirements.\n        explicit_candidates: Set[Candidate] = set()\n        ireqs: List[InstallRequirement] = []\n        for req in requirements[identifier]:\n            cand, ireq = req.get_candidate_lookup()\n            if cand is not None:\n                explicit_candidates.add(cand)\n            if ireq is not None:\n                ireqs.append(ireq)\n\n        # If the current identifier contains extras, add requires and explicit\n        # candidates from entries from extra-less identifier.\n        with contextlib.suppress(InvalidRequirement):\n            parsed_requirement = get_requirement(identifier)\n            if parsed_requirement.name != identifier:\n                explicit_candidates.update(\n                    self._iter_explicit_candidates_from_base(\n                        requirements.get(parsed_requirement.name, ()),\n                        frozenset(parsed_requirement.extras),\n                    ),\n                )\n                for req in requirements.get(parsed_requirement.name, []):\n                    _, ireq = req.get_candidate_lookup()\n                    if ireq is not None:\n                        ireqs.append(ireq)\n\n        # Add explicit candidates from constraints. We only do this if there are\n        # known ireqs, which represent requirements not already explicit. If\n        # there are no ireqs, we're constraining already-explicit requirements,\n        # which is handled later when we return the explicit candidates.\n        if ireqs:\n            try:\n                explicit_candidates.update(\n                    self._iter_candidates_from_constraints(\n                        identifier,\n                        constraint,\n                        template=ireqs[0],\n                    ),\n                )\n            except UnsupportedWheel:\n                # If we're constrained to install a wheel incompatible with the\n                # target architecture, no candidates will ever be valid.\n                return ()\n\n        # Since we cache all the candidates, incompatibility identification\n        # can be made quicker by comparing only the id() values.\n        incompat_ids = {id(c) for c in incompatibilities.get(identifier, ())}\n\n        # If none of the requirements want an explicit candidate, we can ask\n        # the finder for candidates.\n        if not explicit_candidates:\n            return self._iter_found_candidates(\n                ireqs,\n                constraint.specifier,\n                constraint.hashes,\n                prefers_installed,\n                incompat_ids,\n            )\n\n        return (\n            c\n            for c in explicit_candidates\n            if id(c) not in incompat_ids\n            and constraint.is_satisfied_by(c)\n            and all(is_satisfied_by(req, c) for req in requirements[identifier])\n        )\n\n    def _make_requirements_from_install_req(\n        self, ireq: InstallRequirement, requested_extras: Iterable[str]\n    ) -> Iterator[Requirement]:\n        \"\"\"\n        Returns requirement objects associated with the given InstallRequirement. In\n        most cases this will be a single object but the following special cases exist:\n            - the InstallRequirement has markers that do not apply -> result is empty\n            - the InstallRequirement has both a constraint (or link) and extras\n                -> result is split in two requirement objects: one with the constraint\n                (or link) and one with the extra. This allows centralized constraint\n                handling for the base, resulting in fewer candidate rejections.\n        \"\"\"\n        if not ireq.match_markers(requested_extras):\n            logger.info(\n                \"Ignoring %s: markers '%s' don't match your environment\",\n                ireq.name,\n                ireq.markers,\n            )\n        elif not ireq.link:\n            if ireq.extras and ireq.req is not None and ireq.req.specifier:\n                yield SpecifierWithoutExtrasRequirement(ireq)\n            yield SpecifierRequirement(ireq)\n        else:\n            self._fail_if_link_is_unsupported_wheel(ireq.link)\n            # Always make the link candidate for the base requirement to make it\n            # available to `find_candidates` for explicit candidate lookup for any\n            # set of extras.\n            # The extras are required separately via a second requirement.\n            cand = self._make_base_candidate_from_link(\n                ireq.link,\n                template=install_req_drop_extras(ireq) if ireq.extras else ireq,\n                name=canonicalize_name(ireq.name) if ireq.name else None,\n                version=None,\n            )\n            if cand is None:\n                # There's no way we can satisfy a URL requirement if the underlying\n                # candidate fails to build. An unnamed URL must be user-supplied, so\n                # we fail eagerly. If the URL is named, an unsatisfiable requirement\n                # can make the resolver do the right thing, either backtrack (and\n                # maybe find some other requirement that's buildable) or raise a\n                # ResolutionImpossible eventually.\n                if not ireq.name:\n                    raise self._build_failures[ireq.link]\n                yield UnsatisfiableRequirement(canonicalize_name(ireq.name))\n            else:\n                # require the base from the link\n                yield self.make_requirement_from_candidate(cand)\n                if ireq.extras:\n                    # require the extras on top of the base candidate\n                    yield self.make_requirement_from_candidate(\n                        self._make_extras_candidate(cand, frozenset(ireq.extras))\n                    )\n\n    def collect_root_requirements(\n        self, root_ireqs: List[InstallRequirement]\n    ) -> CollectedRootRequirements:\n        collected = CollectedRootRequirements([], {}, {})\n        for i, ireq in enumerate(root_ireqs):\n            if ireq.constraint:\n                # Ensure we only accept valid constraints\n                problem = check_invalid_constraint_type(ireq)\n                if problem:\n                    raise InstallationError(problem)\n                if not ireq.match_markers():\n                    continue\n                assert ireq.name, \"Constraint must be named\"\n                name = canonicalize_name(ireq.name)\n                if name in collected.constraints:\n                    collected.constraints[name] &= ireq\n                else:\n                    collected.constraints[name] = Constraint.from_ireq(ireq)\n            else:\n                reqs = list(\n                    self._make_requirements_from_install_req(\n                        ireq,\n                        requested_extras=(),\n                    )\n                )\n                if not reqs:\n                    continue\n                template = reqs[0]\n                if ireq.user_supplied and template.name not in collected.user_requested:\n                    collected.user_requested[template.name] = i\n                collected.requirements.extend(reqs)\n        # Put requirements with extras at the end of the root requires. This does not\n        # affect resolvelib's picking preference but it does affect its initial criteria\n        # population: by putting extras at the end we enable the candidate finder to\n        # present resolvelib with a smaller set of candidates to resolvelib, already\n        # taking into account any non-transient constraints on the associated base. This\n        # means resolvelib will have fewer candidates to visit and reject.\n        # Python's list sort is stable, meaning relative order is kept for objects with\n        # the same key.\n        collected.requirements.sort(key=lambda r: r.name != r.project_name)\n        return collected\n\n    def make_requirement_from_candidate(\n        self, candidate: Candidate\n    ) -> ExplicitRequirement:\n        return ExplicitRequirement(candidate)\n\n    def make_requirements_from_spec(\n        self,\n        specifier: str,\n        comes_from: Optional[InstallRequirement],\n        requested_extras: Iterable[str] = (),\n    ) -> Iterator[Requirement]:\n        \"\"\"\n        Returns requirement objects associated with the given specifier. In most cases\n        this will be a single object but the following special cases exist:\n            - the specifier has markers that do not apply -> result is empty\n            - the specifier has both a constraint and extras -> result is split\n                in two requirement objects: one with the constraint and one with the\n                extra. This allows centralized constraint handling for the base,\n                resulting in fewer candidate rejections.\n        \"\"\"\n        ireq = self._make_install_req_from_spec(specifier, comes_from)\n        return self._make_requirements_from_install_req(ireq, requested_extras)\n\n    def make_requires_python_requirement(\n        self,\n        specifier: SpecifierSet,\n    ) -> Optional[Requirement]:\n        if self._ignore_requires_python:\n            return None\n        # Don't bother creating a dependency for an empty Requires-Python.\n        if not str(specifier):\n            return None\n        return RequiresPythonRequirement(specifier, self._python_candidate)\n\n    def get_wheel_cache_entry(\n        self, link: Link, name: Optional[str]\n    ) -> Optional[CacheEntry]:\n        \"\"\"Look up the link in the wheel cache.\n\n        If ``preparer.require_hashes`` is True, don't use the wheel cache,\n        because cached wheels, always built locally, have different hashes\n        than the files downloaded from the index server and thus throw false\n        hash mismatches. Furthermore, cached wheels at present have\n        nondeterministic contents due to file modification times.\n        \"\"\"\n        if self._wheel_cache is None:\n            return None\n        return self._wheel_cache.get_cache_entry(\n            link=link,\n            package_name=name,\n            supported_tags=self._supported_tags_cache,\n        )\n\n    def get_dist_to_uninstall(self, candidate: Candidate) -> Optional[BaseDistribution]:\n        # TODO: Are there more cases this needs to return True? Editable?\n        dist = self._installed_dists.get(candidate.project_name)\n        if dist is None:  # Not installed, no uninstallation required.\n            return None\n\n        # We're installing into global site. The current installation must\n        # be uninstalled, no matter it's in global or user site, because the\n        # user site installation has precedence over global.\n        if not self._use_user_site:\n            return dist\n\n        # We're installing into user site. Remove the user site installation.\n        if dist.in_usersite:\n            return dist\n\n        # We're installing into user site, but the installed incompatible\n        # package is in global site. We can't uninstall that, and would let\n        # the new user installation to \"shadow\" it. But shadowing won't work\n        # in virtual environments, so we error out.\n        if running_under_virtualenv() and dist.in_site_packages:\n            message = (\n                f\"Will not install to the user site because it will lack \"\n                f\"sys.path precedence to {dist.raw_name} in {dist.location}\"\n            )\n            raise InstallationError(message)\n        return None\n\n    def _report_requires_python_error(\n        self, causes: Sequence[\"ConflictCause\"]\n    ) -> UnsupportedPythonVersion:\n        assert causes, \"Requires-Python error reported with no cause\"\n\n        version = self._python_candidate.version\n\n        if len(causes) == 1:\n            specifier = str(causes[0].requirement.specifier)\n            message = (\n                f\"Package {causes[0].parent.name!r} requires a different \"\n                f\"Python: {version} not in {specifier!r}\"\n            )\n            return UnsupportedPythonVersion(message)\n\n        message = f\"Packages require a different Python. {version} not in:\"\n        for cause in causes:\n            package = cause.parent.format_for_error()\n            specifier = str(cause.requirement.specifier)\n            message += f\"\\n{specifier!r} (required by {package})\"\n        return UnsupportedPythonVersion(message)\n\n    def _report_single_requirement_conflict(\n        self, req: Requirement, parent: Optional[Candidate]\n    ) -> DistributionNotFound:\n        if parent is None:\n            req_disp = str(req)\n        else:\n            req_disp = f\"{req} (from {parent.name})\"\n\n        cands = self._finder.find_all_candidates(req.project_name)\n        skipped_by_requires_python = self._finder.requires_python_skipped_reasons()\n\n        versions_set: Set[Version] = set()\n        yanked_versions_set: Set[Version] = set()\n        for c in cands:\n            is_yanked = c.link.is_yanked if c.link else False\n            if is_yanked:\n                yanked_versions_set.add(c.version)\n            else:\n                versions_set.add(c.version)\n\n        versions = [str(v) for v in sorted(versions_set)]\n        yanked_versions = [str(v) for v in sorted(yanked_versions_set)]\n\n        if yanked_versions:\n            # Saying \"version X is yanked\" isn't entirely accurate.\n            # https://github.com/pypa/pip/issues/11745#issuecomment-1402805842\n            logger.critical(\n                \"Ignored the following yanked versions: %s\",\n                \", \".join(yanked_versions) or \"none\",\n            )\n        if skipped_by_requires_python:\n            logger.critical(\n                \"Ignored the following versions that require a different python \"\n                \"version: %s\",\n                \"; \".join(skipped_by_requires_python) or \"none\",\n            )\n        logger.critical(\n            \"Could not find a version that satisfies the requirement %s \"\n            \"(from versions: %s)\",\n            req_disp,\n            \", \".join(versions) or \"none\",\n        )\n        if str(req) == \"requirements.txt\":\n            logger.info(\n                \"HINT: You are attempting to install a package literally \"\n                'named \"requirements.txt\" (which cannot exist). Consider '\n                \"using the '-r' flag to install the packages listed in \"\n                \"requirements.txt\"\n            )\n\n        return DistributionNotFound(f\"No matching distribution found for {req}\")\n\n    def get_installation_error(\n        self,\n        e: \"ResolutionImpossible[Requirement, Candidate]\",\n        constraints: Dict[str, Constraint],\n    ) -> InstallationError:\n        assert e.causes, \"Installation error reported with no cause\"\n\n        # If one of the things we can't solve is \"we need Python X.Y\",\n        # that is what we report.\n        requires_python_causes = [\n            cause\n            for cause in e.causes\n            if isinstance(cause.requirement, RequiresPythonRequirement)\n            and not cause.requirement.is_satisfied_by(self._python_candidate)\n        ]\n        if requires_python_causes:\n            # The comprehension above makes sure all Requirement instances are\n            # RequiresPythonRequirement, so let's cast for convenience.\n            return self._report_requires_python_error(\n                cast(\"Sequence[ConflictCause]\", requires_python_causes),\n            )\n\n        # Otherwise, we have a set of causes which can't all be satisfied\n        # at once.\n\n        # The simplest case is when we have *one* cause that can't be\n        # satisfied. We just report that case.\n        if len(e.causes) == 1:\n            req, parent = e.causes[0]\n            if req.name not in constraints:\n                return self._report_single_requirement_conflict(req, parent)\n\n        # OK, we now have a list of requirements that can't all be\n        # satisfied at once.\n\n        # A couple of formatting helpers\n        def text_join(parts: List[str]) -> str:\n            if len(parts) == 1:\n                return parts[0]\n\n            return \", \".join(parts[:-1]) + \" and \" + parts[-1]\n\n        def describe_trigger(parent: Candidate) -> str:\n            ireq = parent.get_install_requirement()\n            if not ireq or not ireq.comes_from:\n                return f\"{parent.name}=={parent.version}\"\n            if isinstance(ireq.comes_from, InstallRequirement):\n                return str(ireq.comes_from.name)\n            return str(ireq.comes_from)\n\n        triggers = set()\n        for req, parent in e.causes:\n            if parent is None:\n                # This is a root requirement, so we can report it directly\n                trigger = req.format_for_error()\n            else:\n                trigger = describe_trigger(parent)\n            triggers.add(trigger)\n\n        if triggers:\n            info = text_join(sorted(triggers))\n        else:\n            info = \"the requested packages\"\n\n        msg = (\n            f\"Cannot install {info} because these package versions \"\n            \"have conflicting dependencies.\"\n        )\n        logger.critical(msg)\n        msg = \"\\nThe conflict is caused by:\"\n\n        relevant_constraints = set()\n        for req, parent in e.causes:\n            if req.name in constraints:\n                relevant_constraints.add(req.name)\n            msg = msg + \"\\n    \"\n            if parent:\n                msg = msg + f\"{parent.name} {parent.version} depends on \"\n            else:\n                msg = msg + \"The user requested \"\n            msg = msg + req.format_for_error()\n        for key in relevant_constraints:\n            spec = constraints[key].specifier\n            msg += f\"\\n    The user requested (constraint) {key}{spec}\"\n\n        msg = (\n            msg\n            + \"\\n\\n\"\n            + \"To fix this you could try to:\\n\"\n            + \"1. loosen the range of package versions you've specified\\n\"\n            + \"2. remove package versions to allow pip to attempt to solve \"\n            + \"the dependency conflict\\n\"\n        )\n\n        logger.info(msg)\n\n        return DistributionNotFound(\n            \"ResolutionImpossible: for help visit \"\n            \"https://pip.pypa.io/en/latest/topics/dependency-resolution/\"\n            \"#dealing-with-dependency-conflicts\"\n        )\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/resolution/resolvelib/found_candidates.py","size":6383,"sha1":"dea8e3a5eb8a9264e36d3efb323b742c285b22af","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"Utilities to lazily create and visit candidates found.\n\nCreating and visiting a candidate is a *very* costly operation. It involves\nfetching, extracting, potentially building modules from source, and verifying\ndistribution metadata. It is therefore crucial for performance to keep\neverything here lazy all the way down, so we only touch candidates that we\nabsolutely need, and not \"download the world\" when we only need one version of\nsomething.\n\"\"\"\n\nimport functools\nimport logging\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any, Callable, Iterator, Optional, Set, Tuple\n\nfrom pip._vendor.packaging.version import _BaseVersion\n\nfrom pip._internal.exceptions import MetadataInvalid\n\nfrom .base import Candidate\n\nlogger = logging.getLogger(__name__)\n\nIndexCandidateInfo = Tuple[_BaseVersion, Callable[[], Optional[Candidate]]]\n\nif TYPE_CHECKING:\n    SequenceCandidate = Sequence[Candidate]\nelse:\n    # For compatibility: Python before 3.9 does not support using [] on the\n    # Sequence class.\n    #\n    # >>> from collections.abc import Sequence\n    # >>> Sequence[str]\n    # Traceback (most recent call last):\n    #   File \"<stdin>\", line 1, in <module>\n    # TypeError: 'ABCMeta' object is not subscriptable\n    #\n    # TODO: Remove this block after dropping Python 3.8 support.\n    SequenceCandidate = Sequence\n\n\ndef _iter_built(infos: Iterator[IndexCandidateInfo]) -> Iterator[Candidate]:\n    \"\"\"Iterator for ``FoundCandidates``.\n\n    This iterator is used when the package is not already installed. Candidates\n    from index come later in their normal ordering.\n    \"\"\"\n    versions_found: Set[_BaseVersion] = set()\n    for version, func in infos:\n        if version in versions_found:\n            continue\n        try:\n            candidate = func()\n        except MetadataInvalid as e:\n            logger.warning(\n                \"Ignoring version %s of %s since it has invalid metadata:\\n\"\n                \"%s\\n\"\n                \"Please use pip<24.1 if you need to use this version.\",\n                version,\n                e.ireq.name,\n                e,\n            )\n            # Mark version as found to avoid trying other candidates with the same\n            # version, since they most likely have invalid metadata as well.\n            versions_found.add(version)\n        else:\n            if candidate is None:\n                continue\n            yield candidate\n            versions_found.add(version)\n\n\ndef _iter_built_with_prepended(\n    installed: Candidate, infos: Iterator[IndexCandidateInfo]\n) -> Iterator[Candidate]:\n    \"\"\"Iterator for ``FoundCandidates``.\n\n    This iterator is used when the resolver prefers the already-installed\n    candidate and NOT to upgrade. The installed candidate is therefore\n    always yielded first, and candidates from index come later in their\n    normal ordering, except skipped when the version is already installed.\n    \"\"\"\n    yield installed\n    versions_found: Set[_BaseVersion] = {installed.version}\n    for version, func in infos:\n        if version in versions_found:\n            continue\n        candidate = func()\n        if candidate is None:\n            continue\n        yield candidate\n        versions_found.add(version)\n\n\ndef _iter_built_with_inserted(\n    installed: Candidate, infos: Iterator[IndexCandidateInfo]\n) -> Iterator[Candidate]:\n    \"\"\"Iterator for ``FoundCandidates``.\n\n    This iterator is used when the resolver prefers to upgrade an\n    already-installed package. Candidates from index are returned in their\n    normal ordering, except replaced when the version is already installed.\n\n    The implementation iterates through and yields other candidates, inserting\n    the installed candidate exactly once before we start yielding older or\n    equivalent candidates, or after all other candidates if they are all newer.\n    \"\"\"\n    versions_found: Set[_BaseVersion] = set()\n    for version, func in infos:\n        if version in versions_found:\n            continue\n        # If the installed candidate is better, yield it first.\n        if installed.version >= version:\n            yield installed\n            versions_found.add(installed.version)\n        candidate = func()\n        if candidate is None:\n            continue\n        yield candidate\n        versions_found.add(version)\n\n    # If the installed candidate is older than all other candidates.\n    if installed.version not in versions_found:\n        yield installed\n\n\nclass FoundCandidates(SequenceCandidate):\n    \"\"\"A lazy sequence to provide candidates to the resolver.\n\n    The intended usage is to return this from `find_matches()` so the resolver\n    can iterate through the sequence multiple times, but only access the index\n    page when remote packages are actually needed. This improve performances\n    when suitable candidates are already installed on disk.\n    \"\"\"\n\n    def __init__(\n        self,\n        get_infos: Callable[[], Iterator[IndexCandidateInfo]],\n        installed: Optional[Candidate],\n        prefers_installed: bool,\n        incompatible_ids: Set[int],\n    ):\n        self._get_infos = get_infos\n        self._installed = installed\n        self._prefers_installed = prefers_installed\n        self._incompatible_ids = incompatible_ids\n\n    def __getitem__(self, index: Any) -> Any:\n        # Implemented to satisfy the ABC check. This is not needed by the\n        # resolver, and should not be used by the provider either (for\n        # performance reasons).\n        raise NotImplementedError(\"don't do this\")\n\n    def __iter__(self) -> Iterator[Candidate]:\n        infos = self._get_infos()\n        if not self._installed:\n            iterator = _iter_built(infos)\n        elif self._prefers_installed:\n            iterator = _iter_built_with_prepended(self._installed, infos)\n        else:\n            iterator = _iter_built_with_inserted(self._installed, infos)\n        return (c for c in iterator if id(c) not in self._incompatible_ids)\n\n    def __len__(self) -> int:\n        # Implemented to satisfy the ABC check. This is not needed by the\n        # resolver, and should not be used by the provider either (for\n        # performance reasons).\n        raise NotImplementedError(\"don't do this\")\n\n    @functools.lru_cache(maxsize=1)\n    def __bool__(self) -> bool:\n        if self._prefers_installed and self._installed:\n            return True\n        return any(self)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/resolution/resolvelib/provider.py","size":9935,"sha1":"3e485dbad5a5e8d46bf99e94ba6fdd57912b15a0","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import collections\nimport math\nfrom functools import lru_cache\nfrom typing import (\n    TYPE_CHECKING,\n    Dict,\n    Iterable,\n    Iterator,\n    Mapping,\n    Sequence,\n    TypeVar,\n    Union,\n)\n\nfrom pip._vendor.resolvelib.providers import AbstractProvider\n\nfrom .base import Candidate, Constraint, Requirement\nfrom .candidates import REQUIRES_PYTHON_IDENTIFIER\nfrom .factory import Factory\n\nif TYPE_CHECKING:\n    from pip._vendor.resolvelib.providers import Preference\n    from pip._vendor.resolvelib.resolvers import RequirementInformation\n\n    PreferenceInformation = RequirementInformation[Requirement, Candidate]\n\n    _ProviderBase = AbstractProvider[Requirement, Candidate, str]\nelse:\n    _ProviderBase = AbstractProvider\n\n# Notes on the relationship between the provider, the factory, and the\n# candidate and requirement classes.\n#\n# The provider is a direct implementation of the resolvelib class. Its role\n# is to deliver the API that resolvelib expects.\n#\n# Rather than work with completely abstract \"requirement\" and \"candidate\"\n# concepts as resolvelib does, pip has concrete classes implementing these two\n# ideas. The API of Requirement and Candidate objects are defined in the base\n# classes, but essentially map fairly directly to the equivalent provider\n# methods. In particular, `find_matches` and `is_satisfied_by` are\n# requirement methods, and `get_dependencies` is a candidate method.\n#\n# The factory is the interface to pip's internal mechanisms. It is stateless,\n# and is created by the resolver and held as a property of the provider. It is\n# responsible for creating Requirement and Candidate objects, and provides\n# services to those objects (access to pip's finder and preparer).\n\n\nD = TypeVar(\"D\")\nV = TypeVar(\"V\")\n\n\ndef _get_with_identifier(\n    mapping: Mapping[str, V],\n    identifier: str,\n    default: D,\n) -> Union[D, V]:\n    \"\"\"Get item from a package name lookup mapping with a resolver identifier.\n\n    This extra logic is needed when the target mapping is keyed by package\n    name, which cannot be directly looked up with an identifier (which may\n    contain requested extras). Additional logic is added to also look up a value\n    by \"cleaning up\" the extras from the identifier.\n    \"\"\"\n    if identifier in mapping:\n        return mapping[identifier]\n    # HACK: Theoretically we should check whether this identifier is a valid\n    # \"NAME[EXTRAS]\" format, and parse out the name part with packaging or\n    # some regular expression. But since pip's resolver only spits out three\n    # kinds of identifiers: normalized PEP 503 names, normalized names plus\n    # extras, and Requires-Python, we can cheat a bit here.\n    name, open_bracket, _ = identifier.partition(\"[\")\n    if open_bracket and name in mapping:\n        return mapping[name]\n    return default\n\n\nclass PipProvider(_ProviderBase):\n    \"\"\"Pip's provider implementation for resolvelib.\n\n    :params constraints: A mapping of constraints specified by the user. Keys\n        are canonicalized project names.\n    :params ignore_dependencies: Whether the user specified ``--no-deps``.\n    :params upgrade_strategy: The user-specified upgrade strategy.\n    :params user_requested: A set of canonicalized package names that the user\n        supplied for pip to install/upgrade.\n    \"\"\"\n\n    def __init__(\n        self,\n        factory: Factory,\n        constraints: Dict[str, Constraint],\n        ignore_dependencies: bool,\n        upgrade_strategy: str,\n        user_requested: Dict[str, int],\n    ) -> None:\n        self._factory = factory\n        self._constraints = constraints\n        self._ignore_dependencies = ignore_dependencies\n        self._upgrade_strategy = upgrade_strategy\n        self._user_requested = user_requested\n        self._known_depths: Dict[str, float] = collections.defaultdict(lambda: math.inf)\n\n    def identify(self, requirement_or_candidate: Union[Requirement, Candidate]) -> str:\n        return requirement_or_candidate.name\n\n    def get_preference(\n        self,\n        identifier: str,\n        resolutions: Mapping[str, Candidate],\n        candidates: Mapping[str, Iterator[Candidate]],\n        information: Mapping[str, Iterable[\"PreferenceInformation\"]],\n        backtrack_causes: Sequence[\"PreferenceInformation\"],\n    ) -> \"Preference\":\n        \"\"\"Produce a sort key for given requirement based on preference.\n\n        The lower the return value is, the more preferred this group of\n        arguments is.\n\n        Currently pip considers the following in order:\n\n        * Prefer if any of the known requirements is \"direct\", e.g. points to an\n          explicit URL.\n        * If equal, prefer if any requirement is \"pinned\", i.e. contains\n          operator ``===`` or ``==``.\n        * If equal, calculate an approximate \"depth\" and resolve requirements\n          closer to the user-specified requirements first. If the depth cannot\n          by determined (eg: due to no matching parents), it is considered\n          infinite.\n        * Order user-specified requirements by the order they are specified.\n        * If equal, prefers \"non-free\" requirements, i.e. contains at least one\n          operator, such as ``>=`` or ``<``.\n        * If equal, order alphabetically for consistency (helps debuggability).\n        \"\"\"\n        try:\n            next(iter(information[identifier]))\n        except StopIteration:\n            # There is no information for this identifier, so there's no known\n            # candidates.\n            has_information = False\n        else:\n            has_information = True\n\n        if has_information:\n            lookups = (r.get_candidate_lookup() for r, _ in information[identifier])\n            candidate, ireqs = zip(*lookups)\n        else:\n            candidate, ireqs = None, ()\n\n        operators = [\n            specifier.operator\n            for specifier_set in (ireq.specifier for ireq in ireqs if ireq)\n            for specifier in specifier_set\n        ]\n\n        direct = candidate is not None\n        pinned = any(op[:2] == \"==\" for op in operators)\n        unfree = bool(operators)\n\n        try:\n            requested_order: Union[int, float] = self._user_requested[identifier]\n        except KeyError:\n            requested_order = math.inf\n            if has_information:\n                parent_depths = (\n                    self._known_depths[parent.name] if parent is not None else 0.0\n                    for _, parent in information[identifier]\n                )\n                inferred_depth = min(d for d in parent_depths) + 1.0\n            else:\n                inferred_depth = math.inf\n        else:\n            inferred_depth = 1.0\n        self._known_depths[identifier] = inferred_depth\n\n        requested_order = self._user_requested.get(identifier, math.inf)\n\n        # Requires-Python has only one candidate and the check is basically\n        # free, so we always do it first to avoid needless work if it fails.\n        requires_python = identifier == REQUIRES_PYTHON_IDENTIFIER\n\n        # Prefer the causes of backtracking on the assumption that the problem\n        # resolving the dependency tree is related to the failures that caused\n        # the backtracking\n        backtrack_cause = self.is_backtrack_cause(identifier, backtrack_causes)\n\n        return (\n            not requires_python,\n            not direct,\n            not pinned,\n            not backtrack_cause,\n            inferred_depth,\n            requested_order,\n            not unfree,\n            identifier,\n        )\n\n    def find_matches(\n        self,\n        identifier: str,\n        requirements: Mapping[str, Iterator[Requirement]],\n        incompatibilities: Mapping[str, Iterator[Candidate]],\n    ) -> Iterable[Candidate]:\n        def _eligible_for_upgrade(identifier: str) -> bool:\n            \"\"\"Are upgrades allowed for this project?\n\n            This checks the upgrade strategy, and whether the project was one\n            that the user specified in the command line, in order to decide\n            whether we should upgrade if there's a newer version available.\n\n            (Note that we don't need access to the `--upgrade` flag, because\n            an upgrade strategy of \"to-satisfy-only\" means that `--upgrade`\n            was not specified).\n            \"\"\"\n            if self._upgrade_strategy == \"eager\":\n                return True\n            elif self._upgrade_strategy == \"only-if-needed\":\n                user_order = _get_with_identifier(\n                    self._user_requested,\n                    identifier,\n                    default=None,\n                )\n                return user_order is not None\n            return False\n\n        constraint = _get_with_identifier(\n            self._constraints,\n            identifier,\n            default=Constraint.empty(),\n        )\n        return self._factory.find_candidates(\n            identifier=identifier,\n            requirements=requirements,\n            constraint=constraint,\n            prefers_installed=(not _eligible_for_upgrade(identifier)),\n            incompatibilities=incompatibilities,\n            is_satisfied_by=self.is_satisfied_by,\n        )\n\n    @lru_cache(maxsize=None)\n    def is_satisfied_by(self, requirement: Requirement, candidate: Candidate) -> bool:\n        return requirement.is_satisfied_by(candidate)\n\n    def get_dependencies(self, candidate: Candidate) -> Sequence[Requirement]:\n        with_requires = not self._ignore_dependencies\n        return [r for r in candidate.iter_dependencies(with_requires) if r is not None]\n\n    @staticmethod\n    def is_backtrack_cause(\n        identifier: str, backtrack_causes: Sequence[\"PreferenceInformation\"]\n    ) -> bool:\n        for backtrack_cause in backtrack_causes:\n            if identifier == backtrack_cause.requirement.name:\n                return True\n            if backtrack_cause.parent and identifier == backtrack_cause.parent.name:\n                return True\n        return False\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/resolution/resolvelib/reporter.py","size":3168,"sha1":"8450449913d8080bac83964245f253e118a3c0d8","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"from collections import defaultdict\nfrom logging import getLogger\nfrom typing import Any, DefaultDict\n\nfrom pip._vendor.resolvelib.reporters import BaseReporter\n\nfrom .base import Candidate, Requirement\n\nlogger = getLogger(__name__)\n\n\nclass PipReporter(BaseReporter):\n    def __init__(self) -> None:\n        self.reject_count_by_package: DefaultDict[str, int] = defaultdict(int)\n\n        self._messages_at_reject_count = {\n            1: (\n                \"pip is looking at multiple versions of {package_name} to \"\n                \"determine which version is compatible with other \"\n                \"requirements. This could take a while.\"\n            ),\n            8: (\n                \"pip is still looking at multiple versions of {package_name} to \"\n                \"determine which version is compatible with other \"\n                \"requirements. This could take a while.\"\n            ),\n            13: (\n                \"This is taking longer than usual. You might need to provide \"\n                \"the dependency resolver with stricter constraints to reduce \"\n                \"runtime. See https://pip.pypa.io/warnings/backtracking for \"\n                \"guidance. If you want to abort this run, press Ctrl + C.\"\n            ),\n        }\n\n    def rejecting_candidate(self, criterion: Any, candidate: Candidate) -> None:\n        self.reject_count_by_package[candidate.name] += 1\n\n        count = self.reject_count_by_package[candidate.name]\n        if count not in self._messages_at_reject_count:\n            return\n\n        message = self._messages_at_reject_count[count]\n        logger.info(\"INFO: %s\", message.format(package_name=candidate.name))\n\n        msg = \"Will try a different candidate, due to conflict:\"\n        for req_info in criterion.information:\n            req, parent = req_info.requirement, req_info.parent\n            # Inspired by Factory.get_installation_error\n            msg += \"\\n    \"\n            if parent:\n                msg += f\"{parent.name} {parent.version} depends on \"\n            else:\n                msg += \"The user requested \"\n            msg += req.format_for_error()\n        logger.debug(msg)\n\n\nclass PipDebuggingReporter(BaseReporter):\n    \"\"\"A reporter that does an info log for every event it sees.\"\"\"\n\n    def starting(self) -> None:\n        logger.info(\"Reporter.starting()\")\n\n    def starting_round(self, index: int) -> None:\n        logger.info(\"Reporter.starting_round(%r)\", index)\n\n    def ending_round(self, index: int, state: Any) -> None:\n        logger.info(\"Reporter.ending_round(%r, state)\", index)\n        logger.debug(\"Reporter.ending_round(%r, %r)\", index, state)\n\n    def ending(self, state: Any) -> None:\n        logger.info(\"Reporter.ending(%r)\", state)\n\n    def adding_requirement(self, requirement: Requirement, parent: Candidate) -> None:\n        logger.info(\"Reporter.adding_requirement(%r, %r)\", requirement, parent)\n\n    def rejecting_candidate(self, criterion: Any, candidate: Candidate) -> None:\n        logger.info(\"Reporter.rejecting_candidate(%r, %r)\", criterion, candidate)\n\n    def pinning(self, candidate: Candidate) -> None:\n        logger.info(\"Reporter.pinning(%r)\", candidate)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/resolution/resolvelib/requirements.py","size":8065,"sha1":"784f726b843ca893bfae2d2db8e4832391e6740b","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"from typing import Any, Optional\n\nfrom pip._vendor.packaging.specifiers import SpecifierSet\nfrom pip._vendor.packaging.utils import NormalizedName, canonicalize_name\n\nfrom pip._internal.req.constructors import install_req_drop_extras\nfrom pip._internal.req.req_install import InstallRequirement\n\nfrom .base import Candidate, CandidateLookup, Requirement, format_name\n\n\nclass ExplicitRequirement(Requirement):\n    def __init__(self, candidate: Candidate) -> None:\n        self.candidate = candidate\n\n    def __str__(self) -> str:\n        return str(self.candidate)\n\n    def __repr__(self) -> str:\n        return f\"{self.__class__.__name__}({self.candidate!r})\"\n\n    def __hash__(self) -> int:\n        return hash(self.candidate)\n\n    def __eq__(self, other: Any) -> bool:\n        if not isinstance(other, ExplicitRequirement):\n            return False\n        return self.candidate == other.candidate\n\n    @property\n    def project_name(self) -> NormalizedName:\n        # No need to canonicalize - the candidate did this\n        return self.candidate.project_name\n\n    @property\n    def name(self) -> str:\n        # No need to canonicalize - the candidate did this\n        return self.candidate.name\n\n    def format_for_error(self) -> str:\n        return self.candidate.format_for_error()\n\n    def get_candidate_lookup(self) -> CandidateLookup:\n        return self.candidate, None\n\n    def is_satisfied_by(self, candidate: Candidate) -> bool:\n        return candidate == self.candidate\n\n\nclass SpecifierRequirement(Requirement):\n    def __init__(self, ireq: InstallRequirement) -> None:\n        assert ireq.link is None, \"This is a link, not a specifier\"\n        self._ireq = ireq\n        self._equal_cache: Optional[str] = None\n        self._hash: Optional[int] = None\n        self._extras = frozenset(canonicalize_name(e) for e in self._ireq.extras)\n\n    @property\n    def _equal(self) -> str:\n        if self._equal_cache is not None:\n            return self._equal_cache\n\n        self._equal_cache = str(self._ireq)\n        return self._equal_cache\n\n    def __str__(self) -> str:\n        return str(self._ireq.req)\n\n    def __repr__(self) -> str:\n        return f\"{self.__class__.__name__}({str(self._ireq.req)!r})\"\n\n    def __eq__(self, other: object) -> bool:\n        if not isinstance(other, SpecifierRequirement):\n            return NotImplemented\n        return self._equal == other._equal\n\n    def __hash__(self) -> int:\n        if self._hash is not None:\n            return self._hash\n\n        self._hash = hash(self._equal)\n        return self._hash\n\n    @property\n    def project_name(self) -> NormalizedName:\n        assert self._ireq.req, \"Specifier-backed ireq is always PEP 508\"\n        return canonicalize_name(self._ireq.req.name)\n\n    @property\n    def name(self) -> str:\n        return format_name(self.project_name, self._extras)\n\n    def format_for_error(self) -> str:\n        # Convert comma-separated specifiers into \"A, B, ..., F and G\"\n        # This makes the specifier a bit more \"human readable\", without\n        # risking a change in meaning. (Hopefully! Not all edge cases have\n        # been checked)\n        parts = [s.strip() for s in str(self).split(\",\")]\n        if len(parts) == 0:\n            return \"\"\n        elif len(parts) == 1:\n            return parts[0]\n\n        return \", \".join(parts[:-1]) + \" and \" + parts[-1]\n\n    def get_candidate_lookup(self) -> CandidateLookup:\n        return None, self._ireq\n\n    def is_satisfied_by(self, candidate: Candidate) -> bool:\n        assert candidate.name == self.name, (\n            f\"Internal issue: Candidate is not for this requirement \"\n            f\"{candidate.name} vs {self.name}\"\n        )\n        # We can safely always allow prereleases here since PackageFinder\n        # already implements the prerelease logic, and would have filtered out\n        # prerelease candidates if the user does not expect them.\n        assert self._ireq.req, \"Specifier-backed ireq is always PEP 508\"\n        spec = self._ireq.req.specifier\n        return spec.contains(candidate.version, prereleases=True)\n\n\nclass SpecifierWithoutExtrasRequirement(SpecifierRequirement):\n    \"\"\"\n    Requirement backed by an install requirement on a base package.\n    Trims extras from its install requirement if there are any.\n    \"\"\"\n\n    def __init__(self, ireq: InstallRequirement) -> None:\n        assert ireq.link is None, \"This is a link, not a specifier\"\n        self._ireq = install_req_drop_extras(ireq)\n        self._equal_cache: Optional[str] = None\n        self._hash: Optional[int] = None\n        self._extras = frozenset(canonicalize_name(e) for e in self._ireq.extras)\n\n    @property\n    def _equal(self) -> str:\n        if self._equal_cache is not None:\n            return self._equal_cache\n\n        self._equal_cache = str(self._ireq)\n        return self._equal_cache\n\n    def __eq__(self, other: object) -> bool:\n        if not isinstance(other, SpecifierWithoutExtrasRequirement):\n            return NotImplemented\n        return self._equal == other._equal\n\n    def __hash__(self) -> int:\n        if self._hash is not None:\n            return self._hash\n\n        self._hash = hash(self._equal)\n        return self._hash\n\n\nclass RequiresPythonRequirement(Requirement):\n    \"\"\"A requirement representing Requires-Python metadata.\"\"\"\n\n    def __init__(self, specifier: SpecifierSet, match: Candidate) -> None:\n        self.specifier = specifier\n        self._specifier_string = str(specifier)  # for faster __eq__\n        self._hash: Optional[int] = None\n        self._candidate = match\n\n    def __str__(self) -> str:\n        return f\"Python {self.specifier}\"\n\n    def __repr__(self) -> str:\n        return f\"{self.__class__.__name__}({str(self.specifier)!r})\"\n\n    def __hash__(self) -> int:\n        if self._hash is not None:\n            return self._hash\n\n        self._hash = hash((self._specifier_string, self._candidate))\n        return self._hash\n\n    def __eq__(self, other: Any) -> bool:\n        if not isinstance(other, RequiresPythonRequirement):\n            return False\n        return (\n            self._specifier_string == other._specifier_string\n            and self._candidate == other._candidate\n        )\n\n    @property\n    def project_name(self) -> NormalizedName:\n        return self._candidate.project_name\n\n    @property\n    def name(self) -> str:\n        return self._candidate.name\n\n    def format_for_error(self) -> str:\n        return str(self)\n\n    def get_candidate_lookup(self) -> CandidateLookup:\n        if self.specifier.contains(self._candidate.version, prereleases=True):\n            return self._candidate, None\n        return None, None\n\n    def is_satisfied_by(self, candidate: Candidate) -> bool:\n        assert candidate.name == self._candidate.name, \"Not Python candidate\"\n        # We can safely always allow prereleases here since PackageFinder\n        # already implements the prerelease logic, and would have filtered out\n        # prerelease candidates if the user does not expect them.\n        return self.specifier.contains(candidate.version, prereleases=True)\n\n\nclass UnsatisfiableRequirement(Requirement):\n    \"\"\"A requirement that cannot be satisfied.\"\"\"\n\n    def __init__(self, name: NormalizedName) -> None:\n        self._name = name\n\n    def __str__(self) -> str:\n        return f\"{self._name} (unavailable)\"\n\n    def __repr__(self) -> str:\n        return f\"{self.__class__.__name__}({str(self._name)!r})\"\n\n    def __eq__(self, other: object) -> bool:\n        if not isinstance(other, UnsatisfiableRequirement):\n            return NotImplemented\n        return self._name == other._name\n\n    def __hash__(self) -> int:\n        return hash(self._name)\n\n    @property\n    def project_name(self) -> NormalizedName:\n        return self._name\n\n    @property\n    def name(self) -> str:\n        return self._name\n\n    def format_for_error(self) -> str:\n        return str(self)\n\n    def get_candidate_lookup(self) -> CandidateLookup:\n        return None, None\n\n    def is_satisfied_by(self, candidate: Candidate) -> bool:\n        return False\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/resolution/resolvelib/resolver.py","size":12592,"sha1":"1fd155fcfa0a1547f514e35c4013a1c214e64d6d","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import contextlib\nimport functools\nimport logging\nimport os\nfrom typing import TYPE_CHECKING, Dict, List, Optional, Set, Tuple, cast\n\nfrom pip._vendor.packaging.utils import canonicalize_name\nfrom pip._vendor.resolvelib import BaseReporter, ResolutionImpossible\nfrom pip._vendor.resolvelib import Resolver as RLResolver\nfrom pip._vendor.resolvelib.structs import DirectedGraph\n\nfrom pip._internal.cache import WheelCache\nfrom pip._internal.index.package_finder import PackageFinder\nfrom pip._internal.operations.prepare import RequirementPreparer\nfrom pip._internal.req.constructors import install_req_extend_extras\nfrom pip._internal.req.req_install import InstallRequirement\nfrom pip._internal.req.req_set import RequirementSet\nfrom pip._internal.resolution.base import BaseResolver, InstallRequirementProvider\nfrom pip._internal.resolution.resolvelib.provider import PipProvider\nfrom pip._internal.resolution.resolvelib.reporter import (\n    PipDebuggingReporter,\n    PipReporter,\n)\nfrom pip._internal.utils.packaging import get_requirement\n\nfrom .base import Candidate, Requirement\nfrom .factory import Factory\n\nif TYPE_CHECKING:\n    from pip._vendor.resolvelib.resolvers import Result as RLResult\n\n    Result = RLResult[Requirement, Candidate, str]\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass Resolver(BaseResolver):\n    _allowed_strategies = {\"eager\", \"only-if-needed\", \"to-satisfy-only\"}\n\n    def __init__(\n        self,\n        preparer: RequirementPreparer,\n        finder: PackageFinder,\n        wheel_cache: Optional[WheelCache],\n        make_install_req: InstallRequirementProvider,\n        use_user_site: bool,\n        ignore_dependencies: bool,\n        ignore_installed: bool,\n        ignore_requires_python: bool,\n        force_reinstall: bool,\n        upgrade_strategy: str,\n        py_version_info: Optional[Tuple[int, ...]] = None,\n    ):\n        super().__init__()\n        assert upgrade_strategy in self._allowed_strategies\n\n        self.factory = Factory(\n            finder=finder,\n            preparer=preparer,\n            make_install_req=make_install_req,\n            wheel_cache=wheel_cache,\n            use_user_site=use_user_site,\n            force_reinstall=force_reinstall,\n            ignore_installed=ignore_installed,\n            ignore_requires_python=ignore_requires_python,\n            py_version_info=py_version_info,\n        )\n        self.ignore_dependencies = ignore_dependencies\n        self.upgrade_strategy = upgrade_strategy\n        self._result: Optional[Result] = None\n\n    def resolve(\n        self, root_reqs: List[InstallRequirement], check_supported_wheels: bool\n    ) -> RequirementSet:\n        collected = self.factory.collect_root_requirements(root_reqs)\n        provider = PipProvider(\n            factory=self.factory,\n            constraints=collected.constraints,\n            ignore_dependencies=self.ignore_dependencies,\n            upgrade_strategy=self.upgrade_strategy,\n            user_requested=collected.user_requested,\n        )\n        if \"PIP_RESOLVER_DEBUG\" in os.environ:\n            reporter: BaseReporter = PipDebuggingReporter()\n        else:\n            reporter = PipReporter()\n        resolver: RLResolver[Requirement, Candidate, str] = RLResolver(\n            provider,\n            reporter,\n        )\n\n        try:\n            limit_how_complex_resolution_can_be = 200000\n            result = self._result = resolver.resolve(\n                collected.requirements, max_rounds=limit_how_complex_resolution_can_be\n            )\n\n        except ResolutionImpossible as e:\n            error = self.factory.get_installation_error(\n                cast(\"ResolutionImpossible[Requirement, Candidate]\", e),\n                collected.constraints,\n            )\n            raise error from e\n\n        req_set = RequirementSet(check_supported_wheels=check_supported_wheels)\n        # process candidates with extras last to ensure their base equivalent is\n        # already in the req_set if appropriate.\n        # Python's sort is stable so using a binary key function keeps relative order\n        # within both subsets.\n        for candidate in sorted(\n            result.mapping.values(), key=lambda c: c.name != c.project_name\n        ):\n            ireq = candidate.get_install_requirement()\n            if ireq is None:\n                if candidate.name != candidate.project_name:\n                    # extend existing req's extras\n                    with contextlib.suppress(KeyError):\n                        req = req_set.get_requirement(candidate.project_name)\n                        req_set.add_named_requirement(\n                            install_req_extend_extras(\n                                req, get_requirement(candidate.name).extras\n                            )\n                        )\n                continue\n\n            # Check if there is already an installation under the same name,\n            # and set a flag for later stages to uninstall it, if needed.\n            installed_dist = self.factory.get_dist_to_uninstall(candidate)\n            if installed_dist is None:\n                # There is no existing installation -- nothing to uninstall.\n                ireq.should_reinstall = False\n            elif self.factory.force_reinstall:\n                # The --force-reinstall flag is set -- reinstall.\n                ireq.should_reinstall = True\n            elif installed_dist.version != candidate.version:\n                # The installation is different in version -- reinstall.\n                ireq.should_reinstall = True\n            elif candidate.is_editable or installed_dist.editable:\n                # The incoming distribution is editable, or different in\n                # editable-ness to installation -- reinstall.\n                ireq.should_reinstall = True\n            elif candidate.source_link and candidate.source_link.is_file:\n                # The incoming distribution is under file://\n                if candidate.source_link.is_wheel:\n                    # is a local wheel -- do nothing.\n                    logger.info(\n                        \"%s is already installed with the same version as the \"\n                        \"provided wheel. Use --force-reinstall to force an \"\n                        \"installation of the wheel.\",\n                        ireq.name,\n                    )\n                    continue\n\n                # is a local sdist or path -- reinstall\n                ireq.should_reinstall = True\n            else:\n                continue\n\n            link = candidate.source_link\n            if link and link.is_yanked:\n                # The reason can contain non-ASCII characters, Unicode\n                # is required for Python 2.\n                msg = (\n                    \"The candidate selected for download or install is a \"\n                    \"yanked version: {name!r} candidate (version {version} \"\n                    \"at {link})\\nReason for being yanked: {reason}\"\n                ).format(\n                    name=candidate.name,\n                    version=candidate.version,\n                    link=link,\n                    reason=link.yanked_reason or \"<none given>\",\n                )\n                logger.warning(msg)\n\n            req_set.add_named_requirement(ireq)\n\n        reqs = req_set.all_requirements\n        self.factory.preparer.prepare_linked_requirements_more(reqs)\n        for req in reqs:\n            req.prepared = True\n            req.needs_more_preparation = False\n        return req_set\n\n    def get_installation_order(\n        self, req_set: RequirementSet\n    ) -> List[InstallRequirement]:\n        \"\"\"Get order for installation of requirements in RequirementSet.\n\n        The returned list contains a requirement before another that depends on\n        it. This helps ensure that the environment is kept consistent as they\n        get installed one-by-one.\n\n        The current implementation creates a topological ordering of the\n        dependency graph, giving more weight to packages with less\n        or no dependencies, while breaking any cycles in the graph at\n        arbitrary points. We make no guarantees about where the cycle\n        would be broken, other than it *would* be broken.\n        \"\"\"\n        assert self._result is not None, \"must call resolve() first\"\n\n        if not req_set.requirements:\n            # Nothing is left to install, so we do not need an order.\n            return []\n\n        graph = self._result.graph\n        weights = get_topological_weights(graph, set(req_set.requirements.keys()))\n\n        sorted_items = sorted(\n            req_set.requirements.items(),\n            key=functools.partial(_req_set_item_sorter, weights=weights),\n            reverse=True,\n        )\n        return [ireq for _, ireq in sorted_items]\n\n\ndef get_topological_weights(\n    graph: \"DirectedGraph[Optional[str]]\", requirement_keys: Set[str]\n) -> Dict[Optional[str], int]:\n    \"\"\"Assign weights to each node based on how \"deep\" they are.\n\n    This implementation may change at any point in the future without prior\n    notice.\n\n    We first simplify the dependency graph by pruning any leaves and giving them\n    the highest weight: a package without any dependencies should be installed\n    first. This is done again and again in the same way, giving ever less weight\n    to the newly found leaves. The loop stops when no leaves are left: all\n    remaining packages have at least one dependency left in the graph.\n\n    Then we continue with the remaining graph, by taking the length for the\n    longest path to any node from root, ignoring any paths that contain a single\n    node twice (i.e. cycles). This is done through a depth-first search through\n    the graph, while keeping track of the path to the node.\n\n    Cycles in the graph result would result in node being revisited while also\n    being on its own path. In this case, take no action. This helps ensure we\n    don't get stuck in a cycle.\n\n    When assigning weight, the longer path (i.e. larger length) is preferred.\n\n    We are only interested in the weights of packages that are in the\n    requirement_keys.\n    \"\"\"\n    path: Set[Optional[str]] = set()\n    weights: Dict[Optional[str], int] = {}\n\n    def visit(node: Optional[str]) -> None:\n        if node in path:\n            # We hit a cycle, so we'll break it here.\n            return\n\n        # Time to visit the children!\n        path.add(node)\n        for child in graph.iter_children(node):\n            visit(child)\n        path.remove(node)\n\n        if node not in requirement_keys:\n            return\n\n        last_known_parent_count = weights.get(node, 0)\n        weights[node] = max(last_known_parent_count, len(path))\n\n    # Simplify the graph, pruning leaves that have no dependencies.\n    # This is needed for large graphs (say over 200 packages) because the\n    # `visit` function is exponentially slower then, taking minutes.\n    # See https://github.com/pypa/pip/issues/10557\n    # We will loop until we explicitly break the loop.\n    while True:\n        leaves = set()\n        for key in graph:\n            if key is None:\n                continue\n            for _child in graph.iter_children(key):\n                # This means we have at least one child\n                break\n            else:\n                # No child.\n                leaves.add(key)\n        if not leaves:\n            # We are done simplifying.\n            break\n        # Calculate the weight for the leaves.\n        weight = len(graph) - 1\n        for leaf in leaves:\n            if leaf not in requirement_keys:\n                continue\n            weights[leaf] = weight\n        # Remove the leaves from the graph, making it simpler.\n        for leaf in leaves:\n            graph.remove(leaf)\n\n    # Visit the remaining graph.\n    # `None` is guaranteed to be the root node by resolvelib.\n    visit(None)\n\n    # Sanity check: all requirement keys should be in the weights,\n    # and no other keys should be in the weights.\n    difference = set(weights.keys()).difference(requirement_keys)\n    assert not difference, difference\n\n    return weights\n\n\ndef _req_set_item_sorter(\n    item: Tuple[str, InstallRequirement],\n    weights: Dict[Optional[str], int],\n) -> Tuple[int, str]:\n    \"\"\"Key function used to sort install requirements for installation.\n\n    Based on the \"weight\" mapping calculated in ``get_installation_order()``.\n    The canonical package name is returned as the second member as a tie-\n    breaker to ensure the result is predictable, which is useful in tests.\n    \"\"\"\n    name = canonicalize_name(item[0])\n    return weights[name], name\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/self_outdated_check.py","size":8318,"sha1":"7d3a637d66bc54b47d7d7b403e761a6ddddfcdc6","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import datetime\nimport functools\nimport hashlib\nimport json\nimport logging\nimport optparse\nimport os.path\nimport sys\nfrom dataclasses import dataclass\nfrom typing import Any, Callable, Dict, Optional\n\nfrom pip._vendor.packaging.version import Version\nfrom pip._vendor.packaging.version import parse as parse_version\nfrom pip._vendor.rich.console import Group\nfrom pip._vendor.rich.markup import escape\nfrom pip._vendor.rich.text import Text\n\nfrom pip._internal.index.collector import LinkCollector\nfrom pip._internal.index.package_finder import PackageFinder\nfrom pip._internal.metadata import get_default_environment\nfrom pip._internal.models.selection_prefs import SelectionPreferences\nfrom pip._internal.network.session import PipSession\nfrom pip._internal.utils.compat import WINDOWS\nfrom pip._internal.utils.entrypoints import (\n    get_best_invocation_for_this_pip,\n    get_best_invocation_for_this_python,\n)\nfrom pip._internal.utils.filesystem import adjacent_tmp_file, check_path_owner, replace\nfrom pip._internal.utils.misc import (\n    ExternallyManagedEnvironment,\n    check_externally_managed,\n    ensure_dir,\n)\n\n_WEEK = datetime.timedelta(days=7)\n\nlogger = logging.getLogger(__name__)\n\n\ndef _get_statefile_name(key: str) -> str:\n    key_bytes = key.encode()\n    name = hashlib.sha224(key_bytes).hexdigest()\n    return name\n\n\ndef _convert_date(isodate: str) -> datetime.datetime:\n    \"\"\"Convert an ISO format string to a date.\n\n    Handles the format 2020-01-22T14:24:01Z (trailing Z)\n    which is not supported by older versions of fromisoformat.\n    \"\"\"\n    return datetime.datetime.fromisoformat(isodate.replace(\"Z\", \"+00:00\"))\n\n\nclass SelfCheckState:\n    def __init__(self, cache_dir: str) -> None:\n        self._state: Dict[str, Any] = {}\n        self._statefile_path = None\n\n        # Try to load the existing state\n        if cache_dir:\n            self._statefile_path = os.path.join(\n                cache_dir, \"selfcheck\", _get_statefile_name(self.key)\n            )\n            try:\n                with open(self._statefile_path, encoding=\"utf-8\") as statefile:\n                    self._state = json.load(statefile)\n            except (OSError, ValueError, KeyError):\n                # Explicitly suppressing exceptions, since we don't want to\n                # error out if the cache file is invalid.\n                pass\n\n    @property\n    def key(self) -> str:\n        return sys.prefix\n\n    def get(self, current_time: datetime.datetime) -> Optional[str]:\n        \"\"\"Check if we have a not-outdated version loaded already.\"\"\"\n        if not self._state:\n            return None\n\n        if \"last_check\" not in self._state:\n            return None\n\n        if \"pypi_version\" not in self._state:\n            return None\n\n        # Determine if we need to refresh the state\n        last_check = _convert_date(self._state[\"last_check\"])\n        time_since_last_check = current_time - last_check\n        if time_since_last_check > _WEEK:\n            return None\n\n        return self._state[\"pypi_version\"]\n\n    def set(self, pypi_version: str, current_time: datetime.datetime) -> None:\n        # If we do not have a path to cache in, don't bother saving.\n        if not self._statefile_path:\n            return\n\n        # Check to make sure that we own the directory\n        if not check_path_owner(os.path.dirname(self._statefile_path)):\n            return\n\n        # Now that we've ensured the directory is owned by this user, we'll go\n        # ahead and make sure that all our directories are created.\n        ensure_dir(os.path.dirname(self._statefile_path))\n\n        state = {\n            # Include the key so it's easy to tell which pip wrote the\n            # file.\n            \"key\": self.key,\n            \"last_check\": current_time.isoformat(),\n            \"pypi_version\": pypi_version,\n        }\n\n        text = json.dumps(state, sort_keys=True, separators=(\",\", \":\"))\n\n        with adjacent_tmp_file(self._statefile_path) as f:\n            f.write(text.encode())\n\n        try:\n            # Since we have a prefix-specific state file, we can just\n            # overwrite whatever is there, no need to check.\n            replace(f.name, self._statefile_path)\n        except OSError:\n            # Best effort.\n            pass\n\n\n@dataclass\nclass UpgradePrompt:\n    old: str\n    new: str\n\n    def __rich__(self) -> Group:\n        if WINDOWS:\n            pip_cmd = f\"{get_best_invocation_for_this_python()} -m pip\"\n        else:\n            pip_cmd = get_best_invocation_for_this_pip()\n\n        notice = \"[bold][[reset][blue]notice[reset][bold]][reset]\"\n        return Group(\n            Text(),\n            Text.from_markup(\n                f\"{notice} A new release of pip is available: \"\n                f\"[red]{self.old}[reset] -> [green]{self.new}[reset]\"\n            ),\n            Text.from_markup(\n                f\"{notice} To update, run: \"\n                f\"[green]{escape(pip_cmd)} install --upgrade pip\"\n            ),\n        )\n\n\ndef was_installed_by_pip(pkg: str) -> bool:\n    \"\"\"Checks whether pkg was installed by pip\n\n    This is used not to display the upgrade message when pip is in fact\n    installed by system package manager, such as dnf on Fedora.\n    \"\"\"\n    dist = get_default_environment().get_distribution(pkg)\n    return dist is not None and \"pip\" == dist.installer\n\n\ndef _get_current_remote_pip_version(\n    session: PipSession, options: optparse.Values\n) -> Optional[str]:\n    # Lets use PackageFinder to see what the latest pip version is\n    link_collector = LinkCollector.create(\n        session,\n        options=options,\n        suppress_no_index=True,\n    )\n\n    # Pass allow_yanked=False so we don't suggest upgrading to a\n    # yanked version.\n    selection_prefs = SelectionPreferences(\n        allow_yanked=False,\n        allow_all_prereleases=False,  # Explicitly set to False\n    )\n\n    finder = PackageFinder.create(\n        link_collector=link_collector,\n        selection_prefs=selection_prefs,\n    )\n    best_candidate = finder.find_best_candidate(\"pip\").best_candidate\n    if best_candidate is None:\n        return None\n\n    return str(best_candidate.version)\n\n\ndef _self_version_check_logic(\n    *,\n    state: SelfCheckState,\n    current_time: datetime.datetime,\n    local_version: Version,\n    get_remote_version: Callable[[], Optional[str]],\n) -> Optional[UpgradePrompt]:\n    remote_version_str = state.get(current_time)\n    if remote_version_str is None:\n        remote_version_str = get_remote_version()\n        if remote_version_str is None:\n            logger.debug(\"No remote pip version found\")\n            return None\n        state.set(remote_version_str, current_time)\n\n    remote_version = parse_version(remote_version_str)\n    logger.debug(\"Remote version of pip: %s\", remote_version)\n    logger.debug(\"Local version of pip:  %s\", local_version)\n\n    pip_installed_by_pip = was_installed_by_pip(\"pip\")\n    logger.debug(\"Was pip installed by pip? %s\", pip_installed_by_pip)\n    if not pip_installed_by_pip:\n        return None  # Only suggest upgrade if pip is installed by pip.\n\n    local_version_is_older = (\n        local_version < remote_version\n        and local_version.base_version != remote_version.base_version\n    )\n    if local_version_is_older:\n        return UpgradePrompt(old=str(local_version), new=remote_version_str)\n\n    return None\n\n\ndef pip_self_version_check(session: PipSession, options: optparse.Values) -> None:\n    \"\"\"Check for an update for pip.\n\n    Limit the frequency of checks to once per week. State is stored either in\n    the active virtualenv or in the user's USER_CACHE_DIR keyed off the prefix\n    of the pip script path.\n    \"\"\"\n    installed_dist = get_default_environment().get_distribution(\"pip\")\n    if not installed_dist:\n        return\n    try:\n        check_externally_managed()\n    except ExternallyManagedEnvironment:\n        return\n\n    upgrade_prompt = _self_version_check_logic(\n        state=SelfCheckState(cache_dir=options.cache_dir),\n        current_time=datetime.datetime.now(datetime.timezone.utc),\n        local_version=installed_dist.version,\n        get_remote_version=functools.partial(\n            _get_current_remote_pip_version, session, options\n        ),\n    )\n    if upgrade_prompt is not None:\n        logger.warning(\"%s\", upgrade_prompt, extra={\"rich\": True})\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/utils/__init__.py","size":0,"sha1":"da39a3ee5e6b4b0d3255bfef95601890afd80709","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":""},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/utils/_jaraco_text.py","size":3350,"sha1":"6aa20e2c27dc77bf1257543461957f52a11b124a","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"Functions brought over from jaraco.text.\n\nThese functions are not supposed to be used within `pip._internal`. These are\nhelper functions brought over from `jaraco.text` to enable vendoring newer\ncopies of `pkg_resources` without having to vendor `jaraco.text` and its entire\ndependency cone; something that our vendoring setup is not currently capable of\nhandling.\n\nLicense reproduced from original source below:\n\nCopyright Jason R. Coombs\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to\ndeal in the Software without restriction, including without limitation the\nrights to use, copy, modify, merge, publish, distribute, sublicense, and/or\nsell copies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\nFROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS\nIN THE SOFTWARE.\n\"\"\"\n\nimport functools\nimport itertools\n\n\ndef _nonblank(str):\n    return str and not str.startswith(\"#\")\n\n\n@functools.singledispatch\ndef yield_lines(iterable):\n    r\"\"\"\n    Yield valid lines of a string or iterable.\n\n    >>> list(yield_lines(''))\n    []\n    >>> list(yield_lines(['foo', 'bar']))\n    ['foo', 'bar']\n    >>> list(yield_lines('foo\\nbar'))\n    ['foo', 'bar']\n    >>> list(yield_lines('\\nfoo\\n#bar\\nbaz #comment'))\n    ['foo', 'baz #comment']\n    >>> list(yield_lines(['foo\\nbar', 'baz', 'bing\\n\\n\\n']))\n    ['foo', 'bar', 'baz', 'bing']\n    \"\"\"\n    return itertools.chain.from_iterable(map(yield_lines, iterable))\n\n\n@yield_lines.register(str)\ndef _(text):\n    return filter(_nonblank, map(str.strip, text.splitlines()))\n\n\ndef drop_comment(line):\n    \"\"\"\n    Drop comments.\n\n    >>> drop_comment('foo # bar')\n    'foo'\n\n    A hash without a space may be in a URL.\n\n    >>> drop_comment('http://example.com/foo#bar')\n    'http://example.com/foo#bar'\n    \"\"\"\n    return line.partition(\" #\")[0]\n\n\ndef join_continuation(lines):\n    r\"\"\"\n    Join lines continued by a trailing backslash.\n\n    >>> list(join_continuation(['foo \\\\', 'bar', 'baz']))\n    ['foobar', 'baz']\n    >>> list(join_continuation(['foo \\\\', 'bar', 'baz']))\n    ['foobar', 'baz']\n    >>> list(join_continuation(['foo \\\\', 'bar \\\\', 'baz']))\n    ['foobarbaz']\n\n    Not sure why, but...\n    The character preceding the backslash is also elided.\n\n    >>> list(join_continuation(['goo\\\\', 'dly']))\n    ['godly']\n\n    A terrible idea, but...\n    If no line is available to continue, suppress the lines.\n\n    >>> list(join_continuation(['foo', 'bar\\\\', 'baz\\\\']))\n    ['foo']\n    \"\"\"\n    lines = iter(lines)\n    for item in lines:\n        while item.endswith(\"\\\\\"):\n            try:\n                item = item[:-2].strip() + next(lines)\n            except StopIteration:\n                return\n        yield item\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/utils/_log.py","size":1015,"sha1":"2c20b7d739a304f3715aea6b90eed634c2217c5f","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"Customize logging\n\nDefines custom logger class for the `logger.verbose(...)` method.\n\ninit_logging() must be called before any other modules that call logging.getLogger.\n\"\"\"\n\nimport logging\nfrom typing import Any, cast\n\n# custom log level for `--verbose` output\n# between DEBUG and INFO\nVERBOSE = 15\n\n\nclass VerboseLogger(logging.Logger):\n    \"\"\"Custom Logger, defining a verbose log-level\n\n    VERBOSE is between INFO and DEBUG.\n    \"\"\"\n\n    def verbose(self, msg: str, *args: Any, **kwargs: Any) -> None:\n        return self.log(VERBOSE, msg, *args, **kwargs)\n\n\ndef getLogger(name: str) -> VerboseLogger:\n    \"\"\"logging.getLogger, but ensures our VerboseLogger class is returned\"\"\"\n    return cast(VerboseLogger, logging.getLogger(name))\n\n\ndef init_logging() -> None:\n    \"\"\"Register our VerboseLogger and VERBOSE log level.\n\n    Should be called before any calls to getLogger(),\n    i.e. in pip._internal.__init__\n    \"\"\"\n    logging.setLoggerClass(VerboseLogger)\n    logging.addLevelName(VERBOSE, \"VERBOSE\")\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/utils/appdirs.py","size":1665,"sha1":"2f7d46108f0818d083ec0fdef4bef65ac5977583","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"\nThis code wraps the vendored appdirs module to so the return values are\ncompatible for the current pip code base.\n\nThe intention is to rewrite current usages gradually, keeping the tests pass,\nand eventually drop this after all usages are changed.\n\"\"\"\n\nimport os\nimport sys\nfrom typing import List\n\nfrom pip._vendor import platformdirs as _appdirs\n\n\ndef user_cache_dir(appname: str) -> str:\n    return _appdirs.user_cache_dir(appname, appauthor=False)\n\n\ndef _macos_user_config_dir(appname: str, roaming: bool = True) -> str:\n    # Use ~/Application Support/pip, if the directory exists.\n    path = _appdirs.user_data_dir(appname, appauthor=False, roaming=roaming)\n    if os.path.isdir(path):\n        return path\n\n    # Use a Linux-like ~/.config/pip, by default.\n    linux_like_path = \"~/.config/\"\n    if appname:\n        linux_like_path = os.path.join(linux_like_path, appname)\n\n    return os.path.expanduser(linux_like_path)\n\n\ndef user_config_dir(appname: str, roaming: bool = True) -> str:\n    if sys.platform == \"darwin\":\n        return _macos_user_config_dir(appname, roaming)\n\n    return _appdirs.user_config_dir(appname, appauthor=False, roaming=roaming)\n\n\n# for the discussion regarding site_config_dir locations\n# see <https://github.com/pypa/pip/issues/1733>\ndef site_config_dirs(appname: str) -> List[str]:\n    if sys.platform == \"darwin\":\n        return [_appdirs.site_data_dir(appname, appauthor=False, multipath=True)]\n\n    dirval = _appdirs.site_config_dir(appname, appauthor=False, multipath=True)\n    if sys.platform == \"win32\":\n        return [dirval]\n\n    # Unix-y system. Look in /etc as well.\n    return dirval.split(os.pathsep) + [\"/etc\"]\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/utils/compat.py","size":2399,"sha1":"5b96aa24f35b6a072b7ce1f2c3df09e01079bf7a","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"Stuff that differs in different Python versions and platform\ndistributions.\"\"\"\n\nimport importlib.resources\nimport logging\nimport os\nimport sys\nfrom typing import IO\n\n__all__ = [\"get_path_uid\", \"stdlib_pkgs\", \"WINDOWS\"]\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef has_tls() -> bool:\n    try:\n        import _ssl  # noqa: F401  # ignore unused\n\n        return True\n    except ImportError:\n        pass\n\n    from pip._vendor.urllib3.util import IS_PYOPENSSL\n\n    return IS_PYOPENSSL\n\n\ndef get_path_uid(path: str) -> int:\n    \"\"\"\n    Return path's uid.\n\n    Does not follow symlinks:\n        https://github.com/pypa/pip/pull/935#discussion_r5307003\n\n    Placed this function in compat due to differences on AIX and\n    Jython, that should eventually go away.\n\n    :raises OSError: When path is a symlink or can't be read.\n    \"\"\"\n    if hasattr(os, \"O_NOFOLLOW\"):\n        fd = os.open(path, os.O_RDONLY | os.O_NOFOLLOW)\n        file_uid = os.fstat(fd).st_uid\n        os.close(fd)\n    else:  # AIX and Jython\n        # WARNING: time of check vulnerability, but best we can do w/o NOFOLLOW\n        if not os.path.islink(path):\n            # older versions of Jython don't have `os.fstat`\n            file_uid = os.stat(path).st_uid\n        else:\n            # raise OSError for parity with os.O_NOFOLLOW above\n            raise OSError(f\"{path} is a symlink; Will not return uid for symlinks\")\n    return file_uid\n\n\n# The importlib.resources.open_text function was deprecated in 3.11 with suggested\n# replacement we use below.\nif sys.version_info < (3, 11):\n    open_text_resource = importlib.resources.open_text\nelse:\n\n    def open_text_resource(\n        package: str, resource: str, encoding: str = \"utf-8\", errors: str = \"strict\"\n    ) -> IO[str]:\n        return (importlib.resources.files(package) / resource).open(\n            \"r\", encoding=encoding, errors=errors\n        )\n\n\n# packages in the stdlib that may have installation metadata, but should not be\n# considered 'installed'.  this theoretically could be determined based on\n# dist.location (py27:`sysconfig.get_paths()['stdlib']`,\n# py26:sysconfig.get_config_vars('LIBDEST')), but fear platform variation may\n# make this ineffective, so hard-coding\nstdlib_pkgs = {\"python\", \"wsgiref\", \"argparse\"}\n\n\n# windows detection, covers cpython and ironpython\nWINDOWS = sys.platform.startswith(\"win\") or (sys.platform == \"cli\" and os.name == \"nt\")\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/utils/compatibility_tags.py","size":6272,"sha1":"4c4c227038a138d68816a032acfd6e48e38c00a7","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"Generate and work with PEP 425 Compatibility Tags.\n\"\"\"\n\nimport re\nfrom typing import List, Optional, Tuple\n\nfrom pip._vendor.packaging.tags import (\n    PythonVersion,\n    Tag,\n    compatible_tags,\n    cpython_tags,\n    generic_tags,\n    interpreter_name,\n    interpreter_version,\n    ios_platforms,\n    mac_platforms,\n)\n\n_apple_arch_pat = re.compile(r\"(.+)_(\\d+)_(\\d+)_(.+)\")\n\n\ndef version_info_to_nodot(version_info: Tuple[int, ...]) -> str:\n    # Only use up to the first two numbers.\n    return \"\".join(map(str, version_info[:2]))\n\n\ndef _mac_platforms(arch: str) -> List[str]:\n    match = _apple_arch_pat.match(arch)\n    if match:\n        name, major, minor, actual_arch = match.groups()\n        mac_version = (int(major), int(minor))\n        arches = [\n            # Since we have always only checked that the platform starts\n            # with \"macosx\", for backwards-compatibility we extract the\n            # actual prefix provided by the user in case they provided\n            # something like \"macosxcustom_\". It may be good to remove\n            # this as undocumented or deprecate it in the future.\n            \"{}_{}\".format(name, arch[len(\"macosx_\") :])\n            for arch in mac_platforms(mac_version, actual_arch)\n        ]\n    else:\n        # arch pattern didn't match (?!)\n        arches = [arch]\n    return arches\n\n\ndef _ios_platforms(arch: str) -> List[str]:\n    match = _apple_arch_pat.match(arch)\n    if match:\n        name, major, minor, actual_multiarch = match.groups()\n        ios_version = (int(major), int(minor))\n        arches = [\n            # Since we have always only checked that the platform starts\n            # with \"ios\", for backwards-compatibility we extract the\n            # actual prefix provided by the user in case they provided\n            # something like \"ioscustom_\". It may be good to remove\n            # this as undocumented or deprecate it in the future.\n            \"{}_{}\".format(name, arch[len(\"ios_\") :])\n            for arch in ios_platforms(ios_version, actual_multiarch)\n        ]\n    else:\n        # arch pattern didn't match (?!)\n        arches = [arch]\n    return arches\n\n\ndef _custom_manylinux_platforms(arch: str) -> List[str]:\n    arches = [arch]\n    arch_prefix, arch_sep, arch_suffix = arch.partition(\"_\")\n    if arch_prefix == \"manylinux2014\":\n        # manylinux1/manylinux2010 wheels run on most manylinux2014 systems\n        # with the exception of wheels depending on ncurses. PEP 599 states\n        # manylinux1/manylinux2010 wheels should be considered\n        # manylinux2014 wheels:\n        # https://www.python.org/dev/peps/pep-0599/#backwards-compatibility-with-manylinux2010-wheels\n        if arch_suffix in {\"i686\", \"x86_64\"}:\n            arches.append(\"manylinux2010\" + arch_sep + arch_suffix)\n            arches.append(\"manylinux1\" + arch_sep + arch_suffix)\n    elif arch_prefix == \"manylinux2010\":\n        # manylinux1 wheels run on most manylinux2010 systems with the\n        # exception of wheels depending on ncurses. PEP 571 states\n        # manylinux1 wheels should be considered manylinux2010 wheels:\n        # https://www.python.org/dev/peps/pep-0571/#backwards-compatibility-with-manylinux1-wheels\n        arches.append(\"manylinux1\" + arch_sep + arch_suffix)\n    return arches\n\n\ndef _get_custom_platforms(arch: str) -> List[str]:\n    arch_prefix, arch_sep, arch_suffix = arch.partition(\"_\")\n    if arch.startswith(\"macosx\"):\n        arches = _mac_platforms(arch)\n    elif arch.startswith(\"ios\"):\n        arches = _ios_platforms(arch)\n    elif arch_prefix in [\"manylinux2014\", \"manylinux2010\"]:\n        arches = _custom_manylinux_platforms(arch)\n    else:\n        arches = [arch]\n    return arches\n\n\ndef _expand_allowed_platforms(platforms: Optional[List[str]]) -> Optional[List[str]]:\n    if not platforms:\n        return None\n\n    seen = set()\n    result = []\n\n    for p in platforms:\n        if p in seen:\n            continue\n        additions = [c for c in _get_custom_platforms(p) if c not in seen]\n        seen.update(additions)\n        result.extend(additions)\n\n    return result\n\n\ndef _get_python_version(version: str) -> PythonVersion:\n    if len(version) > 1:\n        return int(version[0]), int(version[1:])\n    else:\n        return (int(version[0]),)\n\n\ndef _get_custom_interpreter(\n    implementation: Optional[str] = None, version: Optional[str] = None\n) -> str:\n    if implementation is None:\n        implementation = interpreter_name()\n    if version is None:\n        version = interpreter_version()\n    return f\"{implementation}{version}\"\n\n\ndef get_supported(\n    version: Optional[str] = None,\n    platforms: Optional[List[str]] = None,\n    impl: Optional[str] = None,\n    abis: Optional[List[str]] = None,\n) -> List[Tag]:\n    \"\"\"Return a list of supported tags for each version specified in\n    `versions`.\n\n    :param version: a string version, of the form \"33\" or \"32\",\n        or None. The version will be assumed to support our ABI.\n    :param platform: specify a list of platforms you want valid\n        tags for, or None. If None, use the local system platform.\n    :param impl: specify the exact implementation you want valid\n        tags for, or None. If None, use the local interpreter impl.\n    :param abis: specify a list of abis you want valid\n        tags for, or None. If None, use the local interpreter abi.\n    \"\"\"\n    supported: List[Tag] = []\n\n    python_version: Optional[PythonVersion] = None\n    if version is not None:\n        python_version = _get_python_version(version)\n\n    interpreter = _get_custom_interpreter(impl, version)\n\n    platforms = _expand_allowed_platforms(platforms)\n\n    is_cpython = (impl or interpreter_name()) == \"cp\"\n    if is_cpython:\n        supported.extend(\n            cpython_tags(\n                python_version=python_version,\n                abis=abis,\n                platforms=platforms,\n            )\n        )\n    else:\n        supported.extend(\n            generic_tags(\n                interpreter=interpreter,\n                abis=abis,\n                platforms=platforms,\n            )\n        )\n    supported.extend(\n        compatible_tags(\n            python_version=python_version,\n            interpreter=interpreter,\n            platforms=platforms,\n        )\n    )\n\n    return supported\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/utils/datetime.py","size":242,"sha1":"442dc5866a60dac7ca2578cd773c147e9e1c063a","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"For when pip wants to check the date or time.\n\"\"\"\n\nimport datetime\n\n\ndef today_is_later_than(year: int, month: int, day: int) -> bool:\n    today = datetime.date.today()\n    given = datetime.date(year, month, day)\n\n    return today > given\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/utils/deprecation.py","size":3707,"sha1":"9d3e78a239d0174e609be53553d0d823d549df15","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"\nA module that implements tooling to enable easy warnings about deprecations.\n\"\"\"\n\nimport logging\nimport warnings\nfrom typing import Any, Optional, TextIO, Type, Union\n\nfrom pip._vendor.packaging.version import parse\n\nfrom pip import __version__ as current_version  # NOTE: tests patch this name.\n\nDEPRECATION_MSG_PREFIX = \"DEPRECATION: \"\n\n\nclass PipDeprecationWarning(Warning):\n    pass\n\n\n_original_showwarning: Any = None\n\n\n# Warnings <-> Logging Integration\ndef _showwarning(\n    message: Union[Warning, str],\n    category: Type[Warning],\n    filename: str,\n    lineno: int,\n    file: Optional[TextIO] = None,\n    line: Optional[str] = None,\n) -> None:\n    if file is not None:\n        if _original_showwarning is not None:\n            _original_showwarning(message, category, filename, lineno, file, line)\n    elif issubclass(category, PipDeprecationWarning):\n        # We use a specially named logger which will handle all of the\n        # deprecation messages for pip.\n        logger = logging.getLogger(\"pip._internal.deprecations\")\n        logger.warning(message)\n    else:\n        _original_showwarning(message, category, filename, lineno, file, line)\n\n\ndef install_warning_logger() -> None:\n    # Enable our Deprecation Warnings\n    warnings.simplefilter(\"default\", PipDeprecationWarning, append=True)\n\n    global _original_showwarning\n\n    if _original_showwarning is None:\n        _original_showwarning = warnings.showwarning\n        warnings.showwarning = _showwarning\n\n\ndef deprecated(\n    *,\n    reason: str,\n    replacement: Optional[str],\n    gone_in: Optional[str],\n    feature_flag: Optional[str] = None,\n    issue: Optional[int] = None,\n) -> None:\n    \"\"\"Helper to deprecate existing functionality.\n\n    reason:\n        Textual reason shown to the user about why this functionality has\n        been deprecated. Should be a complete sentence.\n    replacement:\n        Textual suggestion shown to the user about what alternative\n        functionality they can use.\n    gone_in:\n        The version of pip does this functionality should get removed in.\n        Raises an error if pip's current version is greater than or equal to\n        this.\n    feature_flag:\n        Command-line flag of the form --use-feature={feature_flag} for testing\n        upcoming functionality.\n    issue:\n        Issue number on the tracker that would serve as a useful place for\n        users to find related discussion and provide feedback.\n    \"\"\"\n\n    # Determine whether or not the feature is already gone in this version.\n    is_gone = gone_in is not None and parse(current_version) >= parse(gone_in)\n\n    message_parts = [\n        (reason, f\"{DEPRECATION_MSG_PREFIX}{{}}\"),\n        (\n            gone_in,\n            (\n                \"pip {} will enforce this behaviour change.\"\n                if not is_gone\n                else \"Since pip {}, this is no longer supported.\"\n            ),\n        ),\n        (\n            replacement,\n            \"A possible replacement is {}.\",\n        ),\n        (\n            feature_flag,\n            (\n                \"You can use the flag --use-feature={} to test the upcoming behaviour.\"\n                if not is_gone\n                else None\n            ),\n        ),\n        (\n            issue,\n            \"Discussion can be found at https://github.com/pypa/pip/issues/{}\",\n        ),\n    ]\n\n    message = \" \".join(\n        format_str.format(value)\n        for value, format_str in message_parts\n        if format_str is not None and value is not None\n    )\n\n    # Raise as an error if this behaviour is deprecated.\n    if is_gone:\n        raise PipDeprecationWarning(message)\n\n    warnings.warn(message, category=PipDeprecationWarning, stacklevel=2)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/utils/direct_url_helpers.py","size":3196,"sha1":"588197601a7ea58749abb9033f8b50097b881549","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"from typing import Optional\n\nfrom pip._internal.models.direct_url import ArchiveInfo, DirectUrl, DirInfo, VcsInfo\nfrom pip._internal.models.link import Link\nfrom pip._internal.utils.urls import path_to_url\nfrom pip._internal.vcs import vcs\n\n\ndef direct_url_as_pep440_direct_reference(direct_url: DirectUrl, name: str) -> str:\n    \"\"\"Convert a DirectUrl to a pip requirement string.\"\"\"\n    direct_url.validate()  # if invalid, this is a pip bug\n    requirement = name + \" @ \"\n    fragments = []\n    if isinstance(direct_url.info, VcsInfo):\n        requirement += (\n            f\"{direct_url.info.vcs}+{direct_url.url}@{direct_url.info.commit_id}\"\n        )\n    elif isinstance(direct_url.info, ArchiveInfo):\n        requirement += direct_url.url\n        if direct_url.info.hash:\n            fragments.append(direct_url.info.hash)\n    else:\n        assert isinstance(direct_url.info, DirInfo)\n        requirement += direct_url.url\n    if direct_url.subdirectory:\n        fragments.append(\"subdirectory=\" + direct_url.subdirectory)\n    if fragments:\n        requirement += \"#\" + \"&\".join(fragments)\n    return requirement\n\n\ndef direct_url_for_editable(source_dir: str) -> DirectUrl:\n    return DirectUrl(\n        url=path_to_url(source_dir),\n        info=DirInfo(editable=True),\n    )\n\n\ndef direct_url_from_link(\n    link: Link, source_dir: Optional[str] = None, link_is_in_wheel_cache: bool = False\n) -> DirectUrl:\n    if link.is_vcs:\n        vcs_backend = vcs.get_backend_for_scheme(link.scheme)\n        assert vcs_backend\n        url, requested_revision, _ = vcs_backend.get_url_rev_and_auth(\n            link.url_without_fragment\n        )\n        # For VCS links, we need to find out and add commit_id.\n        if link_is_in_wheel_cache:\n            # If the requested VCS link corresponds to a cached\n            # wheel, it means the requested revision was an\n            # immutable commit hash, otherwise it would not have\n            # been cached. In that case we don't have a source_dir\n            # with the VCS checkout.\n            assert requested_revision\n            commit_id = requested_revision\n        else:\n            # If the wheel was not in cache, it means we have\n            # had to checkout from VCS to build and we have a source_dir\n            # which we can inspect to find out the commit id.\n            assert source_dir\n            commit_id = vcs_backend.get_revision(source_dir)\n        return DirectUrl(\n            url=url,\n            info=VcsInfo(\n                vcs=vcs_backend.name,\n                commit_id=commit_id,\n                requested_revision=requested_revision,\n            ),\n            subdirectory=link.subdirectory_fragment,\n        )\n    elif link.is_existing_dir():\n        return DirectUrl(\n            url=link.url_without_fragment,\n            info=DirInfo(),\n            subdirectory=link.subdirectory_fragment,\n        )\n    else:\n        hash = None\n        hash_name = link.hash_name\n        if hash_name:\n            hash = f\"{hash_name}={link.hash}\"\n        return DirectUrl(\n            url=link.url_without_fragment,\n            info=ArchiveInfo(hash=hash),\n            subdirectory=link.subdirectory_fragment,\n        )\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/utils/egg_link.py","size":2463,"sha1":"7f16cac0927cdf0a044e03f617a9e94a427b859e","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import os\nimport re\nimport sys\nfrom typing import List, Optional\n\nfrom pip._internal.locations import site_packages, user_site\nfrom pip._internal.utils.virtualenv import (\n    running_under_virtualenv,\n    virtualenv_no_global,\n)\n\n__all__ = [\n    \"egg_link_path_from_sys_path\",\n    \"egg_link_path_from_location\",\n]\n\n\ndef _egg_link_names(raw_name: str) -> List[str]:\n    \"\"\"\n    Convert a Name metadata value to a .egg-link name, by applying\n    the same substitution as pkg_resources's safe_name function.\n    Note: we cannot use canonicalize_name because it has a different logic.\n\n    We also look for the raw name (without normalization) as setuptools 69 changed\n    the way it names .egg-link files (https://github.com/pypa/setuptools/issues/4167).\n    \"\"\"\n    return [\n        re.sub(\"[^A-Za-z0-9.]+\", \"-\", raw_name) + \".egg-link\",\n        f\"{raw_name}.egg-link\",\n    ]\n\n\ndef egg_link_path_from_sys_path(raw_name: str) -> Optional[str]:\n    \"\"\"\n    Look for a .egg-link file for project name, by walking sys.path.\n    \"\"\"\n    egg_link_names = _egg_link_names(raw_name)\n    for path_item in sys.path:\n        for egg_link_name in egg_link_names:\n            egg_link = os.path.join(path_item, egg_link_name)\n            if os.path.isfile(egg_link):\n                return egg_link\n    return None\n\n\ndef egg_link_path_from_location(raw_name: str) -> Optional[str]:\n    \"\"\"\n    Return the path for the .egg-link file if it exists, otherwise, None.\n\n    There's 3 scenarios:\n    1) not in a virtualenv\n       try to find in site.USER_SITE, then site_packages\n    2) in a no-global virtualenv\n       try to find in site_packages\n    3) in a yes-global virtualenv\n       try to find in site_packages, then site.USER_SITE\n       (don't look in global location)\n\n    For #1 and #3, there could be odd cases, where there's an egg-link in 2\n    locations.\n\n    This method will just return the first one found.\n    \"\"\"\n    sites: List[str] = []\n    if running_under_virtualenv():\n        sites.append(site_packages)\n        if not virtualenv_no_global() and user_site:\n            sites.append(user_site)\n    else:\n        if user_site:\n            sites.append(user_site)\n        sites.append(site_packages)\n\n    egg_link_names = _egg_link_names(raw_name)\n    for site in sites:\n        for egg_link_name in egg_link_names:\n            egglink = os.path.join(site, egg_link_name)\n            if os.path.isfile(egglink):\n                return egglink\n    return None\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/utils/entrypoints.py","size":3064,"sha1":"df1011df89a89e8e184b38cf4232cc1b15446cc0","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import itertools\nimport os\nimport shutil\nimport sys\nfrom typing import List, Optional\n\nfrom pip._internal.cli.main import main\nfrom pip._internal.utils.compat import WINDOWS\n\n_EXECUTABLE_NAMES = [\n    \"pip\",\n    f\"pip{sys.version_info.major}\",\n    f\"pip{sys.version_info.major}.{sys.version_info.minor}\",\n]\nif WINDOWS:\n    _allowed_extensions = {\"\", \".exe\"}\n    _EXECUTABLE_NAMES = [\n        \"\".join(parts)\n        for parts in itertools.product(_EXECUTABLE_NAMES, _allowed_extensions)\n    ]\n\n\ndef _wrapper(args: Optional[List[str]] = None) -> int:\n    \"\"\"Central wrapper for all old entrypoints.\n\n    Historically pip has had several entrypoints defined. Because of issues\n    arising from PATH, sys.path, multiple Pythons, their interactions, and most\n    of them having a pip installed, users suffer every time an entrypoint gets\n    moved.\n\n    To alleviate this pain, and provide a mechanism for warning users and\n    directing them to an appropriate place for help, we now define all of\n    our old entrypoints as wrappers for the current one.\n    \"\"\"\n    sys.stderr.write(\n        \"WARNING: pip is being invoked by an old script wrapper. This will \"\n        \"fail in a future version of pip.\\n\"\n        \"Please see https://github.com/pypa/pip/issues/5599 for advice on \"\n        \"fixing the underlying issue.\\n\"\n        \"To avoid this problem you can invoke Python with '-m pip' instead of \"\n        \"running pip directly.\\n\"\n    )\n    return main(args)\n\n\ndef get_best_invocation_for_this_pip() -> str:\n    \"\"\"Try to figure out the best way to invoke pip in the current environment.\"\"\"\n    binary_directory = \"Scripts\" if WINDOWS else \"bin\"\n    binary_prefix = os.path.join(sys.prefix, binary_directory)\n\n    # Try to use pip[X[.Y]] names, if those executables for this environment are\n    # the first on PATH with that name.\n    path_parts = os.path.normcase(os.environ.get(\"PATH\", \"\")).split(os.pathsep)\n    exe_are_in_PATH = os.path.normcase(binary_prefix) in path_parts\n    if exe_are_in_PATH:\n        for exe_name in _EXECUTABLE_NAMES:\n            found_executable = shutil.which(exe_name)\n            binary_executable = os.path.join(binary_prefix, exe_name)\n            if (\n                found_executable\n                and os.path.exists(binary_executable)\n                and os.path.samefile(\n                    found_executable,\n                    binary_executable,\n                )\n            ):\n                return exe_name\n\n    # Use the `-m` invocation, if there's no \"nice\" invocation.\n    return f\"{get_best_invocation_for_this_python()} -m pip\"\n\n\ndef get_best_invocation_for_this_python() -> str:\n    \"\"\"Try to figure out the best way to invoke the current Python.\"\"\"\n    exe = sys.executable\n    exe_name = os.path.basename(exe)\n\n    # Try to use the basename, if it's the first executable.\n    found_executable = shutil.which(exe_name)\n    if found_executable and os.path.samefile(found_executable, exe):\n        return exe_name\n\n    # Use the full executable name, because we couldn't find something simpler.\n    return exe\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/utils/filesystem.py","size":4950,"sha1":"2aca3a5915e77e20cdfd3236c03ce2e4d564e0a5","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import fnmatch\nimport os\nimport os.path\nimport random\nimport sys\nfrom contextlib import contextmanager\nfrom tempfile import NamedTemporaryFile\nfrom typing import Any, BinaryIO, Generator, List, Union, cast\n\nfrom pip._internal.utils.compat import get_path_uid\nfrom pip._internal.utils.misc import format_size\nfrom pip._internal.utils.retry import retry\n\n\ndef check_path_owner(path: str) -> bool:\n    # If we don't have a way to check the effective uid of this process, then\n    # we'll just assume that we own the directory.\n    if sys.platform == \"win32\" or not hasattr(os, \"geteuid\"):\n        return True\n\n    assert os.path.isabs(path)\n\n    previous = None\n    while path != previous:\n        if os.path.lexists(path):\n            # Check if path is writable by current user.\n            if os.geteuid() == 0:\n                # Special handling for root user in order to handle properly\n                # cases where users use sudo without -H flag.\n                try:\n                    path_uid = get_path_uid(path)\n                except OSError:\n                    return False\n                return path_uid == 0\n            else:\n                return os.access(path, os.W_OK)\n        else:\n            previous, path = path, os.path.dirname(path)\n    return False  # assume we don't own the path\n\n\n@contextmanager\ndef adjacent_tmp_file(path: str, **kwargs: Any) -> Generator[BinaryIO, None, None]:\n    \"\"\"Return a file-like object pointing to a tmp file next to path.\n\n    The file is created securely and is ensured to be written to disk\n    after the context reaches its end.\n\n    kwargs will be passed to tempfile.NamedTemporaryFile to control\n    the way the temporary file will be opened.\n    \"\"\"\n    with NamedTemporaryFile(\n        delete=False,\n        dir=os.path.dirname(path),\n        prefix=os.path.basename(path),\n        suffix=\".tmp\",\n        **kwargs,\n    ) as f:\n        result = cast(BinaryIO, f)\n        try:\n            yield result\n        finally:\n            result.flush()\n            os.fsync(result.fileno())\n\n\nreplace = retry(stop_after_delay=1, wait=0.25)(os.replace)\n\n\n# test_writable_dir and _test_writable_dir_win are copied from Flit,\n# with the author's agreement to also place them under pip's license.\ndef test_writable_dir(path: str) -> bool:\n    \"\"\"Check if a directory is writable.\n\n    Uses os.access() on POSIX, tries creating files on Windows.\n    \"\"\"\n    # If the directory doesn't exist, find the closest parent that does.\n    while not os.path.isdir(path):\n        parent = os.path.dirname(path)\n        if parent == path:\n            break  # Should never get here, but infinite loops are bad\n        path = parent\n\n    if os.name == \"posix\":\n        return os.access(path, os.W_OK)\n\n    return _test_writable_dir_win(path)\n\n\ndef _test_writable_dir_win(path: str) -> bool:\n    # os.access doesn't work on Windows: http://bugs.python.org/issue2528\n    # and we can't use tempfile: http://bugs.python.org/issue22107\n    basename = \"accesstest_deleteme_fishfingers_custard_\"\n    alphabet = \"abcdefghijklmnopqrstuvwxyz0123456789\"\n    for _ in range(10):\n        name = basename + \"\".join(random.choice(alphabet) for _ in range(6))\n        file = os.path.join(path, name)\n        try:\n            fd = os.open(file, os.O_RDWR | os.O_CREAT | os.O_EXCL)\n        except FileExistsError:\n            pass\n        except PermissionError:\n            # This could be because there's a directory with the same name.\n            # But it's highly unlikely there's a directory called that,\n            # so we'll assume it's because the parent dir is not writable.\n            # This could as well be because the parent dir is not readable,\n            # due to non-privileged user access.\n            return False\n        else:\n            os.close(fd)\n            os.unlink(file)\n            return True\n\n    # This should never be reached\n    raise OSError(\"Unexpected condition testing for writable directory\")\n\n\ndef find_files(path: str, pattern: str) -> List[str]:\n    \"\"\"Returns a list of absolute paths of files beneath path, recursively,\n    with filenames which match the UNIX-style shell glob pattern.\"\"\"\n    result: List[str] = []\n    for root, _, files in os.walk(path):\n        matches = fnmatch.filter(files, pattern)\n        result.extend(os.path.join(root, f) for f in matches)\n    return result\n\n\ndef file_size(path: str) -> Union[int, float]:\n    # If it's a symlink, return 0.\n    if os.path.islink(path):\n        return 0\n    return os.path.getsize(path)\n\n\ndef format_file_size(path: str) -> str:\n    return format_size(file_size(path))\n\n\ndef directory_size(path: str) -> Union[int, float]:\n    size = 0.0\n    for root, _dirs, files in os.walk(path):\n        for filename in files:\n            file_path = os.path.join(root, filename)\n            size += file_size(file_path)\n    return size\n\n\ndef format_directory_size(path: str) -> str:\n    return format_size(directory_size(path))\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/utils/filetypes.py","size":716,"sha1":"b75e13ef5d44699f0af4ae12882321e63045b936","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"Filetype information.\n\"\"\"\n\nfrom typing import Tuple\n\nfrom pip._internal.utils.misc import splitext\n\nWHEEL_EXTENSION = \".whl\"\nBZ2_EXTENSIONS: Tuple[str, ...] = (\".tar.bz2\", \".tbz\")\nXZ_EXTENSIONS: Tuple[str, ...] = (\n    \".tar.xz\",\n    \".txz\",\n    \".tlz\",\n    \".tar.lz\",\n    \".tar.lzma\",\n)\nZIP_EXTENSIONS: Tuple[str, ...] = (\".zip\", WHEEL_EXTENSION)\nTAR_EXTENSIONS: Tuple[str, ...] = (\".tar.gz\", \".tgz\", \".tar\")\nARCHIVE_EXTENSIONS = ZIP_EXTENSIONS + BZ2_EXTENSIONS + TAR_EXTENSIONS + XZ_EXTENSIONS\n\n\ndef is_archive_file(name: str) -> bool:\n    \"\"\"Return True if `name` is a considered as an archive file.\"\"\"\n    ext = splitext(name)[1].lower()\n    if ext in ARCHIVE_EXTENSIONS:\n        return True\n    return False\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/utils/glibc.py","size":3734,"sha1":"416a32119e7561fe1f0ce65d3245cca5b4a02129","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import os\nimport sys\nfrom typing import Optional, Tuple\n\n\ndef glibc_version_string() -> Optional[str]:\n    \"Returns glibc version string, or None if not using glibc.\"\n    return glibc_version_string_confstr() or glibc_version_string_ctypes()\n\n\ndef glibc_version_string_confstr() -> Optional[str]:\n    \"Primary implementation of glibc_version_string using os.confstr.\"\n    # os.confstr is quite a bit faster than ctypes.DLL. It's also less likely\n    # to be broken or missing. This strategy is used in the standard library\n    # platform module:\n    # https://github.com/python/cpython/blob/fcf1d003bf4f0100c9d0921ff3d70e1127ca1b71/Lib/platform.py#L175-L183\n    if sys.platform == \"win32\":\n        return None\n    try:\n        gnu_libc_version = os.confstr(\"CS_GNU_LIBC_VERSION\")\n        if gnu_libc_version is None:\n            return None\n        # os.confstr(\"CS_GNU_LIBC_VERSION\") returns a string like \"glibc 2.17\":\n        _, version = gnu_libc_version.split()\n    except (AttributeError, OSError, ValueError):\n        # os.confstr() or CS_GNU_LIBC_VERSION not available (or a bad value)...\n        return None\n    return version\n\n\ndef glibc_version_string_ctypes() -> Optional[str]:\n    \"Fallback implementation of glibc_version_string using ctypes.\"\n\n    try:\n        import ctypes\n    except ImportError:\n        return None\n\n    # ctypes.CDLL(None) internally calls dlopen(NULL), and as the dlopen\n    # manpage says, \"If filename is NULL, then the returned handle is for the\n    # main program\". This way we can let the linker do the work to figure out\n    # which libc our process is actually using.\n    #\n    # We must also handle the special case where the executable is not a\n    # dynamically linked executable. This can occur when using musl libc,\n    # for example. In this situation, dlopen() will error, leading to an\n    # OSError. Interestingly, at least in the case of musl, there is no\n    # errno set on the OSError. The single string argument used to construct\n    # OSError comes from libc itself and is therefore not portable to\n    # hard code here. In any case, failure to call dlopen() means we\n    # can't proceed, so we bail on our attempt.\n    try:\n        process_namespace = ctypes.CDLL(None)\n    except OSError:\n        return None\n\n    try:\n        gnu_get_libc_version = process_namespace.gnu_get_libc_version\n    except AttributeError:\n        # Symbol doesn't exist -> therefore, we are not linked to\n        # glibc.\n        return None\n\n    # Call gnu_get_libc_version, which returns a string like \"2.5\"\n    gnu_get_libc_version.restype = ctypes.c_char_p\n    version_str: str = gnu_get_libc_version()\n    # py2 / py3 compatibility:\n    if not isinstance(version_str, str):\n        version_str = version_str.decode(\"ascii\")\n\n    return version_str\n\n\n# platform.libc_ver regularly returns completely nonsensical glibc\n# versions. E.g. on my computer, platform says:\n#\n#   ~$ python2.7 -c 'import platform; print(platform.libc_ver())'\n#   ('glibc', '2.7')\n#   ~$ python3.5 -c 'import platform; print(platform.libc_ver())'\n#   ('glibc', '2.9')\n#\n# But the truth is:\n#\n#   ~$ ldd --version\n#   ldd (Debian GLIBC 2.22-11) 2.22\n#\n# This is unfortunate, because it means that the linehaul data on libc\n# versions that was generated by pip 8.1.2 and earlier is useless and\n# misleading. Solution: instead of using platform, use our code that actually\n# works.\ndef libc_ver() -> Tuple[str, str]:\n    \"\"\"Try to determine the glibc version\n\n    Returns a tuple of strings (lib, version) which default to empty strings\n    in case the lookup fails.\n    \"\"\"\n    glibc_version = glibc_version_string()\n    if glibc_version is None:\n        return (\"\", \"\")\n    else:\n        return (\"glibc\", glibc_version)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/utils/hashes.py","size":4972,"sha1":"c228365c815862e953b287888067ebbb94381b71","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import hashlib\nfrom typing import TYPE_CHECKING, BinaryIO, Dict, Iterable, List, NoReturn, Optional\n\nfrom pip._internal.exceptions import HashMismatch, HashMissing, InstallationError\nfrom pip._internal.utils.misc import read_chunks\n\nif TYPE_CHECKING:\n    from hashlib import _Hash\n\n\n# The recommended hash algo of the moment. Change this whenever the state of\n# the art changes; it won't hurt backward compatibility.\nFAVORITE_HASH = \"sha256\"\n\n\n# Names of hashlib algorithms allowed by the --hash option and ``pip hash``\n# Currently, those are the ones at least as collision-resistant as sha256.\nSTRONG_HASHES = [\"sha256\", \"sha384\", \"sha512\"]\n\n\nclass Hashes:\n    \"\"\"A wrapper that builds multiple hashes at once and checks them against\n    known-good values\n\n    \"\"\"\n\n    def __init__(self, hashes: Optional[Dict[str, List[str]]] = None) -> None:\n        \"\"\"\n        :param hashes: A dict of algorithm names pointing to lists of allowed\n            hex digests\n        \"\"\"\n        allowed = {}\n        if hashes is not None:\n            for alg, keys in hashes.items():\n                # Make sure values are always sorted (to ease equality checks)\n                allowed[alg] = [k.lower() for k in sorted(keys)]\n        self._allowed = allowed\n\n    def __and__(self, other: \"Hashes\") -> \"Hashes\":\n        if not isinstance(other, Hashes):\n            return NotImplemented\n\n        # If either of the Hashes object is entirely empty (i.e. no hash\n        # specified at all), all hashes from the other object are allowed.\n        if not other:\n            return self\n        if not self:\n            return other\n\n        # Otherwise only hashes that present in both objects are allowed.\n        new = {}\n        for alg, values in other._allowed.items():\n            if alg not in self._allowed:\n                continue\n            new[alg] = [v for v in values if v in self._allowed[alg]]\n        return Hashes(new)\n\n    @property\n    def digest_count(self) -> int:\n        return sum(len(digests) for digests in self._allowed.values())\n\n    def is_hash_allowed(self, hash_name: str, hex_digest: str) -> bool:\n        \"\"\"Return whether the given hex digest is allowed.\"\"\"\n        return hex_digest in self._allowed.get(hash_name, [])\n\n    def check_against_chunks(self, chunks: Iterable[bytes]) -> None:\n        \"\"\"Check good hashes against ones built from iterable of chunks of\n        data.\n\n        Raise HashMismatch if none match.\n\n        \"\"\"\n        gots = {}\n        for hash_name in self._allowed.keys():\n            try:\n                gots[hash_name] = hashlib.new(hash_name)\n            except (ValueError, TypeError):\n                raise InstallationError(f\"Unknown hash name: {hash_name}\")\n\n        for chunk in chunks:\n            for hash in gots.values():\n                hash.update(chunk)\n\n        for hash_name, got in gots.items():\n            if got.hexdigest() in self._allowed[hash_name]:\n                return\n        self._raise(gots)\n\n    def _raise(self, gots: Dict[str, \"_Hash\"]) -> \"NoReturn\":\n        raise HashMismatch(self._allowed, gots)\n\n    def check_against_file(self, file: BinaryIO) -> None:\n        \"\"\"Check good hashes against a file-like object\n\n        Raise HashMismatch if none match.\n\n        \"\"\"\n        return self.check_against_chunks(read_chunks(file))\n\n    def check_against_path(self, path: str) -> None:\n        with open(path, \"rb\") as file:\n            return self.check_against_file(file)\n\n    def has_one_of(self, hashes: Dict[str, str]) -> bool:\n        \"\"\"Return whether any of the given hashes are allowed.\"\"\"\n        for hash_name, hex_digest in hashes.items():\n            if self.is_hash_allowed(hash_name, hex_digest):\n                return True\n        return False\n\n    def __bool__(self) -> bool:\n        \"\"\"Return whether I know any known-good hashes.\"\"\"\n        return bool(self._allowed)\n\n    def __eq__(self, other: object) -> bool:\n        if not isinstance(other, Hashes):\n            return NotImplemented\n        return self._allowed == other._allowed\n\n    def __hash__(self) -> int:\n        return hash(\n            \",\".join(\n                sorted(\n                    \":\".join((alg, digest))\n                    for alg, digest_list in self._allowed.items()\n                    for digest in digest_list\n                )\n            )\n        )\n\n\nclass MissingHashes(Hashes):\n    \"\"\"A workalike for Hashes used when we're missing a hash for a requirement\n\n    It computes the actual hash of the requirement and raises a HashMissing\n    exception showing it to the user.\n\n    \"\"\"\n\n    def __init__(self) -> None:\n        \"\"\"Don't offer the ``hashes`` kwarg.\"\"\"\n        # Pass our favorite hash in to generate a \"gotten hash\". With the\n        # empty list, it will never match, so an error will always raise.\n        super().__init__(hashes={FAVORITE_HASH: []})\n\n    def _raise(self, gots: Dict[str, \"_Hash\"]) -> \"NoReturn\":\n        raise HashMissing(gots[FAVORITE_HASH].hexdigest())\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/utils/logging.py","size":11845,"sha1":"1e56653a4b94a8730ec9088349cc42d3f59bc1cd","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import contextlib\nimport errno\nimport logging\nimport logging.handlers\nimport os\nimport sys\nimport threading\nfrom dataclasses import dataclass\nfrom io import TextIOWrapper\nfrom logging import Filter\nfrom typing import Any, ClassVar, Generator, List, Optional, TextIO, Type\n\nfrom pip._vendor.rich.console import (\n    Console,\n    ConsoleOptions,\n    ConsoleRenderable,\n    RenderableType,\n    RenderResult,\n    RichCast,\n)\nfrom pip._vendor.rich.highlighter import NullHighlighter\nfrom pip._vendor.rich.logging import RichHandler\nfrom pip._vendor.rich.segment import Segment\nfrom pip._vendor.rich.style import Style\n\nfrom pip._internal.utils._log import VERBOSE, getLogger\nfrom pip._internal.utils.compat import WINDOWS\nfrom pip._internal.utils.deprecation import DEPRECATION_MSG_PREFIX\nfrom pip._internal.utils.misc import ensure_dir\n\n_log_state = threading.local()\nsubprocess_logger = getLogger(\"pip.subprocessor\")\n\n\nclass BrokenStdoutLoggingError(Exception):\n    \"\"\"\n    Raised if BrokenPipeError occurs for the stdout stream while logging.\n    \"\"\"\n\n\ndef _is_broken_pipe_error(exc_class: Type[BaseException], exc: BaseException) -> bool:\n    if exc_class is BrokenPipeError:\n        return True\n\n    # On Windows, a broken pipe can show up as EINVAL rather than EPIPE:\n    # https://bugs.python.org/issue19612\n    # https://bugs.python.org/issue30418\n    if not WINDOWS:\n        return False\n\n    return isinstance(exc, OSError) and exc.errno in (errno.EINVAL, errno.EPIPE)\n\n\n@contextlib.contextmanager\ndef indent_log(num: int = 2) -> Generator[None, None, None]:\n    \"\"\"\n    A context manager which will cause the log output to be indented for any\n    log messages emitted inside it.\n    \"\"\"\n    # For thread-safety\n    _log_state.indentation = get_indentation()\n    _log_state.indentation += num\n    try:\n        yield\n    finally:\n        _log_state.indentation -= num\n\n\ndef get_indentation() -> int:\n    return getattr(_log_state, \"indentation\", 0)\n\n\nclass IndentingFormatter(logging.Formatter):\n    default_time_format = \"%Y-%m-%dT%H:%M:%S\"\n\n    def __init__(\n        self,\n        *args: Any,\n        add_timestamp: bool = False,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"\n        A logging.Formatter that obeys the indent_log() context manager.\n\n        :param add_timestamp: A bool indicating output lines should be prefixed\n            with their record's timestamp.\n        \"\"\"\n        self.add_timestamp = add_timestamp\n        super().__init__(*args, **kwargs)\n\n    def get_message_start(self, formatted: str, levelno: int) -> str:\n        \"\"\"\n        Return the start of the formatted log message (not counting the\n        prefix to add to each line).\n        \"\"\"\n        if levelno < logging.WARNING:\n            return \"\"\n        if formatted.startswith(DEPRECATION_MSG_PREFIX):\n            # Then the message already has a prefix.  We don't want it to\n            # look like \"WARNING: DEPRECATION: ....\"\n            return \"\"\n        if levelno < logging.ERROR:\n            return \"WARNING: \"\n\n        return \"ERROR: \"\n\n    def format(self, record: logging.LogRecord) -> str:\n        \"\"\"\n        Calls the standard formatter, but will indent all of the log message\n        lines by our current indentation level.\n        \"\"\"\n        formatted = super().format(record)\n        message_start = self.get_message_start(formatted, record.levelno)\n        formatted = message_start + formatted\n\n        prefix = \"\"\n        if self.add_timestamp:\n            prefix = f\"{self.formatTime(record)} \"\n        prefix += \" \" * get_indentation()\n        formatted = \"\".join([prefix + line for line in formatted.splitlines(True)])\n        return formatted\n\n\n@dataclass\nclass IndentedRenderable:\n    renderable: RenderableType\n    indent: int\n\n    def __rich_console__(\n        self, console: Console, options: ConsoleOptions\n    ) -> RenderResult:\n        segments = console.render(self.renderable, options)\n        lines = Segment.split_lines(segments)\n        for line in lines:\n            yield Segment(\" \" * self.indent)\n            yield from line\n            yield Segment(\"\\n\")\n\n\nclass PipConsole(Console):\n    def on_broken_pipe(self) -> None:\n        # Reraise the original exception, rich 13.8.0+ exits by default\n        # instead, preventing our handler from firing.\n        raise BrokenPipeError() from None\n\n\nclass RichPipStreamHandler(RichHandler):\n    KEYWORDS: ClassVar[Optional[List[str]]] = []\n\n    def __init__(self, stream: Optional[TextIO], no_color: bool) -> None:\n        super().__init__(\n            console=PipConsole(file=stream, no_color=no_color, soft_wrap=True),\n            show_time=False,\n            show_level=False,\n            show_path=False,\n            highlighter=NullHighlighter(),\n        )\n\n    # Our custom override on Rich's logger, to make things work as we need them to.\n    def emit(self, record: logging.LogRecord) -> None:\n        style: Optional[Style] = None\n\n        # If we are given a diagnostic error to present, present it with indentation.\n        if getattr(record, \"rich\", False):\n            assert isinstance(record.args, tuple)\n            (rich_renderable,) = record.args\n            assert isinstance(\n                rich_renderable, (ConsoleRenderable, RichCast, str)\n            ), f\"{rich_renderable} is not rich-console-renderable\"\n\n            renderable: RenderableType = IndentedRenderable(\n                rich_renderable, indent=get_indentation()\n            )\n        else:\n            message = self.format(record)\n            renderable = self.render_message(record, message)\n            if record.levelno is not None:\n                if record.levelno >= logging.ERROR:\n                    style = Style(color=\"red\")\n                elif record.levelno >= logging.WARNING:\n                    style = Style(color=\"yellow\")\n\n        try:\n            self.console.print(renderable, overflow=\"ignore\", crop=False, style=style)\n        except Exception:\n            self.handleError(record)\n\n    def handleError(self, record: logging.LogRecord) -> None:\n        \"\"\"Called when logging is unable to log some output.\"\"\"\n\n        exc_class, exc = sys.exc_info()[:2]\n        # If a broken pipe occurred while calling write() or flush() on the\n        # stdout stream in logging's Handler.emit(), then raise our special\n        # exception so we can handle it in main() instead of logging the\n        # broken pipe error and continuing.\n        if (\n            exc_class\n            and exc\n            and self.console.file is sys.stdout\n            and _is_broken_pipe_error(exc_class, exc)\n        ):\n            raise BrokenStdoutLoggingError()\n\n        return super().handleError(record)\n\n\nclass BetterRotatingFileHandler(logging.handlers.RotatingFileHandler):\n    def _open(self) -> TextIOWrapper:\n        ensure_dir(os.path.dirname(self.baseFilename))\n        return super()._open()\n\n\nclass MaxLevelFilter(Filter):\n    def __init__(self, level: int) -> None:\n        self.level = level\n\n    def filter(self, record: logging.LogRecord) -> bool:\n        return record.levelno < self.level\n\n\nclass ExcludeLoggerFilter(Filter):\n    \"\"\"\n    A logging Filter that excludes records from a logger (or its children).\n    \"\"\"\n\n    def filter(self, record: logging.LogRecord) -> bool:\n        # The base Filter class allows only records from a logger (or its\n        # children).\n        return not super().filter(record)\n\n\ndef setup_logging(verbosity: int, no_color: bool, user_log_file: Optional[str]) -> int:\n    \"\"\"Configures and sets up all of the logging\n\n    Returns the requested logging level, as its integer value.\n    \"\"\"\n\n    # Determine the level to be logging at.\n    if verbosity >= 2:\n        level_number = logging.DEBUG\n    elif verbosity == 1:\n        level_number = VERBOSE\n    elif verbosity == -1:\n        level_number = logging.WARNING\n    elif verbosity == -2:\n        level_number = logging.ERROR\n    elif verbosity <= -3:\n        level_number = logging.CRITICAL\n    else:\n        level_number = logging.INFO\n\n    level = logging.getLevelName(level_number)\n\n    # The \"root\" logger should match the \"console\" level *unless* we also need\n    # to log to a user log file.\n    include_user_log = user_log_file is not None\n    if include_user_log:\n        additional_log_file = user_log_file\n        root_level = \"DEBUG\"\n    else:\n        additional_log_file = \"/dev/null\"\n        root_level = level\n\n    # Disable any logging besides WARNING unless we have DEBUG level logging\n    # enabled for vendored libraries.\n    vendored_log_level = \"WARNING\" if level in [\"INFO\", \"ERROR\"] else \"DEBUG\"\n\n    # Shorthands for clarity\n    log_streams = {\n        \"stdout\": \"ext://sys.stdout\",\n        \"stderr\": \"ext://sys.stderr\",\n    }\n    handler_classes = {\n        \"stream\": \"pip._internal.utils.logging.RichPipStreamHandler\",\n        \"file\": \"pip._internal.utils.logging.BetterRotatingFileHandler\",\n    }\n    handlers = [\"console\", \"console_errors\", \"console_subprocess\"] + (\n        [\"user_log\"] if include_user_log else []\n    )\n\n    logging.config.dictConfig(\n        {\n            \"version\": 1,\n            \"disable_existing_loggers\": False,\n            \"filters\": {\n                \"exclude_warnings\": {\n                    \"()\": \"pip._internal.utils.logging.MaxLevelFilter\",\n                    \"level\": logging.WARNING,\n                },\n                \"restrict_to_subprocess\": {\n                    \"()\": \"logging.Filter\",\n                    \"name\": subprocess_logger.name,\n                },\n                \"exclude_subprocess\": {\n                    \"()\": \"pip._internal.utils.logging.ExcludeLoggerFilter\",\n                    \"name\": subprocess_logger.name,\n                },\n            },\n            \"formatters\": {\n                \"indent\": {\n                    \"()\": IndentingFormatter,\n                    \"format\": \"%(message)s\",\n                },\n                \"indent_with_timestamp\": {\n                    \"()\": IndentingFormatter,\n                    \"format\": \"%(message)s\",\n                    \"add_timestamp\": True,\n                },\n            },\n            \"handlers\": {\n                \"console\": {\n                    \"level\": level,\n                    \"class\": handler_classes[\"stream\"],\n                    \"no_color\": no_color,\n                    \"stream\": log_streams[\"stdout\"],\n                    \"filters\": [\"exclude_subprocess\", \"exclude_warnings\"],\n                    \"formatter\": \"indent\",\n                },\n                \"console_errors\": {\n                    \"level\": \"WARNING\",\n                    \"class\": handler_classes[\"stream\"],\n                    \"no_color\": no_color,\n                    \"stream\": log_streams[\"stderr\"],\n                    \"filters\": [\"exclude_subprocess\"],\n                    \"formatter\": \"indent\",\n                },\n                # A handler responsible for logging to the console messages\n                # from the \"subprocessor\" logger.\n                \"console_subprocess\": {\n                    \"level\": level,\n                    \"class\": handler_classes[\"stream\"],\n                    \"stream\": log_streams[\"stderr\"],\n                    \"no_color\": no_color,\n                    \"filters\": [\"restrict_to_subprocess\"],\n                    \"formatter\": \"indent\",\n                },\n                \"user_log\": {\n                    \"level\": \"DEBUG\",\n                    \"class\": handler_classes[\"file\"],\n                    \"filename\": additional_log_file,\n                    \"encoding\": \"utf-8\",\n                    \"delay\": True,\n                    \"formatter\": \"indent_with_timestamp\",\n                },\n            },\n            \"root\": {\n                \"level\": root_level,\n                \"handlers\": handlers,\n            },\n            \"loggers\": {\"pip._vendor\": {\"level\": vendored_log_level}},\n        }\n    )\n\n    return level_number\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/utils/misc.py","size":23450,"sha1":"8bc49854d3b0759d44c3e2fbe90d3c1deae653b7","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import errno\nimport getpass\nimport hashlib\nimport logging\nimport os\nimport posixpath\nimport shutil\nimport stat\nimport sys\nimport sysconfig\nimport urllib.parse\nfrom dataclasses import dataclass\nfrom functools import partial\nfrom io import StringIO\nfrom itertools import filterfalse, tee, zip_longest\nfrom pathlib import Path\nfrom types import FunctionType, TracebackType\nfrom typing import (\n    Any,\n    BinaryIO,\n    Callable,\n    Generator,\n    Iterable,\n    Iterator,\n    List,\n    Mapping,\n    Optional,\n    Sequence,\n    TextIO,\n    Tuple,\n    Type,\n    TypeVar,\n    Union,\n    cast,\n)\n\nfrom pip._vendor.packaging.requirements import Requirement\nfrom pip._vendor.pyproject_hooks import BuildBackendHookCaller\n\nfrom pip import __version__\nfrom pip._internal.exceptions import CommandError, ExternallyManagedEnvironment\nfrom pip._internal.locations import get_major_minor_version\nfrom pip._internal.utils.compat import WINDOWS\nfrom pip._internal.utils.retry import retry\nfrom pip._internal.utils.virtualenv import running_under_virtualenv\n\n__all__ = [\n    \"rmtree\",\n    \"display_path\",\n    \"backup_dir\",\n    \"ask\",\n    \"splitext\",\n    \"format_size\",\n    \"is_installable_dir\",\n    \"normalize_path\",\n    \"renames\",\n    \"get_prog\",\n    \"ensure_dir\",\n    \"remove_auth_from_url\",\n    \"check_externally_managed\",\n    \"ConfiguredBuildBackendHookCaller\",\n]\n\nlogger = logging.getLogger(__name__)\n\nT = TypeVar(\"T\")\nExcInfo = Tuple[Type[BaseException], BaseException, TracebackType]\nVersionInfo = Tuple[int, int, int]\nNetlocTuple = Tuple[str, Tuple[Optional[str], Optional[str]]]\nOnExc = Callable[[FunctionType, Path, BaseException], Any]\nOnErr = Callable[[FunctionType, Path, ExcInfo], Any]\n\nFILE_CHUNK_SIZE = 1024 * 1024\n\n\ndef get_pip_version() -> str:\n    pip_pkg_dir = os.path.join(os.path.dirname(__file__), \"..\", \"..\")\n    pip_pkg_dir = os.path.abspath(pip_pkg_dir)\n\n    return f\"pip {__version__} from {pip_pkg_dir} (python {get_major_minor_version()})\"\n\n\ndef normalize_version_info(py_version_info: Tuple[int, ...]) -> Tuple[int, int, int]:\n    \"\"\"\n    Convert a tuple of ints representing a Python version to one of length\n    three.\n\n    :param py_version_info: a tuple of ints representing a Python version,\n        or None to specify no version. The tuple can have any length.\n\n    :return: a tuple of length three if `py_version_info` is non-None.\n        Otherwise, return `py_version_info` unchanged (i.e. None).\n    \"\"\"\n    if len(py_version_info) < 3:\n        py_version_info += (3 - len(py_version_info)) * (0,)\n    elif len(py_version_info) > 3:\n        py_version_info = py_version_info[:3]\n\n    return cast(\"VersionInfo\", py_version_info)\n\n\ndef ensure_dir(path: str) -> None:\n    \"\"\"os.path.makedirs without EEXIST.\"\"\"\n    try:\n        os.makedirs(path)\n    except OSError as e:\n        # Windows can raise spurious ENOTEMPTY errors. See #6426.\n        if e.errno != errno.EEXIST and e.errno != errno.ENOTEMPTY:\n            raise\n\n\ndef get_prog() -> str:\n    try:\n        prog = os.path.basename(sys.argv[0])\n        if prog in (\"__main__.py\", \"-c\"):\n            return f\"{sys.executable} -m pip\"\n        else:\n            return prog\n    except (AttributeError, TypeError, IndexError):\n        pass\n    return \"pip\"\n\n\n# Retry every half second for up to 3 seconds\n@retry(stop_after_delay=3, wait=0.5)\ndef rmtree(\n    dir: str, ignore_errors: bool = False, onexc: Optional[OnExc] = None\n) -> None:\n    if ignore_errors:\n        onexc = _onerror_ignore\n    if onexc is None:\n        onexc = _onerror_reraise\n    handler: OnErr = partial(rmtree_errorhandler, onexc=onexc)\n    if sys.version_info >= (3, 12):\n        # See https://docs.python.org/3.12/whatsnew/3.12.html#shutil.\n        shutil.rmtree(dir, onexc=handler)  # type: ignore\n    else:\n        shutil.rmtree(dir, onerror=handler)  # type: ignore\n\n\ndef _onerror_ignore(*_args: Any) -> None:\n    pass\n\n\ndef _onerror_reraise(*_args: Any) -> None:\n    raise  # noqa: PLE0704 - Bare exception used to reraise existing exception\n\n\ndef rmtree_errorhandler(\n    func: FunctionType,\n    path: Path,\n    exc_info: Union[ExcInfo, BaseException],\n    *,\n    onexc: OnExc = _onerror_reraise,\n) -> None:\n    \"\"\"\n    `rmtree` error handler to 'force' a file remove (i.e. like `rm -f`).\n\n    * If a file is readonly then it's write flag is set and operation is\n      retried.\n\n    * `onerror` is the original callback from `rmtree(... onerror=onerror)`\n      that is chained at the end if the \"rm -f\" still fails.\n    \"\"\"\n    try:\n        st_mode = os.stat(path).st_mode\n    except OSError:\n        # it's equivalent to os.path.exists\n        return\n\n    if not st_mode & stat.S_IWRITE:\n        # convert to read/write\n        try:\n            os.chmod(path, st_mode | stat.S_IWRITE)\n        except OSError:\n            pass\n        else:\n            # use the original function to repeat the operation\n            try:\n                func(path)\n                return\n            except OSError:\n                pass\n\n    if not isinstance(exc_info, BaseException):\n        _, exc_info, _ = exc_info\n    onexc(func, path, exc_info)\n\n\ndef display_path(path: str) -> str:\n    \"\"\"Gives the display value for a given path, making it relative to cwd\n    if possible.\"\"\"\n    path = os.path.normcase(os.path.abspath(path))\n    if path.startswith(os.getcwd() + os.path.sep):\n        path = \".\" + path[len(os.getcwd()) :]\n    return path\n\n\ndef backup_dir(dir: str, ext: str = \".bak\") -> str:\n    \"\"\"Figure out the name of a directory to back up the given dir to\n    (adding .bak, .bak2, etc)\"\"\"\n    n = 1\n    extension = ext\n    while os.path.exists(dir + extension):\n        n += 1\n        extension = ext + str(n)\n    return dir + extension\n\n\ndef ask_path_exists(message: str, options: Iterable[str]) -> str:\n    for action in os.environ.get(\"PIP_EXISTS_ACTION\", \"\").split():\n        if action in options:\n            return action\n    return ask(message, options)\n\n\ndef _check_no_input(message: str) -> None:\n    \"\"\"Raise an error if no input is allowed.\"\"\"\n    if os.environ.get(\"PIP_NO_INPUT\"):\n        raise Exception(\n            f\"No input was expected ($PIP_NO_INPUT set); question: {message}\"\n        )\n\n\ndef ask(message: str, options: Iterable[str]) -> str:\n    \"\"\"Ask the message interactively, with the given possible responses\"\"\"\n    while 1:\n        _check_no_input(message)\n        response = input(message)\n        response = response.strip().lower()\n        if response not in options:\n            print(\n                \"Your response ({!r}) was not one of the expected responses: \"\n                \"{}\".format(response, \", \".join(options))\n            )\n        else:\n            return response\n\n\ndef ask_input(message: str) -> str:\n    \"\"\"Ask for input interactively.\"\"\"\n    _check_no_input(message)\n    return input(message)\n\n\ndef ask_password(message: str) -> str:\n    \"\"\"Ask for a password interactively.\"\"\"\n    _check_no_input(message)\n    return getpass.getpass(message)\n\n\ndef strtobool(val: str) -> int:\n    \"\"\"Convert a string representation of truth to true (1) or false (0).\n\n    True values are 'y', 'yes', 't', 'true', 'on', and '1'; false values\n    are 'n', 'no', 'f', 'false', 'off', and '0'.  Raises ValueError if\n    'val' is anything else.\n    \"\"\"\n    val = val.lower()\n    if val in (\"y\", \"yes\", \"t\", \"true\", \"on\", \"1\"):\n        return 1\n    elif val in (\"n\", \"no\", \"f\", \"false\", \"off\", \"0\"):\n        return 0\n    else:\n        raise ValueError(f\"invalid truth value {val!r}\")\n\n\ndef format_size(bytes: float) -> str:\n    if bytes > 1000 * 1000:\n        return f\"{bytes / 1000.0 / 1000:.1f} MB\"\n    elif bytes > 10 * 1000:\n        return f\"{int(bytes / 1000)} kB\"\n    elif bytes > 1000:\n        return f\"{bytes / 1000.0:.1f} kB\"\n    else:\n        return f\"{int(bytes)} bytes\"\n\n\ndef tabulate(rows: Iterable[Iterable[Any]]) -> Tuple[List[str], List[int]]:\n    \"\"\"Return a list of formatted rows and a list of column sizes.\n\n    For example::\n\n    >>> tabulate([['foobar', 2000], [0xdeadbeef]])\n    (['foobar     2000', '3735928559'], [10, 4])\n    \"\"\"\n    rows = [tuple(map(str, row)) for row in rows]\n    sizes = [max(map(len, col)) for col in zip_longest(*rows, fillvalue=\"\")]\n    table = [\" \".join(map(str.ljust, row, sizes)).rstrip() for row in rows]\n    return table, sizes\n\n\ndef is_installable_dir(path: str) -> bool:\n    \"\"\"Is path is a directory containing pyproject.toml or setup.py?\n\n    If pyproject.toml exists, this is a PEP 517 project. Otherwise we look for\n    a legacy setuptools layout by identifying setup.py. We don't check for the\n    setup.cfg because using it without setup.py is only available for PEP 517\n    projects, which are already covered by the pyproject.toml check.\n    \"\"\"\n    if not os.path.isdir(path):\n        return False\n    if os.path.isfile(os.path.join(path, \"pyproject.toml\")):\n        return True\n    if os.path.isfile(os.path.join(path, \"setup.py\")):\n        return True\n    return False\n\n\ndef read_chunks(\n    file: BinaryIO, size: int = FILE_CHUNK_SIZE\n) -> Generator[bytes, None, None]:\n    \"\"\"Yield pieces of data from a file-like object until EOF.\"\"\"\n    while True:\n        chunk = file.read(size)\n        if not chunk:\n            break\n        yield chunk\n\n\ndef normalize_path(path: str, resolve_symlinks: bool = True) -> str:\n    \"\"\"\n    Convert a path to its canonical, case-normalized, absolute version.\n\n    \"\"\"\n    path = os.path.expanduser(path)\n    if resolve_symlinks:\n        path = os.path.realpath(path)\n    else:\n        path = os.path.abspath(path)\n    return os.path.normcase(path)\n\n\ndef splitext(path: str) -> Tuple[str, str]:\n    \"\"\"Like os.path.splitext, but take off .tar too\"\"\"\n    base, ext = posixpath.splitext(path)\n    if base.lower().endswith(\".tar\"):\n        ext = base[-4:] + ext\n        base = base[:-4]\n    return base, ext\n\n\ndef renames(old: str, new: str) -> None:\n    \"\"\"Like os.renames(), but handles renaming across devices.\"\"\"\n    # Implementation borrowed from os.renames().\n    head, tail = os.path.split(new)\n    if head and tail and not os.path.exists(head):\n        os.makedirs(head)\n\n    shutil.move(old, new)\n\n    head, tail = os.path.split(old)\n    if head and tail:\n        try:\n            os.removedirs(head)\n        except OSError:\n            pass\n\n\ndef is_local(path: str) -> bool:\n    \"\"\"\n    Return True if path is within sys.prefix, if we're running in a virtualenv.\n\n    If we're not in a virtualenv, all paths are considered \"local.\"\n\n    Caution: this function assumes the head of path has been normalized\n    with normalize_path.\n    \"\"\"\n    if not running_under_virtualenv():\n        return True\n    return path.startswith(normalize_path(sys.prefix))\n\n\ndef write_output(msg: Any, *args: Any) -> None:\n    logger.info(msg, *args)\n\n\nclass StreamWrapper(StringIO):\n    orig_stream: TextIO\n\n    @classmethod\n    def from_stream(cls, orig_stream: TextIO) -> \"StreamWrapper\":\n        ret = cls()\n        ret.orig_stream = orig_stream\n        return ret\n\n    # compileall.compile_dir() needs stdout.encoding to print to stdout\n    # type ignore is because TextIOBase.encoding is writeable\n    @property\n    def encoding(self) -> str:  # type: ignore\n        return self.orig_stream.encoding\n\n\n# Simulates an enum\ndef enum(*sequential: Any, **named: Any) -> Type[Any]:\n    enums = dict(zip(sequential, range(len(sequential))), **named)\n    reverse = {value: key for key, value in enums.items()}\n    enums[\"reverse_mapping\"] = reverse\n    return type(\"Enum\", (), enums)\n\n\ndef build_netloc(host: str, port: Optional[int]) -> str:\n    \"\"\"\n    Build a netloc from a host-port pair\n    \"\"\"\n    if port is None:\n        return host\n    if \":\" in host:\n        # Only wrap host with square brackets when it is IPv6\n        host = f\"[{host}]\"\n    return f\"{host}:{port}\"\n\n\ndef build_url_from_netloc(netloc: str, scheme: str = \"https\") -> str:\n    \"\"\"\n    Build a full URL from a netloc.\n    \"\"\"\n    if netloc.count(\":\") >= 2 and \"@\" not in netloc and \"[\" not in netloc:\n        # It must be a bare IPv6 address, so wrap it with brackets.\n        netloc = f\"[{netloc}]\"\n    return f\"{scheme}://{netloc}\"\n\n\ndef parse_netloc(netloc: str) -> Tuple[Optional[str], Optional[int]]:\n    \"\"\"\n    Return the host-port pair from a netloc.\n    \"\"\"\n    url = build_url_from_netloc(netloc)\n    parsed = urllib.parse.urlparse(url)\n    return parsed.hostname, parsed.port\n\n\ndef split_auth_from_netloc(netloc: str) -> NetlocTuple:\n    \"\"\"\n    Parse out and remove the auth information from a netloc.\n\n    Returns: (netloc, (username, password)).\n    \"\"\"\n    if \"@\" not in netloc:\n        return netloc, (None, None)\n\n    # Split from the right because that's how urllib.parse.urlsplit()\n    # behaves if more than one @ is present (which can be checked using\n    # the password attribute of urlsplit()'s return value).\n    auth, netloc = netloc.rsplit(\"@\", 1)\n    pw: Optional[str] = None\n    if \":\" in auth:\n        # Split from the left because that's how urllib.parse.urlsplit()\n        # behaves if more than one : is present (which again can be checked\n        # using the password attribute of the return value)\n        user, pw = auth.split(\":\", 1)\n    else:\n        user, pw = auth, None\n\n    user = urllib.parse.unquote(user)\n    if pw is not None:\n        pw = urllib.parse.unquote(pw)\n\n    return netloc, (user, pw)\n\n\ndef redact_netloc(netloc: str) -> str:\n    \"\"\"\n    Replace the sensitive data in a netloc with \"****\", if it exists.\n\n    For example:\n        - \"user:pass@example.com\" returns \"user:****@example.com\"\n        - \"accesstoken@example.com\" returns \"****@example.com\"\n    \"\"\"\n    netloc, (user, password) = split_auth_from_netloc(netloc)\n    if user is None:\n        return netloc\n    if password is None:\n        user = \"****\"\n        password = \"\"\n    else:\n        user = urllib.parse.quote(user)\n        password = \":****\"\n    return f\"{user}{password}@{netloc}\"\n\n\ndef _transform_url(\n    url: str, transform_netloc: Callable[[str], Tuple[Any, ...]]\n) -> Tuple[str, NetlocTuple]:\n    \"\"\"Transform and replace netloc in a url.\n\n    transform_netloc is a function taking the netloc and returning a\n    tuple. The first element of this tuple is the new netloc. The\n    entire tuple is returned.\n\n    Returns a tuple containing the transformed url as item 0 and the\n    original tuple returned by transform_netloc as item 1.\n    \"\"\"\n    purl = urllib.parse.urlsplit(url)\n    netloc_tuple = transform_netloc(purl.netloc)\n    # stripped url\n    url_pieces = (purl.scheme, netloc_tuple[0], purl.path, purl.query, purl.fragment)\n    surl = urllib.parse.urlunsplit(url_pieces)\n    return surl, cast(\"NetlocTuple\", netloc_tuple)\n\n\ndef _get_netloc(netloc: str) -> NetlocTuple:\n    return split_auth_from_netloc(netloc)\n\n\ndef _redact_netloc(netloc: str) -> Tuple[str]:\n    return (redact_netloc(netloc),)\n\n\ndef split_auth_netloc_from_url(\n    url: str,\n) -> Tuple[str, str, Tuple[Optional[str], Optional[str]]]:\n    \"\"\"\n    Parse a url into separate netloc, auth, and url with no auth.\n\n    Returns: (url_without_auth, netloc, (username, password))\n    \"\"\"\n    url_without_auth, (netloc, auth) = _transform_url(url, _get_netloc)\n    return url_without_auth, netloc, auth\n\n\ndef remove_auth_from_url(url: str) -> str:\n    \"\"\"Return a copy of url with 'username:password@' removed.\"\"\"\n    # username/pass params are passed to subversion through flags\n    # and are not recognized in the url.\n    return _transform_url(url, _get_netloc)[0]\n\n\ndef redact_auth_from_url(url: str) -> str:\n    \"\"\"Replace the password in a given url with ****.\"\"\"\n    return _transform_url(url, _redact_netloc)[0]\n\n\ndef redact_auth_from_requirement(req: Requirement) -> str:\n    \"\"\"Replace the password in a given requirement url with ****.\"\"\"\n    if not req.url:\n        return str(req)\n    return str(req).replace(req.url, redact_auth_from_url(req.url))\n\n\n@dataclass(frozen=True)\nclass HiddenText:\n    secret: str\n    redacted: str\n\n    def __repr__(self) -> str:\n        return f\"<HiddenText {str(self)!r}>\"\n\n    def __str__(self) -> str:\n        return self.redacted\n\n    # This is useful for testing.\n    def __eq__(self, other: Any) -> bool:\n        if type(self) is not type(other):\n            return False\n\n        # The string being used for redaction doesn't also have to match,\n        # just the raw, original string.\n        return self.secret == other.secret\n\n\ndef hide_value(value: str) -> HiddenText:\n    return HiddenText(value, redacted=\"****\")\n\n\ndef hide_url(url: str) -> HiddenText:\n    redacted = redact_auth_from_url(url)\n    return HiddenText(url, redacted=redacted)\n\n\ndef protect_pip_from_modification_on_windows(modifying_pip: bool) -> None:\n    \"\"\"Protection of pip.exe from modification on Windows\n\n    On Windows, any operation modifying pip should be run as:\n        python -m pip ...\n    \"\"\"\n    pip_names = [\n        \"pip\",\n        f\"pip{sys.version_info.major}\",\n        f\"pip{sys.version_info.major}.{sys.version_info.minor}\",\n    ]\n\n    # See https://github.com/pypa/pip/issues/1299 for more discussion\n    should_show_use_python_msg = (\n        modifying_pip and WINDOWS and os.path.basename(sys.argv[0]) in pip_names\n    )\n\n    if should_show_use_python_msg:\n        new_command = [sys.executable, \"-m\", \"pip\"] + sys.argv[1:]\n        raise CommandError(\n            \"To modify pip, please run the following command:\\n{}\".format(\n                \" \".join(new_command)\n            )\n        )\n\n\ndef check_externally_managed() -> None:\n    \"\"\"Check whether the current environment is externally managed.\n\n    If the ``EXTERNALLY-MANAGED`` config file is found, the current environment\n    is considered externally managed, and an ExternallyManagedEnvironment is\n    raised.\n    \"\"\"\n    if running_under_virtualenv():\n        return\n    marker = os.path.join(sysconfig.get_path(\"stdlib\"), \"EXTERNALLY-MANAGED\")\n    if not os.path.isfile(marker):\n        return\n    raise ExternallyManagedEnvironment.from_config(marker)\n\n\ndef is_console_interactive() -> bool:\n    \"\"\"Is this console interactive?\"\"\"\n    return sys.stdin is not None and sys.stdin.isatty()\n\n\ndef hash_file(path: str, blocksize: int = 1 << 20) -> Tuple[Any, int]:\n    \"\"\"Return (hash, length) for path using hashlib.sha256()\"\"\"\n\n    h = hashlib.sha256()\n    length = 0\n    with open(path, \"rb\") as f:\n        for block in read_chunks(f, size=blocksize):\n            length += len(block)\n            h.update(block)\n    return h, length\n\n\ndef pairwise(iterable: Iterable[Any]) -> Iterator[Tuple[Any, Any]]:\n    \"\"\"\n    Return paired elements.\n\n    For example:\n        s -> (s0, s1), (s2, s3), (s4, s5), ...\n    \"\"\"\n    iterable = iter(iterable)\n    return zip_longest(iterable, iterable)\n\n\ndef partition(\n    pred: Callable[[T], bool], iterable: Iterable[T]\n) -> Tuple[Iterable[T], Iterable[T]]:\n    \"\"\"\n    Use a predicate to partition entries into false entries and true entries,\n    like\n\n        partition(is_odd, range(10)) --> 0 2 4 6 8   and  1 3 5 7 9\n    \"\"\"\n    t1, t2 = tee(iterable)\n    return filterfalse(pred, t1), filter(pred, t2)\n\n\nclass ConfiguredBuildBackendHookCaller(BuildBackendHookCaller):\n    def __init__(\n        self,\n        config_holder: Any,\n        source_dir: str,\n        build_backend: str,\n        backend_path: Optional[str] = None,\n        runner: Optional[Callable[..., None]] = None,\n        python_executable: Optional[str] = None,\n    ):\n        super().__init__(\n            source_dir, build_backend, backend_path, runner, python_executable\n        )\n        self.config_holder = config_holder\n\n    def build_wheel(\n        self,\n        wheel_directory: str,\n        config_settings: Optional[Mapping[str, Any]] = None,\n        metadata_directory: Optional[str] = None,\n    ) -> str:\n        cs = self.config_holder.config_settings\n        return super().build_wheel(\n            wheel_directory, config_settings=cs, metadata_directory=metadata_directory\n        )\n\n    def build_sdist(\n        self,\n        sdist_directory: str,\n        config_settings: Optional[Mapping[str, Any]] = None,\n    ) -> str:\n        cs = self.config_holder.config_settings\n        return super().build_sdist(sdist_directory, config_settings=cs)\n\n    def build_editable(\n        self,\n        wheel_directory: str,\n        config_settings: Optional[Mapping[str, Any]] = None,\n        metadata_directory: Optional[str] = None,\n    ) -> str:\n        cs = self.config_holder.config_settings\n        return super().build_editable(\n            wheel_directory, config_settings=cs, metadata_directory=metadata_directory\n        )\n\n    def get_requires_for_build_wheel(\n        self, config_settings: Optional[Mapping[str, Any]] = None\n    ) -> Sequence[str]:\n        cs = self.config_holder.config_settings\n        return super().get_requires_for_build_wheel(config_settings=cs)\n\n    def get_requires_for_build_sdist(\n        self, config_settings: Optional[Mapping[str, Any]] = None\n    ) -> Sequence[str]:\n        cs = self.config_holder.config_settings\n        return super().get_requires_for_build_sdist(config_settings=cs)\n\n    def get_requires_for_build_editable(\n        self, config_settings: Optional[Mapping[str, Any]] = None\n    ) -> Sequence[str]:\n        cs = self.config_holder.config_settings\n        return super().get_requires_for_build_editable(config_settings=cs)\n\n    def prepare_metadata_for_build_wheel(\n        self,\n        metadata_directory: str,\n        config_settings: Optional[Mapping[str, Any]] = None,\n        _allow_fallback: bool = True,\n    ) -> str:\n        cs = self.config_holder.config_settings\n        return super().prepare_metadata_for_build_wheel(\n            metadata_directory=metadata_directory,\n            config_settings=cs,\n            _allow_fallback=_allow_fallback,\n        )\n\n    def prepare_metadata_for_build_editable(\n        self,\n        metadata_directory: str,\n        config_settings: Optional[Mapping[str, Any]] = None,\n        _allow_fallback: bool = True,\n    ) -> Optional[str]:\n        cs = self.config_holder.config_settings\n        return super().prepare_metadata_for_build_editable(\n            metadata_directory=metadata_directory,\n            config_settings=cs,\n            _allow_fallback=_allow_fallback,\n        )\n\n\ndef warn_if_run_as_root() -> None:\n    \"\"\"Output a warning for sudo users on Unix.\n\n    In a virtual environment, sudo pip still writes to virtualenv.\n    On Windows, users may run pip as Administrator without issues.\n    This warning only applies to Unix root users outside of virtualenv.\n    \"\"\"\n    if running_under_virtualenv():\n        return\n    if not hasattr(os, \"getuid\"):\n        return\n    # On Windows, there are no \"system managed\" Python packages. Installing as\n    # Administrator via pip is the correct way of updating system environments.\n    #\n    # We choose sys.platform over utils.compat.WINDOWS here to enable Mypy platform\n    # checks: https://mypy.readthedocs.io/en/stable/common_issues.html\n    if sys.platform == \"win32\" or sys.platform == \"cygwin\":\n        return\n\n    if os.getuid() != 0:\n        return\n\n    logger.warning(\n        \"Running pip as the 'root' user can result in broken permissions and \"\n        \"conflicting behaviour with the system package manager, possibly \"\n        \"rendering your system unusable. \"\n        \"It is recommended to use a virtual environment instead: \"\n        \"https://pip.pypa.io/warnings/venv. \"\n        \"Use the --root-user-action option if you know what you are doing and \"\n        \"want to suppress this warning.\"\n    )\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/utils/packaging.py","size":2142,"sha1":"abc8ae6d65a11ec4d934e0313d21a6d83d283eb1","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import functools\nimport logging\nimport re\nfrom typing import NewType, Optional, Tuple, cast\n\nfrom pip._vendor.packaging import specifiers, version\nfrom pip._vendor.packaging.requirements import Requirement\n\nNormalizedExtra = NewType(\"NormalizedExtra\", str)\n\nlogger = logging.getLogger(__name__)\n\n\n@functools.lru_cache(maxsize=32)\ndef check_requires_python(\n    requires_python: Optional[str], version_info: Tuple[int, ...]\n) -> bool:\n    \"\"\"\n    Check if the given Python version matches a \"Requires-Python\" specifier.\n\n    :param version_info: A 3-tuple of ints representing a Python\n        major-minor-micro version to check (e.g. `sys.version_info[:3]`).\n\n    :return: `True` if the given Python version satisfies the requirement.\n        Otherwise, return `False`.\n\n    :raises InvalidSpecifier: If `requires_python` has an invalid format.\n    \"\"\"\n    if requires_python is None:\n        # The package provides no information\n        return True\n    requires_python_specifier = specifiers.SpecifierSet(requires_python)\n\n    python_version = version.parse(\".\".join(map(str, version_info)))\n    return python_version in requires_python_specifier\n\n\n@functools.lru_cache(maxsize=2048)\ndef get_requirement(req_string: str) -> Requirement:\n    \"\"\"Construct a packaging.Requirement object with caching\"\"\"\n    # Parsing requirement strings is expensive, and is also expected to happen\n    # with a low diversity of different arguments (at least relative the number\n    # constructed). This method adds a cache to requirement object creation to\n    # minimize repeated parsing of the same string to construct equivalent\n    # Requirement objects.\n    return Requirement(req_string)\n\n\ndef safe_extra(extra: str) -> NormalizedExtra:\n    \"\"\"Convert an arbitrary string to a standard 'extra' name\n\n    Any runs of non-alphanumeric characters are replaced with a single '_',\n    and the result is always lowercased.\n\n    This function is duplicated from ``pkg_resources``. Note that this is not\n    the same to either ``canonicalize_name`` or ``_egg_link_name``.\n    \"\"\"\n    return cast(NormalizedExtra, re.sub(\"[^A-Za-z0-9.-]+\", \"_\", extra).lower())\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/utils/retry.py","size":1392,"sha1":"f269c76b99b17894af50b6aa4b0056917104b04c","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import functools\nfrom time import perf_counter, sleep\nfrom typing import Callable, TypeVar\n\nfrom pip._vendor.typing_extensions import ParamSpec\n\nT = TypeVar(\"T\")\nP = ParamSpec(\"P\")\n\n\ndef retry(\n    wait: float, stop_after_delay: float\n) -> Callable[[Callable[P, T]], Callable[P, T]]:\n    \"\"\"Decorator to automatically retry a function on error.\n\n    If the function raises, the function is recalled with the same arguments\n    until it returns or the time limit is reached. When the time limit is\n    surpassed, the last exception raised is reraised.\n\n    :param wait: The time to wait after an error before retrying, in seconds.\n    :param stop_after_delay: The time limit after which retries will cease,\n        in seconds.\n    \"\"\"\n\n    def wrapper(func: Callable[P, T]) -> Callable[P, T]:\n\n        @functools.wraps(func)\n        def retry_wrapped(*args: P.args, **kwargs: P.kwargs) -> T:\n            # The performance counter is monotonic on all platforms we care\n            # about and has much better resolution than time.monotonic().\n            start_time = perf_counter()\n            while True:\n                try:\n                    return func(*args, **kwargs)\n                except Exception:\n                    if perf_counter() - start_time > stop_after_delay:\n                        raise\n                    sleep(wait)\n\n        return retry_wrapped\n\n    return wrapper\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/utils/setuptools_build.py","size":4435,"sha1":"b3bccb4896004ff798da12a3d87978f7b62efc45","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import sys\nimport textwrap\nfrom typing import List, Optional, Sequence\n\n# Shim to wrap setup.py invocation with setuptools\n# Note that __file__ is handled via two {!r} *and* %r, to ensure that paths on\n# Windows are correctly handled (it should be \"C:\\\\Users\" not \"C:\\Users\").\n_SETUPTOOLS_SHIM = textwrap.dedent(\n    \"\"\"\n    exec(compile('''\n    # This is <pip-setuptools-caller> -- a caller that pip uses to run setup.py\n    #\n    # - It imports setuptools before invoking setup.py, to enable projects that directly\n    #   import from `distutils.core` to work with newer packaging standards.\n    # - It provides a clear error message when setuptools is not installed.\n    # - It sets `sys.argv[0]` to the underlying `setup.py`, when invoking `setup.py` so\n    #   setuptools doesn't think the script is `-c`. This avoids the following warning:\n    #     manifest_maker: standard file '-c' not found\".\n    # - It generates a shim setup.py, for handling setup.cfg-only projects.\n    import os, sys, tokenize\n\n    try:\n        import setuptools\n    except ImportError as error:\n        print(\n            \"ERROR: Can not execute `setup.py` since setuptools is not available in \"\n            \"the build environment.\",\n            file=sys.stderr,\n        )\n        sys.exit(1)\n\n    __file__ = %r\n    sys.argv[0] = __file__\n\n    if os.path.exists(__file__):\n        filename = __file__\n        with tokenize.open(__file__) as f:\n            setup_py_code = f.read()\n    else:\n        filename = \"<auto-generated setuptools caller>\"\n        setup_py_code = \"from setuptools import setup; setup()\"\n\n    exec(compile(setup_py_code, filename, \"exec\"))\n    ''' % ({!r},), \"<pip-setuptools-caller>\", \"exec\"))\n    \"\"\"\n).rstrip()\n\n\ndef make_setuptools_shim_args(\n    setup_py_path: str,\n    global_options: Optional[Sequence[str]] = None,\n    no_user_config: bool = False,\n    unbuffered_output: bool = False,\n) -> List[str]:\n    \"\"\"\n    Get setuptools command arguments with shim wrapped setup file invocation.\n\n    :param setup_py_path: The path to setup.py to be wrapped.\n    :param global_options: Additional global options.\n    :param no_user_config: If True, disables personal user configuration.\n    :param unbuffered_output: If True, adds the unbuffered switch to the\n     argument list.\n    \"\"\"\n    args = [sys.executable]\n    if unbuffered_output:\n        args += [\"-u\"]\n    args += [\"-c\", _SETUPTOOLS_SHIM.format(setup_py_path)]\n    if global_options:\n        args += global_options\n    if no_user_config:\n        args += [\"--no-user-cfg\"]\n    return args\n\n\ndef make_setuptools_bdist_wheel_args(\n    setup_py_path: str,\n    global_options: Sequence[str],\n    build_options: Sequence[str],\n    destination_dir: str,\n) -> List[str]:\n    # NOTE: Eventually, we'd want to also -S to the flags here, when we're\n    # isolating. Currently, it breaks Python in virtualenvs, because it\n    # relies on site.py to find parts of the standard library outside the\n    # virtualenv.\n    args = make_setuptools_shim_args(\n        setup_py_path, global_options=global_options, unbuffered_output=True\n    )\n    args += [\"bdist_wheel\", \"-d\", destination_dir]\n    args += build_options\n    return args\n\n\ndef make_setuptools_clean_args(\n    setup_py_path: str,\n    global_options: Sequence[str],\n) -> List[str]:\n    args = make_setuptools_shim_args(\n        setup_py_path, global_options=global_options, unbuffered_output=True\n    )\n    args += [\"clean\", \"--all\"]\n    return args\n\n\ndef make_setuptools_develop_args(\n    setup_py_path: str,\n    *,\n    global_options: Sequence[str],\n    no_user_config: bool,\n    prefix: Optional[str],\n    home: Optional[str],\n    use_user_site: bool,\n) -> List[str]:\n    assert not (use_user_site and prefix)\n\n    args = make_setuptools_shim_args(\n        setup_py_path,\n        global_options=global_options,\n        no_user_config=no_user_config,\n    )\n\n    args += [\"develop\", \"--no-deps\"]\n\n    if prefix:\n        args += [\"--prefix\", prefix]\n    if home is not None:\n        args += [\"--install-dir\", home]\n\n    if use_user_site:\n        args += [\"--user\", \"--prefix=\"]\n\n    return args\n\n\ndef make_setuptools_egg_info_args(\n    setup_py_path: str,\n    egg_info_dir: Optional[str],\n    no_user_config: bool,\n) -> List[str]:\n    args = make_setuptools_shim_args(setup_py_path, no_user_config=no_user_config)\n\n    args += [\"egg_info\"]\n\n    if egg_info_dir:\n        args += [\"--egg-base\", egg_info_dir]\n\n    return args\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/utils/subprocess.py","size":8988,"sha1":"c6578ade2da5ecf0db20b044a5a17d23e36178b5","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import logging\nimport os\nimport shlex\nimport subprocess\nfrom typing import Any, Callable, Iterable, List, Literal, Mapping, Optional, Union\n\nfrom pip._vendor.rich.markup import escape\n\nfrom pip._internal.cli.spinners import SpinnerInterface, open_spinner\nfrom pip._internal.exceptions import InstallationSubprocessError\nfrom pip._internal.utils.logging import VERBOSE, subprocess_logger\nfrom pip._internal.utils.misc import HiddenText\n\nCommandArgs = List[Union[str, HiddenText]]\n\n\ndef make_command(*args: Union[str, HiddenText, CommandArgs]) -> CommandArgs:\n    \"\"\"\n    Create a CommandArgs object.\n    \"\"\"\n    command_args: CommandArgs = []\n    for arg in args:\n        # Check for list instead of CommandArgs since CommandArgs is\n        # only known during type-checking.\n        if isinstance(arg, list):\n            command_args.extend(arg)\n        else:\n            # Otherwise, arg is str or HiddenText.\n            command_args.append(arg)\n\n    return command_args\n\n\ndef format_command_args(args: Union[List[str], CommandArgs]) -> str:\n    \"\"\"\n    Format command arguments for display.\n    \"\"\"\n    # For HiddenText arguments, display the redacted form by calling str().\n    # Also, we don't apply str() to arguments that aren't HiddenText since\n    # this can trigger a UnicodeDecodeError in Python 2 if the argument\n    # has type unicode and includes a non-ascii character.  (The type\n    # checker doesn't ensure the annotations are correct in all cases.)\n    return \" \".join(\n        shlex.quote(str(arg)) if isinstance(arg, HiddenText) else shlex.quote(arg)\n        for arg in args\n    )\n\n\ndef reveal_command_args(args: Union[List[str], CommandArgs]) -> List[str]:\n    \"\"\"\n    Return the arguments in their raw, unredacted form.\n    \"\"\"\n    return [arg.secret if isinstance(arg, HiddenText) else arg for arg in args]\n\n\ndef call_subprocess(\n    cmd: Union[List[str], CommandArgs],\n    show_stdout: bool = False,\n    cwd: Optional[str] = None,\n    on_returncode: 'Literal[\"raise\", \"warn\", \"ignore\"]' = \"raise\",\n    extra_ok_returncodes: Optional[Iterable[int]] = None,\n    extra_environ: Optional[Mapping[str, Any]] = None,\n    unset_environ: Optional[Iterable[str]] = None,\n    spinner: Optional[SpinnerInterface] = None,\n    log_failed_cmd: Optional[bool] = True,\n    stdout_only: Optional[bool] = False,\n    *,\n    command_desc: str,\n) -> str:\n    \"\"\"\n    Args:\n      show_stdout: if true, use INFO to log the subprocess's stderr and\n        stdout streams.  Otherwise, use DEBUG.  Defaults to False.\n      extra_ok_returncodes: an iterable of integer return codes that are\n        acceptable, in addition to 0. Defaults to None, which means [].\n      unset_environ: an iterable of environment variable names to unset\n        prior to calling subprocess.Popen().\n      log_failed_cmd: if false, failed commands are not logged, only raised.\n      stdout_only: if true, return only stdout, else return both. When true,\n        logging of both stdout and stderr occurs when the subprocess has\n        terminated, else logging occurs as subprocess output is produced.\n    \"\"\"\n    if extra_ok_returncodes is None:\n        extra_ok_returncodes = []\n    if unset_environ is None:\n        unset_environ = []\n    # Most places in pip use show_stdout=False. What this means is--\n    #\n    # - We connect the child's output (combined stderr and stdout) to a\n    #   single pipe, which we read.\n    # - We log this output to stderr at DEBUG level as it is received.\n    # - If DEBUG logging isn't enabled (e.g. if --verbose logging wasn't\n    #   requested), then we show a spinner so the user can still see the\n    #   subprocess is in progress.\n    # - If the subprocess exits with an error, we log the output to stderr\n    #   at ERROR level if it hasn't already been displayed to the console\n    #   (e.g. if --verbose logging wasn't enabled).  This way we don't log\n    #   the output to the console twice.\n    #\n    # If show_stdout=True, then the above is still done, but with DEBUG\n    # replaced by INFO.\n    if show_stdout:\n        # Then log the subprocess output at INFO level.\n        log_subprocess: Callable[..., None] = subprocess_logger.info\n        used_level = logging.INFO\n    else:\n        # Then log the subprocess output using VERBOSE.  This also ensures\n        # it will be logged to the log file (aka user_log), if enabled.\n        log_subprocess = subprocess_logger.verbose\n        used_level = VERBOSE\n\n    # Whether the subprocess will be visible in the console.\n    showing_subprocess = subprocess_logger.getEffectiveLevel() <= used_level\n\n    # Only use the spinner if we're not showing the subprocess output\n    # and we have a spinner.\n    use_spinner = not showing_subprocess and spinner is not None\n\n    log_subprocess(\"Running command %s\", command_desc)\n    env = os.environ.copy()\n    if extra_environ:\n        env.update(extra_environ)\n    for name in unset_environ:\n        env.pop(name, None)\n    try:\n        proc = subprocess.Popen(\n            # Convert HiddenText objects to the underlying str.\n            reveal_command_args(cmd),\n            stdin=subprocess.PIPE,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.STDOUT if not stdout_only else subprocess.PIPE,\n            cwd=cwd,\n            env=env,\n            errors=\"backslashreplace\",\n        )\n    except Exception as exc:\n        if log_failed_cmd:\n            subprocess_logger.critical(\n                \"Error %s while executing command %s\",\n                exc,\n                command_desc,\n            )\n        raise\n    all_output = []\n    if not stdout_only:\n        assert proc.stdout\n        assert proc.stdin\n        proc.stdin.close()\n        # In this mode, stdout and stderr are in the same pipe.\n        while True:\n            line: str = proc.stdout.readline()\n            if not line:\n                break\n            line = line.rstrip()\n            all_output.append(line + \"\\n\")\n\n            # Show the line immediately.\n            log_subprocess(line)\n            # Update the spinner.\n            if use_spinner:\n                assert spinner\n                spinner.spin()\n        try:\n            proc.wait()\n        finally:\n            if proc.stdout:\n                proc.stdout.close()\n        output = \"\".join(all_output)\n    else:\n        # In this mode, stdout and stderr are in different pipes.\n        # We must use communicate() which is the only safe way to read both.\n        out, err = proc.communicate()\n        # log line by line to preserve pip log indenting\n        for out_line in out.splitlines():\n            log_subprocess(out_line)\n        all_output.append(out)\n        for err_line in err.splitlines():\n            log_subprocess(err_line)\n        all_output.append(err)\n        output = out\n\n    proc_had_error = proc.returncode and proc.returncode not in extra_ok_returncodes\n    if use_spinner:\n        assert spinner\n        if proc_had_error:\n            spinner.finish(\"error\")\n        else:\n            spinner.finish(\"done\")\n    if proc_had_error:\n        if on_returncode == \"raise\":\n            error = InstallationSubprocessError(\n                command_description=command_desc,\n                exit_code=proc.returncode,\n                output_lines=all_output if not showing_subprocess else None,\n            )\n            if log_failed_cmd:\n                subprocess_logger.error(\"%s\", error, extra={\"rich\": True})\n                subprocess_logger.verbose(\n                    \"[bold magenta]full command[/]: [blue]%s[/]\",\n                    escape(format_command_args(cmd)),\n                    extra={\"markup\": True},\n                )\n                subprocess_logger.verbose(\n                    \"[bold magenta]cwd[/]: %s\",\n                    escape(cwd or \"[inherit]\"),\n                    extra={\"markup\": True},\n                )\n\n            raise error\n        elif on_returncode == \"warn\":\n            subprocess_logger.warning(\n                'Command \"%s\" had error code %s in %s',\n                command_desc,\n                proc.returncode,\n                cwd,\n            )\n        elif on_returncode == \"ignore\":\n            pass\n        else:\n            raise ValueError(f\"Invalid value: on_returncode={on_returncode!r}\")\n    return output\n\n\ndef runner_with_spinner_message(message: str) -> Callable[..., None]:\n    \"\"\"Provide a subprocess_runner that shows a spinner message.\n\n    Intended for use with for BuildBackendHookCaller. Thus, the runner has\n    an API that matches what's expected by BuildBackendHookCaller.subprocess_runner.\n    \"\"\"\n\n    def runner(\n        cmd: List[str],\n        cwd: Optional[str] = None,\n        extra_environ: Optional[Mapping[str, Any]] = None,\n    ) -> None:\n        with open_spinner(message) as spinner:\n            call_subprocess(\n                cmd,\n                command_desc=message,\n                cwd=cwd,\n                extra_environ=extra_environ,\n                spinner=spinner,\n            )\n\n    return runner\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/utils/temp_dir.py","size":9310,"sha1":"433d2bc933f4ba7cbe4470386025e4dc3e289022","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import errno\nimport itertools\nimport logging\nimport os.path\nimport tempfile\nimport traceback\nfrom contextlib import ExitStack, contextmanager\nfrom pathlib import Path\nfrom typing import (\n    Any,\n    Callable,\n    Dict,\n    Generator,\n    List,\n    Optional,\n    TypeVar,\n    Union,\n)\n\nfrom pip._internal.utils.misc import enum, rmtree\n\nlogger = logging.getLogger(__name__)\n\n_T = TypeVar(\"_T\", bound=\"TempDirectory\")\n\n\n# Kinds of temporary directories. Only needed for ones that are\n# globally-managed.\ntempdir_kinds = enum(\n    BUILD_ENV=\"build-env\",\n    EPHEM_WHEEL_CACHE=\"ephem-wheel-cache\",\n    REQ_BUILD=\"req-build\",\n)\n\n\n_tempdir_manager: Optional[ExitStack] = None\n\n\n@contextmanager\ndef global_tempdir_manager() -> Generator[None, None, None]:\n    global _tempdir_manager\n    with ExitStack() as stack:\n        old_tempdir_manager, _tempdir_manager = _tempdir_manager, stack\n        try:\n            yield\n        finally:\n            _tempdir_manager = old_tempdir_manager\n\n\nclass TempDirectoryTypeRegistry:\n    \"\"\"Manages temp directory behavior\"\"\"\n\n    def __init__(self) -> None:\n        self._should_delete: Dict[str, bool] = {}\n\n    def set_delete(self, kind: str, value: bool) -> None:\n        \"\"\"Indicate whether a TempDirectory of the given kind should be\n        auto-deleted.\n        \"\"\"\n        self._should_delete[kind] = value\n\n    def get_delete(self, kind: str) -> bool:\n        \"\"\"Get configured auto-delete flag for a given TempDirectory type,\n        default True.\n        \"\"\"\n        return self._should_delete.get(kind, True)\n\n\n_tempdir_registry: Optional[TempDirectoryTypeRegistry] = None\n\n\n@contextmanager\ndef tempdir_registry() -> Generator[TempDirectoryTypeRegistry, None, None]:\n    \"\"\"Provides a scoped global tempdir registry that can be used to dictate\n    whether directories should be deleted.\n    \"\"\"\n    global _tempdir_registry\n    old_tempdir_registry = _tempdir_registry\n    _tempdir_registry = TempDirectoryTypeRegistry()\n    try:\n        yield _tempdir_registry\n    finally:\n        _tempdir_registry = old_tempdir_registry\n\n\nclass _Default:\n    pass\n\n\n_default = _Default()\n\n\nclass TempDirectory:\n    \"\"\"Helper class that owns and cleans up a temporary directory.\n\n    This class can be used as a context manager or as an OO representation of a\n    temporary directory.\n\n    Attributes:\n        path\n            Location to the created temporary directory\n        delete\n            Whether the directory should be deleted when exiting\n            (when used as a contextmanager)\n\n    Methods:\n        cleanup()\n            Deletes the temporary directory\n\n    When used as a context manager, if the delete attribute is True, on\n    exiting the context the temporary directory is deleted.\n    \"\"\"\n\n    def __init__(\n        self,\n        path: Optional[str] = None,\n        delete: Union[bool, None, _Default] = _default,\n        kind: str = \"temp\",\n        globally_managed: bool = False,\n        ignore_cleanup_errors: bool = True,\n    ):\n        super().__init__()\n\n        if delete is _default:\n            if path is not None:\n                # If we were given an explicit directory, resolve delete option\n                # now.\n                delete = False\n            else:\n                # Otherwise, we wait until cleanup and see what\n                # tempdir_registry says.\n                delete = None\n\n        # The only time we specify path is in for editables where it\n        # is the value of the --src option.\n        if path is None:\n            path = self._create(kind)\n\n        self._path = path\n        self._deleted = False\n        self.delete = delete\n        self.kind = kind\n        self.ignore_cleanup_errors = ignore_cleanup_errors\n\n        if globally_managed:\n            assert _tempdir_manager is not None\n            _tempdir_manager.enter_context(self)\n\n    @property\n    def path(self) -> str:\n        assert not self._deleted, f\"Attempted to access deleted path: {self._path}\"\n        return self._path\n\n    def __repr__(self) -> str:\n        return f\"<{self.__class__.__name__} {self.path!r}>\"\n\n    def __enter__(self: _T) -> _T:\n        return self\n\n    def __exit__(self, exc: Any, value: Any, tb: Any) -> None:\n        if self.delete is not None:\n            delete = self.delete\n        elif _tempdir_registry:\n            delete = _tempdir_registry.get_delete(self.kind)\n        else:\n            delete = True\n\n        if delete:\n            self.cleanup()\n\n    def _create(self, kind: str) -> str:\n        \"\"\"Create a temporary directory and store its path in self.path\"\"\"\n        # We realpath here because some systems have their default tmpdir\n        # symlinked to another directory.  This tends to confuse build\n        # scripts, so we canonicalize the path by traversing potential\n        # symlinks here.\n        path = os.path.realpath(tempfile.mkdtemp(prefix=f\"pip-{kind}-\"))\n        logger.debug(\"Created temporary directory: %s\", path)\n        return path\n\n    def cleanup(self) -> None:\n        \"\"\"Remove the temporary directory created and reset state\"\"\"\n        self._deleted = True\n        if not os.path.exists(self._path):\n            return\n\n        errors: List[BaseException] = []\n\n        def onerror(\n            func: Callable[..., Any],\n            path: Path,\n            exc_val: BaseException,\n        ) -> None:\n            \"\"\"Log a warning for a `rmtree` error and continue\"\"\"\n            formatted_exc = \"\\n\".join(\n                traceback.format_exception_only(type(exc_val), exc_val)\n            )\n            formatted_exc = formatted_exc.rstrip()  # remove trailing new line\n            if func in (os.unlink, os.remove, os.rmdir):\n                logger.debug(\n                    \"Failed to remove a temporary file '%s' due to %s.\\n\",\n                    path,\n                    formatted_exc,\n                )\n            else:\n                logger.debug(\"%s failed with %s.\", func.__qualname__, formatted_exc)\n            errors.append(exc_val)\n\n        if self.ignore_cleanup_errors:\n            try:\n                # first try with @retry; retrying to handle ephemeral errors\n                rmtree(self._path, ignore_errors=False)\n            except OSError:\n                # last pass ignore/log all errors\n                rmtree(self._path, onexc=onerror)\n            if errors:\n                logger.warning(\n                    \"Failed to remove contents in a temporary directory '%s'.\\n\"\n                    \"You can safely remove it manually.\",\n                    self._path,\n                )\n        else:\n            rmtree(self._path)\n\n\nclass AdjacentTempDirectory(TempDirectory):\n    \"\"\"Helper class that creates a temporary directory adjacent to a real one.\n\n    Attributes:\n        original\n            The original directory to create a temp directory for.\n        path\n            After calling create() or entering, contains the full\n            path to the temporary directory.\n        delete\n            Whether the directory should be deleted when exiting\n            (when used as a contextmanager)\n\n    \"\"\"\n\n    # The characters that may be used to name the temp directory\n    # We always prepend a ~ and then rotate through these until\n    # a usable name is found.\n    # pkg_resources raises a different error for .dist-info folder\n    # with leading '-' and invalid metadata\n    LEADING_CHARS = \"-~.=%0123456789\"\n\n    def __init__(self, original: str, delete: Optional[bool] = None) -> None:\n        self.original = original.rstrip(\"/\\\\\")\n        super().__init__(delete=delete)\n\n    @classmethod\n    def _generate_names(cls, name: str) -> Generator[str, None, None]:\n        \"\"\"Generates a series of temporary names.\n\n        The algorithm replaces the leading characters in the name\n        with ones that are valid filesystem characters, but are not\n        valid package names (for both Python and pip definitions of\n        package).\n        \"\"\"\n        for i in range(1, len(name)):\n            for candidate in itertools.combinations_with_replacement(\n                cls.LEADING_CHARS, i - 1\n            ):\n                new_name = \"~\" + \"\".join(candidate) + name[i:]\n                if new_name != name:\n                    yield new_name\n\n        # If we make it this far, we will have to make a longer name\n        for i in range(len(cls.LEADING_CHARS)):\n            for candidate in itertools.combinations_with_replacement(\n                cls.LEADING_CHARS, i\n            ):\n                new_name = \"~\" + \"\".join(candidate) + name\n                if new_name != name:\n                    yield new_name\n\n    def _create(self, kind: str) -> str:\n        root, name = os.path.split(self.original)\n        for candidate in self._generate_names(name):\n            path = os.path.join(root, candidate)\n            try:\n                os.mkdir(path)\n            except OSError as ex:\n                # Continue if the name exists already\n                if ex.errno != errno.EEXIST:\n                    raise\n            else:\n                path = os.path.realpath(path)\n                break\n        else:\n            # Final fallback on the default behavior.\n            path = os.path.realpath(tempfile.mkdtemp(prefix=f\"pip-{kind}-\"))\n\n        logger.debug(\"Created temporary directory: %s\", path)\n        return path\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/utils/unpacking.py","size":11967,"sha1":"c2afbc2ec1949d35b8469534be6d109d94903148","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"Utilities related archives.\n\"\"\"\n\nimport logging\nimport os\nimport shutil\nimport stat\nimport sys\nimport tarfile\nimport zipfile\nfrom typing import Iterable, List, Optional\nfrom zipfile import ZipInfo\n\nfrom pip._internal.exceptions import InstallationError\nfrom pip._internal.utils.filetypes import (\n    BZ2_EXTENSIONS,\n    TAR_EXTENSIONS,\n    XZ_EXTENSIONS,\n    ZIP_EXTENSIONS,\n)\nfrom pip._internal.utils.misc import ensure_dir\n\nlogger = logging.getLogger(__name__)\n\n\nSUPPORTED_EXTENSIONS = ZIP_EXTENSIONS + TAR_EXTENSIONS\n\ntry:\n    import bz2  # noqa\n\n    SUPPORTED_EXTENSIONS += BZ2_EXTENSIONS\nexcept ImportError:\n    logger.debug(\"bz2 module is not available\")\n\ntry:\n    # Only for Python 3.3+\n    import lzma  # noqa\n\n    SUPPORTED_EXTENSIONS += XZ_EXTENSIONS\nexcept ImportError:\n    logger.debug(\"lzma module is not available\")\n\n\ndef current_umask() -> int:\n    \"\"\"Get the current umask which involves having to set it temporarily.\"\"\"\n    mask = os.umask(0)\n    os.umask(mask)\n    return mask\n\n\ndef split_leading_dir(path: str) -> List[str]:\n    path = path.lstrip(\"/\").lstrip(\"\\\\\")\n    if \"/\" in path and (\n        (\"\\\\\" in path and path.find(\"/\") < path.find(\"\\\\\")) or \"\\\\\" not in path\n    ):\n        return path.split(\"/\", 1)\n    elif \"\\\\\" in path:\n        return path.split(\"\\\\\", 1)\n    else:\n        return [path, \"\"]\n\n\ndef has_leading_dir(paths: Iterable[str]) -> bool:\n    \"\"\"Returns true if all the paths have the same leading path name\n    (i.e., everything is in one subdirectory in an archive)\"\"\"\n    common_prefix = None\n    for path in paths:\n        prefix, rest = split_leading_dir(path)\n        if not prefix:\n            return False\n        elif common_prefix is None:\n            common_prefix = prefix\n        elif prefix != common_prefix:\n            return False\n    return True\n\n\ndef is_within_directory(directory: str, target: str) -> bool:\n    \"\"\"\n    Return true if the absolute path of target is within the directory\n    \"\"\"\n    abs_directory = os.path.abspath(directory)\n    abs_target = os.path.abspath(target)\n\n    prefix = os.path.commonprefix([abs_directory, abs_target])\n    return prefix == abs_directory\n\n\ndef _get_default_mode_plus_executable() -> int:\n    return 0o777 & ~current_umask() | 0o111\n\n\ndef set_extracted_file_to_default_mode_plus_executable(path: str) -> None:\n    \"\"\"\n    Make file present at path have execute for user/group/world\n    (chmod +x) is no-op on windows per python docs\n    \"\"\"\n    os.chmod(path, _get_default_mode_plus_executable())\n\n\ndef zip_item_is_executable(info: ZipInfo) -> bool:\n    mode = info.external_attr >> 16\n    # if mode and regular file and any execute permissions for\n    # user/group/world?\n    return bool(mode and stat.S_ISREG(mode) and mode & 0o111)\n\n\ndef unzip_file(filename: str, location: str, flatten: bool = True) -> None:\n    \"\"\"\n    Unzip the file (with path `filename`) to the destination `location`.  All\n    files are written based on system defaults and umask (i.e. permissions are\n    not preserved), except that regular file members with any execute\n    permissions (user, group, or world) have \"chmod +x\" applied after being\n    written. Note that for windows, any execute changes using os.chmod are\n    no-ops per the python docs.\n    \"\"\"\n    ensure_dir(location)\n    zipfp = open(filename, \"rb\")\n    try:\n        zip = zipfile.ZipFile(zipfp, allowZip64=True)\n        leading = has_leading_dir(zip.namelist()) and flatten\n        for info in zip.infolist():\n            name = info.filename\n            fn = name\n            if leading:\n                fn = split_leading_dir(name)[1]\n            fn = os.path.join(location, fn)\n            dir = os.path.dirname(fn)\n            if not is_within_directory(location, fn):\n                message = (\n                    \"The zip file ({}) has a file ({}) trying to install \"\n                    \"outside target directory ({})\"\n                )\n                raise InstallationError(message.format(filename, fn, location))\n            if fn.endswith(\"/\") or fn.endswith(\"\\\\\"):\n                # A directory\n                ensure_dir(fn)\n            else:\n                ensure_dir(dir)\n                # Don't use read() to avoid allocating an arbitrarily large\n                # chunk of memory for the file's content\n                fp = zip.open(name)\n                try:\n                    with open(fn, \"wb\") as destfp:\n                        shutil.copyfileobj(fp, destfp)\n                finally:\n                    fp.close()\n                    if zip_item_is_executable(info):\n                        set_extracted_file_to_default_mode_plus_executable(fn)\n    finally:\n        zipfp.close()\n\n\ndef untar_file(filename: str, location: str) -> None:\n    \"\"\"\n    Untar the file (with path `filename`) to the destination `location`.\n    All files are written based on system defaults and umask (i.e. permissions\n    are not preserved), except that regular file members with any execute\n    permissions (user, group, or world) have \"chmod +x\" applied on top of the\n    default.  Note that for windows, any execute changes using os.chmod are\n    no-ops per the python docs.\n    \"\"\"\n    ensure_dir(location)\n    if filename.lower().endswith(\".gz\") or filename.lower().endswith(\".tgz\"):\n        mode = \"r:gz\"\n    elif filename.lower().endswith(BZ2_EXTENSIONS):\n        mode = \"r:bz2\"\n    elif filename.lower().endswith(XZ_EXTENSIONS):\n        mode = \"r:xz\"\n    elif filename.lower().endswith(\".tar\"):\n        mode = \"r\"\n    else:\n        logger.warning(\n            \"Cannot determine compression type for file %s\",\n            filename,\n        )\n        mode = \"r:*\"\n\n    tar = tarfile.open(filename, mode, encoding=\"utf-8\")  # type: ignore\n    try:\n        leading = has_leading_dir([member.name for member in tar.getmembers()])\n\n        # PEP 706 added `tarfile.data_filter`, and made some other changes to\n        # Python's tarfile module (see below). The features were backported to\n        # security releases.\n        try:\n            data_filter = tarfile.data_filter\n        except AttributeError:\n            _untar_without_filter(filename, location, tar, leading)\n        else:\n            default_mode_plus_executable = _get_default_mode_plus_executable()\n\n            if leading:\n                # Strip the leading directory from all files in the archive,\n                # including hardlink targets (which are relative to the\n                # unpack location).\n                for member in tar.getmembers():\n                    name_lead, name_rest = split_leading_dir(member.name)\n                    member.name = name_rest\n                    if member.islnk():\n                        lnk_lead, lnk_rest = split_leading_dir(member.linkname)\n                        if lnk_lead == name_lead:\n                            member.linkname = lnk_rest\n\n            def pip_filter(member: tarfile.TarInfo, path: str) -> tarfile.TarInfo:\n                orig_mode = member.mode\n                try:\n                    try:\n                        member = data_filter(member, location)\n                    except tarfile.LinkOutsideDestinationError:\n                        if sys.version_info[:3] in {\n                            (3, 8, 17),\n                            (3, 9, 17),\n                            (3, 10, 12),\n                            (3, 11, 4),\n                        }:\n                            # The tarfile filter in specific Python versions\n                            # raises LinkOutsideDestinationError on valid input\n                            # (https://github.com/python/cpython/issues/107845)\n                            # Ignore the error there, but do use the\n                            # more lax `tar_filter`\n                            member = tarfile.tar_filter(member, location)\n                        else:\n                            raise\n                except tarfile.TarError as exc:\n                    message = \"Invalid member in the tar file {}: {}\"\n                    # Filter error messages mention the member name.\n                    # No need to add it here.\n                    raise InstallationError(\n                        message.format(\n                            filename,\n                            exc,\n                        )\n                    )\n                if member.isfile() and orig_mode & 0o111:\n                    member.mode = default_mode_plus_executable\n                else:\n                    # See PEP 706 note above.\n                    # The PEP changed this from `int` to `Optional[int]`,\n                    # where None means \"use the default\". Mypy doesn't\n                    # know this yet.\n                    member.mode = None  # type: ignore [assignment]\n                return member\n\n            tar.extractall(location, filter=pip_filter)\n\n    finally:\n        tar.close()\n\n\ndef _untar_without_filter(\n    filename: str,\n    location: str,\n    tar: tarfile.TarFile,\n    leading: bool,\n) -> None:\n    \"\"\"Fallback for Python without tarfile.data_filter\"\"\"\n    for member in tar.getmembers():\n        fn = member.name\n        if leading:\n            fn = split_leading_dir(fn)[1]\n        path = os.path.join(location, fn)\n        if not is_within_directory(location, path):\n            message = (\n                \"The tar file ({}) has a file ({}) trying to install \"\n                \"outside target directory ({})\"\n            )\n            raise InstallationError(message.format(filename, path, location))\n        if member.isdir():\n            ensure_dir(path)\n        elif member.issym():\n            try:\n                tar._extract_member(member, path)\n            except Exception as exc:\n                # Some corrupt tar files seem to produce this\n                # (specifically bad symlinks)\n                logger.warning(\n                    \"In the tar file %s the member %s is invalid: %s\",\n                    filename,\n                    member.name,\n                    exc,\n                )\n                continue\n        else:\n            try:\n                fp = tar.extractfile(member)\n            except (KeyError, AttributeError) as exc:\n                # Some corrupt tar files seem to produce this\n                # (specifically bad symlinks)\n                logger.warning(\n                    \"In the tar file %s the member %s is invalid: %s\",\n                    filename,\n                    member.name,\n                    exc,\n                )\n                continue\n            ensure_dir(os.path.dirname(path))\n            assert fp is not None\n            with open(path, \"wb\") as destfp:\n                shutil.copyfileobj(fp, destfp)\n            fp.close()\n            # Update the timestamp (useful for cython compiled files)\n            tar.utime(member, path)\n            # member have any execute permissions for user/group/world?\n            if member.mode & 0o111:\n                set_extracted_file_to_default_mode_plus_executable(path)\n\n\ndef unpack_file(\n    filename: str,\n    location: str,\n    content_type: Optional[str] = None,\n) -> None:\n    filename = os.path.realpath(filename)\n    if (\n        content_type == \"application/zip\"\n        or filename.lower().endswith(ZIP_EXTENSIONS)\n        or zipfile.is_zipfile(filename)\n    ):\n        unzip_file(filename, location, flatten=not filename.endswith(\".whl\"))\n    elif (\n        content_type == \"application/x-gzip\"\n        or tarfile.is_tarfile(filename)\n        or filename.lower().endswith(TAR_EXTENSIONS + BZ2_EXTENSIONS + XZ_EXTENSIONS)\n    ):\n        untar_file(filename, location)\n    else:\n        # FIXME: handle?\n        # FIXME: magic signatures?\n        logger.critical(\n            \"Cannot unpack file %s (downloaded from %s, content-type: %s); \"\n            \"cannot detect archive format\",\n            filename,\n            location,\n            content_type,\n        )\n        raise InstallationError(f\"Cannot determine archive format of {location}\")\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/utils/urls.py","size":1599,"sha1":"bfd9cb6ce0fdefb138f2bca35fee7cd97d064c3e","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import os\nimport string\nimport urllib.parse\nimport urllib.request\n\nfrom .compat import WINDOWS\n\n\ndef path_to_url(path: str) -> str:\n    \"\"\"\n    Convert a path to a file: URL.  The path will be made absolute and have\n    quoted path parts.\n    \"\"\"\n    path = os.path.normpath(os.path.abspath(path))\n    url = urllib.parse.urljoin(\"file:\", urllib.request.pathname2url(path))\n    return url\n\n\ndef url_to_path(url: str) -> str:\n    \"\"\"\n    Convert a file: URL to a path.\n    \"\"\"\n    assert url.startswith(\n        \"file:\"\n    ), f\"You can only turn file: urls into filenames (not {url!r})\"\n\n    _, netloc, path, _, _ = urllib.parse.urlsplit(url)\n\n    if not netloc or netloc == \"localhost\":\n        # According to RFC 8089, same as empty authority.\n        netloc = \"\"\n    elif WINDOWS:\n        # If we have a UNC path, prepend UNC share notation.\n        netloc = \"\\\\\\\\\" + netloc\n    else:\n        raise ValueError(\n            f\"non-local file URIs are not supported on this platform: {url!r}\"\n        )\n\n    path = urllib.request.url2pathname(netloc + path)\n\n    # On Windows, urlsplit parses the path as something like \"/C:/Users/foo\".\n    # This creates issues for path-related functions like io.open(), so we try\n    # to detect and strip the leading slash.\n    if (\n        WINDOWS\n        and not netloc  # Not UNC.\n        and len(path) >= 3\n        and path[0] == \"/\"  # Leading slash to strip.\n        and path[1] in string.ascii_letters  # Drive letter.\n        and path[2:4] in (\":\", \":/\")  # Colon + end of string, or colon + absolute path.\n    ):\n        path = path[1:]\n\n    return path\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/utils/virtualenv.py","size":3456,"sha1":"14cb66aa6ea7945b643769280466da0fd10febac","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import logging\nimport os\nimport re\nimport site\nimport sys\nfrom typing import List, Optional\n\nlogger = logging.getLogger(__name__)\n_INCLUDE_SYSTEM_SITE_PACKAGES_REGEX = re.compile(\n    r\"include-system-site-packages\\s*=\\s*(?P<value>true|false)\"\n)\n\n\ndef _running_under_venv() -> bool:\n    \"\"\"Checks if sys.base_prefix and sys.prefix match.\n\n    This handles PEP 405 compliant virtual environments.\n    \"\"\"\n    return sys.prefix != getattr(sys, \"base_prefix\", sys.prefix)\n\n\ndef _running_under_legacy_virtualenv() -> bool:\n    \"\"\"Checks if sys.real_prefix is set.\n\n    This handles virtual environments created with pypa's virtualenv.\n    \"\"\"\n    # pypa/virtualenv case\n    return hasattr(sys, \"real_prefix\")\n\n\ndef running_under_virtualenv() -> bool:\n    \"\"\"True if we're running inside a virtual environment, False otherwise.\"\"\"\n    return _running_under_venv() or _running_under_legacy_virtualenv()\n\n\ndef _get_pyvenv_cfg_lines() -> Optional[List[str]]:\n    \"\"\"Reads {sys.prefix}/pyvenv.cfg and returns its contents as list of lines\n\n    Returns None, if it could not read/access the file.\n    \"\"\"\n    pyvenv_cfg_file = os.path.join(sys.prefix, \"pyvenv.cfg\")\n    try:\n        # Although PEP 405 does not specify, the built-in venv module always\n        # writes with UTF-8. (pypa/pip#8717)\n        with open(pyvenv_cfg_file, encoding=\"utf-8\") as f:\n            return f.read().splitlines()  # avoids trailing newlines\n    except OSError:\n        return None\n\n\ndef _no_global_under_venv() -> bool:\n    \"\"\"Check `{sys.prefix}/pyvenv.cfg` for system site-packages inclusion\n\n    PEP 405 specifies that when system site-packages are not supposed to be\n    visible from a virtual environment, `pyvenv.cfg` must contain the following\n    line:\n\n        include-system-site-packages = false\n\n    Additionally, log a warning if accessing the file fails.\n    \"\"\"\n    cfg_lines = _get_pyvenv_cfg_lines()\n    if cfg_lines is None:\n        # We're not in a \"sane\" venv, so assume there is no system\n        # site-packages access (since that's PEP 405's default state).\n        logger.warning(\n            \"Could not access 'pyvenv.cfg' despite a virtual environment \"\n            \"being active. Assuming global site-packages is not accessible \"\n            \"in this environment.\"\n        )\n        return True\n\n    for line in cfg_lines:\n        match = _INCLUDE_SYSTEM_SITE_PACKAGES_REGEX.match(line)\n        if match is not None and match.group(\"value\") == \"false\":\n            return True\n    return False\n\n\ndef _no_global_under_legacy_virtualenv() -> bool:\n    \"\"\"Check if \"no-global-site-packages.txt\" exists beside site.py\n\n    This mirrors logic in pypa/virtualenv for determining whether system\n    site-packages are visible in the virtual environment.\n    \"\"\"\n    site_mod_dir = os.path.dirname(os.path.abspath(site.__file__))\n    no_global_site_packages_file = os.path.join(\n        site_mod_dir,\n        \"no-global-site-packages.txt\",\n    )\n    return os.path.exists(no_global_site_packages_file)\n\n\ndef virtualenv_no_global() -> bool:\n    \"\"\"Returns a boolean, whether running in venv with no system site-packages.\"\"\"\n    # PEP 405 compliance needs to be checked first since virtualenv >=20 would\n    # return True for both checks, but is only able to use the PEP 405 config.\n    if _running_under_venv():\n        return _no_global_under_venv()\n\n    if _running_under_legacy_virtualenv():\n        return _no_global_under_legacy_virtualenv()\n\n    return False\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/utils/wheel.py","size":4494,"sha1":"67f8f69dfc250eeaf1b80b9d316a9142cd35a588","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"Support functions for working with wheel files.\n\"\"\"\n\nimport logging\nfrom email.message import Message\nfrom email.parser import Parser\nfrom typing import Tuple\nfrom zipfile import BadZipFile, ZipFile\n\nfrom pip._vendor.packaging.utils import canonicalize_name\n\nfrom pip._internal.exceptions import UnsupportedWheel\n\nVERSION_COMPATIBLE = (1, 0)\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef parse_wheel(wheel_zip: ZipFile, name: str) -> Tuple[str, Message]:\n    \"\"\"Extract information from the provided wheel, ensuring it meets basic\n    standards.\n\n    Returns the name of the .dist-info directory and the parsed WHEEL metadata.\n    \"\"\"\n    try:\n        info_dir = wheel_dist_info_dir(wheel_zip, name)\n        metadata = wheel_metadata(wheel_zip, info_dir)\n        version = wheel_version(metadata)\n    except UnsupportedWheel as e:\n        raise UnsupportedWheel(f\"{name} has an invalid wheel, {e}\")\n\n    check_compatibility(version, name)\n\n    return info_dir, metadata\n\n\ndef wheel_dist_info_dir(source: ZipFile, name: str) -> str:\n    \"\"\"Returns the name of the contained .dist-info directory.\n\n    Raises AssertionError or UnsupportedWheel if not found, >1 found, or\n    it doesn't match the provided name.\n    \"\"\"\n    # Zip file path separators must be /\n    subdirs = {p.split(\"/\", 1)[0] for p in source.namelist()}\n\n    info_dirs = [s for s in subdirs if s.endswith(\".dist-info\")]\n\n    if not info_dirs:\n        raise UnsupportedWheel(\".dist-info directory not found\")\n\n    if len(info_dirs) > 1:\n        raise UnsupportedWheel(\n            \"multiple .dist-info directories found: {}\".format(\", \".join(info_dirs))\n        )\n\n    info_dir = info_dirs[0]\n\n    info_dir_name = canonicalize_name(info_dir)\n    canonical_name = canonicalize_name(name)\n    if not info_dir_name.startswith(canonical_name):\n        raise UnsupportedWheel(\n            f\".dist-info directory {info_dir!r} does not start with {canonical_name!r}\"\n        )\n\n    return info_dir\n\n\ndef read_wheel_metadata_file(source: ZipFile, path: str) -> bytes:\n    try:\n        return source.read(path)\n        # BadZipFile for general corruption, KeyError for missing entry,\n        # and RuntimeError for password-protected files\n    except (BadZipFile, KeyError, RuntimeError) as e:\n        raise UnsupportedWheel(f\"could not read {path!r} file: {e!r}\")\n\n\ndef wheel_metadata(source: ZipFile, dist_info_dir: str) -> Message:\n    \"\"\"Return the WHEEL metadata of an extracted wheel, if possible.\n    Otherwise, raise UnsupportedWheel.\n    \"\"\"\n    path = f\"{dist_info_dir}/WHEEL\"\n    # Zip file path separators must be /\n    wheel_contents = read_wheel_metadata_file(source, path)\n\n    try:\n        wheel_text = wheel_contents.decode()\n    except UnicodeDecodeError as e:\n        raise UnsupportedWheel(f\"error decoding {path!r}: {e!r}\")\n\n    # FeedParser (used by Parser) does not raise any exceptions. The returned\n    # message may have .defects populated, but for backwards-compatibility we\n    # currently ignore them.\n    return Parser().parsestr(wheel_text)\n\n\ndef wheel_version(wheel_data: Message) -> Tuple[int, ...]:\n    \"\"\"Given WHEEL metadata, return the parsed Wheel-Version.\n    Otherwise, raise UnsupportedWheel.\n    \"\"\"\n    version_text = wheel_data[\"Wheel-Version\"]\n    if version_text is None:\n        raise UnsupportedWheel(\"WHEEL is missing Wheel-Version\")\n\n    version = version_text.strip()\n\n    try:\n        return tuple(map(int, version.split(\".\")))\n    except ValueError:\n        raise UnsupportedWheel(f\"invalid Wheel-Version: {version!r}\")\n\n\ndef check_compatibility(version: Tuple[int, ...], name: str) -> None:\n    \"\"\"Raises errors or warns if called with an incompatible Wheel-Version.\n\n    pip should refuse to install a Wheel-Version that's a major series\n    ahead of what it's compatible with (e.g 2.0 > 1.1); and warn when\n    installing a version only minor version ahead (e.g 1.2 > 1.1).\n\n    version: a 2-tuple representing a Wheel-Version (Major, Minor)\n    name: name of wheel or package to raise exception about\n\n    :raises UnsupportedWheel: when an incompatible Wheel-Version is given\n    \"\"\"\n    if version[0] > VERSION_COMPATIBLE[0]:\n        raise UnsupportedWheel(\n            \"{}'s Wheel-Version ({}) is not compatible with this version \"\n            \"of pip\".format(name, \".\".join(map(str, version)))\n        )\n    elif version > VERSION_COMPATIBLE:\n        logger.warning(\n            \"Installing from a newer Wheel-Version (%s)\",\n            \".\".join(map(str, version)),\n        )\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/vcs/__init__.py","size":596,"sha1":"ddf20f97603f281dc422347d7b063a0c31d728b7","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"# Expose a limited set of classes and functions so callers outside of\n# the vcs package don't need to import deeper than `pip._internal.vcs`.\n# (The test directory may still need to import from a vcs sub-package.)\n# Import all vcs modules to register each VCS in the VcsSupport object.\nimport pip._internal.vcs.bazaar\nimport pip._internal.vcs.git\nimport pip._internal.vcs.mercurial\nimport pip._internal.vcs.subversion  # noqa: F401\nfrom pip._internal.vcs.versioncontrol import (  # noqa: F401\n    RemoteNotFoundError,\n    RemoteNotValidError,\n    is_url,\n    make_vcs_requirement_url,\n    vcs,\n)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/vcs/bazaar.py","size":3528,"sha1":"7de44a798522565ddebab655ceb470da33738db4","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import logging\nfrom typing import List, Optional, Tuple\n\nfrom pip._internal.utils.misc import HiddenText, display_path\nfrom pip._internal.utils.subprocess import make_command\nfrom pip._internal.utils.urls import path_to_url\nfrom pip._internal.vcs.versioncontrol import (\n    AuthInfo,\n    RemoteNotFoundError,\n    RevOptions,\n    VersionControl,\n    vcs,\n)\n\nlogger = logging.getLogger(__name__)\n\n\nclass Bazaar(VersionControl):\n    name = \"bzr\"\n    dirname = \".bzr\"\n    repo_name = \"branch\"\n    schemes = (\n        \"bzr+http\",\n        \"bzr+https\",\n        \"bzr+ssh\",\n        \"bzr+sftp\",\n        \"bzr+ftp\",\n        \"bzr+lp\",\n        \"bzr+file\",\n    )\n\n    @staticmethod\n    def get_base_rev_args(rev: str) -> List[str]:\n        return [\"-r\", rev]\n\n    def fetch_new(\n        self, dest: str, url: HiddenText, rev_options: RevOptions, verbosity: int\n    ) -> None:\n        rev_display = rev_options.to_display()\n        logger.info(\n            \"Checking out %s%s to %s\",\n            url,\n            rev_display,\n            display_path(dest),\n        )\n        if verbosity <= 0:\n            flags = [\"--quiet\"]\n        elif verbosity == 1:\n            flags = []\n        else:\n            flags = [f\"-{'v'*verbosity}\"]\n        cmd_args = make_command(\n            \"checkout\", \"--lightweight\", *flags, rev_options.to_args(), url, dest\n        )\n        self.run_command(cmd_args)\n\n    def switch(self, dest: str, url: HiddenText, rev_options: RevOptions) -> None:\n        self.run_command(make_command(\"switch\", url), cwd=dest)\n\n    def update(self, dest: str, url: HiddenText, rev_options: RevOptions) -> None:\n        output = self.run_command(\n            make_command(\"info\"), show_stdout=False, stdout_only=True, cwd=dest\n        )\n        if output.startswith(\"Standalone \"):\n            # Older versions of pip used to create standalone branches.\n            # Convert the standalone branch to a checkout by calling \"bzr bind\".\n            cmd_args = make_command(\"bind\", \"-q\", url)\n            self.run_command(cmd_args, cwd=dest)\n\n        cmd_args = make_command(\"update\", \"-q\", rev_options.to_args())\n        self.run_command(cmd_args, cwd=dest)\n\n    @classmethod\n    def get_url_rev_and_auth(cls, url: str) -> Tuple[str, Optional[str], AuthInfo]:\n        # hotfix the URL scheme after removing bzr+ from bzr+ssh:// re-add it\n        url, rev, user_pass = super().get_url_rev_and_auth(url)\n        if url.startswith(\"ssh://\"):\n            url = \"bzr+\" + url\n        return url, rev, user_pass\n\n    @classmethod\n    def get_remote_url(cls, location: str) -> str:\n        urls = cls.run_command(\n            [\"info\"], show_stdout=False, stdout_only=True, cwd=location\n        )\n        for line in urls.splitlines():\n            line = line.strip()\n            for x in (\"checkout of branch: \", \"parent branch: \"):\n                if line.startswith(x):\n                    repo = line.split(x)[1]\n                    if cls._is_local_repository(repo):\n                        return path_to_url(repo)\n                    return repo\n        raise RemoteNotFoundError\n\n    @classmethod\n    def get_revision(cls, location: str) -> str:\n        revision = cls.run_command(\n            [\"revno\"],\n            show_stdout=False,\n            stdout_only=True,\n            cwd=location,\n        )\n        return revision.splitlines()[-1]\n\n    @classmethod\n    def is_commit_id_equal(cls, dest: str, name: Optional[str]) -> bool:\n        \"\"\"Always assume the versions don't match\"\"\"\n        return False\n\n\nvcs.register(Bazaar)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/vcs/git.py","size":18177,"sha1":"0bc472cd9430defd1886ca436a694f356c920540","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import logging\nimport os.path\nimport pathlib\nimport re\nimport urllib.parse\nimport urllib.request\nfrom dataclasses import replace\nfrom typing import List, Optional, Tuple\n\nfrom pip._internal.exceptions import BadCommand, InstallationError\nfrom pip._internal.utils.misc import HiddenText, display_path, hide_url\nfrom pip._internal.utils.subprocess import make_command\nfrom pip._internal.vcs.versioncontrol import (\n    AuthInfo,\n    RemoteNotFoundError,\n    RemoteNotValidError,\n    RevOptions,\n    VersionControl,\n    find_path_to_project_root_from_repo_root,\n    vcs,\n)\n\nurlsplit = urllib.parse.urlsplit\nurlunsplit = urllib.parse.urlunsplit\n\n\nlogger = logging.getLogger(__name__)\n\n\nGIT_VERSION_REGEX = re.compile(\n    r\"^git version \"  # Prefix.\n    r\"(\\d+)\"  # Major.\n    r\"\\.(\\d+)\"  # Dot, minor.\n    r\"(?:\\.(\\d+))?\"  # Optional dot, patch.\n    r\".*$\"  # Suffix, including any pre- and post-release segments we don't care about.\n)\n\nHASH_REGEX = re.compile(\"^[a-fA-F0-9]{40}$\")\n\n# SCP (Secure copy protocol) shorthand. e.g. 'git@example.com:foo/bar.git'\nSCP_REGEX = re.compile(\n    r\"\"\"^\n    # Optional user, e.g. 'git@'\n    (\\w+@)?\n    # Server, e.g. 'github.com'.\n    ([^/:]+):\n    # The server-side path. e.g. 'user/project.git'. Must start with an\n    # alphanumeric character so as not to be confusable with a Windows paths\n    # like 'C:/foo/bar' or 'C:\\foo\\bar'.\n    (\\w[^:]*)\n    $\"\"\",\n    re.VERBOSE,\n)\n\n\ndef looks_like_hash(sha: str) -> bool:\n    return bool(HASH_REGEX.match(sha))\n\n\nclass Git(VersionControl):\n    name = \"git\"\n    dirname = \".git\"\n    repo_name = \"clone\"\n    schemes = (\n        \"git+http\",\n        \"git+https\",\n        \"git+ssh\",\n        \"git+git\",\n        \"git+file\",\n    )\n    # Prevent the user's environment variables from interfering with pip:\n    # https://github.com/pypa/pip/issues/1130\n    unset_environ = (\"GIT_DIR\", \"GIT_WORK_TREE\")\n    default_arg_rev = \"HEAD\"\n\n    @staticmethod\n    def get_base_rev_args(rev: str) -> List[str]:\n        return [rev]\n\n    def is_immutable_rev_checkout(self, url: str, dest: str) -> bool:\n        _, rev_options = self.get_url_rev_options(hide_url(url))\n        if not rev_options.rev:\n            return False\n        if not self.is_commit_id_equal(dest, rev_options.rev):\n            # the current commit is different from rev,\n            # which means rev was something else than a commit hash\n            return False\n        # return False in the rare case rev is both a commit hash\n        # and a tag or a branch; we don't want to cache in that case\n        # because that branch/tag could point to something else in the future\n        is_tag_or_branch = bool(self.get_revision_sha(dest, rev_options.rev)[0])\n        return not is_tag_or_branch\n\n    def get_git_version(self) -> Tuple[int, ...]:\n        version = self.run_command(\n            [\"version\"],\n            command_desc=\"git version\",\n            show_stdout=False,\n            stdout_only=True,\n        )\n        match = GIT_VERSION_REGEX.match(version)\n        if not match:\n            logger.warning(\"Can't parse git version: %s\", version)\n            return ()\n        return (int(match.group(1)), int(match.group(2)))\n\n    @classmethod\n    def get_current_branch(cls, location: str) -> Optional[str]:\n        \"\"\"\n        Return the current branch, or None if HEAD isn't at a branch\n        (e.g. detached HEAD).\n        \"\"\"\n        # git-symbolic-ref exits with empty stdout if \"HEAD\" is a detached\n        # HEAD rather than a symbolic ref.  In addition, the -q causes the\n        # command to exit with status code 1 instead of 128 in this case\n        # and to suppress the message to stderr.\n        args = [\"symbolic-ref\", \"-q\", \"HEAD\"]\n        output = cls.run_command(\n            args,\n            extra_ok_returncodes=(1,),\n            show_stdout=False,\n            stdout_only=True,\n            cwd=location,\n        )\n        ref = output.strip()\n\n        if ref.startswith(\"refs/heads/\"):\n            return ref[len(\"refs/heads/\") :]\n\n        return None\n\n    @classmethod\n    def get_revision_sha(cls, dest: str, rev: str) -> Tuple[Optional[str], bool]:\n        \"\"\"\n        Return (sha_or_none, is_branch), where sha_or_none is a commit hash\n        if the revision names a remote branch or tag, otherwise None.\n\n        Args:\n          dest: the repository directory.\n          rev: the revision name.\n        \"\"\"\n        # Pass rev to pre-filter the list.\n        output = cls.run_command(\n            [\"show-ref\", rev],\n            cwd=dest,\n            show_stdout=False,\n            stdout_only=True,\n            on_returncode=\"ignore\",\n        )\n        refs = {}\n        # NOTE: We do not use splitlines here since that would split on other\n        #       unicode separators, which can be maliciously used to install a\n        #       different revision.\n        for line in output.strip().split(\"\\n\"):\n            line = line.rstrip(\"\\r\")\n            if not line:\n                continue\n            try:\n                ref_sha, ref_name = line.split(\" \", maxsplit=2)\n            except ValueError:\n                # Include the offending line to simplify troubleshooting if\n                # this error ever occurs.\n                raise ValueError(f\"unexpected show-ref line: {line!r}\")\n\n            refs[ref_name] = ref_sha\n\n        branch_ref = f\"refs/remotes/origin/{rev}\"\n        tag_ref = f\"refs/tags/{rev}\"\n\n        sha = refs.get(branch_ref)\n        if sha is not None:\n            return (sha, True)\n\n        sha = refs.get(tag_ref)\n\n        return (sha, False)\n\n    @classmethod\n    def _should_fetch(cls, dest: str, rev: str) -> bool:\n        \"\"\"\n        Return true if rev is a ref or is a commit that we don't have locally.\n\n        Branches and tags are not considered in this method because they are\n        assumed to be always available locally (which is a normal outcome of\n        ``git clone`` and ``git fetch --tags``).\n        \"\"\"\n        if rev.startswith(\"refs/\"):\n            # Always fetch remote refs.\n            return True\n\n        if not looks_like_hash(rev):\n            # Git fetch would fail with abbreviated commits.\n            return False\n\n        if cls.has_commit(dest, rev):\n            # Don't fetch if we have the commit locally.\n            return False\n\n        return True\n\n    @classmethod\n    def resolve_revision(\n        cls, dest: str, url: HiddenText, rev_options: RevOptions\n    ) -> RevOptions:\n        \"\"\"\n        Resolve a revision to a new RevOptions object with the SHA1 of the\n        branch, tag, or ref if found.\n\n        Args:\n          rev_options: a RevOptions object.\n        \"\"\"\n        rev = rev_options.arg_rev\n        # The arg_rev property's implementation for Git ensures that the\n        # rev return value is always non-None.\n        assert rev is not None\n\n        sha, is_branch = cls.get_revision_sha(dest, rev)\n\n        if sha is not None:\n            rev_options = rev_options.make_new(sha)\n            rev_options = replace(rev_options, branch_name=(rev if is_branch else None))\n\n            return rev_options\n\n        # Do not show a warning for the common case of something that has\n        # the form of a Git commit hash.\n        if not looks_like_hash(rev):\n            logger.warning(\n                \"Did not find branch or tag '%s', assuming revision or ref.\",\n                rev,\n            )\n\n        if not cls._should_fetch(dest, rev):\n            return rev_options\n\n        # fetch the requested revision\n        cls.run_command(\n            make_command(\"fetch\", \"-q\", url, rev_options.to_args()),\n            cwd=dest,\n        )\n        # Change the revision to the SHA of the ref we fetched\n        sha = cls.get_revision(dest, rev=\"FETCH_HEAD\")\n        rev_options = rev_options.make_new(sha)\n\n        return rev_options\n\n    @classmethod\n    def is_commit_id_equal(cls, dest: str, name: Optional[str]) -> bool:\n        \"\"\"\n        Return whether the current commit hash equals the given name.\n\n        Args:\n          dest: the repository directory.\n          name: a string name.\n        \"\"\"\n        if not name:\n            # Then avoid an unnecessary subprocess call.\n            return False\n\n        return cls.get_revision(dest) == name\n\n    def fetch_new(\n        self, dest: str, url: HiddenText, rev_options: RevOptions, verbosity: int\n    ) -> None:\n        rev_display = rev_options.to_display()\n        logger.info(\"Cloning %s%s to %s\", url, rev_display, display_path(dest))\n        if verbosity <= 0:\n            flags: Tuple[str, ...] = (\"--quiet\",)\n        elif verbosity == 1:\n            flags = ()\n        else:\n            flags = (\"--verbose\", \"--progress\")\n        if self.get_git_version() >= (2, 17):\n            # Git added support for partial clone in 2.17\n            # https://git-scm.com/docs/partial-clone\n            # Speeds up cloning by functioning without a complete copy of repository\n            self.run_command(\n                make_command(\n                    \"clone\",\n                    \"--filter=blob:none\",\n                    *flags,\n                    url,\n                    dest,\n                )\n            )\n        else:\n            self.run_command(make_command(\"clone\", *flags, url, dest))\n\n        if rev_options.rev:\n            # Then a specific revision was requested.\n            rev_options = self.resolve_revision(dest, url, rev_options)\n            branch_name = getattr(rev_options, \"branch_name\", None)\n            logger.debug(\"Rev options %s, branch_name %s\", rev_options, branch_name)\n            if branch_name is None:\n                # Only do a checkout if the current commit id doesn't match\n                # the requested revision.\n                if not self.is_commit_id_equal(dest, rev_options.rev):\n                    cmd_args = make_command(\n                        \"checkout\",\n                        \"-q\",\n                        rev_options.to_args(),\n                    )\n                    self.run_command(cmd_args, cwd=dest)\n            elif self.get_current_branch(dest) != branch_name:\n                # Then a specific branch was requested, and that branch\n                # is not yet checked out.\n                track_branch = f\"origin/{branch_name}\"\n                cmd_args = [\n                    \"checkout\",\n                    \"-b\",\n                    branch_name,\n                    \"--track\",\n                    track_branch,\n                ]\n                self.run_command(cmd_args, cwd=dest)\n        else:\n            sha = self.get_revision(dest)\n            rev_options = rev_options.make_new(sha)\n\n        logger.info(\"Resolved %s to commit %s\", url, rev_options.rev)\n\n        #: repo may contain submodules\n        self.update_submodules(dest)\n\n    def switch(self, dest: str, url: HiddenText, rev_options: RevOptions) -> None:\n        self.run_command(\n            make_command(\"config\", \"remote.origin.url\", url),\n            cwd=dest,\n        )\n        cmd_args = make_command(\"checkout\", \"-q\", rev_options.to_args())\n        self.run_command(cmd_args, cwd=dest)\n\n        self.update_submodules(dest)\n\n    def update(self, dest: str, url: HiddenText, rev_options: RevOptions) -> None:\n        # First fetch changes from the default remote\n        if self.get_git_version() >= (1, 9):\n            # fetch tags in addition to everything else\n            self.run_command([\"fetch\", \"-q\", \"--tags\"], cwd=dest)\n        else:\n            self.run_command([\"fetch\", \"-q\"], cwd=dest)\n        # Then reset to wanted revision (maybe even origin/master)\n        rev_options = self.resolve_revision(dest, url, rev_options)\n        cmd_args = make_command(\"reset\", \"--hard\", \"-q\", rev_options.to_args())\n        self.run_command(cmd_args, cwd=dest)\n        #: update submodules\n        self.update_submodules(dest)\n\n    @classmethod\n    def get_remote_url(cls, location: str) -> str:\n        \"\"\"\n        Return URL of the first remote encountered.\n\n        Raises RemoteNotFoundError if the repository does not have a remote\n        url configured.\n        \"\"\"\n        # We need to pass 1 for extra_ok_returncodes since the command\n        # exits with return code 1 if there are no matching lines.\n        stdout = cls.run_command(\n            [\"config\", \"--get-regexp\", r\"remote\\..*\\.url\"],\n            extra_ok_returncodes=(1,),\n            show_stdout=False,\n            stdout_only=True,\n            cwd=location,\n        )\n        remotes = stdout.splitlines()\n        try:\n            found_remote = remotes[0]\n        except IndexError:\n            raise RemoteNotFoundError\n\n        for remote in remotes:\n            if remote.startswith(\"remote.origin.url \"):\n                found_remote = remote\n                break\n        url = found_remote.split(\" \")[1]\n        return cls._git_remote_to_pip_url(url.strip())\n\n    @staticmethod\n    def _git_remote_to_pip_url(url: str) -> str:\n        \"\"\"\n        Convert a remote url from what git uses to what pip accepts.\n\n        There are 3 legal forms **url** may take:\n\n            1. A fully qualified url: ssh://git@example.com/foo/bar.git\n            2. A local project.git folder: /path/to/bare/repository.git\n            3. SCP shorthand for form 1: git@example.com:foo/bar.git\n\n        Form 1 is output as-is. Form 2 must be converted to URI and form 3 must\n        be converted to form 1.\n\n        See the corresponding test test_git_remote_url_to_pip() for examples of\n        sample inputs/outputs.\n        \"\"\"\n        if re.match(r\"\\w+://\", url):\n            # This is already valid. Pass it though as-is.\n            return url\n        if os.path.exists(url):\n            # A local bare remote (git clone --mirror).\n            # Needs a file:// prefix.\n            return pathlib.PurePath(url).as_uri()\n        scp_match = SCP_REGEX.match(url)\n        if scp_match:\n            # Add an ssh:// prefix and replace the ':' with a '/'.\n            return scp_match.expand(r\"ssh://\\1\\2/\\3\")\n        # Otherwise, bail out.\n        raise RemoteNotValidError(url)\n\n    @classmethod\n    def has_commit(cls, location: str, rev: str) -> bool:\n        \"\"\"\n        Check if rev is a commit that is available in the local repository.\n        \"\"\"\n        try:\n            cls.run_command(\n                [\"rev-parse\", \"-q\", \"--verify\", \"sha^\" + rev],\n                cwd=location,\n                log_failed_cmd=False,\n            )\n        except InstallationError:\n            return False\n        else:\n            return True\n\n    @classmethod\n    def get_revision(cls, location: str, rev: Optional[str] = None) -> str:\n        if rev is None:\n            rev = \"HEAD\"\n        current_rev = cls.run_command(\n            [\"rev-parse\", rev],\n            show_stdout=False,\n            stdout_only=True,\n            cwd=location,\n        )\n        return current_rev.strip()\n\n    @classmethod\n    def get_subdirectory(cls, location: str) -> Optional[str]:\n        \"\"\"\n        Return the path to Python project root, relative to the repo root.\n        Return None if the project root is in the repo root.\n        \"\"\"\n        # find the repo root\n        git_dir = cls.run_command(\n            [\"rev-parse\", \"--git-dir\"],\n            show_stdout=False,\n            stdout_only=True,\n            cwd=location,\n        ).strip()\n        if not os.path.isabs(git_dir):\n            git_dir = os.path.join(location, git_dir)\n        repo_root = os.path.abspath(os.path.join(git_dir, \"..\"))\n        return find_path_to_project_root_from_repo_root(location, repo_root)\n\n    @classmethod\n    def get_url_rev_and_auth(cls, url: str) -> Tuple[str, Optional[str], AuthInfo]:\n        \"\"\"\n        Prefixes stub URLs like 'user@hostname:user/repo.git' with 'ssh://'.\n        That's required because although they use SSH they sometimes don't\n        work with a ssh:// scheme (e.g. GitHub). But we need a scheme for\n        parsing. Hence we remove it again afterwards and return it as a stub.\n        \"\"\"\n        # Works around an apparent Git bug\n        # (see https://article.gmane.org/gmane.comp.version-control.git/146500)\n        scheme, netloc, path, query, fragment = urlsplit(url)\n        if scheme.endswith(\"file\"):\n            initial_slashes = path[: -len(path.lstrip(\"/\"))]\n            newpath = initial_slashes + urllib.request.url2pathname(path).replace(\n                \"\\\\\", \"/\"\n            ).lstrip(\"/\")\n            after_plus = scheme.find(\"+\") + 1\n            url = scheme[:after_plus] + urlunsplit(\n                (scheme[after_plus:], netloc, newpath, query, fragment),\n            )\n\n        if \"://\" not in url:\n            assert \"file:\" not in url\n            url = url.replace(\"git+\", \"git+ssh://\")\n            url, rev, user_pass = super().get_url_rev_and_auth(url)\n            url = url.replace(\"ssh://\", \"\")\n        else:\n            url, rev, user_pass = super().get_url_rev_and_auth(url)\n\n        return url, rev, user_pass\n\n    @classmethod\n    def update_submodules(cls, location: str) -> None:\n        if not os.path.exists(os.path.join(location, \".gitmodules\")):\n            return\n        cls.run_command(\n            [\"submodule\", \"update\", \"--init\", \"--recursive\", \"-q\"],\n            cwd=location,\n        )\n\n    @classmethod\n    def get_repository_root(cls, location: str) -> Optional[str]:\n        loc = super().get_repository_root(location)\n        if loc:\n            return loc\n        try:\n            r = cls.run_command(\n                [\"rev-parse\", \"--show-toplevel\"],\n                cwd=location,\n                show_stdout=False,\n                stdout_only=True,\n                on_returncode=\"raise\",\n                log_failed_cmd=False,\n            )\n        except BadCommand:\n            logger.debug(\n                \"could not determine if %s is under git control \"\n                \"because git is not available\",\n                location,\n            )\n            return None\n        except InstallationError:\n            return None\n        return os.path.normpath(r.rstrip(\"\\r\\n\"))\n\n    @staticmethod\n    def should_add_vcs_url_prefix(repo_url: str) -> bool:\n        \"\"\"In either https or ssh form, requirements must be prefixed with git+.\"\"\"\n        return True\n\n\nvcs.register(Git)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/vcs/mercurial.py","size":5249,"sha1":"9f7531c4ab899404633da9ab7de6e0610da47ec0","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import configparser\nimport logging\nimport os\nfrom typing import List, Optional, Tuple\n\nfrom pip._internal.exceptions import BadCommand, InstallationError\nfrom pip._internal.utils.misc import HiddenText, display_path\nfrom pip._internal.utils.subprocess import make_command\nfrom pip._internal.utils.urls import path_to_url\nfrom pip._internal.vcs.versioncontrol import (\n    RevOptions,\n    VersionControl,\n    find_path_to_project_root_from_repo_root,\n    vcs,\n)\n\nlogger = logging.getLogger(__name__)\n\n\nclass Mercurial(VersionControl):\n    name = \"hg\"\n    dirname = \".hg\"\n    repo_name = \"clone\"\n    schemes = (\n        \"hg+file\",\n        \"hg+http\",\n        \"hg+https\",\n        \"hg+ssh\",\n        \"hg+static-http\",\n    )\n\n    @staticmethod\n    def get_base_rev_args(rev: str) -> List[str]:\n        return [f\"--rev={rev}\"]\n\n    def fetch_new(\n        self, dest: str, url: HiddenText, rev_options: RevOptions, verbosity: int\n    ) -> None:\n        rev_display = rev_options.to_display()\n        logger.info(\n            \"Cloning hg %s%s to %s\",\n            url,\n            rev_display,\n            display_path(dest),\n        )\n        if verbosity <= 0:\n            flags: Tuple[str, ...] = (\"--quiet\",)\n        elif verbosity == 1:\n            flags = ()\n        elif verbosity == 2:\n            flags = (\"--verbose\",)\n        else:\n            flags = (\"--verbose\", \"--debug\")\n        self.run_command(make_command(\"clone\", \"--noupdate\", *flags, url, dest))\n        self.run_command(\n            make_command(\"update\", *flags, rev_options.to_args()),\n            cwd=dest,\n        )\n\n    def switch(self, dest: str, url: HiddenText, rev_options: RevOptions) -> None:\n        repo_config = os.path.join(dest, self.dirname, \"hgrc\")\n        config = configparser.RawConfigParser()\n        try:\n            config.read(repo_config)\n            config.set(\"paths\", \"default\", url.secret)\n            with open(repo_config, \"w\") as config_file:\n                config.write(config_file)\n        except (OSError, configparser.NoSectionError) as exc:\n            logger.warning(\"Could not switch Mercurial repository to %s: %s\", url, exc)\n        else:\n            cmd_args = make_command(\"update\", \"-q\", rev_options.to_args())\n            self.run_command(cmd_args, cwd=dest)\n\n    def update(self, dest: str, url: HiddenText, rev_options: RevOptions) -> None:\n        self.run_command([\"pull\", \"-q\"], cwd=dest)\n        cmd_args = make_command(\"update\", \"-q\", rev_options.to_args())\n        self.run_command(cmd_args, cwd=dest)\n\n    @classmethod\n    def get_remote_url(cls, location: str) -> str:\n        url = cls.run_command(\n            [\"showconfig\", \"paths.default\"],\n            show_stdout=False,\n            stdout_only=True,\n            cwd=location,\n        ).strip()\n        if cls._is_local_repository(url):\n            url = path_to_url(url)\n        return url.strip()\n\n    @classmethod\n    def get_revision(cls, location: str) -> str:\n        \"\"\"\n        Return the repository-local changeset revision number, as an integer.\n        \"\"\"\n        current_revision = cls.run_command(\n            [\"parents\", \"--template={rev}\"],\n            show_stdout=False,\n            stdout_only=True,\n            cwd=location,\n        ).strip()\n        return current_revision\n\n    @classmethod\n    def get_requirement_revision(cls, location: str) -> str:\n        \"\"\"\n        Return the changeset identification hash, as a 40-character\n        hexadecimal string\n        \"\"\"\n        current_rev_hash = cls.run_command(\n            [\"parents\", \"--template={node}\"],\n            show_stdout=False,\n            stdout_only=True,\n            cwd=location,\n        ).strip()\n        return current_rev_hash\n\n    @classmethod\n    def is_commit_id_equal(cls, dest: str, name: Optional[str]) -> bool:\n        \"\"\"Always assume the versions don't match\"\"\"\n        return False\n\n    @classmethod\n    def get_subdirectory(cls, location: str) -> Optional[str]:\n        \"\"\"\n        Return the path to Python project root, relative to the repo root.\n        Return None if the project root is in the repo root.\n        \"\"\"\n        # find the repo root\n        repo_root = cls.run_command(\n            [\"root\"], show_stdout=False, stdout_only=True, cwd=location\n        ).strip()\n        if not os.path.isabs(repo_root):\n            repo_root = os.path.abspath(os.path.join(location, repo_root))\n        return find_path_to_project_root_from_repo_root(location, repo_root)\n\n    @classmethod\n    def get_repository_root(cls, location: str) -> Optional[str]:\n        loc = super().get_repository_root(location)\n        if loc:\n            return loc\n        try:\n            r = cls.run_command(\n                [\"root\"],\n                cwd=location,\n                show_stdout=False,\n                stdout_only=True,\n                on_returncode=\"raise\",\n                log_failed_cmd=False,\n            )\n        except BadCommand:\n            logger.debug(\n                \"could not determine if %s is under hg control \"\n                \"because hg is not available\",\n                location,\n            )\n            return None\n        except InstallationError:\n            return None\n        return os.path.normpath(r.rstrip(\"\\r\\n\"))\n\n\nvcs.register(Mercurial)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/vcs/subversion.py","size":11735,"sha1":"523bb3d4d9c413640c2794b139ee8a198f97bb98","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import logging\nimport os\nimport re\nfrom typing import List, Optional, Tuple\n\nfrom pip._internal.utils.misc import (\n    HiddenText,\n    display_path,\n    is_console_interactive,\n    is_installable_dir,\n    split_auth_from_netloc,\n)\nfrom pip._internal.utils.subprocess import CommandArgs, make_command\nfrom pip._internal.vcs.versioncontrol import (\n    AuthInfo,\n    RemoteNotFoundError,\n    RevOptions,\n    VersionControl,\n    vcs,\n)\n\nlogger = logging.getLogger(__name__)\n\n_svn_xml_url_re = re.compile('url=\"([^\"]+)\"')\n_svn_rev_re = re.compile(r'committed-rev=\"(\\d+)\"')\n_svn_info_xml_rev_re = re.compile(r'\\s*revision=\"(\\d+)\"')\n_svn_info_xml_url_re = re.compile(r\"<url>(.*)</url>\")\n\n\nclass Subversion(VersionControl):\n    name = \"svn\"\n    dirname = \".svn\"\n    repo_name = \"checkout\"\n    schemes = (\"svn+ssh\", \"svn+http\", \"svn+https\", \"svn+svn\", \"svn+file\")\n\n    @classmethod\n    def should_add_vcs_url_prefix(cls, remote_url: str) -> bool:\n        return True\n\n    @staticmethod\n    def get_base_rev_args(rev: str) -> List[str]:\n        return [\"-r\", rev]\n\n    @classmethod\n    def get_revision(cls, location: str) -> str:\n        \"\"\"\n        Return the maximum revision for all files under a given location\n        \"\"\"\n        # Note: taken from setuptools.command.egg_info\n        revision = 0\n\n        for base, dirs, _ in os.walk(location):\n            if cls.dirname not in dirs:\n                dirs[:] = []\n                continue  # no sense walking uncontrolled subdirs\n            dirs.remove(cls.dirname)\n            entries_fn = os.path.join(base, cls.dirname, \"entries\")\n            if not os.path.exists(entries_fn):\n                # FIXME: should we warn?\n                continue\n\n            dirurl, localrev = cls._get_svn_url_rev(base)\n\n            if base == location:\n                assert dirurl is not None\n                base = dirurl + \"/\"  # save the root url\n            elif not dirurl or not dirurl.startswith(base):\n                dirs[:] = []\n                continue  # not part of the same svn tree, skip it\n            revision = max(revision, localrev)\n        return str(revision)\n\n    @classmethod\n    def get_netloc_and_auth(\n        cls, netloc: str, scheme: str\n    ) -> Tuple[str, Tuple[Optional[str], Optional[str]]]:\n        \"\"\"\n        This override allows the auth information to be passed to svn via the\n        --username and --password options instead of via the URL.\n        \"\"\"\n        if scheme == \"ssh\":\n            # The --username and --password options can't be used for\n            # svn+ssh URLs, so keep the auth information in the URL.\n            return super().get_netloc_and_auth(netloc, scheme)\n\n        return split_auth_from_netloc(netloc)\n\n    @classmethod\n    def get_url_rev_and_auth(cls, url: str) -> Tuple[str, Optional[str], AuthInfo]:\n        # hotfix the URL scheme after removing svn+ from svn+ssh:// re-add it\n        url, rev, user_pass = super().get_url_rev_and_auth(url)\n        if url.startswith(\"ssh://\"):\n            url = \"svn+\" + url\n        return url, rev, user_pass\n\n    @staticmethod\n    def make_rev_args(\n        username: Optional[str], password: Optional[HiddenText]\n    ) -> CommandArgs:\n        extra_args: CommandArgs = []\n        if username:\n            extra_args += [\"--username\", username]\n        if password:\n            extra_args += [\"--password\", password]\n\n        return extra_args\n\n    @classmethod\n    def get_remote_url(cls, location: str) -> str:\n        # In cases where the source is in a subdirectory, we have to look up in\n        # the location until we find a valid project root.\n        orig_location = location\n        while not is_installable_dir(location):\n            last_location = location\n            location = os.path.dirname(location)\n            if location == last_location:\n                # We've traversed up to the root of the filesystem without\n                # finding a Python project.\n                logger.warning(\n                    \"Could not find Python project for directory %s (tried all \"\n                    \"parent directories)\",\n                    orig_location,\n                )\n                raise RemoteNotFoundError\n\n        url, _rev = cls._get_svn_url_rev(location)\n        if url is None:\n            raise RemoteNotFoundError\n\n        return url\n\n    @classmethod\n    def _get_svn_url_rev(cls, location: str) -> Tuple[Optional[str], int]:\n        from pip._internal.exceptions import InstallationError\n\n        entries_path = os.path.join(location, cls.dirname, \"entries\")\n        if os.path.exists(entries_path):\n            with open(entries_path) as f:\n                data = f.read()\n        else:  # subversion >= 1.7 does not have the 'entries' file\n            data = \"\"\n\n        url = None\n        if data.startswith(\"8\") or data.startswith(\"9\") or data.startswith(\"10\"):\n            entries = list(map(str.splitlines, data.split(\"\\n\\x0c\\n\")))\n            del entries[0][0]  # get rid of the '8'\n            url = entries[0][3]\n            revs = [int(d[9]) for d in entries if len(d) > 9 and d[9]] + [0]\n        elif data.startswith(\"<?xml\"):\n            match = _svn_xml_url_re.search(data)\n            if not match:\n                raise ValueError(f\"Badly formatted data: {data!r}\")\n            url = match.group(1)  # get repository URL\n            revs = [int(m.group(1)) for m in _svn_rev_re.finditer(data)] + [0]\n        else:\n            try:\n                # subversion >= 1.7\n                # Note that using get_remote_call_options is not necessary here\n                # because `svn info` is being run against a local directory.\n                # We don't need to worry about making sure interactive mode\n                # is being used to prompt for passwords, because passwords\n                # are only potentially needed for remote server requests.\n                xml = cls.run_command(\n                    [\"info\", \"--xml\", location],\n                    show_stdout=False,\n                    stdout_only=True,\n                )\n                match = _svn_info_xml_url_re.search(xml)\n                assert match is not None\n                url = match.group(1)\n                revs = [int(m.group(1)) for m in _svn_info_xml_rev_re.finditer(xml)]\n            except InstallationError:\n                url, revs = None, []\n\n        if revs:\n            rev = max(revs)\n        else:\n            rev = 0\n\n        return url, rev\n\n    @classmethod\n    def is_commit_id_equal(cls, dest: str, name: Optional[str]) -> bool:\n        \"\"\"Always assume the versions don't match\"\"\"\n        return False\n\n    def __init__(self, use_interactive: Optional[bool] = None) -> None:\n        if use_interactive is None:\n            use_interactive = is_console_interactive()\n        self.use_interactive = use_interactive\n\n        # This member is used to cache the fetched version of the current\n        # ``svn`` client.\n        # Special value definitions:\n        #   None: Not evaluated yet.\n        #   Empty tuple: Could not parse version.\n        self._vcs_version: Optional[Tuple[int, ...]] = None\n\n        super().__init__()\n\n    def call_vcs_version(self) -> Tuple[int, ...]:\n        \"\"\"Query the version of the currently installed Subversion client.\n\n        :return: A tuple containing the parts of the version information or\n            ``()`` if the version returned from ``svn`` could not be parsed.\n        :raises: BadCommand: If ``svn`` is not installed.\n        \"\"\"\n        # Example versions:\n        #   svn, version 1.10.3 (r1842928)\n        #      compiled Feb 25 2019, 14:20:39 on x86_64-apple-darwin17.0.0\n        #   svn, version 1.7.14 (r1542130)\n        #      compiled Mar 28 2018, 08:49:13 on x86_64-pc-linux-gnu\n        #   svn, version 1.12.0-SlikSvn (SlikSvn/1.12.0)\n        #      compiled May 28 2019, 13:44:56 on x86_64-microsoft-windows6.2\n        version_prefix = \"svn, version \"\n        version = self.run_command([\"--version\"], show_stdout=False, stdout_only=True)\n        if not version.startswith(version_prefix):\n            return ()\n\n        version = version[len(version_prefix) :].split()[0]\n        version_list = version.partition(\"-\")[0].split(\".\")\n        try:\n            parsed_version = tuple(map(int, version_list))\n        except ValueError:\n            return ()\n\n        return parsed_version\n\n    def get_vcs_version(self) -> Tuple[int, ...]:\n        \"\"\"Return the version of the currently installed Subversion client.\n\n        If the version of the Subversion client has already been queried,\n        a cached value will be used.\n\n        :return: A tuple containing the parts of the version information or\n            ``()`` if the version returned from ``svn`` could not be parsed.\n        :raises: BadCommand: If ``svn`` is not installed.\n        \"\"\"\n        if self._vcs_version is not None:\n            # Use cached version, if available.\n            # If parsing the version failed previously (empty tuple),\n            # do not attempt to parse it again.\n            return self._vcs_version\n\n        vcs_version = self.call_vcs_version()\n        self._vcs_version = vcs_version\n        return vcs_version\n\n    def get_remote_call_options(self) -> CommandArgs:\n        \"\"\"Return options to be used on calls to Subversion that contact the server.\n\n        These options are applicable for the following ``svn`` subcommands used\n        in this class.\n\n            - checkout\n            - switch\n            - update\n\n        :return: A list of command line arguments to pass to ``svn``.\n        \"\"\"\n        if not self.use_interactive:\n            # --non-interactive switch is available since Subversion 0.14.4.\n            # Subversion < 1.8 runs in interactive mode by default.\n            return [\"--non-interactive\"]\n\n        svn_version = self.get_vcs_version()\n        # By default, Subversion >= 1.8 runs in non-interactive mode if\n        # stdin is not a TTY. Since that is how pip invokes SVN, in\n        # call_subprocess(), pip must pass --force-interactive to ensure\n        # the user can be prompted for a password, if required.\n        #   SVN added the --force-interactive option in SVN 1.8. Since\n        # e.g. RHEL/CentOS 7, which is supported until 2024, ships with\n        # SVN 1.7, pip should continue to support SVN 1.7. Therefore, pip\n        # can't safely add the option if the SVN version is < 1.8 (or unknown).\n        if svn_version >= (1, 8):\n            return [\"--force-interactive\"]\n\n        return []\n\n    def fetch_new(\n        self, dest: str, url: HiddenText, rev_options: RevOptions, verbosity: int\n    ) -> None:\n        rev_display = rev_options.to_display()\n        logger.info(\n            \"Checking out %s%s to %s\",\n            url,\n            rev_display,\n            display_path(dest),\n        )\n        if verbosity <= 0:\n            flags = [\"--quiet\"]\n        else:\n            flags = []\n        cmd_args = make_command(\n            \"checkout\",\n            *flags,\n            self.get_remote_call_options(),\n            rev_options.to_args(),\n            url,\n            dest,\n        )\n        self.run_command(cmd_args)\n\n    def switch(self, dest: str, url: HiddenText, rev_options: RevOptions) -> None:\n        cmd_args = make_command(\n            \"switch\",\n            self.get_remote_call_options(),\n            rev_options.to_args(),\n            url,\n            dest,\n        )\n        self.run_command(cmd_args)\n\n    def update(self, dest: str, url: HiddenText, rev_options: RevOptions) -> None:\n        cmd_args = make_command(\n            \"update\",\n            self.get_remote_call_options(),\n            rev_options.to_args(),\n            dest,\n        )\n        self.run_command(cmd_args)\n\n\nvcs.register(Subversion)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/vcs/versioncontrol.py","size":22440,"sha1":"db3d199e42699d804165fb344d88225218d53cec","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"Handles all VCS (version control) support\"\"\"\n\nimport logging\nimport os\nimport shutil\nimport sys\nimport urllib.parse\nfrom dataclasses import dataclass, field\nfrom typing import (\n    Any,\n    Dict,\n    Iterable,\n    Iterator,\n    List,\n    Literal,\n    Mapping,\n    Optional,\n    Tuple,\n    Type,\n    Union,\n)\n\nfrom pip._internal.cli.spinners import SpinnerInterface\nfrom pip._internal.exceptions import BadCommand, InstallationError\nfrom pip._internal.utils.misc import (\n    HiddenText,\n    ask_path_exists,\n    backup_dir,\n    display_path,\n    hide_url,\n    hide_value,\n    is_installable_dir,\n    rmtree,\n)\nfrom pip._internal.utils.subprocess import (\n    CommandArgs,\n    call_subprocess,\n    format_command_args,\n    make_command,\n)\n\n__all__ = [\"vcs\"]\n\n\nlogger = logging.getLogger(__name__)\n\nAuthInfo = Tuple[Optional[str], Optional[str]]\n\n\ndef is_url(name: str) -> bool:\n    \"\"\"\n    Return true if the name looks like a URL.\n    \"\"\"\n    scheme = urllib.parse.urlsplit(name).scheme\n    if not scheme:\n        return False\n    return scheme in [\"http\", \"https\", \"file\", \"ftp\"] + vcs.all_schemes\n\n\ndef make_vcs_requirement_url(\n    repo_url: str, rev: str, project_name: str, subdir: Optional[str] = None\n) -> str:\n    \"\"\"\n    Return the URL for a VCS requirement.\n\n    Args:\n      repo_url: the remote VCS url, with any needed VCS prefix (e.g. \"git+\").\n      project_name: the (unescaped) project name.\n    \"\"\"\n    egg_project_name = project_name.replace(\"-\", \"_\")\n    req = f\"{repo_url}@{rev}#egg={egg_project_name}\"\n    if subdir:\n        req += f\"&subdirectory={subdir}\"\n\n    return req\n\n\ndef find_path_to_project_root_from_repo_root(\n    location: str, repo_root: str\n) -> Optional[str]:\n    \"\"\"\n    Find the the Python project's root by searching up the filesystem from\n    `location`. Return the path to project root relative to `repo_root`.\n    Return None if the project root is `repo_root`, or cannot be found.\n    \"\"\"\n    # find project root.\n    orig_location = location\n    while not is_installable_dir(location):\n        last_location = location\n        location = os.path.dirname(location)\n        if location == last_location:\n            # We've traversed up to the root of the filesystem without\n            # finding a Python project.\n            logger.warning(\n                \"Could not find a Python project for directory %s (tried all \"\n                \"parent directories)\",\n                orig_location,\n            )\n            return None\n\n    if os.path.samefile(repo_root, location):\n        return None\n\n    return os.path.relpath(location, repo_root)\n\n\nclass RemoteNotFoundError(Exception):\n    pass\n\n\nclass RemoteNotValidError(Exception):\n    def __init__(self, url: str):\n        super().__init__(url)\n        self.url = url\n\n\n@dataclass(frozen=True)\nclass RevOptions:\n    \"\"\"\n    Encapsulates a VCS-specific revision to install, along with any VCS\n    install options.\n\n    Args:\n        vc_class: a VersionControl subclass.\n        rev: the name of the revision to install.\n        extra_args: a list of extra options.\n    \"\"\"\n\n    vc_class: Type[\"VersionControl\"]\n    rev: Optional[str] = None\n    extra_args: CommandArgs = field(default_factory=list)\n    branch_name: Optional[str] = None\n\n    def __repr__(self) -> str:\n        return f\"<RevOptions {self.vc_class.name}: rev={self.rev!r}>\"\n\n    @property\n    def arg_rev(self) -> Optional[str]:\n        if self.rev is None:\n            return self.vc_class.default_arg_rev\n\n        return self.rev\n\n    def to_args(self) -> CommandArgs:\n        \"\"\"\n        Return the VCS-specific command arguments.\n        \"\"\"\n        args: CommandArgs = []\n        rev = self.arg_rev\n        if rev is not None:\n            args += self.vc_class.get_base_rev_args(rev)\n        args += self.extra_args\n\n        return args\n\n    def to_display(self) -> str:\n        if not self.rev:\n            return \"\"\n\n        return f\" (to revision {self.rev})\"\n\n    def make_new(self, rev: str) -> \"RevOptions\":\n        \"\"\"\n        Make a copy of the current instance, but with a new rev.\n\n        Args:\n          rev: the name of the revision for the new object.\n        \"\"\"\n        return self.vc_class.make_rev_options(rev, extra_args=self.extra_args)\n\n\nclass VcsSupport:\n    _registry: Dict[str, \"VersionControl\"] = {}\n    schemes = [\"ssh\", \"git\", \"hg\", \"bzr\", \"sftp\", \"svn\"]\n\n    def __init__(self) -> None:\n        # Register more schemes with urlparse for various version control\n        # systems\n        urllib.parse.uses_netloc.extend(self.schemes)\n        super().__init__()\n\n    def __iter__(self) -> Iterator[str]:\n        return self._registry.__iter__()\n\n    @property\n    def backends(self) -> List[\"VersionControl\"]:\n        return list(self._registry.values())\n\n    @property\n    def dirnames(self) -> List[str]:\n        return [backend.dirname for backend in self.backends]\n\n    @property\n    def all_schemes(self) -> List[str]:\n        schemes: List[str] = []\n        for backend in self.backends:\n            schemes.extend(backend.schemes)\n        return schemes\n\n    def register(self, cls: Type[\"VersionControl\"]) -> None:\n        if not hasattr(cls, \"name\"):\n            logger.warning(\"Cannot register VCS %s\", cls.__name__)\n            return\n        if cls.name not in self._registry:\n            self._registry[cls.name] = cls()\n            logger.debug(\"Registered VCS backend: %s\", cls.name)\n\n    def unregister(self, name: str) -> None:\n        if name in self._registry:\n            del self._registry[name]\n\n    def get_backend_for_dir(self, location: str) -> Optional[\"VersionControl\"]:\n        \"\"\"\n        Return a VersionControl object if a repository of that type is found\n        at the given directory.\n        \"\"\"\n        vcs_backends = {}\n        for vcs_backend in self._registry.values():\n            repo_path = vcs_backend.get_repository_root(location)\n            if not repo_path:\n                continue\n            logger.debug(\"Determine that %s uses VCS: %s\", location, vcs_backend.name)\n            vcs_backends[repo_path] = vcs_backend\n\n        if not vcs_backends:\n            return None\n\n        # Choose the VCS in the inner-most directory. Since all repository\n        # roots found here would be either `location` or one of its\n        # parents, the longest path should have the most path components,\n        # i.e. the backend representing the inner-most repository.\n        inner_most_repo_path = max(vcs_backends, key=len)\n        return vcs_backends[inner_most_repo_path]\n\n    def get_backend_for_scheme(self, scheme: str) -> Optional[\"VersionControl\"]:\n        \"\"\"\n        Return a VersionControl object or None.\n        \"\"\"\n        for vcs_backend in self._registry.values():\n            if scheme in vcs_backend.schemes:\n                return vcs_backend\n        return None\n\n    def get_backend(self, name: str) -> Optional[\"VersionControl\"]:\n        \"\"\"\n        Return a VersionControl object or None.\n        \"\"\"\n        name = name.lower()\n        return self._registry.get(name)\n\n\nvcs = VcsSupport()\n\n\nclass VersionControl:\n    name = \"\"\n    dirname = \"\"\n    repo_name = \"\"\n    # List of supported schemes for this Version Control\n    schemes: Tuple[str, ...] = ()\n    # Iterable of environment variable names to pass to call_subprocess().\n    unset_environ: Tuple[str, ...] = ()\n    default_arg_rev: Optional[str] = None\n\n    @classmethod\n    def should_add_vcs_url_prefix(cls, remote_url: str) -> bool:\n        \"\"\"\n        Return whether the vcs prefix (e.g. \"git+\") should be added to a\n        repository's remote url when used in a requirement.\n        \"\"\"\n        return not remote_url.lower().startswith(f\"{cls.name}:\")\n\n    @classmethod\n    def get_subdirectory(cls, location: str) -> Optional[str]:\n        \"\"\"\n        Return the path to Python project root, relative to the repo root.\n        Return None if the project root is in the repo root.\n        \"\"\"\n        return None\n\n    @classmethod\n    def get_requirement_revision(cls, repo_dir: str) -> str:\n        \"\"\"\n        Return the revision string that should be used in a requirement.\n        \"\"\"\n        return cls.get_revision(repo_dir)\n\n    @classmethod\n    def get_src_requirement(cls, repo_dir: str, project_name: str) -> str:\n        \"\"\"\n        Return the requirement string to use to redownload the files\n        currently at the given repository directory.\n\n        Args:\n          project_name: the (unescaped) project name.\n\n        The return value has a form similar to the following:\n\n            {repository_url}@{revision}#egg={project_name}\n        \"\"\"\n        repo_url = cls.get_remote_url(repo_dir)\n\n        if cls.should_add_vcs_url_prefix(repo_url):\n            repo_url = f\"{cls.name}+{repo_url}\"\n\n        revision = cls.get_requirement_revision(repo_dir)\n        subdir = cls.get_subdirectory(repo_dir)\n        req = make_vcs_requirement_url(repo_url, revision, project_name, subdir=subdir)\n\n        return req\n\n    @staticmethod\n    def get_base_rev_args(rev: str) -> List[str]:\n        \"\"\"\n        Return the base revision arguments for a vcs command.\n\n        Args:\n          rev: the name of a revision to install.  Cannot be None.\n        \"\"\"\n        raise NotImplementedError\n\n    def is_immutable_rev_checkout(self, url: str, dest: str) -> bool:\n        \"\"\"\n        Return true if the commit hash checked out at dest matches\n        the revision in url.\n\n        Always return False, if the VCS does not support immutable commit\n        hashes.\n\n        This method does not check if there are local uncommitted changes\n        in dest after checkout, as pip currently has no use case for that.\n        \"\"\"\n        return False\n\n    @classmethod\n    def make_rev_options(\n        cls, rev: Optional[str] = None, extra_args: Optional[CommandArgs] = None\n    ) -> RevOptions:\n        \"\"\"\n        Return a RevOptions object.\n\n        Args:\n          rev: the name of a revision to install.\n          extra_args: a list of extra options.\n        \"\"\"\n        return RevOptions(cls, rev, extra_args=extra_args or [])\n\n    @classmethod\n    def _is_local_repository(cls, repo: str) -> bool:\n        \"\"\"\n        posix absolute paths start with os.path.sep,\n        win32 ones start with drive (like c:\\\\folder)\n        \"\"\"\n        drive, tail = os.path.splitdrive(repo)\n        return repo.startswith(os.path.sep) or bool(drive)\n\n    @classmethod\n    def get_netloc_and_auth(\n        cls, netloc: str, scheme: str\n    ) -> Tuple[str, Tuple[Optional[str], Optional[str]]]:\n        \"\"\"\n        Parse the repository URL's netloc, and return the new netloc to use\n        along with auth information.\n\n        Args:\n          netloc: the original repository URL netloc.\n          scheme: the repository URL's scheme without the vcs prefix.\n\n        This is mainly for the Subversion class to override, so that auth\n        information can be provided via the --username and --password options\n        instead of through the URL.  For other subclasses like Git without\n        such an option, auth information must stay in the URL.\n\n        Returns: (netloc, (username, password)).\n        \"\"\"\n        return netloc, (None, None)\n\n    @classmethod\n    def get_url_rev_and_auth(cls, url: str) -> Tuple[str, Optional[str], AuthInfo]:\n        \"\"\"\n        Parse the repository URL to use, and return the URL, revision,\n        and auth info to use.\n\n        Returns: (url, rev, (username, password)).\n        \"\"\"\n        scheme, netloc, path, query, frag = urllib.parse.urlsplit(url)\n        if \"+\" not in scheme:\n            raise ValueError(\n                f\"Sorry, {url!r} is a malformed VCS url. \"\n                \"The format is <vcs>+<protocol>://<url>, \"\n                \"e.g. svn+http://myrepo/svn/MyApp#egg=MyApp\"\n            )\n        # Remove the vcs prefix.\n        scheme = scheme.split(\"+\", 1)[1]\n        netloc, user_pass = cls.get_netloc_and_auth(netloc, scheme)\n        rev = None\n        if \"@\" in path:\n            path, rev = path.rsplit(\"@\", 1)\n            if not rev:\n                raise InstallationError(\n                    f\"The URL {url!r} has an empty revision (after @) \"\n                    \"which is not supported. Include a revision after @ \"\n                    \"or remove @ from the URL.\"\n                )\n        url = urllib.parse.urlunsplit((scheme, netloc, path, query, \"\"))\n        return url, rev, user_pass\n\n    @staticmethod\n    def make_rev_args(\n        username: Optional[str], password: Optional[HiddenText]\n    ) -> CommandArgs:\n        \"\"\"\n        Return the RevOptions \"extra arguments\" to use in obtain().\n        \"\"\"\n        return []\n\n    def get_url_rev_options(self, url: HiddenText) -> Tuple[HiddenText, RevOptions]:\n        \"\"\"\n        Return the URL and RevOptions object to use in obtain(),\n        as a tuple (url, rev_options).\n        \"\"\"\n        secret_url, rev, user_pass = self.get_url_rev_and_auth(url.secret)\n        username, secret_password = user_pass\n        password: Optional[HiddenText] = None\n        if secret_password is not None:\n            password = hide_value(secret_password)\n        extra_args = self.make_rev_args(username, password)\n        rev_options = self.make_rev_options(rev, extra_args=extra_args)\n\n        return hide_url(secret_url), rev_options\n\n    @staticmethod\n    def normalize_url(url: str) -> str:\n        \"\"\"\n        Normalize a URL for comparison by unquoting it and removing any\n        trailing slash.\n        \"\"\"\n        return urllib.parse.unquote(url).rstrip(\"/\")\n\n    @classmethod\n    def compare_urls(cls, url1: str, url2: str) -> bool:\n        \"\"\"\n        Compare two repo URLs for identity, ignoring incidental differences.\n        \"\"\"\n        return cls.normalize_url(url1) == cls.normalize_url(url2)\n\n    def fetch_new(\n        self, dest: str, url: HiddenText, rev_options: RevOptions, verbosity: int\n    ) -> None:\n        \"\"\"\n        Fetch a revision from a repository, in the case that this is the\n        first fetch from the repository.\n\n        Args:\n          dest: the directory to fetch the repository to.\n          rev_options: a RevOptions object.\n          verbosity: verbosity level.\n        \"\"\"\n        raise NotImplementedError\n\n    def switch(self, dest: str, url: HiddenText, rev_options: RevOptions) -> None:\n        \"\"\"\n        Switch the repo at ``dest`` to point to ``URL``.\n\n        Args:\n          rev_options: a RevOptions object.\n        \"\"\"\n        raise NotImplementedError\n\n    def update(self, dest: str, url: HiddenText, rev_options: RevOptions) -> None:\n        \"\"\"\n        Update an already-existing repo to the given ``rev_options``.\n\n        Args:\n          rev_options: a RevOptions object.\n        \"\"\"\n        raise NotImplementedError\n\n    @classmethod\n    def is_commit_id_equal(cls, dest: str, name: Optional[str]) -> bool:\n        \"\"\"\n        Return whether the id of the current commit equals the given name.\n\n        Args:\n          dest: the repository directory.\n          name: a string name.\n        \"\"\"\n        raise NotImplementedError\n\n    def obtain(self, dest: str, url: HiddenText, verbosity: int) -> None:\n        \"\"\"\n        Install or update in editable mode the package represented by this\n        VersionControl object.\n\n        :param dest: the repository directory in which to install or update.\n        :param url: the repository URL starting with a vcs prefix.\n        :param verbosity: verbosity level.\n        \"\"\"\n        url, rev_options = self.get_url_rev_options(url)\n\n        if not os.path.exists(dest):\n            self.fetch_new(dest, url, rev_options, verbosity=verbosity)\n            return\n\n        rev_display = rev_options.to_display()\n        if self.is_repository_directory(dest):\n            existing_url = self.get_remote_url(dest)\n            if self.compare_urls(existing_url, url.secret):\n                logger.debug(\n                    \"%s in %s exists, and has correct URL (%s)\",\n                    self.repo_name.title(),\n                    display_path(dest),\n                    url,\n                )\n                if not self.is_commit_id_equal(dest, rev_options.rev):\n                    logger.info(\n                        \"Updating %s %s%s\",\n                        display_path(dest),\n                        self.repo_name,\n                        rev_display,\n                    )\n                    self.update(dest, url, rev_options)\n                else:\n                    logger.info(\"Skipping because already up-to-date.\")\n                return\n\n            logger.warning(\n                \"%s %s in %s exists with URL %s\",\n                self.name,\n                self.repo_name,\n                display_path(dest),\n                existing_url,\n            )\n            prompt = (\"(s)witch, (i)gnore, (w)ipe, (b)ackup \", (\"s\", \"i\", \"w\", \"b\"))\n        else:\n            logger.warning(\n                \"Directory %s already exists, and is not a %s %s.\",\n                dest,\n                self.name,\n                self.repo_name,\n            )\n            # https://github.com/python/mypy/issues/1174\n            prompt = (\"(i)gnore, (w)ipe, (b)ackup \", (\"i\", \"w\", \"b\"))  # type: ignore\n\n        logger.warning(\n            \"The plan is to install the %s repository %s\",\n            self.name,\n            url,\n        )\n        response = ask_path_exists(f\"What to do?  {prompt[0]}\", prompt[1])\n\n        if response == \"a\":\n            sys.exit(-1)\n\n        if response == \"w\":\n            logger.warning(\"Deleting %s\", display_path(dest))\n            rmtree(dest)\n            self.fetch_new(dest, url, rev_options, verbosity=verbosity)\n            return\n\n        if response == \"b\":\n            dest_dir = backup_dir(dest)\n            logger.warning(\"Backing up %s to %s\", display_path(dest), dest_dir)\n            shutil.move(dest, dest_dir)\n            self.fetch_new(dest, url, rev_options, verbosity=verbosity)\n            return\n\n        # Do nothing if the response is \"i\".\n        if response == \"s\":\n            logger.info(\n                \"Switching %s %s to %s%s\",\n                self.repo_name,\n                display_path(dest),\n                url,\n                rev_display,\n            )\n            self.switch(dest, url, rev_options)\n\n    def unpack(self, location: str, url: HiddenText, verbosity: int) -> None:\n        \"\"\"\n        Clean up current location and download the url repository\n        (and vcs infos) into location\n\n        :param url: the repository URL starting with a vcs prefix.\n        :param verbosity: verbosity level.\n        \"\"\"\n        if os.path.exists(location):\n            rmtree(location)\n        self.obtain(location, url=url, verbosity=verbosity)\n\n    @classmethod\n    def get_remote_url(cls, location: str) -> str:\n        \"\"\"\n        Return the url used at location\n\n        Raises RemoteNotFoundError if the repository does not have a remote\n        url configured.\n        \"\"\"\n        raise NotImplementedError\n\n    @classmethod\n    def get_revision(cls, location: str) -> str:\n        \"\"\"\n        Return the current commit id of the files at the given location.\n        \"\"\"\n        raise NotImplementedError\n\n    @classmethod\n    def run_command(\n        cls,\n        cmd: Union[List[str], CommandArgs],\n        show_stdout: bool = True,\n        cwd: Optional[str] = None,\n        on_returncode: 'Literal[\"raise\", \"warn\", \"ignore\"]' = \"raise\",\n        extra_ok_returncodes: Optional[Iterable[int]] = None,\n        command_desc: Optional[str] = None,\n        extra_environ: Optional[Mapping[str, Any]] = None,\n        spinner: Optional[SpinnerInterface] = None,\n        log_failed_cmd: bool = True,\n        stdout_only: bool = False,\n    ) -> str:\n        \"\"\"\n        Run a VCS subcommand\n        This is simply a wrapper around call_subprocess that adds the VCS\n        command name, and checks that the VCS is available\n        \"\"\"\n        cmd = make_command(cls.name, *cmd)\n        if command_desc is None:\n            command_desc = format_command_args(cmd)\n        try:\n            return call_subprocess(\n                cmd,\n                show_stdout,\n                cwd,\n                on_returncode=on_returncode,\n                extra_ok_returncodes=extra_ok_returncodes,\n                command_desc=command_desc,\n                extra_environ=extra_environ,\n                unset_environ=cls.unset_environ,\n                spinner=spinner,\n                log_failed_cmd=log_failed_cmd,\n                stdout_only=stdout_only,\n            )\n        except NotADirectoryError:\n            raise BadCommand(f\"Cannot find command {cls.name!r} - invalid PATH\")\n        except FileNotFoundError:\n            # errno.ENOENT = no such file or directory\n            # In other words, the VCS executable isn't available\n            raise BadCommand(\n                f\"Cannot find command {cls.name!r} - do you have \"\n                f\"{cls.name!r} installed and in your PATH?\"\n            )\n        except PermissionError:\n            # errno.EACCES = Permission denied\n            # This error occurs, for instance, when the command is installed\n            # only for another user. So, the current user don't have\n            # permission to call the other user command.\n            raise BadCommand(\n                f\"No permission to execute {cls.name!r} - install it \"\n                f\"locally, globally (ask admin), or check your PATH. \"\n                f\"See possible solutions at \"\n                f\"https://pip.pypa.io/en/latest/reference/pip_freeze/\"\n                f\"#fixing-permission-denied.\"\n            )\n\n    @classmethod\n    def is_repository_directory(cls, path: str) -> bool:\n        \"\"\"\n        Return whether a directory path is a repository directory.\n        \"\"\"\n        logger.debug(\"Checking in %s for %s (%s)...\", path, cls.dirname, cls.name)\n        return os.path.exists(os.path.join(path, cls.dirname))\n\n    @classmethod\n    def get_repository_root(cls, location: str) -> Optional[str]:\n        \"\"\"\n        Return the \"root\" (top-level) directory controlled by the vcs,\n        or `None` if the directory is not in any.\n\n        It is meant to be overridden to implement smarter detection\n        mechanisms for specific vcs.\n\n        This can do more than is_repository_directory() alone. For\n        example, the Git override checks that Git is actually available.\n        \"\"\"\n        if cls.is_repository_directory(location):\n            return location\n        return None\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_internal/wheel_builder.py","size":11799,"sha1":"b8624ad9f42eea9be96e2bf69e6d2f61216eacc4","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"Orchestrator for building wheels from InstallRequirements.\n\"\"\"\n\nimport logging\nimport os.path\nimport re\nimport shutil\nfrom typing import Iterable, List, Optional, Tuple\n\nfrom pip._vendor.packaging.utils import canonicalize_name, canonicalize_version\nfrom pip._vendor.packaging.version import InvalidVersion, Version\n\nfrom pip._internal.cache import WheelCache\nfrom pip._internal.exceptions import InvalidWheelFilename, UnsupportedWheel\nfrom pip._internal.metadata import FilesystemWheel, get_wheel_distribution\nfrom pip._internal.models.link import Link\nfrom pip._internal.models.wheel import Wheel\nfrom pip._internal.operations.build.wheel import build_wheel_pep517\nfrom pip._internal.operations.build.wheel_editable import build_wheel_editable\nfrom pip._internal.operations.build.wheel_legacy import build_wheel_legacy\nfrom pip._internal.req.req_install import InstallRequirement\nfrom pip._internal.utils.logging import indent_log\nfrom pip._internal.utils.misc import ensure_dir, hash_file\nfrom pip._internal.utils.setuptools_build import make_setuptools_clean_args\nfrom pip._internal.utils.subprocess import call_subprocess\nfrom pip._internal.utils.temp_dir import TempDirectory\nfrom pip._internal.utils.urls import path_to_url\nfrom pip._internal.vcs import vcs\n\nlogger = logging.getLogger(__name__)\n\n_egg_info_re = re.compile(r\"([a-z0-9_.]+)-([a-z0-9_.!+-]+)\", re.IGNORECASE)\n\nBuildResult = Tuple[List[InstallRequirement], List[InstallRequirement]]\n\n\ndef _contains_egg_info(s: str) -> bool:\n    \"\"\"Determine whether the string looks like an egg_info.\n\n    :param s: The string to parse. E.g. foo-2.1\n    \"\"\"\n    return bool(_egg_info_re.search(s))\n\n\ndef _should_build(\n    req: InstallRequirement,\n    need_wheel: bool,\n) -> bool:\n    \"\"\"Return whether an InstallRequirement should be built into a wheel.\"\"\"\n    if req.constraint:\n        # never build requirements that are merely constraints\n        return False\n    if req.is_wheel:\n        if need_wheel:\n            logger.info(\n                \"Skipping %s, due to already being wheel.\",\n                req.name,\n            )\n        return False\n\n    if need_wheel:\n        # i.e. pip wheel, not pip install\n        return True\n\n    # From this point, this concerns the pip install command only\n    # (need_wheel=False).\n\n    if not req.source_dir:\n        return False\n\n    if req.editable:\n        # we only build PEP 660 editable requirements\n        return req.supports_pyproject_editable\n\n    return True\n\n\ndef should_build_for_wheel_command(\n    req: InstallRequirement,\n) -> bool:\n    return _should_build(req, need_wheel=True)\n\n\ndef should_build_for_install_command(\n    req: InstallRequirement,\n) -> bool:\n    return _should_build(req, need_wheel=False)\n\n\ndef _should_cache(\n    req: InstallRequirement,\n) -> Optional[bool]:\n    \"\"\"\n    Return whether a built InstallRequirement can be stored in the persistent\n    wheel cache, assuming the wheel cache is available, and _should_build()\n    has determined a wheel needs to be built.\n    \"\"\"\n    if req.editable or not req.source_dir:\n        # never cache editable requirements\n        return False\n\n    if req.link and req.link.is_vcs:\n        # VCS checkout. Do not cache\n        # unless it points to an immutable commit hash.\n        assert not req.editable\n        assert req.source_dir\n        vcs_backend = vcs.get_backend_for_scheme(req.link.scheme)\n        assert vcs_backend\n        if vcs_backend.is_immutable_rev_checkout(req.link.url, req.source_dir):\n            return True\n        return False\n\n    assert req.link\n    base, ext = req.link.splitext()\n    if _contains_egg_info(base):\n        return True\n\n    # Otherwise, do not cache.\n    return False\n\n\ndef _get_cache_dir(\n    req: InstallRequirement,\n    wheel_cache: WheelCache,\n) -> str:\n    \"\"\"Return the persistent or temporary cache directory where the built\n    wheel need to be stored.\n    \"\"\"\n    cache_available = bool(wheel_cache.cache_dir)\n    assert req.link\n    if cache_available and _should_cache(req):\n        cache_dir = wheel_cache.get_path_for_link(req.link)\n    else:\n        cache_dir = wheel_cache.get_ephem_path_for_link(req.link)\n    return cache_dir\n\n\ndef _verify_one(req: InstallRequirement, wheel_path: str) -> None:\n    canonical_name = canonicalize_name(req.name or \"\")\n    w = Wheel(os.path.basename(wheel_path))\n    if canonicalize_name(w.name) != canonical_name:\n        raise InvalidWheelFilename(\n            f\"Wheel has unexpected file name: expected {canonical_name!r}, \"\n            f\"got {w.name!r}\",\n        )\n    dist = get_wheel_distribution(FilesystemWheel(wheel_path), canonical_name)\n    dist_verstr = str(dist.version)\n    if canonicalize_version(dist_verstr) != canonicalize_version(w.version):\n        raise InvalidWheelFilename(\n            f\"Wheel has unexpected file name: expected {dist_verstr!r}, \"\n            f\"got {w.version!r}\",\n        )\n    metadata_version_value = dist.metadata_version\n    if metadata_version_value is None:\n        raise UnsupportedWheel(\"Missing Metadata-Version\")\n    try:\n        metadata_version = Version(metadata_version_value)\n    except InvalidVersion:\n        msg = f\"Invalid Metadata-Version: {metadata_version_value}\"\n        raise UnsupportedWheel(msg)\n    if metadata_version >= Version(\"1.2\") and not isinstance(dist.version, Version):\n        raise UnsupportedWheel(\n            f\"Metadata 1.2 mandates PEP 440 version, but {dist_verstr!r} is not\"\n        )\n\n\ndef _build_one(\n    req: InstallRequirement,\n    output_dir: str,\n    verify: bool,\n    build_options: List[str],\n    global_options: List[str],\n    editable: bool,\n) -> Optional[str]:\n    \"\"\"Build one wheel.\n\n    :return: The filename of the built wheel, or None if the build failed.\n    \"\"\"\n    artifact = \"editable\" if editable else \"wheel\"\n    try:\n        ensure_dir(output_dir)\n    except OSError as e:\n        logger.warning(\n            \"Building %s for %s failed: %s\",\n            artifact,\n            req.name,\n            e,\n        )\n        return None\n\n    # Install build deps into temporary directory (PEP 518)\n    with req.build_env:\n        wheel_path = _build_one_inside_env(\n            req, output_dir, build_options, global_options, editable\n        )\n    if wheel_path and verify:\n        try:\n            _verify_one(req, wheel_path)\n        except (InvalidWheelFilename, UnsupportedWheel) as e:\n            logger.warning(\"Built %s for %s is invalid: %s\", artifact, req.name, e)\n            return None\n    return wheel_path\n\n\ndef _build_one_inside_env(\n    req: InstallRequirement,\n    output_dir: str,\n    build_options: List[str],\n    global_options: List[str],\n    editable: bool,\n) -> Optional[str]:\n    with TempDirectory(kind=\"wheel\") as temp_dir:\n        assert req.name\n        if req.use_pep517:\n            assert req.metadata_directory\n            assert req.pep517_backend\n            if global_options:\n                logger.warning(\n                    \"Ignoring --global-option when building %s using PEP 517\", req.name\n                )\n            if build_options:\n                logger.warning(\n                    \"Ignoring --build-option when building %s using PEP 517\", req.name\n                )\n            if editable:\n                wheel_path = build_wheel_editable(\n                    name=req.name,\n                    backend=req.pep517_backend,\n                    metadata_directory=req.metadata_directory,\n                    tempd=temp_dir.path,\n                )\n            else:\n                wheel_path = build_wheel_pep517(\n                    name=req.name,\n                    backend=req.pep517_backend,\n                    metadata_directory=req.metadata_directory,\n                    tempd=temp_dir.path,\n                )\n        else:\n            wheel_path = build_wheel_legacy(\n                name=req.name,\n                setup_py_path=req.setup_py_path,\n                source_dir=req.unpacked_source_directory,\n                global_options=global_options,\n                build_options=build_options,\n                tempd=temp_dir.path,\n            )\n\n        if wheel_path is not None:\n            wheel_name = os.path.basename(wheel_path)\n            dest_path = os.path.join(output_dir, wheel_name)\n            try:\n                wheel_hash, length = hash_file(wheel_path)\n                shutil.move(wheel_path, dest_path)\n                logger.info(\n                    \"Created wheel for %s: filename=%s size=%d sha256=%s\",\n                    req.name,\n                    wheel_name,\n                    length,\n                    wheel_hash.hexdigest(),\n                )\n                logger.info(\"Stored in directory: %s\", output_dir)\n                return dest_path\n            except Exception as e:\n                logger.warning(\n                    \"Building wheel for %s failed: %s\",\n                    req.name,\n                    e,\n                )\n        # Ignore return, we can't do anything else useful.\n        if not req.use_pep517:\n            _clean_one_legacy(req, global_options)\n        return None\n\n\ndef _clean_one_legacy(req: InstallRequirement, global_options: List[str]) -> bool:\n    clean_args = make_setuptools_clean_args(\n        req.setup_py_path,\n        global_options=global_options,\n    )\n\n    logger.info(\"Running setup.py clean for %s\", req.name)\n    try:\n        call_subprocess(\n            clean_args, command_desc=\"python setup.py clean\", cwd=req.source_dir\n        )\n        return True\n    except Exception:\n        logger.error(\"Failed cleaning build dir for %s\", req.name)\n        return False\n\n\ndef build(\n    requirements: Iterable[InstallRequirement],\n    wheel_cache: WheelCache,\n    verify: bool,\n    build_options: List[str],\n    global_options: List[str],\n) -> BuildResult:\n    \"\"\"Build wheels.\n\n    :return: The list of InstallRequirement that succeeded to build and\n        the list of InstallRequirement that failed to build.\n    \"\"\"\n    if not requirements:\n        return [], []\n\n    # Build the wheels.\n    logger.info(\n        \"Building wheels for collected packages: %s\",\n        \", \".join(req.name for req in requirements),  # type: ignore\n    )\n\n    with indent_log():\n        build_successes, build_failures = [], []\n        for req in requirements:\n            assert req.name\n            cache_dir = _get_cache_dir(req, wheel_cache)\n            wheel_file = _build_one(\n                req,\n                cache_dir,\n                verify,\n                build_options,\n                global_options,\n                req.editable and req.permit_editable_wheels,\n            )\n            if wheel_file:\n                # Record the download origin in the cache\n                if req.download_info is not None:\n                    # download_info is guaranteed to be set because when we build an\n                    # InstallRequirement it has been through the preparer before, but\n                    # let's be cautious.\n                    wheel_cache.record_download_origin(cache_dir, req.download_info)\n                # Update the link for this.\n                req.link = Link(path_to_url(wheel_file))\n                req.local_file_path = req.link.file_path\n                assert req.link.is_wheel\n                build_successes.append(req)\n            else:\n                build_failures.append(req)\n\n    # notify success/failure\n    if build_successes:\n        logger.info(\n            \"Successfully built %s\",\n            \" \".join([req.name for req in build_successes]),  # type: ignore\n        )\n    if build_failures:\n        logger.info(\n            \"Failed to build %s\",\n            \" \".join([req.name for req in build_failures]),  # type: ignore\n        )\n    # Return a list of requirements that failed to build\n    return build_successes, build_failures\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_vendor/__init__.py","size":4873,"sha1":"d13b2a27d462159d11b364077c3b48492f3bfa78","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"\npip._vendor is for vendoring dependencies of pip to prevent needing pip to\ndepend on something external.\n\nFiles inside of pip._vendor should be considered immutable and should only be\nupdated to versions from upstream.\n\"\"\"\nfrom __future__ import absolute_import\n\nimport glob\nimport os.path\nimport sys\n\n# Downstream redistributors which have debundled our dependencies should also\n# patch this value to be true. This will trigger the additional patching\n# to cause things like \"six\" to be available as pip.\nDEBUNDLED = False\n\n# By default, look in this directory for a bunch of .whl files which we will\n# add to the beginning of sys.path before attempting to import anything. This\n# is done to support downstream re-distributors like Debian and Fedora who\n# wish to create their own Wheels for our dependencies to aid in debundling.\nWHEEL_DIR = os.path.abspath(os.path.dirname(__file__))\n\n\n# Define a small helper function to alias our vendored modules to the real ones\n# if the vendored ones do not exist. This idea of this was taken from\n# https://github.com/kennethreitz/requests/pull/2567.\ndef vendored(modulename):\n    vendored_name = \"{0}.{1}\".format(__name__, modulename)\n\n    try:\n        __import__(modulename, globals(), locals(), level=0)\n    except ImportError:\n        # We can just silently allow import failures to pass here. If we\n        # got to this point it means that ``import pip._vendor.whatever``\n        # failed and so did ``import whatever``. Since we're importing this\n        # upfront in an attempt to alias imports, not erroring here will\n        # just mean we get a regular import error whenever pip *actually*\n        # tries to import one of these modules to use it, which actually\n        # gives us a better error message than we would have otherwise\n        # gotten.\n        pass\n    else:\n        sys.modules[vendored_name] = sys.modules[modulename]\n        base, head = vendored_name.rsplit(\".\", 1)\n        setattr(sys.modules[base], head, sys.modules[modulename])\n\n\n# If we're operating in a debundled setup, then we want to go ahead and trigger\n# the aliasing of our vendored libraries as well as looking for wheels to add\n# to our sys.path. This will cause all of this code to be a no-op typically\n# however downstream redistributors can enable it in a consistent way across\n# all platforms.\nif DEBUNDLED:\n    # Actually look inside of WHEEL_DIR to find .whl files and add them to the\n    # front of our sys.path.\n    sys.path[:] = glob.glob(os.path.join(WHEEL_DIR, \"*.whl\")) + sys.path\n\n    # Actually alias all of our vendored dependencies.\n    vendored(\"cachecontrol\")\n    vendored(\"certifi\")\n    vendored(\"distlib\")\n    vendored(\"distro\")\n    vendored(\"packaging\")\n    vendored(\"packaging.version\")\n    vendored(\"packaging.specifiers\")\n    vendored(\"pkg_resources\")\n    vendored(\"platformdirs\")\n    vendored(\"progress\")\n    vendored(\"pyproject_hooks\")\n    vendored(\"requests\")\n    vendored(\"requests.exceptions\")\n    vendored(\"requests.packages\")\n    vendored(\"requests.packages.urllib3\")\n    vendored(\"requests.packages.urllib3._collections\")\n    vendored(\"requests.packages.urllib3.connection\")\n    vendored(\"requests.packages.urllib3.connectionpool\")\n    vendored(\"requests.packages.urllib3.contrib\")\n    vendored(\"requests.packages.urllib3.contrib.ntlmpool\")\n    vendored(\"requests.packages.urllib3.contrib.pyopenssl\")\n    vendored(\"requests.packages.urllib3.exceptions\")\n    vendored(\"requests.packages.urllib3.fields\")\n    vendored(\"requests.packages.urllib3.filepost\")\n    vendored(\"requests.packages.urllib3.packages\")\n    vendored(\"requests.packages.urllib3.packages.ordered_dict\")\n    vendored(\"requests.packages.urllib3.packages.six\")\n    vendored(\"requests.packages.urllib3.packages.ssl_match_hostname\")\n    vendored(\"requests.packages.urllib3.packages.ssl_match_hostname.\"\n             \"_implementation\")\n    vendored(\"requests.packages.urllib3.poolmanager\")\n    vendored(\"requests.packages.urllib3.request\")\n    vendored(\"requests.packages.urllib3.response\")\n    vendored(\"requests.packages.urllib3.util\")\n    vendored(\"requests.packages.urllib3.util.connection\")\n    vendored(\"requests.packages.urllib3.util.request\")\n    vendored(\"requests.packages.urllib3.util.response\")\n    vendored(\"requests.packages.urllib3.util.retry\")\n    vendored(\"requests.packages.urllib3.util.ssl_\")\n    vendored(\"requests.packages.urllib3.util.timeout\")\n    vendored(\"requests.packages.urllib3.util.url\")\n    vendored(\"resolvelib\")\n    vendored(\"rich\")\n    vendored(\"rich.console\")\n    vendored(\"rich.highlighter\")\n    vendored(\"rich.logging\")\n    vendored(\"rich.markup\")\n    vendored(\"rich.progress\")\n    vendored(\"rich.segment\")\n    vendored(\"rich.style\")\n    vendored(\"rich.text\")\n    vendored(\"rich.traceback\")\n    if sys.version_info < (3, 11):\n        vendored(\"tomli\")\n    vendored(\"truststore\")\n    vendored(\"urllib3\")\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_vendor/cachecontrol/__init__.py","size":677,"sha1":"a07bd43d049f98c298e23fbf6698d60d493c9b7e","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"# SPDX-FileCopyrightText: 2015 Eric Larson\n#\n# SPDX-License-Identifier: Apache-2.0\n\n\"\"\"CacheControl import Interface.\n\nMake it easy to import from cachecontrol without long namespaces.\n\"\"\"\n\n__author__ = \"Eric Larson\"\n__email__ = \"eric@ionrock.org\"\n__version__ = \"0.14.1\"\n\nfrom pip._vendor.cachecontrol.adapter import CacheControlAdapter\nfrom pip._vendor.cachecontrol.controller import CacheController\nfrom pip._vendor.cachecontrol.wrapper import CacheControl\n\n__all__ = [\n    \"__author__\",\n    \"__email__\",\n    \"__version__\",\n    \"CacheControlAdapter\",\n    \"CacheController\",\n    \"CacheControl\",\n]\n\nimport logging\n\nlogging.getLogger(__name__).addHandler(logging.NullHandler())\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_vendor/cachecontrol/_cmd.py","size":1737,"sha1":"f9f1c294b57d2432b802625d40549f21a22e3ce9","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"# SPDX-FileCopyrightText: 2015 Eric Larson\n#\n# SPDX-License-Identifier: Apache-2.0\nfrom __future__ import annotations\n\nimport logging\nfrom argparse import ArgumentParser\nfrom typing import TYPE_CHECKING\n\nfrom pip._vendor import requests\n\nfrom pip._vendor.cachecontrol.adapter import CacheControlAdapter\nfrom pip._vendor.cachecontrol.cache import DictCache\nfrom pip._vendor.cachecontrol.controller import logger\n\nif TYPE_CHECKING:\n    from argparse import Namespace\n\n    from pip._vendor.cachecontrol.controller import CacheController\n\n\ndef setup_logging() -> None:\n    logger.setLevel(logging.DEBUG)\n    handler = logging.StreamHandler()\n    logger.addHandler(handler)\n\n\ndef get_session() -> requests.Session:\n    adapter = CacheControlAdapter(\n        DictCache(), cache_etags=True, serializer=None, heuristic=None\n    )\n    sess = requests.Session()\n    sess.mount(\"http://\", adapter)\n    sess.mount(\"https://\", adapter)\n\n    sess.cache_controller = adapter.controller  # type: ignore[attr-defined]\n    return sess\n\n\ndef get_args() -> Namespace:\n    parser = ArgumentParser()\n    parser.add_argument(\"url\", help=\"The URL to try and cache\")\n    return parser.parse_args()\n\n\ndef main() -> None:\n    args = get_args()\n    sess = get_session()\n\n    # Make a request to get a response\n    resp = sess.get(args.url)\n\n    # Turn on logging\n    setup_logging()\n\n    # try setting the cache\n    cache_controller: CacheController = (\n        sess.cache_controller  # type: ignore[attr-defined]\n    )\n    cache_controller.cache_response(resp.request, resp.raw)\n\n    # Now try to get it\n    if cache_controller.cached_request(resp.request):\n        print(\"Cached!\")\n    else:\n        print(\"Not cached :(\")\n\n\nif __name__ == \"__main__\":\n    main()\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_vendor/cachecontrol/adapter.py","size":6348,"sha1":"cd1f3b2a360ade6674bc0db33d9f0baef02cdace","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"# SPDX-FileCopyrightText: 2015 Eric Larson\n#\n# SPDX-License-Identifier: Apache-2.0\nfrom __future__ import annotations\n\nimport functools\nimport types\nimport zlib\nfrom typing import TYPE_CHECKING, Any, Collection, Mapping\n\nfrom pip._vendor.requests.adapters import HTTPAdapter\n\nfrom pip._vendor.cachecontrol.cache import DictCache\nfrom pip._vendor.cachecontrol.controller import PERMANENT_REDIRECT_STATUSES, CacheController\nfrom pip._vendor.cachecontrol.filewrapper import CallbackFileWrapper\n\nif TYPE_CHECKING:\n    from pip._vendor.requests import PreparedRequest, Response\n    from pip._vendor.urllib3 import HTTPResponse\n\n    from pip._vendor.cachecontrol.cache import BaseCache\n    from pip._vendor.cachecontrol.heuristics import BaseHeuristic\n    from pip._vendor.cachecontrol.serialize import Serializer\n\n\nclass CacheControlAdapter(HTTPAdapter):\n    invalidating_methods = {\"PUT\", \"PATCH\", \"DELETE\"}\n\n    def __init__(\n        self,\n        cache: BaseCache | None = None,\n        cache_etags: bool = True,\n        controller_class: type[CacheController] | None = None,\n        serializer: Serializer | None = None,\n        heuristic: BaseHeuristic | None = None,\n        cacheable_methods: Collection[str] | None = None,\n        *args: Any,\n        **kw: Any,\n    ) -> None:\n        super().__init__(*args, **kw)\n        self.cache = DictCache() if cache is None else cache\n        self.heuristic = heuristic\n        self.cacheable_methods = cacheable_methods or (\"GET\",)\n\n        controller_factory = controller_class or CacheController\n        self.controller = controller_factory(\n            self.cache, cache_etags=cache_etags, serializer=serializer\n        )\n\n    def send(\n        self,\n        request: PreparedRequest,\n        stream: bool = False,\n        timeout: None | float | tuple[float, float] | tuple[float, None] = None,\n        verify: bool | str = True,\n        cert: (None | bytes | str | tuple[bytes | str, bytes | str]) = None,\n        proxies: Mapping[str, str] | None = None,\n        cacheable_methods: Collection[str] | None = None,\n    ) -> Response:\n        \"\"\"\n        Send a request. Use the request information to see if it\n        exists in the cache and cache the response if we need to and can.\n        \"\"\"\n        cacheable = cacheable_methods or self.cacheable_methods\n        if request.method in cacheable:\n            try:\n                cached_response = self.controller.cached_request(request)\n            except zlib.error:\n                cached_response = None\n            if cached_response:\n                return self.build_response(request, cached_response, from_cache=True)\n\n            # check for etags and add headers if appropriate\n            request.headers.update(self.controller.conditional_headers(request))\n\n        resp = super().send(request, stream, timeout, verify, cert, proxies)\n\n        return resp\n\n    def build_response(  # type: ignore[override]\n        self,\n        request: PreparedRequest,\n        response: HTTPResponse,\n        from_cache: bool = False,\n        cacheable_methods: Collection[str] | None = None,\n    ) -> Response:\n        \"\"\"\n        Build a response by making a request or using the cache.\n\n        This will end up calling send and returning a potentially\n        cached response\n        \"\"\"\n        cacheable = cacheable_methods or self.cacheable_methods\n        if not from_cache and request.method in cacheable:\n            # Check for any heuristics that might update headers\n            # before trying to cache.\n            if self.heuristic:\n                response = self.heuristic.apply(response)\n\n            # apply any expiration heuristics\n            if response.status == 304:\n                # We must have sent an ETag request. This could mean\n                # that we've been expired already or that we simply\n                # have an etag. In either case, we want to try and\n                # update the cache if that is the case.\n                cached_response = self.controller.update_cached_response(\n                    request, response\n                )\n\n                if cached_response is not response:\n                    from_cache = True\n\n                # We are done with the server response, read a\n                # possible response body (compliant servers will\n                # not return one, but we cannot be 100% sure) and\n                # release the connection back to the pool.\n                response.read(decode_content=False)\n                response.release_conn()\n\n                response = cached_response\n\n            # We always cache the 301 responses\n            elif int(response.status) in PERMANENT_REDIRECT_STATUSES:\n                self.controller.cache_response(request, response)\n            else:\n                # Wrap the response file with a wrapper that will cache the\n                #   response when the stream has been consumed.\n                response._fp = CallbackFileWrapper(  # type: ignore[assignment]\n                    response._fp,  # type: ignore[arg-type]\n                    functools.partial(\n                        self.controller.cache_response, request, response\n                    ),\n                )\n                if response.chunked:\n                    super_update_chunk_length = response._update_chunk_length\n\n                    def _update_chunk_length(self: HTTPResponse) -> None:\n                        super_update_chunk_length()\n                        if self.chunk_left == 0:\n                            self._fp._close()  # type: ignore[union-attr]\n\n                    response._update_chunk_length = types.MethodType(  # type: ignore[method-assign]\n                        _update_chunk_length, response\n                    )\n\n        resp: Response = super().build_response(request, response)\n\n        # See if we should invalidate the cache.\n        if request.method in self.invalidating_methods and resp.ok:\n            assert request.url is not None\n            cache_url = self.controller.cache_url(request.url)\n            self.cache.delete(cache_url)\n\n        # Give the request a from_cache attr to let people use it\n        resp.from_cache = from_cache  # type: ignore[attr-defined]\n\n        return resp\n\n    def close(self) -> None:\n        self.cache.close()\n        super().close()  # type: ignore[no-untyped-call]\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_vendor/cachecontrol/cache.py","size":1953,"sha1":"0fc135d6085998d99464e8086bfbd22906d2f52b","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"# SPDX-FileCopyrightText: 2015 Eric Larson\n#\n# SPDX-License-Identifier: Apache-2.0\n\n\"\"\"\nThe cache object API for implementing caches. The default is a thread\nsafe in-memory dictionary.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom threading import Lock\nfrom typing import IO, TYPE_CHECKING, MutableMapping\n\nif TYPE_CHECKING:\n    from datetime import datetime\n\n\nclass BaseCache:\n    def get(self, key: str) -> bytes | None:\n        raise NotImplementedError()\n\n    def set(\n        self, key: str, value: bytes, expires: int | datetime | None = None\n    ) -> None:\n        raise NotImplementedError()\n\n    def delete(self, key: str) -> None:\n        raise NotImplementedError()\n\n    def close(self) -> None:\n        pass\n\n\nclass DictCache(BaseCache):\n    def __init__(self, init_dict: MutableMapping[str, bytes] | None = None) -> None:\n        self.lock = Lock()\n        self.data = init_dict or {}\n\n    def get(self, key: str) -> bytes | None:\n        return self.data.get(key, None)\n\n    def set(\n        self, key: str, value: bytes, expires: int | datetime | None = None\n    ) -> None:\n        with self.lock:\n            self.data.update({key: value})\n\n    def delete(self, key: str) -> None:\n        with self.lock:\n            if key in self.data:\n                self.data.pop(key)\n\n\nclass SeparateBodyBaseCache(BaseCache):\n    \"\"\"\n    In this variant, the body is not stored mixed in with the metadata, but is\n    passed in (as a bytes-like object) in a separate call to ``set_body()``.\n\n    That is, the expected interaction pattern is::\n\n        cache.set(key, serialized_metadata)\n        cache.set_body(key)\n\n    Similarly, the body should be loaded separately via ``get_body()``.\n    \"\"\"\n\n    def set_body(self, key: str, body: bytes) -> None:\n        raise NotImplementedError()\n\n    def get_body(self, key: str) -> IO[bytes] | None:\n        \"\"\"\n        Return the body as file-like object.\n        \"\"\"\n        raise NotImplementedError()\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_vendor/cachecontrol/caches/__init__.py","size":303,"sha1":"1e2ecfdebcf43feb2056d6e6aa58d6cd5123c782","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"# SPDX-FileCopyrightText: 2015 Eric Larson\n#\n# SPDX-License-Identifier: Apache-2.0\n\nfrom pip._vendor.cachecontrol.caches.file_cache import FileCache, SeparateBodyFileCache\nfrom pip._vendor.cachecontrol.caches.redis_cache import RedisCache\n\n__all__ = [\"FileCache\", \"SeparateBodyFileCache\", \"RedisCache\"]\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_vendor/cachecontrol/caches/file_cache.py","size":5399,"sha1":"9e846e1b85a91ee94cb4ae4caec29ad25ef85dfb","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"# SPDX-FileCopyrightText: 2015 Eric Larson\n#\n# SPDX-License-Identifier: Apache-2.0\nfrom __future__ import annotations\n\nimport hashlib\nimport os\nfrom textwrap import dedent\nfrom typing import IO, TYPE_CHECKING\nfrom pathlib import Path\n\nfrom pip._vendor.cachecontrol.cache import BaseCache, SeparateBodyBaseCache\nfrom pip._vendor.cachecontrol.controller import CacheController\n\nif TYPE_CHECKING:\n    from datetime import datetime\n\n    from filelock import BaseFileLock\n\n\ndef _secure_open_write(filename: str, fmode: int) -> IO[bytes]:\n    # We only want to write to this file, so open it in write only mode\n    flags = os.O_WRONLY\n\n    # os.O_CREAT | os.O_EXCL will fail if the file already exists, so we only\n    #  will open *new* files.\n    # We specify this because we want to ensure that the mode we pass is the\n    # mode of the file.\n    flags |= os.O_CREAT | os.O_EXCL\n\n    # Do not follow symlinks to prevent someone from making a symlink that\n    # we follow and insecurely open a cache file.\n    if hasattr(os, \"O_NOFOLLOW\"):\n        flags |= os.O_NOFOLLOW\n\n    # On Windows we'll mark this file as binary\n    if hasattr(os, \"O_BINARY\"):\n        flags |= os.O_BINARY\n\n    # Before we open our file, we want to delete any existing file that is\n    # there\n    try:\n        os.remove(filename)\n    except OSError:\n        # The file must not exist already, so we can just skip ahead to opening\n        pass\n\n    # Open our file, the use of os.O_CREAT | os.O_EXCL will ensure that if a\n    # race condition happens between the os.remove and this line, that an\n    # error will be raised. Because we utilize a lockfile this should only\n    # happen if someone is attempting to attack us.\n    fd = os.open(filename, flags, fmode)\n    try:\n        return os.fdopen(fd, \"wb\")\n\n    except:\n        # An error occurred wrapping our FD in a file object\n        os.close(fd)\n        raise\n\n\nclass _FileCacheMixin:\n    \"\"\"Shared implementation for both FileCache variants.\"\"\"\n\n    def __init__(\n        self,\n        directory: str | Path,\n        forever: bool = False,\n        filemode: int = 0o0600,\n        dirmode: int = 0o0700,\n        lock_class: type[BaseFileLock] | None = None,\n    ) -> None:\n        try:\n            if lock_class is None:\n                from filelock import FileLock\n\n                lock_class = FileLock\n        except ImportError:\n            notice = dedent(\n                \"\"\"\n            NOTE: In order to use the FileCache you must have\n            filelock installed. You can install it via pip:\n              pip install cachecontrol[filecache]\n            \"\"\"\n            )\n            raise ImportError(notice)\n\n        self.directory = directory\n        self.forever = forever\n        self.filemode = filemode\n        self.dirmode = dirmode\n        self.lock_class = lock_class\n\n    @staticmethod\n    def encode(x: str) -> str:\n        return hashlib.sha224(x.encode()).hexdigest()\n\n    def _fn(self, name: str) -> str:\n        # NOTE: This method should not change as some may depend on it.\n        #       See: https://github.com/ionrock/cachecontrol/issues/63\n        hashed = self.encode(name)\n        parts = list(hashed[:5]) + [hashed]\n        return os.path.join(self.directory, *parts)\n\n    def get(self, key: str) -> bytes | None:\n        name = self._fn(key)\n        try:\n            with open(name, \"rb\") as fh:\n                return fh.read()\n\n        except FileNotFoundError:\n            return None\n\n    def set(\n        self, key: str, value: bytes, expires: int | datetime | None = None\n    ) -> None:\n        name = self._fn(key)\n        self._write(name, value)\n\n    def _write(self, path: str, data: bytes) -> None:\n        \"\"\"\n        Safely write the data to the given path.\n        \"\"\"\n        # Make sure the directory exists\n        try:\n            os.makedirs(os.path.dirname(path), self.dirmode)\n        except OSError:\n            pass\n\n        with self.lock_class(path + \".lock\"):\n            # Write our actual file\n            with _secure_open_write(path, self.filemode) as fh:\n                fh.write(data)\n\n    def _delete(self, key: str, suffix: str) -> None:\n        name = self._fn(key) + suffix\n        if not self.forever:\n            try:\n                os.remove(name)\n            except FileNotFoundError:\n                pass\n\n\nclass FileCache(_FileCacheMixin, BaseCache):\n    \"\"\"\n    Traditional FileCache: body is stored in memory, so not suitable for large\n    downloads.\n    \"\"\"\n\n    def delete(self, key: str) -> None:\n        self._delete(key, \"\")\n\n\nclass SeparateBodyFileCache(_FileCacheMixin, SeparateBodyBaseCache):\n    \"\"\"\n    Memory-efficient FileCache: body is stored in a separate file, reducing\n    peak memory usage.\n    \"\"\"\n\n    def get_body(self, key: str) -> IO[bytes] | None:\n        name = self._fn(key) + \".body\"\n        try:\n            return open(name, \"rb\")\n        except FileNotFoundError:\n            return None\n\n    def set_body(self, key: str, body: bytes) -> None:\n        name = self._fn(key) + \".body\"\n        self._write(name, body)\n\n    def delete(self, key: str) -> None:\n        self._delete(key, \"\")\n        self._delete(key, \".body\")\n\n\ndef url_to_file_path(url: str, filecache: FileCache) -> str:\n    \"\"\"Return the file cache path based on the URL.\n\n    This does not ensure the file exists!\n    \"\"\"\n    key = CacheController.cache_url(url)\n    return filecache._fn(key)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_vendor/cachecontrol/caches/redis_cache.py","size":1386,"sha1":"4049dd3d0f66a44f5ff886d4177dfc9ccb83dc03","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"# SPDX-FileCopyrightText: 2015 Eric Larson\n#\n# SPDX-License-Identifier: Apache-2.0\nfrom __future__ import annotations\n\n\nfrom datetime import datetime, timezone\nfrom typing import TYPE_CHECKING\n\nfrom pip._vendor.cachecontrol.cache import BaseCache\n\nif TYPE_CHECKING:\n    from redis import Redis\n\n\nclass RedisCache(BaseCache):\n    def __init__(self, conn: Redis[bytes]) -> None:\n        self.conn = conn\n\n    def get(self, key: str) -> bytes | None:\n        return self.conn.get(key)\n\n    def set(\n        self, key: str, value: bytes, expires: int | datetime | None = None\n    ) -> None:\n        if not expires:\n            self.conn.set(key, value)\n        elif isinstance(expires, datetime):\n            now_utc = datetime.now(timezone.utc)\n            if expires.tzinfo is None:\n                now_utc = now_utc.replace(tzinfo=None)\n            delta = expires - now_utc\n            self.conn.setex(key, int(delta.total_seconds()), value)\n        else:\n            self.conn.setex(key, expires, value)\n\n    def delete(self, key: str) -> None:\n        self.conn.delete(key)\n\n    def clear(self) -> None:\n        \"\"\"Helper for clearing all the keys in a database. Use with\n        caution!\"\"\"\n        for key in self.conn.keys():\n            self.conn.delete(key)\n\n    def close(self) -> None:\n        \"\"\"Redis uses connection pooling, no need to close the connection.\"\"\"\n        pass\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_vendor/cachecontrol/controller.py","size":18576,"sha1":"a00c009d4fd2dcb72ec84233a8485e4b9ca0925d","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"# SPDX-FileCopyrightText: 2015 Eric Larson\n#\n# SPDX-License-Identifier: Apache-2.0\n\n\"\"\"\nThe httplib2 algorithms ported for use with requests.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport calendar\nimport logging\nimport re\nimport time\nfrom email.utils import parsedate_tz\nfrom typing import TYPE_CHECKING, Collection, Mapping\n\nfrom pip._vendor.requests.structures import CaseInsensitiveDict\n\nfrom pip._vendor.cachecontrol.cache import DictCache, SeparateBodyBaseCache\nfrom pip._vendor.cachecontrol.serialize import Serializer\n\nif TYPE_CHECKING:\n    from typing import Literal\n\n    from pip._vendor.requests import PreparedRequest\n    from pip._vendor.urllib3 import HTTPResponse\n\n    from pip._vendor.cachecontrol.cache import BaseCache\n\nlogger = logging.getLogger(__name__)\n\nURI = re.compile(r\"^(([^:/?#]+):)?(//([^/?#]*))?([^?#]*)(\\?([^#]*))?(#(.*))?\")\n\nPERMANENT_REDIRECT_STATUSES = (301, 308)\n\n\ndef parse_uri(uri: str) -> tuple[str, str, str, str, str]:\n    \"\"\"Parses a URI using the regex given in Appendix B of RFC 3986.\n\n    (scheme, authority, path, query, fragment) = parse_uri(uri)\n    \"\"\"\n    match = URI.match(uri)\n    assert match is not None\n    groups = match.groups()\n    return (groups[1], groups[3], groups[4], groups[6], groups[8])\n\n\nclass CacheController:\n    \"\"\"An interface to see if request should cached or not.\"\"\"\n\n    def __init__(\n        self,\n        cache: BaseCache | None = None,\n        cache_etags: bool = True,\n        serializer: Serializer | None = None,\n        status_codes: Collection[int] | None = None,\n    ):\n        self.cache = DictCache() if cache is None else cache\n        self.cache_etags = cache_etags\n        self.serializer = serializer or Serializer()\n        self.cacheable_status_codes = status_codes or (200, 203, 300, 301, 308)\n\n    @classmethod\n    def _urlnorm(cls, uri: str) -> str:\n        \"\"\"Normalize the URL to create a safe key for the cache\"\"\"\n        (scheme, authority, path, query, fragment) = parse_uri(uri)\n        if not scheme or not authority:\n            raise Exception(\"Only absolute URIs are allowed. uri = %s\" % uri)\n\n        scheme = scheme.lower()\n        authority = authority.lower()\n\n        if not path:\n            path = \"/\"\n\n        # Could do syntax based normalization of the URI before\n        # computing the digest. See Section 6.2.2 of Std 66.\n        request_uri = query and \"?\".join([path, query]) or path\n        defrag_uri = scheme + \"://\" + authority + request_uri\n\n        return defrag_uri\n\n    @classmethod\n    def cache_url(cls, uri: str) -> str:\n        return cls._urlnorm(uri)\n\n    def parse_cache_control(self, headers: Mapping[str, str]) -> dict[str, int | None]:\n        known_directives = {\n            # https://tools.ietf.org/html/rfc7234#section-5.2\n            \"max-age\": (int, True),\n            \"max-stale\": (int, False),\n            \"min-fresh\": (int, True),\n            \"no-cache\": (None, False),\n            \"no-store\": (None, False),\n            \"no-transform\": (None, False),\n            \"only-if-cached\": (None, False),\n            \"must-revalidate\": (None, False),\n            \"public\": (None, False),\n            \"private\": (None, False),\n            \"proxy-revalidate\": (None, False),\n            \"s-maxage\": (int, True),\n        }\n\n        cc_headers = headers.get(\"cache-control\", headers.get(\"Cache-Control\", \"\"))\n\n        retval: dict[str, int | None] = {}\n\n        for cc_directive in cc_headers.split(\",\"):\n            if not cc_directive.strip():\n                continue\n\n            parts = cc_directive.split(\"=\", 1)\n            directive = parts[0].strip()\n\n            try:\n                typ, required = known_directives[directive]\n            except KeyError:\n                logger.debug(\"Ignoring unknown cache-control directive: %s\", directive)\n                continue\n\n            if not typ or not required:\n                retval[directive] = None\n            if typ:\n                try:\n                    retval[directive] = typ(parts[1].strip())\n                except IndexError:\n                    if required:\n                        logger.debug(\n                            \"Missing value for cache-control \" \"directive: %s\",\n                            directive,\n                        )\n                except ValueError:\n                    logger.debug(\n                        \"Invalid value for cache-control directive \" \"%s, must be %s\",\n                        directive,\n                        typ.__name__,\n                    )\n\n        return retval\n\n    def _load_from_cache(self, request: PreparedRequest) -> HTTPResponse | None:\n        \"\"\"\n        Load a cached response, or return None if it's not available.\n        \"\"\"\n        # We do not support caching of partial content: so if the request contains a\n        # Range header then we don't want to load anything from the cache.\n        if \"Range\" in request.headers:\n            return None\n\n        cache_url = request.url\n        assert cache_url is not None\n        cache_data = self.cache.get(cache_url)\n        if cache_data is None:\n            logger.debug(\"No cache entry available\")\n            return None\n\n        if isinstance(self.cache, SeparateBodyBaseCache):\n            body_file = self.cache.get_body(cache_url)\n        else:\n            body_file = None\n\n        result = self.serializer.loads(request, cache_data, body_file)\n        if result is None:\n            logger.warning(\"Cache entry deserialization failed, entry ignored\")\n        return result\n\n    def cached_request(self, request: PreparedRequest) -> HTTPResponse | Literal[False]:\n        \"\"\"\n        Return a cached response if it exists in the cache, otherwise\n        return False.\n        \"\"\"\n        assert request.url is not None\n        cache_url = self.cache_url(request.url)\n        logger.debug('Looking up \"%s\" in the cache', cache_url)\n        cc = self.parse_cache_control(request.headers)\n\n        # Bail out if the request insists on fresh data\n        if \"no-cache\" in cc:\n            logger.debug('Request header has \"no-cache\", cache bypassed')\n            return False\n\n        if \"max-age\" in cc and cc[\"max-age\"] == 0:\n            logger.debug('Request header has \"max_age\" as 0, cache bypassed')\n            return False\n\n        # Check whether we can load the response from the cache:\n        resp = self._load_from_cache(request)\n        if not resp:\n            return False\n\n        # If we have a cached permanent redirect, return it immediately. We\n        # don't need to test our response for other headers b/c it is\n        # intrinsically \"cacheable\" as it is Permanent.\n        #\n        # See:\n        #   https://tools.ietf.org/html/rfc7231#section-6.4.2\n        #\n        # Client can try to refresh the value by repeating the request\n        # with cache busting headers as usual (ie no-cache).\n        if int(resp.status) in PERMANENT_REDIRECT_STATUSES:\n            msg = (\n                \"Returning cached permanent redirect response \"\n                \"(ignoring date and etag information)\"\n            )\n            logger.debug(msg)\n            return resp\n\n        headers: CaseInsensitiveDict[str] = CaseInsensitiveDict(resp.headers)\n        if not headers or \"date\" not in headers:\n            if \"etag\" not in headers:\n                # Without date or etag, the cached response can never be used\n                # and should be deleted.\n                logger.debug(\"Purging cached response: no date or etag\")\n                self.cache.delete(cache_url)\n            logger.debug(\"Ignoring cached response: no date\")\n            return False\n\n        now = time.time()\n        time_tuple = parsedate_tz(headers[\"date\"])\n        assert time_tuple is not None\n        date = calendar.timegm(time_tuple[:6])\n        current_age = max(0, now - date)\n        logger.debug(\"Current age based on date: %i\", current_age)\n\n        # TODO: There is an assumption that the result will be a\n        #       urllib3 response object. This may not be best since we\n        #       could probably avoid instantiating or constructing the\n        #       response until we know we need it.\n        resp_cc = self.parse_cache_control(headers)\n\n        # determine freshness\n        freshness_lifetime = 0\n\n        # Check the max-age pragma in the cache control header\n        max_age = resp_cc.get(\"max-age\")\n        if max_age is not None:\n            freshness_lifetime = max_age\n            logger.debug(\"Freshness lifetime from max-age: %i\", freshness_lifetime)\n\n        # If there isn't a max-age, check for an expires header\n        elif \"expires\" in headers:\n            expires = parsedate_tz(headers[\"expires\"])\n            if expires is not None:\n                expire_time = calendar.timegm(expires[:6]) - date\n                freshness_lifetime = max(0, expire_time)\n                logger.debug(\"Freshness lifetime from expires: %i\", freshness_lifetime)\n\n        # Determine if we are setting freshness limit in the\n        # request. Note, this overrides what was in the response.\n        max_age = cc.get(\"max-age\")\n        if max_age is not None:\n            freshness_lifetime = max_age\n            logger.debug(\n                \"Freshness lifetime from request max-age: %i\", freshness_lifetime\n            )\n\n        min_fresh = cc.get(\"min-fresh\")\n        if min_fresh is not None:\n            # adjust our current age by our min fresh\n            current_age += min_fresh\n            logger.debug(\"Adjusted current age from min-fresh: %i\", current_age)\n\n        # Return entry if it is fresh enough\n        if freshness_lifetime > current_age:\n            logger.debug('The response is \"fresh\", returning cached response')\n            logger.debug(\"%i > %i\", freshness_lifetime, current_age)\n            return resp\n\n        # we're not fresh. If we don't have an Etag, clear it out\n        if \"etag\" not in headers:\n            logger.debug('The cached response is \"stale\" with no etag, purging')\n            self.cache.delete(cache_url)\n\n        # return the original handler\n        return False\n\n    def conditional_headers(self, request: PreparedRequest) -> dict[str, str]:\n        resp = self._load_from_cache(request)\n        new_headers = {}\n\n        if resp:\n            headers: CaseInsensitiveDict[str] = CaseInsensitiveDict(resp.headers)\n\n            if \"etag\" in headers:\n                new_headers[\"If-None-Match\"] = headers[\"ETag\"]\n\n            if \"last-modified\" in headers:\n                new_headers[\"If-Modified-Since\"] = headers[\"Last-Modified\"]\n\n        return new_headers\n\n    def _cache_set(\n        self,\n        cache_url: str,\n        request: PreparedRequest,\n        response: HTTPResponse,\n        body: bytes | None = None,\n        expires_time: int | None = None,\n    ) -> None:\n        \"\"\"\n        Store the data in the cache.\n        \"\"\"\n        if isinstance(self.cache, SeparateBodyBaseCache):\n            # We pass in the body separately; just put a placeholder empty\n            # string in the metadata.\n            self.cache.set(\n                cache_url,\n                self.serializer.dumps(request, response, b\"\"),\n                expires=expires_time,\n            )\n            # body is None can happen when, for example, we're only updating\n            # headers, as is the case in update_cached_response().\n            if body is not None:\n                self.cache.set_body(cache_url, body)\n        else:\n            self.cache.set(\n                cache_url,\n                self.serializer.dumps(request, response, body),\n                expires=expires_time,\n            )\n\n    def cache_response(\n        self,\n        request: PreparedRequest,\n        response: HTTPResponse,\n        body: bytes | None = None,\n        status_codes: Collection[int] | None = None,\n    ) -> None:\n        \"\"\"\n        Algorithm for caching requests.\n\n        This assumes a requests Response object.\n        \"\"\"\n        # From httplib2: Don't cache 206's since we aren't going to\n        #                handle byte range requests\n        cacheable_status_codes = status_codes or self.cacheable_status_codes\n        if response.status not in cacheable_status_codes:\n            logger.debug(\n                \"Status code %s not in %s\", response.status, cacheable_status_codes\n            )\n            return\n\n        response_headers: CaseInsensitiveDict[str] = CaseInsensitiveDict(\n            response.headers\n        )\n\n        if \"date\" in response_headers:\n            time_tuple = parsedate_tz(response_headers[\"date\"])\n            assert time_tuple is not None\n            date = calendar.timegm(time_tuple[:6])\n        else:\n            date = 0\n\n        # If we've been given a body, our response has a Content-Length, that\n        # Content-Length is valid then we can check to see if the body we've\n        # been given matches the expected size, and if it doesn't we'll just\n        # skip trying to cache it.\n        if (\n            body is not None\n            and \"content-length\" in response_headers\n            and response_headers[\"content-length\"].isdigit()\n            and int(response_headers[\"content-length\"]) != len(body)\n        ):\n            return\n\n        cc_req = self.parse_cache_control(request.headers)\n        cc = self.parse_cache_control(response_headers)\n\n        assert request.url is not None\n        cache_url = self.cache_url(request.url)\n        logger.debug('Updating cache with response from \"%s\"', cache_url)\n\n        # Delete it from the cache if we happen to have it stored there\n        no_store = False\n        if \"no-store\" in cc:\n            no_store = True\n            logger.debug('Response header has \"no-store\"')\n        if \"no-store\" in cc_req:\n            no_store = True\n            logger.debug('Request header has \"no-store\"')\n        if no_store and self.cache.get(cache_url):\n            logger.debug('Purging existing cache entry to honor \"no-store\"')\n            self.cache.delete(cache_url)\n        if no_store:\n            return\n\n        # https://tools.ietf.org/html/rfc7234#section-4.1:\n        # A Vary header field-value of \"*\" always fails to match.\n        # Storing such a response leads to a deserialization warning\n        # during cache lookup and is not allowed to ever be served,\n        # so storing it can be avoided.\n        if \"*\" in response_headers.get(\"vary\", \"\"):\n            logger.debug('Response header has \"Vary: *\"')\n            return\n\n        # If we've been given an etag, then keep the response\n        if self.cache_etags and \"etag\" in response_headers:\n            expires_time = 0\n            if response_headers.get(\"expires\"):\n                expires = parsedate_tz(response_headers[\"expires\"])\n                if expires is not None:\n                    expires_time = calendar.timegm(expires[:6]) - date\n\n            expires_time = max(expires_time, 14 * 86400)\n\n            logger.debug(f\"etag object cached for {expires_time} seconds\")\n            logger.debug(\"Caching due to etag\")\n            self._cache_set(cache_url, request, response, body, expires_time)\n\n        # Add to the cache any permanent redirects. We do this before looking\n        # that the Date headers.\n        elif int(response.status) in PERMANENT_REDIRECT_STATUSES:\n            logger.debug(\"Caching permanent redirect\")\n            self._cache_set(cache_url, request, response, b\"\")\n\n        # Add to the cache if the response headers demand it. If there\n        # is no date header then we can't do anything about expiring\n        # the cache.\n        elif \"date\" in response_headers:\n            time_tuple = parsedate_tz(response_headers[\"date\"])\n            assert time_tuple is not None\n            date = calendar.timegm(time_tuple[:6])\n            # cache when there is a max-age > 0\n            max_age = cc.get(\"max-age\")\n            if max_age is not None and max_age > 0:\n                logger.debug(\"Caching b/c date exists and max-age > 0\")\n                expires_time = max_age\n                self._cache_set(\n                    cache_url,\n                    request,\n                    response,\n                    body,\n                    expires_time,\n                )\n\n            # If the request can expire, it means we should cache it\n            # in the meantime.\n            elif \"expires\" in response_headers:\n                if response_headers[\"expires\"]:\n                    expires = parsedate_tz(response_headers[\"expires\"])\n                    if expires is not None:\n                        expires_time = calendar.timegm(expires[:6]) - date\n                    else:\n                        expires_time = None\n\n                    logger.debug(\n                        \"Caching b/c of expires header. expires in {} seconds\".format(\n                            expires_time\n                        )\n                    )\n                    self._cache_set(\n                        cache_url,\n                        request,\n                        response,\n                        body,\n                        expires_time,\n                    )\n\n    def update_cached_response(\n        self, request: PreparedRequest, response: HTTPResponse\n    ) -> HTTPResponse:\n        \"\"\"On a 304 we will get a new set of headers that we want to\n        update our cached value with, assuming we have one.\n\n        This should only ever be called when we've sent an ETag and\n        gotten a 304 as the response.\n        \"\"\"\n        assert request.url is not None\n        cache_url = self.cache_url(request.url)\n        cached_response = self._load_from_cache(request)\n\n        if not cached_response:\n            # we didn't have a cached response\n            return response\n\n        # Lets update our headers with the headers from the new request:\n        # http://tools.ietf.org/html/draft-ietf-httpbis-p4-conditional-26#section-4.1\n        #\n        # The server isn't supposed to send headers that would make\n        # the cached body invalid. But... just in case, we'll be sure\n        # to strip out ones we know that might be problmatic due to\n        # typical assumptions.\n        excluded_headers = [\"content-length\"]\n\n        cached_response.headers.update(\n            {\n                k: v\n                for k, v in response.headers.items()\n                if k.lower() not in excluded_headers\n            }\n        )\n\n        # we want a 200 b/c we have content via the cache\n        cached_response.status = 200\n\n        # update our cache\n        self._cache_set(cache_url, request, cached_response)\n\n        return cached_response\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_vendor/cachecontrol/filewrapper.py","size":4291,"sha1":"ba704c17823c56ef9a2fcb9decb055cd16d27fe9","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"# SPDX-FileCopyrightText: 2015 Eric Larson\n#\n# SPDX-License-Identifier: Apache-2.0\nfrom __future__ import annotations\n\nimport mmap\nfrom tempfile import NamedTemporaryFile\nfrom typing import TYPE_CHECKING, Any, Callable\n\nif TYPE_CHECKING:\n    from http.client import HTTPResponse\n\n\nclass CallbackFileWrapper:\n    \"\"\"\n    Small wrapper around a fp object which will tee everything read into a\n    buffer, and when that file is closed it will execute a callback with the\n    contents of that buffer.\n\n    All attributes are proxied to the underlying file object.\n\n    This class uses members with a double underscore (__) leading prefix so as\n    not to accidentally shadow an attribute.\n\n    The data is stored in a temporary file until it is all available.  As long\n    as the temporary files directory is disk-based (sometimes it's a\n    memory-backed-``tmpfs`` on Linux), data will be unloaded to disk if memory\n    pressure is high.  For small files the disk usually won't be used at all,\n    it'll all be in the filesystem memory cache, so there should be no\n    performance impact.\n    \"\"\"\n\n    def __init__(\n        self, fp: HTTPResponse, callback: Callable[[bytes], None] | None\n    ) -> None:\n        self.__buf = NamedTemporaryFile(\"rb+\", delete=True)\n        self.__fp = fp\n        self.__callback = callback\n\n    def __getattr__(self, name: str) -> Any:\n        # The vagaries of garbage collection means that self.__fp is\n        # not always set.  By using __getattribute__ and the private\n        # name[0] allows looking up the attribute value and raising an\n        # AttributeError when it doesn't exist. This stop things from\n        # infinitely recursing calls to getattr in the case where\n        # self.__fp hasn't been set.\n        #\n        # [0] https://docs.python.org/2/reference/expressions.html#atom-identifiers\n        fp = self.__getattribute__(\"_CallbackFileWrapper__fp\")\n        return getattr(fp, name)\n\n    def __is_fp_closed(self) -> bool:\n        try:\n            return self.__fp.fp is None\n\n        except AttributeError:\n            pass\n\n        try:\n            closed: bool = self.__fp.closed\n            return closed\n\n        except AttributeError:\n            pass\n\n        # We just don't cache it then.\n        # TODO: Add some logging here...\n        return False\n\n    def _close(self) -> None:\n        if self.__callback:\n            if self.__buf.tell() == 0:\n                # Empty file:\n                result = b\"\"\n            else:\n                # Return the data without actually loading it into memory,\n                # relying on Python's buffer API and mmap(). mmap() just gives\n                # a view directly into the filesystem's memory cache, so it\n                # doesn't result in duplicate memory use.\n                self.__buf.seek(0, 0)\n                result = memoryview(\n                    mmap.mmap(self.__buf.fileno(), 0, access=mmap.ACCESS_READ)\n                )\n            self.__callback(result)\n\n        # We assign this to None here, because otherwise we can get into\n        # really tricky problems where the CPython interpreter dead locks\n        # because the callback is holding a reference to something which\n        # has a __del__ method. Setting this to None breaks the cycle\n        # and allows the garbage collector to do it's thing normally.\n        self.__callback = None\n\n        # Closing the temporary file releases memory and frees disk space.\n        # Important when caching big files.\n        self.__buf.close()\n\n    def read(self, amt: int | None = None) -> bytes:\n        data: bytes = self.__fp.read(amt)\n        if data:\n            # We may be dealing with b'', a sign that things are over:\n            # it's passed e.g. after we've already closed self.__buf.\n            self.__buf.write(data)\n        if self.__is_fp_closed():\n            self._close()\n\n        return data\n\n    def _safe_read(self, amt: int) -> bytes:\n        data: bytes = self.__fp._safe_read(amt)  # type: ignore[attr-defined]\n        if amt == 2 and data == b\"\\r\\n\":\n            # urllib executes this read to toss the CRLF at the end\n            # of the chunk.\n            return data\n\n        self.__buf.write(data)\n        if self.__is_fp_closed():\n            self._close()\n\n        return data\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_vendor/cachecontrol/heuristics.py","size":4881,"sha1":"4af5a786e9ccb4a19f9d1cccaaa09ea78ecf6ff2","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"# SPDX-FileCopyrightText: 2015 Eric Larson\n#\n# SPDX-License-Identifier: Apache-2.0\nfrom __future__ import annotations\n\nimport calendar\nimport time\nfrom datetime import datetime, timedelta, timezone\nfrom email.utils import formatdate, parsedate, parsedate_tz\nfrom typing import TYPE_CHECKING, Any, Mapping\n\nif TYPE_CHECKING:\n    from pip._vendor.urllib3 import HTTPResponse\n\nTIME_FMT = \"%a, %d %b %Y %H:%M:%S GMT\"\n\n\ndef expire_after(delta: timedelta, date: datetime | None = None) -> datetime:\n    date = date or datetime.now(timezone.utc)\n    return date + delta\n\n\ndef datetime_to_header(dt: datetime) -> str:\n    return formatdate(calendar.timegm(dt.timetuple()))\n\n\nclass BaseHeuristic:\n    def warning(self, response: HTTPResponse) -> str | None:\n        \"\"\"\n        Return a valid 1xx warning header value describing the cache\n        adjustments.\n\n        The response is provided too allow warnings like 113\n        http://tools.ietf.org/html/rfc7234#section-5.5.4 where we need\n        to explicitly say response is over 24 hours old.\n        \"\"\"\n        return '110 - \"Response is Stale\"'\n\n    def update_headers(self, response: HTTPResponse) -> dict[str, str]:\n        \"\"\"Update the response headers with any new headers.\n\n        NOTE: This SHOULD always include some Warning header to\n              signify that the response was cached by the client, not\n              by way of the provided headers.\n        \"\"\"\n        return {}\n\n    def apply(self, response: HTTPResponse) -> HTTPResponse:\n        updated_headers = self.update_headers(response)\n\n        if updated_headers:\n            response.headers.update(updated_headers)\n            warning_header_value = self.warning(response)\n            if warning_header_value is not None:\n                response.headers.update({\"Warning\": warning_header_value})\n\n        return response\n\n\nclass OneDayCache(BaseHeuristic):\n    \"\"\"\n    Cache the response by providing an expires 1 day in the\n    future.\n    \"\"\"\n\n    def update_headers(self, response: HTTPResponse) -> dict[str, str]:\n        headers = {}\n\n        if \"expires\" not in response.headers:\n            date = parsedate(response.headers[\"date\"])\n            expires = expire_after(\n                timedelta(days=1),\n                date=datetime(*date[:6], tzinfo=timezone.utc),  # type: ignore[index,misc]\n            )\n            headers[\"expires\"] = datetime_to_header(expires)\n            headers[\"cache-control\"] = \"public\"\n        return headers\n\n\nclass ExpiresAfter(BaseHeuristic):\n    \"\"\"\n    Cache **all** requests for a defined time period.\n    \"\"\"\n\n    def __init__(self, **kw: Any) -> None:\n        self.delta = timedelta(**kw)\n\n    def update_headers(self, response: HTTPResponse) -> dict[str, str]:\n        expires = expire_after(self.delta)\n        return {\"expires\": datetime_to_header(expires), \"cache-control\": \"public\"}\n\n    def warning(self, response: HTTPResponse) -> str | None:\n        tmpl = \"110 - Automatically cached for %s. Response might be stale\"\n        return tmpl % self.delta\n\n\nclass LastModified(BaseHeuristic):\n    \"\"\"\n    If there is no Expires header already, fall back on Last-Modified\n    using the heuristic from\n    http://tools.ietf.org/html/rfc7234#section-4.2.2\n    to calculate a reasonable value.\n\n    Firefox also does something like this per\n    https://developer.mozilla.org/en-US/docs/Web/HTTP/Caching_FAQ\n    http://lxr.mozilla.org/mozilla-release/source/netwerk/protocol/http/nsHttpResponseHead.cpp#397\n    Unlike mozilla we limit this to 24-hr.\n    \"\"\"\n\n    cacheable_by_default_statuses = {\n        200,\n        203,\n        204,\n        206,\n        300,\n        301,\n        404,\n        405,\n        410,\n        414,\n        501,\n    }\n\n    def update_headers(self, resp: HTTPResponse) -> dict[str, str]:\n        headers: Mapping[str, str] = resp.headers\n\n        if \"expires\" in headers:\n            return {}\n\n        if \"cache-control\" in headers and headers[\"cache-control\"] != \"public\":\n            return {}\n\n        if resp.status not in self.cacheable_by_default_statuses:\n            return {}\n\n        if \"date\" not in headers or \"last-modified\" not in headers:\n            return {}\n\n        time_tuple = parsedate_tz(headers[\"date\"])\n        assert time_tuple is not None\n        date = calendar.timegm(time_tuple[:6])\n        last_modified = parsedate(headers[\"last-modified\"])\n        if last_modified is None:\n            return {}\n\n        now = time.time()\n        current_age = max(0, now - date)\n        delta = date - calendar.timegm(last_modified)\n        freshness_lifetime = max(0, min(delta / 10, 24 * 3600))\n        if freshness_lifetime <= current_age:\n            return {}\n\n        expires = date + freshness_lifetime\n        return {\"expires\": time.strftime(TIME_FMT, time.gmtime(expires))}\n\n    def warning(self, resp: HTTPResponse) -> str | None:\n        return None\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_vendor/cachecontrol/serialize.py","size":5163,"sha1":"05f19d4dd2fb8b16c1228561b74e76ff38d3b723","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"# SPDX-FileCopyrightText: 2015 Eric Larson\n#\n# SPDX-License-Identifier: Apache-2.0\nfrom __future__ import annotations\n\nimport io\nfrom typing import IO, TYPE_CHECKING, Any, Mapping, cast\n\nfrom pip._vendor import msgpack\nfrom pip._vendor.requests.structures import CaseInsensitiveDict\nfrom pip._vendor.urllib3 import HTTPResponse\n\nif TYPE_CHECKING:\n    from pip._vendor.requests import PreparedRequest\n\n\nclass Serializer:\n    serde_version = \"4\"\n\n    def dumps(\n        self,\n        request: PreparedRequest,\n        response: HTTPResponse,\n        body: bytes | None = None,\n    ) -> bytes:\n        response_headers: CaseInsensitiveDict[str] = CaseInsensitiveDict(\n            response.headers\n        )\n\n        if body is None:\n            # When a body isn't passed in, we'll read the response. We\n            # also update the response with a new file handler to be\n            # sure it acts as though it was never read.\n            body = response.read(decode_content=False)\n            response._fp = io.BytesIO(body)  # type: ignore[assignment]\n            response.length_remaining = len(body)\n\n        data = {\n            \"response\": {\n                \"body\": body,  # Empty bytestring if body is stored separately\n                \"headers\": {str(k): str(v) for k, v in response.headers.items()},\n                \"status\": response.status,\n                \"version\": response.version,\n                \"reason\": str(response.reason),\n                \"decode_content\": response.decode_content,\n            }\n        }\n\n        # Construct our vary headers\n        data[\"vary\"] = {}\n        if \"vary\" in response_headers:\n            varied_headers = response_headers[\"vary\"].split(\",\")\n            for header in varied_headers:\n                header = str(header).strip()\n                header_value = request.headers.get(header, None)\n                if header_value is not None:\n                    header_value = str(header_value)\n                data[\"vary\"][header] = header_value\n\n        return b\",\".join([f\"cc={self.serde_version}\".encode(), self.serialize(data)])\n\n    def serialize(self, data: dict[str, Any]) -> bytes:\n        return cast(bytes, msgpack.dumps(data, use_bin_type=True))\n\n    def loads(\n        self,\n        request: PreparedRequest,\n        data: bytes,\n        body_file: IO[bytes] | None = None,\n    ) -> HTTPResponse | None:\n        # Short circuit if we've been given an empty set of data\n        if not data:\n            return None\n\n        # Previous versions of this library supported other serialization\n        # formats, but these have all been removed.\n        if not data.startswith(f\"cc={self.serde_version},\".encode()):\n            return None\n\n        data = data[5:]\n        return self._loads_v4(request, data, body_file)\n\n    def prepare_response(\n        self,\n        request: PreparedRequest,\n        cached: Mapping[str, Any],\n        body_file: IO[bytes] | None = None,\n    ) -> HTTPResponse | None:\n        \"\"\"Verify our vary headers match and construct a real urllib3\n        HTTPResponse object.\n        \"\"\"\n        # Special case the '*' Vary value as it means we cannot actually\n        # determine if the cached response is suitable for this request.\n        # This case is also handled in the controller code when creating\n        # a cache entry, but is left here for backwards compatibility.\n        if \"*\" in cached.get(\"vary\", {}):\n            return None\n\n        # Ensure that the Vary headers for the cached response match our\n        # request\n        for header, value in cached.get(\"vary\", {}).items():\n            if request.headers.get(header, None) != value:\n                return None\n\n        body_raw = cached[\"response\"].pop(\"body\")\n\n        headers: CaseInsensitiveDict[str] = CaseInsensitiveDict(\n            data=cached[\"response\"][\"headers\"]\n        )\n        if headers.get(\"transfer-encoding\", \"\") == \"chunked\":\n            headers.pop(\"transfer-encoding\")\n\n        cached[\"response\"][\"headers\"] = headers\n\n        try:\n            body: IO[bytes]\n            if body_file is None:\n                body = io.BytesIO(body_raw)\n            else:\n                body = body_file\n        except TypeError:\n            # This can happen if cachecontrol serialized to v1 format (pickle)\n            # using Python 2. A Python 2 str(byte string) will be unpickled as\n            # a Python 3 str (unicode string), which will cause the above to\n            # fail with:\n            #\n            #     TypeError: 'str' does not support the buffer interface\n            body = io.BytesIO(body_raw.encode(\"utf8\"))\n\n        # Discard any `strict` parameter serialized by older version of cachecontrol.\n        cached[\"response\"].pop(\"strict\", None)\n\n        return HTTPResponse(body=body, preload_content=False, **cached[\"response\"])\n\n    def _loads_v4(\n        self,\n        request: PreparedRequest,\n        data: bytes,\n        body_file: IO[bytes] | None = None,\n    ) -> HTTPResponse | None:\n        try:\n            cached = msgpack.loads(data, raw=False)\n        except ValueError:\n            return None\n\n        return self.prepare_response(request, cached, body_file)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_vendor/cachecontrol/wrapper.py","size":1417,"sha1":"b18a7cf7fafcb793f2fa944b155c690b209960ca","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"# SPDX-FileCopyrightText: 2015 Eric Larson\n#\n# SPDX-License-Identifier: Apache-2.0\nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING, Collection\n\nfrom pip._vendor.cachecontrol.adapter import CacheControlAdapter\nfrom pip._vendor.cachecontrol.cache import DictCache\n\nif TYPE_CHECKING:\n    from pip._vendor import requests\n\n    from pip._vendor.cachecontrol.cache import BaseCache\n    from pip._vendor.cachecontrol.controller import CacheController\n    from pip._vendor.cachecontrol.heuristics import BaseHeuristic\n    from pip._vendor.cachecontrol.serialize import Serializer\n\n\ndef CacheControl(\n    sess: requests.Session,\n    cache: BaseCache | None = None,\n    cache_etags: bool = True,\n    serializer: Serializer | None = None,\n    heuristic: BaseHeuristic | None = None,\n    controller_class: type[CacheController] | None = None,\n    adapter_class: type[CacheControlAdapter] | None = None,\n    cacheable_methods: Collection[str] | None = None,\n) -> requests.Session:\n    cache = DictCache() if cache is None else cache\n    adapter_class = adapter_class or CacheControlAdapter\n    adapter = adapter_class(\n        cache,\n        cache_etags=cache_etags,\n        serializer=serializer,\n        heuristic=heuristic,\n        controller_class=controller_class,\n        cacheable_methods=cacheable_methods,\n    )\n    sess.mount(\"http://\", adapter)\n    sess.mount(\"https://\", adapter)\n\n    return sess\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_vendor/certifi/__init__.py","size":94,"sha1":"33e92fdf397e4e36a738baef1c0f23a3aeacf424","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"from .core import contents, where\n\n__all__ = [\"contents\", \"where\"]\n__version__ = \"2024.08.30\"\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_vendor/certifi/__main__.py","size":255,"sha1":"94de655e7e05b44b77efbb710287fe7ac57bfe4e","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import argparse\n\nfrom pip._vendor.certifi import contents, where\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\"-c\", \"--contents\", action=\"store_true\")\nargs = parser.parse_args()\n\nif args.contents:\n    print(contents())\nelse:\n    print(where())\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_vendor/certifi/core.py","size":4486,"sha1":"134c12ec63cc0c7fddae8794376f27c842d0ff46","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"\ncertifi.py\n~~~~~~~~~~\n\nThis module returns the installation location of cacert.pem or its contents.\n\"\"\"\nimport sys\nimport atexit\n\ndef exit_cacert_ctx() -> None:\n    _CACERT_CTX.__exit__(None, None, None)  # type: ignore[union-attr]\n\n\nif sys.version_info >= (3, 11):\n\n    from importlib.resources import as_file, files\n\n    _CACERT_CTX = None\n    _CACERT_PATH = None\n\n    def where() -> str:\n        # This is slightly terrible, but we want to delay extracting the file\n        # in cases where we're inside of a zipimport situation until someone\n        # actually calls where(), but we don't want to re-extract the file\n        # on every call of where(), so we'll do it once then store it in a\n        # global variable.\n        global _CACERT_CTX\n        global _CACERT_PATH\n        if _CACERT_PATH is None:\n            # This is slightly janky, the importlib.resources API wants you to\n            # manage the cleanup of this file, so it doesn't actually return a\n            # path, it returns a context manager that will give you the path\n            # when you enter it and will do any cleanup when you leave it. In\n            # the common case of not needing a temporary file, it will just\n            # return the file system location and the __exit__() is a no-op.\n            #\n            # We also have to hold onto the actual context manager, because\n            # it will do the cleanup whenever it gets garbage collected, so\n            # we will also store that at the global level as well.\n            _CACERT_CTX = as_file(files(\"pip._vendor.certifi\").joinpath(\"cacert.pem\"))\n            _CACERT_PATH = str(_CACERT_CTX.__enter__())\n            atexit.register(exit_cacert_ctx)\n\n        return _CACERT_PATH\n\n    def contents() -> str:\n        return files(\"pip._vendor.certifi\").joinpath(\"cacert.pem\").read_text(encoding=\"ascii\")\n\nelif sys.version_info >= (3, 7):\n\n    from importlib.resources import path as get_path, read_text\n\n    _CACERT_CTX = None\n    _CACERT_PATH = None\n\n    def where() -> str:\n        # This is slightly terrible, but we want to delay extracting the\n        # file in cases where we're inside of a zipimport situation until\n        # someone actually calls where(), but we don't want to re-extract\n        # the file on every call of where(), so we'll do it once then store\n        # it in a global variable.\n        global _CACERT_CTX\n        global _CACERT_PATH\n        if _CACERT_PATH is None:\n            # This is slightly janky, the importlib.resources API wants you\n            # to manage the cleanup of this file, so it doesn't actually\n            # return a path, it returns a context manager that will give\n            # you the path when you enter it and will do any cleanup when\n            # you leave it. In the common case of not needing a temporary\n            # file, it will just return the file system location and the\n            # __exit__() is a no-op.\n            #\n            # We also have to hold onto the actual context manager, because\n            # it will do the cleanup whenever it gets garbage collected, so\n            # we will also store that at the global level as well.\n            _CACERT_CTX = get_path(\"pip._vendor.certifi\", \"cacert.pem\")\n            _CACERT_PATH = str(_CACERT_CTX.__enter__())\n            atexit.register(exit_cacert_ctx)\n\n        return _CACERT_PATH\n\n    def contents() -> str:\n        return read_text(\"pip._vendor.certifi\", \"cacert.pem\", encoding=\"ascii\")\n\nelse:\n    import os\n    import types\n    from typing import Union\n\n    Package = Union[types.ModuleType, str]\n    Resource = Union[str, \"os.PathLike\"]\n\n    # This fallback will work for Python versions prior to 3.7 that lack the\n    # importlib.resources module but relies on the existing `where` function\n    # so won't address issues with environments like PyOxidizer that don't set\n    # __file__ on modules.\n    def read_text(\n        package: Package,\n        resource: Resource,\n        encoding: str = 'utf-8',\n        errors: str = 'strict'\n    ) -> str:\n        with open(where(), encoding=encoding) as data:\n            return data.read()\n\n    # If we don't have importlib.resources, then we will just do the old logic\n    # of assuming we're on the filesystem and munge the path directly.\n    def where() -> str:\n        f = os.path.dirname(__file__)\n\n        return os.path.join(f, \"cacert.pem\")\n\n    def contents() -> str:\n        return read_text(\"pip._vendor.certifi\", \"cacert.pem\", encoding=\"ascii\")\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_vendor/distlib/__init__.py","size":625,"sha1":"f8e81bd8c110fd8fcba1c13eda047b067636bb6d","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"# -*- coding: utf-8 -*-\n#\n# Copyright (C) 2012-2023 Vinay Sajip.\n# Licensed to the Python Software Foundation under a contributor agreement.\n# See LICENSE.txt and CONTRIBUTORS.txt.\n#\nimport logging\n\n__version__ = '0.3.9'\n\n\nclass DistlibException(Exception):\n    pass\n\n\ntry:\n    from logging import NullHandler\nexcept ImportError:  # pragma: no cover\n\n    class NullHandler(logging.Handler):\n\n        def handle(self, record):\n            pass\n\n        def emit(self, record):\n            pass\n\n        def createLock(self):\n            self.lock = None\n\n\nlogger = logging.getLogger(__name__)\nlogger.addHandler(NullHandler())\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_vendor/distlib/compat.py","size":41467,"sha1":"d4f491ec2a8dbb2c62df0c54da9fb76223d12005","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"# -*- coding: utf-8 -*-\n#\n# Copyright (C) 2013-2017 Vinay Sajip.\n# Licensed to the Python Software Foundation under a contributor agreement.\n# See LICENSE.txt and CONTRIBUTORS.txt.\n#\nfrom __future__ import absolute_import\n\nimport os\nimport re\nimport shutil\nimport sys\n\ntry:\n    import ssl\nexcept ImportError:  # pragma: no cover\n    ssl = None\n\nif sys.version_info[0] < 3:  # pragma: no cover\n    from StringIO import StringIO\n    string_types = basestring,\n    text_type = unicode\n    from types import FileType as file_type\n    import __builtin__ as builtins\n    import ConfigParser as configparser\n    from urlparse import urlparse, urlunparse, urljoin, urlsplit, urlunsplit\n    from urllib import (urlretrieve, quote as _quote, unquote, url2pathname,\n                        pathname2url, ContentTooShortError, splittype)\n\n    def quote(s):\n        if isinstance(s, unicode):\n            s = s.encode('utf-8')\n        return _quote(s)\n\n    import urllib2\n    from urllib2 import (Request, urlopen, URLError, HTTPError,\n                         HTTPBasicAuthHandler, HTTPPasswordMgr, HTTPHandler,\n                         HTTPRedirectHandler, build_opener)\n    if ssl:\n        from urllib2 import HTTPSHandler\n    import httplib\n    import xmlrpclib\n    import Queue as queue\n    from HTMLParser import HTMLParser\n    import htmlentitydefs\n    raw_input = raw_input\n    from itertools import ifilter as filter\n    from itertools import ifilterfalse as filterfalse\n\n    # Leaving this around for now, in case it needs resurrecting in some way\n    # _userprog = None\n    # def splituser(host):\n    # \"\"\"splituser('user[:passwd]@host[:port]') --> 'user[:passwd]', 'host[:port]'.\"\"\"\n    # global _userprog\n    # if _userprog is None:\n    # import re\n    # _userprog = re.compile('^(.*)@(.*)$')\n\n    # match = _userprog.match(host)\n    # if match: return match.group(1, 2)\n    # return None, host\n\nelse:  # pragma: no cover\n    from io import StringIO\n    string_types = str,\n    text_type = str\n    from io import TextIOWrapper as file_type\n    import builtins\n    import configparser\n    from urllib.parse import (urlparse, urlunparse, urljoin, quote, unquote,\n                              urlsplit, urlunsplit, splittype)\n    from urllib.request import (urlopen, urlretrieve, Request, url2pathname,\n                                pathname2url, HTTPBasicAuthHandler,\n                                HTTPPasswordMgr, HTTPHandler,\n                                HTTPRedirectHandler, build_opener)\n    if ssl:\n        from urllib.request import HTTPSHandler\n    from urllib.error import HTTPError, URLError, ContentTooShortError\n    import http.client as httplib\n    import urllib.request as urllib2\n    import xmlrpc.client as xmlrpclib\n    import queue\n    from html.parser import HTMLParser\n    import html.entities as htmlentitydefs\n    raw_input = input\n    from itertools import filterfalse\n    filter = filter\n\ntry:\n    from ssl import match_hostname, CertificateError\nexcept ImportError:  # pragma: no cover\n\n    class CertificateError(ValueError):\n        pass\n\n    def _dnsname_match(dn, hostname, max_wildcards=1):\n        \"\"\"Matching according to RFC 6125, section 6.4.3\n\n        http://tools.ietf.org/html/rfc6125#section-6.4.3\n        \"\"\"\n        pats = []\n        if not dn:\n            return False\n\n        parts = dn.split('.')\n        leftmost, remainder = parts[0], parts[1:]\n\n        wildcards = leftmost.count('*')\n        if wildcards > max_wildcards:\n            # Issue #17980: avoid denials of service by refusing more\n            # than one wildcard per fragment.  A survey of established\n            # policy among SSL implementations showed it to be a\n            # reasonable choice.\n            raise CertificateError(\n                \"too many wildcards in certificate DNS name: \" + repr(dn))\n\n        # speed up common case w/o wildcards\n        if not wildcards:\n            return dn.lower() == hostname.lower()\n\n        # RFC 6125, section 6.4.3, subitem 1.\n        # The client SHOULD NOT attempt to match a presented identifier in which\n        # the wildcard character comprises a label other than the left-most label.\n        if leftmost == '*':\n            # When '*' is a fragment by itself, it matches a non-empty dotless\n            # fragment.\n            pats.append('[^.]+')\n        elif leftmost.startswith('xn--') or hostname.startswith('xn--'):\n            # RFC 6125, section 6.4.3, subitem 3.\n            # The client SHOULD NOT attempt to match a presented identifier\n            # where the wildcard character is embedded within an A-label or\n            # U-label of an internationalized domain name.\n            pats.append(re.escape(leftmost))\n        else:\n            # Otherwise, '*' matches any dotless string, e.g. www*\n            pats.append(re.escape(leftmost).replace(r'\\*', '[^.]*'))\n\n        # add the remaining fragments, ignore any wildcards\n        for frag in remainder:\n            pats.append(re.escape(frag))\n\n        pat = re.compile(r'\\A' + r'\\.'.join(pats) + r'\\Z', re.IGNORECASE)\n        return pat.match(hostname)\n\n    def match_hostname(cert, hostname):\n        \"\"\"Verify that *cert* (in decoded format as returned by\n        SSLSocket.getpeercert()) matches the *hostname*.  RFC 2818 and RFC 6125\n        rules are followed, but IP addresses are not accepted for *hostname*.\n\n        CertificateError is raised on failure. On success, the function\n        returns nothing.\n        \"\"\"\n        if not cert:\n            raise ValueError(\"empty or no certificate, match_hostname needs a \"\n                             \"SSL socket or SSL context with either \"\n                             \"CERT_OPTIONAL or CERT_REQUIRED\")\n        dnsnames = []\n        san = cert.get('subjectAltName', ())\n        for key, value in san:\n            if key == 'DNS':\n                if _dnsname_match(value, hostname):\n                    return\n                dnsnames.append(value)\n        if not dnsnames:\n            # The subject is only checked when there is no dNSName entry\n            # in subjectAltName\n            for sub in cert.get('subject', ()):\n                for key, value in sub:\n                    # XXX according to RFC 2818, the most specific Common Name\n                    # must be used.\n                    if key == 'commonName':\n                        if _dnsname_match(value, hostname):\n                            return\n                        dnsnames.append(value)\n        if len(dnsnames) > 1:\n            raise CertificateError(\"hostname %r \"\n                                   \"doesn't match either of %s\" %\n                                   (hostname, ', '.join(map(repr, dnsnames))))\n        elif len(dnsnames) == 1:\n            raise CertificateError(\"hostname %r \"\n                                   \"doesn't match %r\" %\n                                   (hostname, dnsnames[0]))\n        else:\n            raise CertificateError(\"no appropriate commonName or \"\n                                   \"subjectAltName fields were found\")\n\n\ntry:\n    from types import SimpleNamespace as Container\nexcept ImportError:  # pragma: no cover\n\n    class Container(object):\n        \"\"\"\n        A generic container for when multiple values need to be returned\n        \"\"\"\n\n        def __init__(self, **kwargs):\n            self.__dict__.update(kwargs)\n\n\ntry:\n    from shutil import which\nexcept ImportError:  # pragma: no cover\n    # Implementation from Python 3.3\n    def which(cmd, mode=os.F_OK | os.X_OK, path=None):\n        \"\"\"Given a command, mode, and a PATH string, return the path which\n        conforms to the given mode on the PATH, or None if there is no such\n        file.\n\n        `mode` defaults to os.F_OK | os.X_OK. `path` defaults to the result\n        of os.environ.get(\"PATH\"), or can be overridden with a custom search\n        path.\n\n        \"\"\"\n\n        # Check that a given file can be accessed with the correct mode.\n        # Additionally check that `file` is not a directory, as on Windows\n        # directories pass the os.access check.\n        def _access_check(fn, mode):\n            return (os.path.exists(fn) and os.access(fn, mode) and not os.path.isdir(fn))\n\n        # If we're given a path with a directory part, look it up directly rather\n        # than referring to PATH directories. This includes checking relative to the\n        # current directory, e.g. ./script\n        if os.path.dirname(cmd):\n            if _access_check(cmd, mode):\n                return cmd\n            return None\n\n        if path is None:\n            path = os.environ.get(\"PATH\", os.defpath)\n        if not path:\n            return None\n        path = path.split(os.pathsep)\n\n        if sys.platform == \"win32\":\n            # The current directory takes precedence on Windows.\n            if os.curdir not in path:\n                path.insert(0, os.curdir)\n\n            # PATHEXT is necessary to check on Windows.\n            pathext = os.environ.get(\"PATHEXT\", \"\").split(os.pathsep)\n            # See if the given file matches any of the expected path extensions.\n            # This will allow us to short circuit when given \"python.exe\".\n            # If it does match, only test that one, otherwise we have to try\n            # others.\n            if any(cmd.lower().endswith(ext.lower()) for ext in pathext):\n                files = [cmd]\n            else:\n                files = [cmd + ext for ext in pathext]\n        else:\n            # On other platforms you don't have things like PATHEXT to tell you\n            # what file suffixes are executable, so just pass on cmd as-is.\n            files = [cmd]\n\n        seen = set()\n        for dir in path:\n            normdir = os.path.normcase(dir)\n            if normdir not in seen:\n                seen.add(normdir)\n                for thefile in files:\n                    name = os.path.join(dir, thefile)\n                    if _access_check(name, mode):\n                        return name\n        return None\n\n\n# ZipFile is a context manager in 2.7, but not in 2.6\n\nfrom zipfile import ZipFile as BaseZipFile\n\nif hasattr(BaseZipFile, '__enter__'):  # pragma: no cover\n    ZipFile = BaseZipFile\nelse:  # pragma: no cover\n    from zipfile import ZipExtFile as BaseZipExtFile\n\n    class ZipExtFile(BaseZipExtFile):\n\n        def __init__(self, base):\n            self.__dict__.update(base.__dict__)\n\n        def __enter__(self):\n            return self\n\n        def __exit__(self, *exc_info):\n            self.close()\n            # return None, so if an exception occurred, it will propagate\n\n    class ZipFile(BaseZipFile):\n\n        def __enter__(self):\n            return self\n\n        def __exit__(self, *exc_info):\n            self.close()\n            # return None, so if an exception occurred, it will propagate\n\n        def open(self, *args, **kwargs):\n            base = BaseZipFile.open(self, *args, **kwargs)\n            return ZipExtFile(base)\n\n\ntry:\n    from platform import python_implementation\nexcept ImportError:  # pragma: no cover\n\n    def python_implementation():\n        \"\"\"Return a string identifying the Python implementation.\"\"\"\n        if 'PyPy' in sys.version:\n            return 'PyPy'\n        if os.name == 'java':\n            return 'Jython'\n        if sys.version.startswith('IronPython'):\n            return 'IronPython'\n        return 'CPython'\n\n\nimport sysconfig\n\ntry:\n    callable = callable\nexcept NameError:  # pragma: no cover\n    from collections.abc import Callable\n\n    def callable(obj):\n        return isinstance(obj, Callable)\n\n\ntry:\n    fsencode = os.fsencode\n    fsdecode = os.fsdecode\nexcept AttributeError:  # pragma: no cover\n    # Issue #99: on some systems (e.g. containerised),\n    # sys.getfilesystemencoding() returns None, and we need a real value,\n    # so fall back to utf-8. From the CPython 2.7 docs relating to Unix and\n    # sys.getfilesystemencoding(): the return value is \"the users preference\n    # according to the result of nl_langinfo(CODESET), or None if the\n    # nl_langinfo(CODESET) failed.\"\n    _fsencoding = sys.getfilesystemencoding() or 'utf-8'\n    if _fsencoding == 'mbcs':\n        _fserrors = 'strict'\n    else:\n        _fserrors = 'surrogateescape'\n\n    def fsencode(filename):\n        if isinstance(filename, bytes):\n            return filename\n        elif isinstance(filename, text_type):\n            return filename.encode(_fsencoding, _fserrors)\n        else:\n            raise TypeError(\"expect bytes or str, not %s\" %\n                            type(filename).__name__)\n\n    def fsdecode(filename):\n        if isinstance(filename, text_type):\n            return filename\n        elif isinstance(filename, bytes):\n            return filename.decode(_fsencoding, _fserrors)\n        else:\n            raise TypeError(\"expect bytes or str, not %s\" %\n                            type(filename).__name__)\n\n\ntry:\n    from tokenize import detect_encoding\nexcept ImportError:  # pragma: no cover\n    from codecs import BOM_UTF8, lookup\n\n    cookie_re = re.compile(r\"coding[:=]\\s*([-\\w.]+)\")\n\n    def _get_normal_name(orig_enc):\n        \"\"\"Imitates get_normal_name in tokenizer.c.\"\"\"\n        # Only care about the first 12 characters.\n        enc = orig_enc[:12].lower().replace(\"_\", \"-\")\n        if enc == \"utf-8\" or enc.startswith(\"utf-8-\"):\n            return \"utf-8\"\n        if enc in (\"latin-1\", \"iso-8859-1\", \"iso-latin-1\") or \\\n           enc.startswith((\"latin-1-\", \"iso-8859-1-\", \"iso-latin-1-\")):\n            return \"iso-8859-1\"\n        return orig_enc\n\n    def detect_encoding(readline):\n        \"\"\"\n        The detect_encoding() function is used to detect the encoding that should\n        be used to decode a Python source file.  It requires one argument, readline,\n        in the same way as the tokenize() generator.\n\n        It will call readline a maximum of twice, and return the encoding used\n        (as a string) and a list of any lines (left as bytes) it has read in.\n\n        It detects the encoding from the presence of a utf-8 bom or an encoding\n        cookie as specified in pep-0263.  If both a bom and a cookie are present,\n        but disagree, a SyntaxError will be raised.  If the encoding cookie is an\n        invalid charset, raise a SyntaxError.  Note that if a utf-8 bom is found,\n        'utf-8-sig' is returned.\n\n        If no encoding is specified, then the default of 'utf-8' will be returned.\n        \"\"\"\n        try:\n            filename = readline.__self__.name\n        except AttributeError:\n            filename = None\n        bom_found = False\n        encoding = None\n        default = 'utf-8'\n\n        def read_or_stop():\n            try:\n                return readline()\n            except StopIteration:\n                return b''\n\n        def find_cookie(line):\n            try:\n                # Decode as UTF-8. Either the line is an encoding declaration,\n                # in which case it should be pure ASCII, or it must be UTF-8\n                # per default encoding.\n                line_string = line.decode('utf-8')\n            except UnicodeDecodeError:\n                msg = \"invalid or missing encoding declaration\"\n                if filename is not None:\n                    msg = '{} for {!r}'.format(msg, filename)\n                raise SyntaxError(msg)\n\n            matches = cookie_re.findall(line_string)\n            if not matches:\n                return None\n            encoding = _get_normal_name(matches[0])\n            try:\n                codec = lookup(encoding)\n            except LookupError:\n                # This behaviour mimics the Python interpreter\n                if filename is None:\n                    msg = \"unknown encoding: \" + encoding\n                else:\n                    msg = \"unknown encoding for {!r}: {}\".format(\n                        filename, encoding)\n                raise SyntaxError(msg)\n\n            if bom_found:\n                if codec.name != 'utf-8':\n                    # This behaviour mimics the Python interpreter\n                    if filename is None:\n                        msg = 'encoding problem: utf-8'\n                    else:\n                        msg = 'encoding problem for {!r}: utf-8'.format(\n                            filename)\n                    raise SyntaxError(msg)\n                encoding += '-sig'\n            return encoding\n\n        first = read_or_stop()\n        if first.startswith(BOM_UTF8):\n            bom_found = True\n            first = first[3:]\n            default = 'utf-8-sig'\n        if not first:\n            return default, []\n\n        encoding = find_cookie(first)\n        if encoding:\n            return encoding, [first]\n\n        second = read_or_stop()\n        if not second:\n            return default, [first]\n\n        encoding = find_cookie(second)\n        if encoding:\n            return encoding, [first, second]\n\n        return default, [first, second]\n\n\n# For converting & <-> &amp; etc.\ntry:\n    from html import escape\nexcept ImportError:\n    from cgi import escape\nif sys.version_info[:2] < (3, 4):\n    unescape = HTMLParser().unescape\nelse:\n    from html import unescape\n\ntry:\n    from collections import ChainMap\nexcept ImportError:  # pragma: no cover\n    from collections import MutableMapping\n\n    try:\n        from reprlib import recursive_repr as _recursive_repr\n    except ImportError:\n\n        def _recursive_repr(fillvalue='...'):\n            '''\n            Decorator to make a repr function return fillvalue for a recursive\n            call\n            '''\n\n            def decorating_function(user_function):\n                repr_running = set()\n\n                def wrapper(self):\n                    key = id(self), get_ident()\n                    if key in repr_running:\n                        return fillvalue\n                    repr_running.add(key)\n                    try:\n                        result = user_function(self)\n                    finally:\n                        repr_running.discard(key)\n                    return result\n\n                # Can't use functools.wraps() here because of bootstrap issues\n                wrapper.__module__ = getattr(user_function, '__module__')\n                wrapper.__doc__ = getattr(user_function, '__doc__')\n                wrapper.__name__ = getattr(user_function, '__name__')\n                wrapper.__annotations__ = getattr(user_function,\n                                                  '__annotations__', {})\n                return wrapper\n\n            return decorating_function\n\n    class ChainMap(MutableMapping):\n        '''\n        A ChainMap groups multiple dicts (or other mappings) together\n        to create a single, updateable view.\n\n        The underlying mappings are stored in a list.  That list is public and can\n        accessed or updated using the *maps* attribute.  There is no other state.\n\n        Lookups search the underlying mappings successively until a key is found.\n        In contrast, writes, updates, and deletions only operate on the first\n        mapping.\n        '''\n\n        def __init__(self, *maps):\n            '''Initialize a ChainMap by setting *maps* to the given mappings.\n            If no mappings are provided, a single empty dictionary is used.\n\n            '''\n            self.maps = list(maps) or [{}]  # always at least one map\n\n        def __missing__(self, key):\n            raise KeyError(key)\n\n        def __getitem__(self, key):\n            for mapping in self.maps:\n                try:\n                    return mapping[\n                        key]  # can't use 'key in mapping' with defaultdict\n                except KeyError:\n                    pass\n            return self.__missing__(\n                key)  # support subclasses that define __missing__\n\n        def get(self, key, default=None):\n            return self[key] if key in self else default\n\n        def __len__(self):\n            return len(set().union(\n                *self.maps))  # reuses stored hash values if possible\n\n        def __iter__(self):\n            return iter(set().union(*self.maps))\n\n        def __contains__(self, key):\n            return any(key in m for m in self.maps)\n\n        def __bool__(self):\n            return any(self.maps)\n\n        @_recursive_repr()\n        def __repr__(self):\n            return '{0.__class__.__name__}({1})'.format(\n                self, ', '.join(map(repr, self.maps)))\n\n        @classmethod\n        def fromkeys(cls, iterable, *args):\n            'Create a ChainMap with a single dict created from the iterable.'\n            return cls(dict.fromkeys(iterable, *args))\n\n        def copy(self):\n            'New ChainMap or subclass with a new copy of maps[0] and refs to maps[1:]'\n            return self.__class__(self.maps[0].copy(), *self.maps[1:])\n\n        __copy__ = copy\n\n        def new_child(self):  # like Django's Context.push()\n            'New ChainMap with a new dict followed by all previous maps.'\n            return self.__class__({}, *self.maps)\n\n        @property\n        def parents(self):  # like Django's Context.pop()\n            'New ChainMap from maps[1:].'\n            return self.__class__(*self.maps[1:])\n\n        def __setitem__(self, key, value):\n            self.maps[0][key] = value\n\n        def __delitem__(self, key):\n            try:\n                del self.maps[0][key]\n            except KeyError:\n                raise KeyError(\n                    'Key not found in the first mapping: {!r}'.format(key))\n\n        def popitem(self):\n            'Remove and return an item pair from maps[0]. Raise KeyError is maps[0] is empty.'\n            try:\n                return self.maps[0].popitem()\n            except KeyError:\n                raise KeyError('No keys found in the first mapping.')\n\n        def pop(self, key, *args):\n            'Remove *key* from maps[0] and return its value. Raise KeyError if *key* not in maps[0].'\n            try:\n                return self.maps[0].pop(key, *args)\n            except KeyError:\n                raise KeyError(\n                    'Key not found in the first mapping: {!r}'.format(key))\n\n        def clear(self):\n            'Clear maps[0], leaving maps[1:] intact.'\n            self.maps[0].clear()\n\n\ntry:\n    from importlib.util import cache_from_source  # Python >= 3.4\nexcept ImportError:  # pragma: no cover\n\n    def cache_from_source(path, debug_override=None):\n        assert path.endswith('.py')\n        if debug_override is None:\n            debug_override = __debug__\n        if debug_override:\n            suffix = 'c'\n        else:\n            suffix = 'o'\n        return path + suffix\n\n\ntry:\n    from collections import OrderedDict\nexcept ImportError:  # pragma: no cover\n    # {{{ http://code.activestate.com/recipes/576693/ (r9)\n    # Backport of OrderedDict() class that runs on Python 2.4, 2.5, 2.6, 2.7 and pypy.\n    # Passes Python2.7's test suite and incorporates all the latest updates.\n    try:\n        from thread import get_ident as _get_ident\n    except ImportError:\n        from dummy_thread import get_ident as _get_ident\n\n    try:\n        from _abcoll import KeysView, ValuesView, ItemsView\n    except ImportError:\n        pass\n\n    class OrderedDict(dict):\n        'Dictionary that remembers insertion order'\n\n        # An inherited dict maps keys to values.\n        # The inherited dict provides __getitem__, __len__, __contains__, and get.\n        # The remaining methods are order-aware.\n        # Big-O running times for all methods are the same as for regular dictionaries.\n\n        # The internal self.__map dictionary maps keys to links in a doubly linked list.\n        # The circular doubly linked list starts and ends with a sentinel element.\n        # The sentinel element never gets deleted (this simplifies the algorithm).\n        # Each link is stored as a list of length three:  [PREV, NEXT, KEY].\n\n        def __init__(self, *args, **kwds):\n            '''Initialize an ordered dictionary.  Signature is the same as for\n            regular dictionaries, but keyword arguments are not recommended\n            because their insertion order is arbitrary.\n\n            '''\n            if len(args) > 1:\n                raise TypeError('expected at most 1 arguments, got %d' %\n                                len(args))\n            try:\n                self.__root\n            except AttributeError:\n                self.__root = root = []  # sentinel node\n                root[:] = [root, root, None]\n                self.__map = {}\n            self.__update(*args, **kwds)\n\n        def __setitem__(self, key, value, dict_setitem=dict.__setitem__):\n            'od.__setitem__(i, y) <==> od[i]=y'\n            # Setting a new item creates a new link which goes at the end of the linked\n            # list, and the inherited dictionary is updated with the new key/value pair.\n            if key not in self:\n                root = self.__root\n                last = root[0]\n                last[1] = root[0] = self.__map[key] = [last, root, key]\n            dict_setitem(self, key, value)\n\n        def __delitem__(self, key, dict_delitem=dict.__delitem__):\n            'od.__delitem__(y) <==> del od[y]'\n            # Deleting an existing item uses self.__map to find the link which is\n            # then removed by updating the links in the predecessor and successor nodes.\n            dict_delitem(self, key)\n            link_prev, link_next, key = self.__map.pop(key)\n            link_prev[1] = link_next\n            link_next[0] = link_prev\n\n        def __iter__(self):\n            'od.__iter__() <==> iter(od)'\n            root = self.__root\n            curr = root[1]\n            while curr is not root:\n                yield curr[2]\n                curr = curr[1]\n\n        def __reversed__(self):\n            'od.__reversed__() <==> reversed(od)'\n            root = self.__root\n            curr = root[0]\n            while curr is not root:\n                yield curr[2]\n                curr = curr[0]\n\n        def clear(self):\n            'od.clear() -> None.  Remove all items from od.'\n            try:\n                for node in self.__map.itervalues():\n                    del node[:]\n                root = self.__root\n                root[:] = [root, root, None]\n                self.__map.clear()\n            except AttributeError:\n                pass\n            dict.clear(self)\n\n        def popitem(self, last=True):\n            '''od.popitem() -> (k, v), return and remove a (key, value) pair.\n            Pairs are returned in LIFO order if last is true or FIFO order if false.\n\n            '''\n            if not self:\n                raise KeyError('dictionary is empty')\n            root = self.__root\n            if last:\n                link = root[0]\n                link_prev = link[0]\n                link_prev[1] = root\n                root[0] = link_prev\n            else:\n                link = root[1]\n                link_next = link[1]\n                root[1] = link_next\n                link_next[0] = root\n            key = link[2]\n            del self.__map[key]\n            value = dict.pop(self, key)\n            return key, value\n\n        # -- the following methods do not depend on the internal structure --\n\n        def keys(self):\n            'od.keys() -> list of keys in od'\n            return list(self)\n\n        def values(self):\n            'od.values() -> list of values in od'\n            return [self[key] for key in self]\n\n        def items(self):\n            'od.items() -> list of (key, value) pairs in od'\n            return [(key, self[key]) for key in self]\n\n        def iterkeys(self):\n            'od.iterkeys() -> an iterator over the keys in od'\n            return iter(self)\n\n        def itervalues(self):\n            'od.itervalues -> an iterator over the values in od'\n            for k in self:\n                yield self[k]\n\n        def iteritems(self):\n            'od.iteritems -> an iterator over the (key, value) items in od'\n            for k in self:\n                yield (k, self[k])\n\n        def update(*args, **kwds):\n            '''od.update(E, **F) -> None.  Update od from dict/iterable E and F.\n\n            If E is a dict instance, does:           for k in E: od[k] = E[k]\n            If E has a .keys() method, does:         for k in E.keys(): od[k] = E[k]\n            Or if E is an iterable of items, does:   for k, v in E: od[k] = v\n            In either case, this is followed by:     for k, v in F.items(): od[k] = v\n\n            '''\n            if len(args) > 2:\n                raise TypeError('update() takes at most 2 positional '\n                                'arguments (%d given)' % (len(args), ))\n            elif not args:\n                raise TypeError('update() takes at least 1 argument (0 given)')\n            self = args[0]\n            # Make progressively weaker assumptions about \"other\"\n            other = ()\n            if len(args) == 2:\n                other = args[1]\n            if isinstance(other, dict):\n                for key in other:\n                    self[key] = other[key]\n            elif hasattr(other, 'keys'):\n                for key in other.keys():\n                    self[key] = other[key]\n            else:\n                for key, value in other:\n                    self[key] = value\n            for key, value in kwds.items():\n                self[key] = value\n\n        __update = update  # let subclasses override update without breaking __init__\n\n        __marker = object()\n\n        def pop(self, key, default=__marker):\n            '''od.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n            If key is not found, d is returned if given, otherwise KeyError is raised.\n\n            '''\n            if key in self:\n                result = self[key]\n                del self[key]\n                return result\n            if default is self.__marker:\n                raise KeyError(key)\n            return default\n\n        def setdefault(self, key, default=None):\n            'od.setdefault(k[,d]) -> od.get(k,d), also set od[k]=d if k not in od'\n            if key in self:\n                return self[key]\n            self[key] = default\n            return default\n\n        def __repr__(self, _repr_running=None):\n            'od.__repr__() <==> repr(od)'\n            if not _repr_running:\n                _repr_running = {}\n            call_key = id(self), _get_ident()\n            if call_key in _repr_running:\n                return '...'\n            _repr_running[call_key] = 1\n            try:\n                if not self:\n                    return '%s()' % (self.__class__.__name__, )\n                return '%s(%r)' % (self.__class__.__name__, self.items())\n            finally:\n                del _repr_running[call_key]\n\n        def __reduce__(self):\n            'Return state information for pickling'\n            items = [[k, self[k]] for k in self]\n            inst_dict = vars(self).copy()\n            for k in vars(OrderedDict()):\n                inst_dict.pop(k, None)\n            if inst_dict:\n                return (self.__class__, (items, ), inst_dict)\n            return self.__class__, (items, )\n\n        def copy(self):\n            'od.copy() -> a shallow copy of od'\n            return self.__class__(self)\n\n        @classmethod\n        def fromkeys(cls, iterable, value=None):\n            '''OD.fromkeys(S[, v]) -> New ordered dictionary with keys from S\n            and values equal to v (which defaults to None).\n\n            '''\n            d = cls()\n            for key in iterable:\n                d[key] = value\n            return d\n\n        def __eq__(self, other):\n            '''od.__eq__(y) <==> od==y.  Comparison to another OD is order-sensitive\n            while comparison to a regular mapping is order-insensitive.\n\n            '''\n            if isinstance(other, OrderedDict):\n                return len(self) == len(\n                    other) and self.items() == other.items()\n            return dict.__eq__(self, other)\n\n        def __ne__(self, other):\n            return not self == other\n\n        # -- the following methods are only used in Python 2.7 --\n\n        def viewkeys(self):\n            \"od.viewkeys() -> a set-like object providing a view on od's keys\"\n            return KeysView(self)\n\n        def viewvalues(self):\n            \"od.viewvalues() -> an object providing a view on od's values\"\n            return ValuesView(self)\n\n        def viewitems(self):\n            \"od.viewitems() -> a set-like object providing a view on od's items\"\n            return ItemsView(self)\n\n\ntry:\n    from logging.config import BaseConfigurator, valid_ident\nexcept ImportError:  # pragma: no cover\n    IDENTIFIER = re.compile('^[a-z_][a-z0-9_]*$', re.I)\n\n    def valid_ident(s):\n        m = IDENTIFIER.match(s)\n        if not m:\n            raise ValueError('Not a valid Python identifier: %r' % s)\n        return True\n\n    # The ConvertingXXX classes are wrappers around standard Python containers,\n    # and they serve to convert any suitable values in the container. The\n    # conversion converts base dicts, lists and tuples to their wrapped\n    # equivalents, whereas strings which match a conversion format are converted\n    # appropriately.\n    #\n    # Each wrapper should have a configurator attribute holding the actual\n    # configurator to use for conversion.\n\n    class ConvertingDict(dict):\n        \"\"\"A converting dictionary wrapper.\"\"\"\n\n        def __getitem__(self, key):\n            value = dict.__getitem__(self, key)\n            result = self.configurator.convert(value)\n            # If the converted value is different, save for next time\n            if value is not result:\n                self[key] = result\n                if type(result) in (ConvertingDict, ConvertingList,\n                                    ConvertingTuple):\n                    result.parent = self\n                    result.key = key\n            return result\n\n        def get(self, key, default=None):\n            value = dict.get(self, key, default)\n            result = self.configurator.convert(value)\n            # If the converted value is different, save for next time\n            if value is not result:\n                self[key] = result\n                if type(result) in (ConvertingDict, ConvertingList,\n                                    ConvertingTuple):\n                    result.parent = self\n                    result.key = key\n            return result\n\n    def pop(self, key, default=None):\n        value = dict.pop(self, key, default)\n        result = self.configurator.convert(value)\n        if value is not result:\n            if type(result) in (ConvertingDict, ConvertingList,\n                                ConvertingTuple):\n                result.parent = self\n                result.key = key\n        return result\n\n    class ConvertingList(list):\n        \"\"\"A converting list wrapper.\"\"\"\n\n        def __getitem__(self, key):\n            value = list.__getitem__(self, key)\n            result = self.configurator.convert(value)\n            # If the converted value is different, save for next time\n            if value is not result:\n                self[key] = result\n                if type(result) in (ConvertingDict, ConvertingList,\n                                    ConvertingTuple):\n                    result.parent = self\n                    result.key = key\n            return result\n\n        def pop(self, idx=-1):\n            value = list.pop(self, idx)\n            result = self.configurator.convert(value)\n            if value is not result:\n                if type(result) in (ConvertingDict, ConvertingList,\n                                    ConvertingTuple):\n                    result.parent = self\n            return result\n\n    class ConvertingTuple(tuple):\n        \"\"\"A converting tuple wrapper.\"\"\"\n\n        def __getitem__(self, key):\n            value = tuple.__getitem__(self, key)\n            result = self.configurator.convert(value)\n            if value is not result:\n                if type(result) in (ConvertingDict, ConvertingList,\n                                    ConvertingTuple):\n                    result.parent = self\n                    result.key = key\n            return result\n\n    class BaseConfigurator(object):\n        \"\"\"\n        The configurator base class which defines some useful defaults.\n        \"\"\"\n\n        CONVERT_PATTERN = re.compile(r'^(?P<prefix>[a-z]+)://(?P<suffix>.*)$')\n\n        WORD_PATTERN = re.compile(r'^\\s*(\\w+)\\s*')\n        DOT_PATTERN = re.compile(r'^\\.\\s*(\\w+)\\s*')\n        INDEX_PATTERN = re.compile(r'^\\[\\s*(\\w+)\\s*\\]\\s*')\n        DIGIT_PATTERN = re.compile(r'^\\d+$')\n\n        value_converters = {\n            'ext': 'ext_convert',\n            'cfg': 'cfg_convert',\n        }\n\n        # We might want to use a different one, e.g. importlib\n        importer = staticmethod(__import__)\n\n        def __init__(self, config):\n            self.config = ConvertingDict(config)\n            self.config.configurator = self\n\n        def resolve(self, s):\n            \"\"\"\n            Resolve strings to objects using standard import and attribute\n            syntax.\n            \"\"\"\n            name = s.split('.')\n            used = name.pop(0)\n            try:\n                found = self.importer(used)\n                for frag in name:\n                    used += '.' + frag\n                    try:\n                        found = getattr(found, frag)\n                    except AttributeError:\n                        self.importer(used)\n                        found = getattr(found, frag)\n                return found\n            except ImportError:\n                e, tb = sys.exc_info()[1:]\n                v = ValueError('Cannot resolve %r: %s' % (s, e))\n                v.__cause__, v.__traceback__ = e, tb\n                raise v\n\n        def ext_convert(self, value):\n            \"\"\"Default converter for the ext:// protocol.\"\"\"\n            return self.resolve(value)\n\n        def cfg_convert(self, value):\n            \"\"\"Default converter for the cfg:// protocol.\"\"\"\n            rest = value\n            m = self.WORD_PATTERN.match(rest)\n            if m is None:\n                raise ValueError(\"Unable to convert %r\" % value)\n            else:\n                rest = rest[m.end():]\n                d = self.config[m.groups()[0]]\n                while rest:\n                    m = self.DOT_PATTERN.match(rest)\n                    if m:\n                        d = d[m.groups()[0]]\n                    else:\n                        m = self.INDEX_PATTERN.match(rest)\n                        if m:\n                            idx = m.groups()[0]\n                            if not self.DIGIT_PATTERN.match(idx):\n                                d = d[idx]\n                            else:\n                                try:\n                                    n = int(\n                                        idx\n                                    )  # try as number first (most likely)\n                                    d = d[n]\n                                except TypeError:\n                                    d = d[idx]\n                    if m:\n                        rest = rest[m.end():]\n                    else:\n                        raise ValueError('Unable to convert '\n                                         '%r at %r' % (value, rest))\n            # rest should be empty\n            return d\n\n        def convert(self, value):\n            \"\"\"\n            Convert values to an appropriate type. dicts, lists and tuples are\n            replaced by their converting alternatives. Strings are checked to\n            see if they have a conversion format and are converted if they do.\n            \"\"\"\n            if not isinstance(value, ConvertingDict) and isinstance(\n                    value, dict):\n                value = ConvertingDict(value)\n                value.configurator = self\n            elif not isinstance(value, ConvertingList) and isinstance(\n                    value, list):\n                value = ConvertingList(value)\n                value.configurator = self\n            elif not isinstance(value, ConvertingTuple) and isinstance(value, tuple):\n                value = ConvertingTuple(value)\n                value.configurator = self\n            elif isinstance(value, string_types):\n                m = self.CONVERT_PATTERN.match(value)\n                if m:\n                    d = m.groupdict()\n                    prefix = d['prefix']\n                    converter = self.value_converters.get(prefix, None)\n                    if converter:\n                        suffix = d['suffix']\n                        converter = getattr(self, converter)\n                        value = converter(suffix)\n            return value\n\n        def configure_custom(self, config):\n            \"\"\"Configure an object with a user-supplied factory.\"\"\"\n            c = config.pop('()')\n            if not callable(c):\n                c = self.resolve(c)\n            props = config.pop('.', None)\n            # Check for valid identifiers\n            kwargs = dict([(k, config[k]) for k in config if valid_ident(k)])\n            result = c(**kwargs)\n            if props:\n                for name, value in props.items():\n                    setattr(result, name, value)\n            return result\n\n        def as_tuple(self, value):\n            \"\"\"Utility function which converts lists to tuples.\"\"\"\n            if isinstance(value, list):\n                value = tuple(value)\n            return value\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_vendor/distlib/database.py","size":51160,"sha1":"567a99b381e58a01f507597463bfa0a71e8ad968","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"# -*- coding: utf-8 -*-\n#\n# Copyright (C) 2012-2023 The Python Software Foundation.\n# See LICENSE.txt and CONTRIBUTORS.txt.\n#\n\"\"\"PEP 376 implementation.\"\"\"\n\nfrom __future__ import unicode_literals\n\nimport base64\nimport codecs\nimport contextlib\nimport hashlib\nimport logging\nimport os\nimport posixpath\nimport sys\nimport zipimport\n\nfrom . import DistlibException, resources\nfrom .compat import StringIO\nfrom .version import get_scheme, UnsupportedVersionError\nfrom .metadata import (Metadata, METADATA_FILENAME, WHEEL_METADATA_FILENAME, LEGACY_METADATA_FILENAME)\nfrom .util import (parse_requirement, cached_property, parse_name_and_version, read_exports, write_exports, CSVReader,\n                   CSVWriter)\n\n__all__ = [\n    'Distribution', 'BaseInstalledDistribution', 'InstalledDistribution', 'EggInfoDistribution', 'DistributionPath'\n]\n\nlogger = logging.getLogger(__name__)\n\nEXPORTS_FILENAME = 'pydist-exports.json'\nCOMMANDS_FILENAME = 'pydist-commands.json'\n\nDIST_FILES = ('INSTALLER', METADATA_FILENAME, 'RECORD', 'REQUESTED', 'RESOURCES', EXPORTS_FILENAME, 'SHARED')\n\nDISTINFO_EXT = '.dist-info'\n\n\nclass _Cache(object):\n    \"\"\"\n    A simple cache mapping names and .dist-info paths to distributions\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initialise an instance. There is normally one for each DistributionPath.\n        \"\"\"\n        self.name = {}\n        self.path = {}\n        self.generated = False\n\n    def clear(self):\n        \"\"\"\n        Clear the cache, setting it to its initial state.\n        \"\"\"\n        self.name.clear()\n        self.path.clear()\n        self.generated = False\n\n    def add(self, dist):\n        \"\"\"\n        Add a distribution to the cache.\n        :param dist: The distribution to add.\n        \"\"\"\n        if dist.path not in self.path:\n            self.path[dist.path] = dist\n            self.name.setdefault(dist.key, []).append(dist)\n\n\nclass DistributionPath(object):\n    \"\"\"\n    Represents a set of distributions installed on a path (typically sys.path).\n    \"\"\"\n\n    def __init__(self, path=None, include_egg=False):\n        \"\"\"\n        Create an instance from a path, optionally including legacy (distutils/\n        setuptools/distribute) distributions.\n        :param path: The path to use, as a list of directories. If not specified,\n                     sys.path is used.\n        :param include_egg: If True, this instance will look for and return legacy\n                            distributions as well as those based on PEP 376.\n        \"\"\"\n        if path is None:\n            path = sys.path\n        self.path = path\n        self._include_dist = True\n        self._include_egg = include_egg\n\n        self._cache = _Cache()\n        self._cache_egg = _Cache()\n        self._cache_enabled = True\n        self._scheme = get_scheme('default')\n\n    def _get_cache_enabled(self):\n        return self._cache_enabled\n\n    def _set_cache_enabled(self, value):\n        self._cache_enabled = value\n\n    cache_enabled = property(_get_cache_enabled, _set_cache_enabled)\n\n    def clear_cache(self):\n        \"\"\"\n        Clears the internal cache.\n        \"\"\"\n        self._cache.clear()\n        self._cache_egg.clear()\n\n    def _yield_distributions(self):\n        \"\"\"\n        Yield .dist-info and/or .egg(-info) distributions.\n        \"\"\"\n        # We need to check if we've seen some resources already, because on\n        # some Linux systems (e.g. some Debian/Ubuntu variants) there are\n        # symlinks which alias other files in the environment.\n        seen = set()\n        for path in self.path:\n            finder = resources.finder_for_path(path)\n            if finder is None:\n                continue\n            r = finder.find('')\n            if not r or not r.is_container:\n                continue\n            rset = sorted(r.resources)\n            for entry in rset:\n                r = finder.find(entry)\n                if not r or r.path in seen:\n                    continue\n                try:\n                    if self._include_dist and entry.endswith(DISTINFO_EXT):\n                        possible_filenames = [METADATA_FILENAME, WHEEL_METADATA_FILENAME, LEGACY_METADATA_FILENAME]\n                        for metadata_filename in possible_filenames:\n                            metadata_path = posixpath.join(entry, metadata_filename)\n                            pydist = finder.find(metadata_path)\n                            if pydist:\n                                break\n                        else:\n                            continue\n\n                        with contextlib.closing(pydist.as_stream()) as stream:\n                            metadata = Metadata(fileobj=stream, scheme='legacy')\n                        logger.debug('Found %s', r.path)\n                        seen.add(r.path)\n                        yield new_dist_class(r.path, metadata=metadata, env=self)\n                    elif self._include_egg and entry.endswith(('.egg-info', '.egg')):\n                        logger.debug('Found %s', r.path)\n                        seen.add(r.path)\n                        yield old_dist_class(r.path, self)\n                except Exception as e:\n                    msg = 'Unable to read distribution at %s, perhaps due to bad metadata: %s'\n                    logger.warning(msg, r.path, e)\n                    import warnings\n                    warnings.warn(msg % (r.path, e), stacklevel=2)\n\n    def _generate_cache(self):\n        \"\"\"\n        Scan the path for distributions and populate the cache with\n        those that are found.\n        \"\"\"\n        gen_dist = not self._cache.generated\n        gen_egg = self._include_egg and not self._cache_egg.generated\n        if gen_dist or gen_egg:\n            for dist in self._yield_distributions():\n                if isinstance(dist, InstalledDistribution):\n                    self._cache.add(dist)\n                else:\n                    self._cache_egg.add(dist)\n\n            if gen_dist:\n                self._cache.generated = True\n            if gen_egg:\n                self._cache_egg.generated = True\n\n    @classmethod\n    def distinfo_dirname(cls, name, version):\n        \"\"\"\n        The *name* and *version* parameters are converted into their\n        filename-escaped form, i.e. any ``'-'`` characters are replaced\n        with ``'_'`` other than the one in ``'dist-info'`` and the one\n        separating the name from the version number.\n\n        :parameter name: is converted to a standard distribution name by replacing\n                         any runs of non- alphanumeric characters with a single\n                         ``'-'``.\n        :type name: string\n        :parameter version: is converted to a standard version string. Spaces\n                            become dots, and all other non-alphanumeric characters\n                            (except dots) become dashes, with runs of multiple\n                            dashes condensed to a single dash.\n        :type version: string\n        :returns: directory name\n        :rtype: string\"\"\"\n        name = name.replace('-', '_')\n        return '-'.join([name, version]) + DISTINFO_EXT\n\n    def get_distributions(self):\n        \"\"\"\n        Provides an iterator that looks for distributions and returns\n        :class:`InstalledDistribution` or\n        :class:`EggInfoDistribution` instances for each one of them.\n\n        :rtype: iterator of :class:`InstalledDistribution` and\n                :class:`EggInfoDistribution` instances\n        \"\"\"\n        if not self._cache_enabled:\n            for dist in self._yield_distributions():\n                yield dist\n        else:\n            self._generate_cache()\n\n            for dist in self._cache.path.values():\n                yield dist\n\n            if self._include_egg:\n                for dist in self._cache_egg.path.values():\n                    yield dist\n\n    def get_distribution(self, name):\n        \"\"\"\n        Looks for a named distribution on the path.\n\n        This function only returns the first result found, as no more than one\n        value is expected. If nothing is found, ``None`` is returned.\n\n        :rtype: :class:`InstalledDistribution`, :class:`EggInfoDistribution`\n                or ``None``\n        \"\"\"\n        result = None\n        name = name.lower()\n        if not self._cache_enabled:\n            for dist in self._yield_distributions():\n                if dist.key == name:\n                    result = dist\n                    break\n        else:\n            self._generate_cache()\n\n            if name in self._cache.name:\n                result = self._cache.name[name][0]\n            elif self._include_egg and name in self._cache_egg.name:\n                result = self._cache_egg.name[name][0]\n        return result\n\n    def provides_distribution(self, name, version=None):\n        \"\"\"\n        Iterates over all distributions to find which distributions provide *name*.\n        If a *version* is provided, it will be used to filter the results.\n\n        This function only returns the first result found, since no more than\n        one values are expected. If the directory is not found, returns ``None``.\n\n        :parameter version: a version specifier that indicates the version\n                            required, conforming to the format in ``PEP-345``\n\n        :type name: string\n        :type version: string\n        \"\"\"\n        matcher = None\n        if version is not None:\n            try:\n                matcher = self._scheme.matcher('%s (%s)' % (name, version))\n            except ValueError:\n                raise DistlibException('invalid name or version: %r, %r' % (name, version))\n\n        for dist in self.get_distributions():\n            # We hit a problem on Travis where enum34 was installed and doesn't\n            # have a provides attribute ...\n            if not hasattr(dist, 'provides'):\n                logger.debug('No \"provides\": %s', dist)\n            else:\n                provided = dist.provides\n\n                for p in provided:\n                    p_name, p_ver = parse_name_and_version(p)\n                    if matcher is None:\n                        if p_name == name:\n                            yield dist\n                            break\n                    else:\n                        if p_name == name and matcher.match(p_ver):\n                            yield dist\n                            break\n\n    def get_file_path(self, name, relative_path):\n        \"\"\"\n        Return the path to a resource file.\n        \"\"\"\n        dist = self.get_distribution(name)\n        if dist is None:\n            raise LookupError('no distribution named %r found' % name)\n        return dist.get_resource_path(relative_path)\n\n    def get_exported_entries(self, category, name=None):\n        \"\"\"\n        Return all of the exported entries in a particular category.\n\n        :param category: The category to search for entries.\n        :param name: If specified, only entries with that name are returned.\n        \"\"\"\n        for dist in self.get_distributions():\n            r = dist.exports\n            if category in r:\n                d = r[category]\n                if name is not None:\n                    if name in d:\n                        yield d[name]\n                else:\n                    for v in d.values():\n                        yield v\n\n\nclass Distribution(object):\n    \"\"\"\n    A base class for distributions, whether installed or from indexes.\n    Either way, it must have some metadata, so that's all that's needed\n    for construction.\n    \"\"\"\n\n    build_time_dependency = False\n    \"\"\"\n    Set to True if it's known to be only a build-time dependency (i.e.\n    not needed after installation).\n    \"\"\"\n\n    requested = False\n    \"\"\"A boolean that indicates whether the ``REQUESTED`` metadata file is\n    present (in other words, whether the package was installed by user\n    request or it was installed as a dependency).\"\"\"\n\n    def __init__(self, metadata):\n        \"\"\"\n        Initialise an instance.\n        :param metadata: The instance of :class:`Metadata` describing this\n        distribution.\n        \"\"\"\n        self.metadata = metadata\n        self.name = metadata.name\n        self.key = self.name.lower()  # for case-insensitive comparisons\n        self.version = metadata.version\n        self.locator = None\n        self.digest = None\n        self.extras = None  # additional features requested\n        self.context = None  # environment marker overrides\n        self.download_urls = set()\n        self.digests = {}\n\n    @property\n    def source_url(self):\n        \"\"\"\n        The source archive download URL for this distribution.\n        \"\"\"\n        return self.metadata.source_url\n\n    download_url = source_url  # Backward compatibility\n\n    @property\n    def name_and_version(self):\n        \"\"\"\n        A utility property which displays the name and version in parentheses.\n        \"\"\"\n        return '%s (%s)' % (self.name, self.version)\n\n    @property\n    def provides(self):\n        \"\"\"\n        A set of distribution names and versions provided by this distribution.\n        :return: A set of \"name (version)\" strings.\n        \"\"\"\n        plist = self.metadata.provides\n        s = '%s (%s)' % (self.name, self.version)\n        if s not in plist:\n            plist.append(s)\n        return plist\n\n    def _get_requirements(self, req_attr):\n        md = self.metadata\n        reqts = getattr(md, req_attr)\n        logger.debug('%s: got requirements %r from metadata: %r', self.name, req_attr, reqts)\n        return set(md.get_requirements(reqts, extras=self.extras, env=self.context))\n\n    @property\n    def run_requires(self):\n        return self._get_requirements('run_requires')\n\n    @property\n    def meta_requires(self):\n        return self._get_requirements('meta_requires')\n\n    @property\n    def build_requires(self):\n        return self._get_requirements('build_requires')\n\n    @property\n    def test_requires(self):\n        return self._get_requirements('test_requires')\n\n    @property\n    def dev_requires(self):\n        return self._get_requirements('dev_requires')\n\n    def matches_requirement(self, req):\n        \"\"\"\n        Say if this instance matches (fulfills) a requirement.\n        :param req: The requirement to match.\n        :rtype req: str\n        :return: True if it matches, else False.\n        \"\"\"\n        # Requirement may contain extras - parse to lose those\n        # from what's passed to the matcher\n        r = parse_requirement(req)\n        scheme = get_scheme(self.metadata.scheme)\n        try:\n            matcher = scheme.matcher(r.requirement)\n        except UnsupportedVersionError:\n            # XXX compat-mode if cannot read the version\n            logger.warning('could not read version %r - using name only', req)\n            name = req.split()[0]\n            matcher = scheme.matcher(name)\n\n        name = matcher.key  # case-insensitive\n\n        result = False\n        for p in self.provides:\n            p_name, p_ver = parse_name_and_version(p)\n            if p_name != name:\n                continue\n            try:\n                result = matcher.match(p_ver)\n                break\n            except UnsupportedVersionError:\n                pass\n        return result\n\n    def __repr__(self):\n        \"\"\"\n        Return a textual representation of this instance,\n        \"\"\"\n        if self.source_url:\n            suffix = ' [%s]' % self.source_url\n        else:\n            suffix = ''\n        return '<Distribution %s (%s)%s>' % (self.name, self.version, suffix)\n\n    def __eq__(self, other):\n        \"\"\"\n        See if this distribution is the same as another.\n        :param other: The distribution to compare with. To be equal to one\n                      another. distributions must have the same type, name,\n                      version and source_url.\n        :return: True if it is the same, else False.\n        \"\"\"\n        if type(other) is not type(self):\n            result = False\n        else:\n            result = (self.name == other.name and self.version == other.version and self.source_url == other.source_url)\n        return result\n\n    def __hash__(self):\n        \"\"\"\n        Compute hash in a way which matches the equality test.\n        \"\"\"\n        return hash(self.name) + hash(self.version) + hash(self.source_url)\n\n\nclass BaseInstalledDistribution(Distribution):\n    \"\"\"\n    This is the base class for installed distributions (whether PEP 376 or\n    legacy).\n    \"\"\"\n\n    hasher = None\n\n    def __init__(self, metadata, path, env=None):\n        \"\"\"\n        Initialise an instance.\n        :param metadata: An instance of :class:`Metadata` which describes the\n                         distribution. This will normally have been initialised\n                         from a metadata file in the ``path``.\n        :param path:     The path of the ``.dist-info`` or ``.egg-info``\n                         directory for the distribution.\n        :param env:      This is normally the :class:`DistributionPath`\n                         instance where this distribution was found.\n        \"\"\"\n        super(BaseInstalledDistribution, self).__init__(metadata)\n        self.path = path\n        self.dist_path = env\n\n    def get_hash(self, data, hasher=None):\n        \"\"\"\n        Get the hash of some data, using a particular hash algorithm, if\n        specified.\n\n        :param data: The data to be hashed.\n        :type data: bytes\n        :param hasher: The name of a hash implementation, supported by hashlib,\n                       or ``None``. Examples of valid values are ``'sha1'``,\n                       ``'sha224'``, ``'sha384'``, '``sha256'``, ``'md5'`` and\n                       ``'sha512'``. If no hasher is specified, the ``hasher``\n                       attribute of the :class:`InstalledDistribution` instance\n                       is used. If the hasher is determined to be ``None``, MD5\n                       is used as the hashing algorithm.\n        :returns: The hash of the data. If a hasher was explicitly specified,\n                  the returned hash will be prefixed with the specified hasher\n                  followed by '='.\n        :rtype: str\n        \"\"\"\n        if hasher is None:\n            hasher = self.hasher\n        if hasher is None:\n            hasher = hashlib.md5\n            prefix = ''\n        else:\n            hasher = getattr(hashlib, hasher)\n            prefix = '%s=' % self.hasher\n        digest = hasher(data).digest()\n        digest = base64.urlsafe_b64encode(digest).rstrip(b'=').decode('ascii')\n        return '%s%s' % (prefix, digest)\n\n\nclass InstalledDistribution(BaseInstalledDistribution):\n    \"\"\"\n    Created with the *path* of the ``.dist-info`` directory provided to the\n    constructor. It reads the metadata contained in ``pydist.json`` when it is\n    instantiated., or uses a passed in Metadata instance (useful for when\n    dry-run mode is being used).\n    \"\"\"\n\n    hasher = 'sha256'\n\n    def __init__(self, path, metadata=None, env=None):\n        self.modules = []\n        self.finder = finder = resources.finder_for_path(path)\n        if finder is None:\n            raise ValueError('finder unavailable for %s' % path)\n        if env and env._cache_enabled and path in env._cache.path:\n            metadata = env._cache.path[path].metadata\n        elif metadata is None:\n            r = finder.find(METADATA_FILENAME)\n            # Temporary - for Wheel 0.23 support\n            if r is None:\n                r = finder.find(WHEEL_METADATA_FILENAME)\n            # Temporary - for legacy support\n            if r is None:\n                r = finder.find(LEGACY_METADATA_FILENAME)\n            if r is None:\n                raise ValueError('no %s found in %s' % (METADATA_FILENAME, path))\n            with contextlib.closing(r.as_stream()) as stream:\n                metadata = Metadata(fileobj=stream, scheme='legacy')\n\n        super(InstalledDistribution, self).__init__(metadata, path, env)\n\n        if env and env._cache_enabled:\n            env._cache.add(self)\n\n        r = finder.find('REQUESTED')\n        self.requested = r is not None\n        p = os.path.join(path, 'top_level.txt')\n        if os.path.exists(p):\n            with open(p, 'rb') as f:\n                data = f.read().decode('utf-8')\n            self.modules = data.splitlines()\n\n    def __repr__(self):\n        return '<InstalledDistribution %r %s at %r>' % (self.name, self.version, self.path)\n\n    def __str__(self):\n        return \"%s %s\" % (self.name, self.version)\n\n    def _get_records(self):\n        \"\"\"\n        Get the list of installed files for the distribution\n        :return: A list of tuples of path, hash and size. Note that hash and\n                 size might be ``None`` for some entries. The path is exactly\n                 as stored in the file (which is as in PEP 376).\n        \"\"\"\n        results = []\n        r = self.get_distinfo_resource('RECORD')\n        with contextlib.closing(r.as_stream()) as stream:\n            with CSVReader(stream=stream) as record_reader:\n                # Base location is parent dir of .dist-info dir\n                # base_location = os.path.dirname(self.path)\n                # base_location = os.path.abspath(base_location)\n                for row in record_reader:\n                    missing = [None for i in range(len(row), 3)]\n                    path, checksum, size = row + missing\n                    # if not os.path.isabs(path):\n                    #     path = path.replace('/', os.sep)\n                    #     path = os.path.join(base_location, path)\n                    results.append((path, checksum, size))\n        return results\n\n    @cached_property\n    def exports(self):\n        \"\"\"\n        Return the information exported by this distribution.\n        :return: A dictionary of exports, mapping an export category to a dict\n                 of :class:`ExportEntry` instances describing the individual\n                 export entries, and keyed by name.\n        \"\"\"\n        result = {}\n        r = self.get_distinfo_resource(EXPORTS_FILENAME)\n        if r:\n            result = self.read_exports()\n        return result\n\n    def read_exports(self):\n        \"\"\"\n        Read exports data from a file in .ini format.\n\n        :return: A dictionary of exports, mapping an export category to a list\n                 of :class:`ExportEntry` instances describing the individual\n                 export entries.\n        \"\"\"\n        result = {}\n        r = self.get_distinfo_resource(EXPORTS_FILENAME)\n        if r:\n            with contextlib.closing(r.as_stream()) as stream:\n                result = read_exports(stream)\n        return result\n\n    def write_exports(self, exports):\n        \"\"\"\n        Write a dictionary of exports to a file in .ini format.\n        :param exports: A dictionary of exports, mapping an export category to\n                        a list of :class:`ExportEntry` instances describing the\n                        individual export entries.\n        \"\"\"\n        rf = self.get_distinfo_file(EXPORTS_FILENAME)\n        with open(rf, 'w') as f:\n            write_exports(exports, f)\n\n    def get_resource_path(self, relative_path):\n        \"\"\"\n        NOTE: This API may change in the future.\n\n        Return the absolute path to a resource file with the given relative\n        path.\n\n        :param relative_path: The path, relative to .dist-info, of the resource\n                              of interest.\n        :return: The absolute path where the resource is to be found.\n        \"\"\"\n        r = self.get_distinfo_resource('RESOURCES')\n        with contextlib.closing(r.as_stream()) as stream:\n            with CSVReader(stream=stream) as resources_reader:\n                for relative, destination in resources_reader:\n                    if relative == relative_path:\n                        return destination\n        raise KeyError('no resource file with relative path %r '\n                       'is installed' % relative_path)\n\n    def list_installed_files(self):\n        \"\"\"\n        Iterates over the ``RECORD`` entries and returns a tuple\n        ``(path, hash, size)`` for each line.\n\n        :returns: iterator of (path, hash, size)\n        \"\"\"\n        for result in self._get_records():\n            yield result\n\n    def write_installed_files(self, paths, prefix, dry_run=False):\n        \"\"\"\n        Writes the ``RECORD`` file, using the ``paths`` iterable passed in. Any\n        existing ``RECORD`` file is silently overwritten.\n\n        prefix is used to determine when to write absolute paths.\n        \"\"\"\n        prefix = os.path.join(prefix, '')\n        base = os.path.dirname(self.path)\n        base_under_prefix = base.startswith(prefix)\n        base = os.path.join(base, '')\n        record_path = self.get_distinfo_file('RECORD')\n        logger.info('creating %s', record_path)\n        if dry_run:\n            return None\n        with CSVWriter(record_path) as writer:\n            for path in paths:\n                if os.path.isdir(path) or path.endswith(('.pyc', '.pyo')):\n                    # do not put size and hash, as in PEP-376\n                    hash_value = size = ''\n                else:\n                    size = '%d' % os.path.getsize(path)\n                    with open(path, 'rb') as fp:\n                        hash_value = self.get_hash(fp.read())\n                if path.startswith(base) or (base_under_prefix and path.startswith(prefix)):\n                    path = os.path.relpath(path, base)\n                writer.writerow((path, hash_value, size))\n\n            # add the RECORD file itself\n            if record_path.startswith(base):\n                record_path = os.path.relpath(record_path, base)\n            writer.writerow((record_path, '', ''))\n        return record_path\n\n    def check_installed_files(self):\n        \"\"\"\n        Checks that the hashes and sizes of the files in ``RECORD`` are\n        matched by the files themselves. Returns a (possibly empty) list of\n        mismatches. Each entry in the mismatch list will be a tuple consisting\n        of the path, 'exists', 'size' or 'hash' according to what didn't match\n        (existence is checked first, then size, then hash), the expected\n        value and the actual value.\n        \"\"\"\n        mismatches = []\n        base = os.path.dirname(self.path)\n        record_path = self.get_distinfo_file('RECORD')\n        for path, hash_value, size in self.list_installed_files():\n            if not os.path.isabs(path):\n                path = os.path.join(base, path)\n            if path == record_path:\n                continue\n            if not os.path.exists(path):\n                mismatches.append((path, 'exists', True, False))\n            elif os.path.isfile(path):\n                actual_size = str(os.path.getsize(path))\n                if size and actual_size != size:\n                    mismatches.append((path, 'size', size, actual_size))\n                elif hash_value:\n                    if '=' in hash_value:\n                        hasher = hash_value.split('=', 1)[0]\n                    else:\n                        hasher = None\n\n                    with open(path, 'rb') as f:\n                        actual_hash = self.get_hash(f.read(), hasher)\n                        if actual_hash != hash_value:\n                            mismatches.append((path, 'hash', hash_value, actual_hash))\n        return mismatches\n\n    @cached_property\n    def shared_locations(self):\n        \"\"\"\n        A dictionary of shared locations whose keys are in the set 'prefix',\n        'purelib', 'platlib', 'scripts', 'headers', 'data' and 'namespace'.\n        The corresponding value is the absolute path of that category for\n        this distribution, and takes into account any paths selected by the\n        user at installation time (e.g. via command-line arguments). In the\n        case of the 'namespace' key, this would be a list of absolute paths\n        for the roots of namespace packages in this distribution.\n\n        The first time this property is accessed, the relevant information is\n        read from the SHARED file in the .dist-info directory.\n        \"\"\"\n        result = {}\n        shared_path = os.path.join(self.path, 'SHARED')\n        if os.path.isfile(shared_path):\n            with codecs.open(shared_path, 'r', encoding='utf-8') as f:\n                lines = f.read().splitlines()\n            for line in lines:\n                key, value = line.split('=', 1)\n                if key == 'namespace':\n                    result.setdefault(key, []).append(value)\n                else:\n                    result[key] = value\n        return result\n\n    def write_shared_locations(self, paths, dry_run=False):\n        \"\"\"\n        Write shared location information to the SHARED file in .dist-info.\n        :param paths: A dictionary as described in the documentation for\n        :meth:`shared_locations`.\n        :param dry_run: If True, the action is logged but no file is actually\n                        written.\n        :return: The path of the file written to.\n        \"\"\"\n        shared_path = os.path.join(self.path, 'SHARED')\n        logger.info('creating %s', shared_path)\n        if dry_run:\n            return None\n        lines = []\n        for key in ('prefix', 'lib', 'headers', 'scripts', 'data'):\n            path = paths[key]\n            if os.path.isdir(paths[key]):\n                lines.append('%s=%s' % (key, path))\n        for ns in paths.get('namespace', ()):\n            lines.append('namespace=%s' % ns)\n\n        with codecs.open(shared_path, 'w', encoding='utf-8') as f:\n            f.write('\\n'.join(lines))\n        return shared_path\n\n    def get_distinfo_resource(self, path):\n        if path not in DIST_FILES:\n            raise DistlibException('invalid path for a dist-info file: '\n                                   '%r at %r' % (path, self.path))\n        finder = resources.finder_for_path(self.path)\n        if finder is None:\n            raise DistlibException('Unable to get a finder for %s' % self.path)\n        return finder.find(path)\n\n    def get_distinfo_file(self, path):\n        \"\"\"\n        Returns a path located under the ``.dist-info`` directory. Returns a\n        string representing the path.\n\n        :parameter path: a ``'/'``-separated path relative to the\n                         ``.dist-info`` directory or an absolute path;\n                         If *path* is an absolute path and doesn't start\n                         with the ``.dist-info`` directory path,\n                         a :class:`DistlibException` is raised\n        :type path: str\n        :rtype: str\n        \"\"\"\n        # Check if it is an absolute path  # XXX use relpath, add tests\n        if path.find(os.sep) >= 0:\n            # it's an absolute path?\n            distinfo_dirname, path = path.split(os.sep)[-2:]\n            if distinfo_dirname != self.path.split(os.sep)[-1]:\n                raise DistlibException('dist-info file %r does not belong to the %r %s '\n                                       'distribution' % (path, self.name, self.version))\n\n        # The file must be relative\n        if path not in DIST_FILES:\n            raise DistlibException('invalid path for a dist-info file: '\n                                   '%r at %r' % (path, self.path))\n\n        return os.path.join(self.path, path)\n\n    def list_distinfo_files(self):\n        \"\"\"\n        Iterates over the ``RECORD`` entries and returns paths for each line if\n        the path is pointing to a file located in the ``.dist-info`` directory\n        or one of its subdirectories.\n\n        :returns: iterator of paths\n        \"\"\"\n        base = os.path.dirname(self.path)\n        for path, checksum, size in self._get_records():\n            # XXX add separator or use real relpath algo\n            if not os.path.isabs(path):\n                path = os.path.join(base, path)\n            if path.startswith(self.path):\n                yield path\n\n    def __eq__(self, other):\n        return (isinstance(other, InstalledDistribution) and self.path == other.path)\n\n    # See http://docs.python.org/reference/datamodel#object.__hash__\n    __hash__ = object.__hash__\n\n\nclass EggInfoDistribution(BaseInstalledDistribution):\n    \"\"\"Created with the *path* of the ``.egg-info`` directory or file provided\n    to the constructor. It reads the metadata contained in the file itself, or\n    if the given path happens to be a directory, the metadata is read from the\n    file ``PKG-INFO`` under that directory.\"\"\"\n\n    requested = True  # as we have no way of knowing, assume it was\n    shared_locations = {}\n\n    def __init__(self, path, env=None):\n\n        def set_name_and_version(s, n, v):\n            s.name = n\n            s.key = n.lower()  # for case-insensitive comparisons\n            s.version = v\n\n        self.path = path\n        self.dist_path = env\n        if env and env._cache_enabled and path in env._cache_egg.path:\n            metadata = env._cache_egg.path[path].metadata\n            set_name_and_version(self, metadata.name, metadata.version)\n        else:\n            metadata = self._get_metadata(path)\n\n            # Need to be set before caching\n            set_name_and_version(self, metadata.name, metadata.version)\n\n            if env and env._cache_enabled:\n                env._cache_egg.add(self)\n        super(EggInfoDistribution, self).__init__(metadata, path, env)\n\n    def _get_metadata(self, path):\n        requires = None\n\n        def parse_requires_data(data):\n            \"\"\"Create a list of dependencies from a requires.txt file.\n\n            *data*: the contents of a setuptools-produced requires.txt file.\n            \"\"\"\n            reqs = []\n            lines = data.splitlines()\n            for line in lines:\n                line = line.strip()\n                # sectioned files have bare newlines (separating sections)\n                if not line:  # pragma: no cover\n                    continue\n                if line.startswith('['):  # pragma: no cover\n                    logger.warning('Unexpected line: quitting requirement scan: %r', line)\n                    break\n                r = parse_requirement(line)\n                if not r:  # pragma: no cover\n                    logger.warning('Not recognised as a requirement: %r', line)\n                    continue\n                if r.extras:  # pragma: no cover\n                    logger.warning('extra requirements in requires.txt are '\n                                   'not supported')\n                if not r.constraints:\n                    reqs.append(r.name)\n                else:\n                    cons = ', '.join('%s%s' % c for c in r.constraints)\n                    reqs.append('%s (%s)' % (r.name, cons))\n            return reqs\n\n        def parse_requires_path(req_path):\n            \"\"\"Create a list of dependencies from a requires.txt file.\n\n            *req_path*: the path to a setuptools-produced requires.txt file.\n            \"\"\"\n\n            reqs = []\n            try:\n                with codecs.open(req_path, 'r', 'utf-8') as fp:\n                    reqs = parse_requires_data(fp.read())\n            except IOError:\n                pass\n            return reqs\n\n        tl_path = tl_data = None\n        if path.endswith('.egg'):\n            if os.path.isdir(path):\n                p = os.path.join(path, 'EGG-INFO')\n                meta_path = os.path.join(p, 'PKG-INFO')\n                metadata = Metadata(path=meta_path, scheme='legacy')\n                req_path = os.path.join(p, 'requires.txt')\n                tl_path = os.path.join(p, 'top_level.txt')\n                requires = parse_requires_path(req_path)\n            else:\n                # FIXME handle the case where zipfile is not available\n                zipf = zipimport.zipimporter(path)\n                fileobj = StringIO(zipf.get_data('EGG-INFO/PKG-INFO').decode('utf8'))\n                metadata = Metadata(fileobj=fileobj, scheme='legacy')\n                try:\n                    data = zipf.get_data('EGG-INFO/requires.txt')\n                    tl_data = zipf.get_data('EGG-INFO/top_level.txt').decode('utf-8')\n                    requires = parse_requires_data(data.decode('utf-8'))\n                except IOError:\n                    requires = None\n        elif path.endswith('.egg-info'):\n            if os.path.isdir(path):\n                req_path = os.path.join(path, 'requires.txt')\n                requires = parse_requires_path(req_path)\n                path = os.path.join(path, 'PKG-INFO')\n                tl_path = os.path.join(path, 'top_level.txt')\n            metadata = Metadata(path=path, scheme='legacy')\n        else:\n            raise DistlibException('path must end with .egg-info or .egg, '\n                                   'got %r' % path)\n\n        if requires:\n            metadata.add_requirements(requires)\n        # look for top-level modules in top_level.txt, if present\n        if tl_data is None:\n            if tl_path is not None and os.path.exists(tl_path):\n                with open(tl_path, 'rb') as f:\n                    tl_data = f.read().decode('utf-8')\n        if not tl_data:\n            tl_data = []\n        else:\n            tl_data = tl_data.splitlines()\n        self.modules = tl_data\n        return metadata\n\n    def __repr__(self):\n        return '<EggInfoDistribution %r %s at %r>' % (self.name, self.version, self.path)\n\n    def __str__(self):\n        return \"%s %s\" % (self.name, self.version)\n\n    def check_installed_files(self):\n        \"\"\"\n        Checks that the hashes and sizes of the files in ``RECORD`` are\n        matched by the files themselves. Returns a (possibly empty) list of\n        mismatches. Each entry in the mismatch list will be a tuple consisting\n        of the path, 'exists', 'size' or 'hash' according to what didn't match\n        (existence is checked first, then size, then hash), the expected\n        value and the actual value.\n        \"\"\"\n        mismatches = []\n        record_path = os.path.join(self.path, 'installed-files.txt')\n        if os.path.exists(record_path):\n            for path, _, _ in self.list_installed_files():\n                if path == record_path:\n                    continue\n                if not os.path.exists(path):\n                    mismatches.append((path, 'exists', True, False))\n        return mismatches\n\n    def list_installed_files(self):\n        \"\"\"\n        Iterates over the ``installed-files.txt`` entries and returns a tuple\n        ``(path, hash, size)`` for each line.\n\n        :returns: a list of (path, hash, size)\n        \"\"\"\n\n        def _md5(path):\n            f = open(path, 'rb')\n            try:\n                content = f.read()\n            finally:\n                f.close()\n            return hashlib.md5(content).hexdigest()\n\n        def _size(path):\n            return os.stat(path).st_size\n\n        record_path = os.path.join(self.path, 'installed-files.txt')\n        result = []\n        if os.path.exists(record_path):\n            with codecs.open(record_path, 'r', encoding='utf-8') as f:\n                for line in f:\n                    line = line.strip()\n                    p = os.path.normpath(os.path.join(self.path, line))\n                    # \"./\" is present as a marker between installed files\n                    # and installation metadata files\n                    if not os.path.exists(p):\n                        logger.warning('Non-existent file: %s', p)\n                        if p.endswith(('.pyc', '.pyo')):\n                            continue\n                        # otherwise fall through and fail\n                    if not os.path.isdir(p):\n                        result.append((p, _md5(p), _size(p)))\n            result.append((record_path, None, None))\n        return result\n\n    def list_distinfo_files(self, absolute=False):\n        \"\"\"\n        Iterates over the ``installed-files.txt`` entries and returns paths for\n        each line if the path is pointing to a file located in the\n        ``.egg-info`` directory or one of its subdirectories.\n\n        :parameter absolute: If *absolute* is ``True``, each returned path is\n                          transformed into a local absolute path. Otherwise the\n                          raw value from ``installed-files.txt`` is returned.\n        :type absolute: boolean\n        :returns: iterator of paths\n        \"\"\"\n        record_path = os.path.join(self.path, 'installed-files.txt')\n        if os.path.exists(record_path):\n            skip = True\n            with codecs.open(record_path, 'r', encoding='utf-8') as f:\n                for line in f:\n                    line = line.strip()\n                    if line == './':\n                        skip = False\n                        continue\n                    if not skip:\n                        p = os.path.normpath(os.path.join(self.path, line))\n                        if p.startswith(self.path):\n                            if absolute:\n                                yield p\n                            else:\n                                yield line\n\n    def __eq__(self, other):\n        return (isinstance(other, EggInfoDistribution) and self.path == other.path)\n\n    # See http://docs.python.org/reference/datamodel#object.__hash__\n    __hash__ = object.__hash__\n\n\nnew_dist_class = InstalledDistribution\nold_dist_class = EggInfoDistribution\n\n\nclass DependencyGraph(object):\n    \"\"\"\n    Represents a dependency graph between distributions.\n\n    The dependency relationships are stored in an ``adjacency_list`` that maps\n    distributions to a list of ``(other, label)`` tuples where  ``other``\n    is a distribution and the edge is labeled with ``label`` (i.e. the version\n    specifier, if such was provided). Also, for more efficient traversal, for\n    every distribution ``x``, a list of predecessors is kept in\n    ``reverse_list[x]``. An edge from distribution ``a`` to\n    distribution ``b`` means that ``a`` depends on ``b``. If any missing\n    dependencies are found, they are stored in ``missing``, which is a\n    dictionary that maps distributions to a list of requirements that were not\n    provided by any other distributions.\n    \"\"\"\n\n    def __init__(self):\n        self.adjacency_list = {}\n        self.reverse_list = {}\n        self.missing = {}\n\n    def add_distribution(self, distribution):\n        \"\"\"Add the *distribution* to the graph.\n\n        :type distribution: :class:`distutils2.database.InstalledDistribution`\n                            or :class:`distutils2.database.EggInfoDistribution`\n        \"\"\"\n        self.adjacency_list[distribution] = []\n        self.reverse_list[distribution] = []\n        # self.missing[distribution] = []\n\n    def add_edge(self, x, y, label=None):\n        \"\"\"Add an edge from distribution *x* to distribution *y* with the given\n        *label*.\n\n        :type x: :class:`distutils2.database.InstalledDistribution` or\n                 :class:`distutils2.database.EggInfoDistribution`\n        :type y: :class:`distutils2.database.InstalledDistribution` or\n                 :class:`distutils2.database.EggInfoDistribution`\n        :type label: ``str`` or ``None``\n        \"\"\"\n        self.adjacency_list[x].append((y, label))\n        # multiple edges are allowed, so be careful\n        if x not in self.reverse_list[y]:\n            self.reverse_list[y].append(x)\n\n    def add_missing(self, distribution, requirement):\n        \"\"\"\n        Add a missing *requirement* for the given *distribution*.\n\n        :type distribution: :class:`distutils2.database.InstalledDistribution`\n                            or :class:`distutils2.database.EggInfoDistribution`\n        :type requirement: ``str``\n        \"\"\"\n        logger.debug('%s missing %r', distribution, requirement)\n        self.missing.setdefault(distribution, []).append(requirement)\n\n    def _repr_dist(self, dist):\n        return '%s %s' % (dist.name, dist.version)\n\n    def repr_node(self, dist, level=1):\n        \"\"\"Prints only a subgraph\"\"\"\n        output = [self._repr_dist(dist)]\n        for other, label in self.adjacency_list[dist]:\n            dist = self._repr_dist(other)\n            if label is not None:\n                dist = '%s [%s]' % (dist, label)\n            output.append('    ' * level + str(dist))\n            suboutput = self.repr_node(other, level + 1)\n            subs = suboutput.split('\\n')\n            output.extend(subs[1:])\n        return '\\n'.join(output)\n\n    def to_dot(self, f, skip_disconnected=True):\n        \"\"\"Writes a DOT output for the graph to the provided file *f*.\n\n        If *skip_disconnected* is set to ``True``, then all distributions\n        that are not dependent on any other distribution are skipped.\n\n        :type f: has to support ``file``-like operations\n        :type skip_disconnected: ``bool``\n        \"\"\"\n        disconnected = []\n\n        f.write(\"digraph dependencies {\\n\")\n        for dist, adjs in self.adjacency_list.items():\n            if len(adjs) == 0 and not skip_disconnected:\n                disconnected.append(dist)\n            for other, label in adjs:\n                if label is not None:\n                    f.write('\"%s\" -> \"%s\" [label=\"%s\"]\\n' % (dist.name, other.name, label))\n                else:\n                    f.write('\"%s\" -> \"%s\"\\n' % (dist.name, other.name))\n        if not skip_disconnected and len(disconnected) > 0:\n            f.write('subgraph disconnected {\\n')\n            f.write('label = \"Disconnected\"\\n')\n            f.write('bgcolor = red\\n')\n\n            for dist in disconnected:\n                f.write('\"%s\"' % dist.name)\n                f.write('\\n')\n            f.write('}\\n')\n        f.write('}\\n')\n\n    def topological_sort(self):\n        \"\"\"\n        Perform a topological sort of the graph.\n        :return: A tuple, the first element of which is a topologically sorted\n                 list of distributions, and the second element of which is a\n                 list of distributions that cannot be sorted because they have\n                 circular dependencies and so form a cycle.\n        \"\"\"\n        result = []\n        # Make a shallow copy of the adjacency list\n        alist = {}\n        for k, v in self.adjacency_list.items():\n            alist[k] = v[:]\n        while True:\n            # See what we can remove in this run\n            to_remove = []\n            for k, v in list(alist.items())[:]:\n                if not v:\n                    to_remove.append(k)\n                    del alist[k]\n            if not to_remove:\n                # What's left in alist (if anything) is a cycle.\n                break\n            # Remove from the adjacency list of others\n            for k, v in alist.items():\n                alist[k] = [(d, r) for d, r in v if d not in to_remove]\n            logger.debug('Moving to result: %s', ['%s (%s)' % (d.name, d.version) for d in to_remove])\n            result.extend(to_remove)\n        return result, list(alist.keys())\n\n    def __repr__(self):\n        \"\"\"Representation of the graph\"\"\"\n        output = []\n        for dist, adjs in self.adjacency_list.items():\n            output.append(self.repr_node(dist))\n        return '\\n'.join(output)\n\n\ndef make_graph(dists, scheme='default'):\n    \"\"\"Makes a dependency graph from the given distributions.\n\n    :parameter dists: a list of distributions\n    :type dists: list of :class:`distutils2.database.InstalledDistribution` and\n                 :class:`distutils2.database.EggInfoDistribution` instances\n    :rtype: a :class:`DependencyGraph` instance\n    \"\"\"\n    scheme = get_scheme(scheme)\n    graph = DependencyGraph()\n    provided = {}  # maps names to lists of (version, dist) tuples\n\n    # first, build the graph and find out what's provided\n    for dist in dists:\n        graph.add_distribution(dist)\n\n        for p in dist.provides:\n            name, version = parse_name_and_version(p)\n            logger.debug('Add to provided: %s, %s, %s', name, version, dist)\n            provided.setdefault(name, []).append((version, dist))\n\n    # now make the edges\n    for dist in dists:\n        requires = (dist.run_requires | dist.meta_requires | dist.build_requires | dist.dev_requires)\n        for req in requires:\n            try:\n                matcher = scheme.matcher(req)\n            except UnsupportedVersionError:\n                # XXX compat-mode if cannot read the version\n                logger.warning('could not read version %r - using name only', req)\n                name = req.split()[0]\n                matcher = scheme.matcher(name)\n\n            name = matcher.key  # case-insensitive\n\n            matched = False\n            if name in provided:\n                for version, provider in provided[name]:\n                    try:\n                        match = matcher.match(version)\n                    except UnsupportedVersionError:\n                        match = False\n\n                    if match:\n                        graph.add_edge(dist, provider, req)\n                        matched = True\n                        break\n            if not matched:\n                graph.add_missing(dist, req)\n    return graph\n\n\ndef get_dependent_dists(dists, dist):\n    \"\"\"Recursively generate a list of distributions from *dists* that are\n    dependent on *dist*.\n\n    :param dists: a list of distributions\n    :param dist: a distribution, member of *dists* for which we are interested\n    \"\"\"\n    if dist not in dists:\n        raise DistlibException('given distribution %r is not a member '\n                               'of the list' % dist.name)\n    graph = make_graph(dists)\n\n    dep = [dist]  # dependent distributions\n    todo = graph.reverse_list[dist]  # list of nodes we should inspect\n\n    while todo:\n        d = todo.pop()\n        dep.append(d)\n        for succ in graph.reverse_list[d]:\n            if succ not in dep:\n                todo.append(succ)\n\n    dep.pop(0)  # remove dist from dep, was there to prevent infinite loops\n    return dep\n\n\ndef get_required_dists(dists, dist):\n    \"\"\"Recursively generate a list of distributions from *dists* that are\n    required by *dist*.\n\n    :param dists: a list of distributions\n    :param dist: a distribution, member of *dists* for which we are interested\n                 in finding the dependencies.\n    \"\"\"\n    if dist not in dists:\n        raise DistlibException('given distribution %r is not a member '\n                               'of the list' % dist.name)\n    graph = make_graph(dists)\n\n    req = set()  # required distributions\n    todo = graph.adjacency_list[dist]  # list of nodes we should inspect\n    seen = set(t[0] for t in todo)  # already added to todo\n\n    while todo:\n        d = todo.pop()[0]\n        req.add(d)\n        pred_list = graph.adjacency_list[d]\n        for pred in pred_list:\n            d = pred[0]\n            if d not in req and d not in seen:\n                seen.add(d)\n                todo.append(pred)\n    return req\n\n\ndef make_dist(name, version, **kwargs):\n    \"\"\"\n    A convenience method for making a dist given just a name and version.\n    \"\"\"\n    summary = kwargs.pop('summary', 'Placeholder for summary')\n    md = Metadata(**kwargs)\n    md.name = name\n    md.version = version\n    md.summary = summary or 'Placeholder for summary'\n    return Distribution(md)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_vendor/distlib/index.py","size":20797,"sha1":"742277dd9d3c629a01057e27fdf3ab7233024167","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"# -*- coding: utf-8 -*-\n#\n# Copyright (C) 2013-2023 Vinay Sajip.\n# Licensed to the Python Software Foundation under a contributor agreement.\n# See LICENSE.txt and CONTRIBUTORS.txt.\n#\nimport hashlib\nimport logging\nimport os\nimport shutil\nimport subprocess\nimport tempfile\ntry:\n    from threading import Thread\nexcept ImportError:  # pragma: no cover\n    from dummy_threading import Thread\n\nfrom . import DistlibException\nfrom .compat import (HTTPBasicAuthHandler, Request, HTTPPasswordMgr,\n                     urlparse, build_opener, string_types)\nfrom .util import zip_dir, ServerProxy\n\nlogger = logging.getLogger(__name__)\n\nDEFAULT_INDEX = 'https://pypi.org/pypi'\nDEFAULT_REALM = 'pypi'\n\n\nclass PackageIndex(object):\n    \"\"\"\n    This class represents a package index compatible with PyPI, the Python\n    Package Index.\n    \"\"\"\n\n    boundary = b'----------ThIs_Is_tHe_distlib_index_bouNdaRY_$'\n\n    def __init__(self, url=None):\n        \"\"\"\n        Initialise an instance.\n\n        :param url: The URL of the index. If not specified, the URL for PyPI is\n                    used.\n        \"\"\"\n        self.url = url or DEFAULT_INDEX\n        self.read_configuration()\n        scheme, netloc, path, params, query, frag = urlparse(self.url)\n        if params or query or frag or scheme not in ('http', 'https'):\n            raise DistlibException('invalid repository: %s' % self.url)\n        self.password_handler = None\n        self.ssl_verifier = None\n        self.gpg = None\n        self.gpg_home = None\n        with open(os.devnull, 'w') as sink:\n            # Use gpg by default rather than gpg2, as gpg2 insists on\n            # prompting for passwords\n            for s in ('gpg', 'gpg2'):\n                try:\n                    rc = subprocess.check_call([s, '--version'], stdout=sink,\n                                               stderr=sink)\n                    if rc == 0:\n                        self.gpg = s\n                        break\n                except OSError:\n                    pass\n\n    def _get_pypirc_command(self):\n        \"\"\"\n        Get the distutils command for interacting with PyPI configurations.\n        :return: the command.\n        \"\"\"\n        from .util import _get_pypirc_command as cmd\n        return cmd()\n\n    def read_configuration(self):\n        \"\"\"\n        Read the PyPI access configuration as supported by distutils. This populates\n        ``username``, ``password``, ``realm`` and ``url`` attributes from the\n        configuration.\n        \"\"\"\n        from .util import _load_pypirc\n        cfg = _load_pypirc(self)\n        self.username = cfg.get('username')\n        self.password = cfg.get('password')\n        self.realm = cfg.get('realm', 'pypi')\n        self.url = cfg.get('repository', self.url)\n\n    def save_configuration(self):\n        \"\"\"\n        Save the PyPI access configuration. You must have set ``username`` and\n        ``password`` attributes before calling this method.\n        \"\"\"\n        self.check_credentials()\n        from .util import _store_pypirc\n        _store_pypirc(self)\n\n    def check_credentials(self):\n        \"\"\"\n        Check that ``username`` and ``password`` have been set, and raise an\n        exception if not.\n        \"\"\"\n        if self.username is None or self.password is None:\n            raise DistlibException('username and password must be set')\n        pm = HTTPPasswordMgr()\n        _, netloc, _, _, _, _ = urlparse(self.url)\n        pm.add_password(self.realm, netloc, self.username, self.password)\n        self.password_handler = HTTPBasicAuthHandler(pm)\n\n    def register(self, metadata):  # pragma: no cover\n        \"\"\"\n        Register a distribution on PyPI, using the provided metadata.\n\n        :param metadata: A :class:`Metadata` instance defining at least a name\n                         and version number for the distribution to be\n                         registered.\n        :return: The HTTP response received from PyPI upon submission of the\n                request.\n        \"\"\"\n        self.check_credentials()\n        metadata.validate()\n        d = metadata.todict()\n        d[':action'] = 'verify'\n        request = self.encode_request(d.items(), [])\n        self.send_request(request)\n        d[':action'] = 'submit'\n        request = self.encode_request(d.items(), [])\n        return self.send_request(request)\n\n    def _reader(self, name, stream, outbuf):\n        \"\"\"\n        Thread runner for reading lines of from a subprocess into a buffer.\n\n        :param name: The logical name of the stream (used for logging only).\n        :param stream: The stream to read from. This will typically a pipe\n                       connected to the output stream of a subprocess.\n        :param outbuf: The list to append the read lines to.\n        \"\"\"\n        while True:\n            s = stream.readline()\n            if not s:\n                break\n            s = s.decode('utf-8').rstrip()\n            outbuf.append(s)\n            logger.debug('%s: %s' % (name, s))\n        stream.close()\n\n    def get_sign_command(self, filename, signer, sign_password, keystore=None):  # pragma: no cover\n        \"\"\"\n        Return a suitable command for signing a file.\n\n        :param filename: The pathname to the file to be signed.\n        :param signer: The identifier of the signer of the file.\n        :param sign_password: The passphrase for the signer's\n                              private key used for signing.\n        :param keystore: The path to a directory which contains the keys\n                         used in verification. If not specified, the\n                         instance's ``gpg_home`` attribute is used instead.\n        :return: The signing command as a list suitable to be\n                 passed to :class:`subprocess.Popen`.\n        \"\"\"\n        cmd = [self.gpg, '--status-fd', '2', '--no-tty']\n        if keystore is None:\n            keystore = self.gpg_home\n        if keystore:\n            cmd.extend(['--homedir', keystore])\n        if sign_password is not None:\n            cmd.extend(['--batch', '--passphrase-fd', '0'])\n        td = tempfile.mkdtemp()\n        sf = os.path.join(td, os.path.basename(filename) + '.asc')\n        cmd.extend(['--detach-sign', '--armor', '--local-user',\n                    signer, '--output', sf, filename])\n        logger.debug('invoking: %s', ' '.join(cmd))\n        return cmd, sf\n\n    def run_command(self, cmd, input_data=None):\n        \"\"\"\n        Run a command in a child process , passing it any input data specified.\n\n        :param cmd: The command to run.\n        :param input_data: If specified, this must be a byte string containing\n                           data to be sent to the child process.\n        :return: A tuple consisting of the subprocess' exit code, a list of\n                 lines read from the subprocess' ``stdout``, and a list of\n                 lines read from the subprocess' ``stderr``.\n        \"\"\"\n        kwargs = {\n            'stdout': subprocess.PIPE,\n            'stderr': subprocess.PIPE,\n        }\n        if input_data is not None:\n            kwargs['stdin'] = subprocess.PIPE\n        stdout = []\n        stderr = []\n        p = subprocess.Popen(cmd, **kwargs)\n        # We don't use communicate() here because we may need to\n        # get clever with interacting with the command\n        t1 = Thread(target=self._reader, args=('stdout', p.stdout, stdout))\n        t1.start()\n        t2 = Thread(target=self._reader, args=('stderr', p.stderr, stderr))\n        t2.start()\n        if input_data is not None:\n            p.stdin.write(input_data)\n            p.stdin.close()\n\n        p.wait()\n        t1.join()\n        t2.join()\n        return p.returncode, stdout, stderr\n\n    def sign_file(self, filename, signer, sign_password, keystore=None):  # pragma: no cover\n        \"\"\"\n        Sign a file.\n\n        :param filename: The pathname to the file to be signed.\n        :param signer: The identifier of the signer of the file.\n        :param sign_password: The passphrase for the signer's\n                              private key used for signing.\n        :param keystore: The path to a directory which contains the keys\n                         used in signing. If not specified, the instance's\n                         ``gpg_home`` attribute is used instead.\n        :return: The absolute pathname of the file where the signature is\n                 stored.\n        \"\"\"\n        cmd, sig_file = self.get_sign_command(filename, signer, sign_password,\n                                              keystore)\n        rc, stdout, stderr = self.run_command(cmd,\n                                              sign_password.encode('utf-8'))\n        if rc != 0:\n            raise DistlibException('sign command failed with error '\n                                   'code %s' % rc)\n        return sig_file\n\n    def upload_file(self, metadata, filename, signer=None, sign_password=None,\n                    filetype='sdist', pyversion='source', keystore=None):\n        \"\"\"\n        Upload a release file to the index.\n\n        :param metadata: A :class:`Metadata` instance defining at least a name\n                         and version number for the file to be uploaded.\n        :param filename: The pathname of the file to be uploaded.\n        :param signer: The identifier of the signer of the file.\n        :param sign_password: The passphrase for the signer's\n                              private key used for signing.\n        :param filetype: The type of the file being uploaded. This is the\n                        distutils command which produced that file, e.g.\n                        ``sdist`` or ``bdist_wheel``.\n        :param pyversion: The version of Python which the release relates\n                          to. For code compatible with any Python, this would\n                          be ``source``, otherwise it would be e.g. ``3.2``.\n        :param keystore: The path to a directory which contains the keys\n                         used in signing. If not specified, the instance's\n                         ``gpg_home`` attribute is used instead.\n        :return: The HTTP response received from PyPI upon submission of the\n                request.\n        \"\"\"\n        self.check_credentials()\n        if not os.path.exists(filename):\n            raise DistlibException('not found: %s' % filename)\n        metadata.validate()\n        d = metadata.todict()\n        sig_file = None\n        if signer:\n            if not self.gpg:\n                logger.warning('no signing program available - not signed')\n            else:\n                sig_file = self.sign_file(filename, signer, sign_password,\n                                          keystore)\n        with open(filename, 'rb') as f:\n            file_data = f.read()\n        md5_digest = hashlib.md5(file_data).hexdigest()\n        sha256_digest = hashlib.sha256(file_data).hexdigest()\n        d.update({\n            ':action': 'file_upload',\n            'protocol_version': '1',\n            'filetype': filetype,\n            'pyversion': pyversion,\n            'md5_digest': md5_digest,\n            'sha256_digest': sha256_digest,\n        })\n        files = [('content', os.path.basename(filename), file_data)]\n        if sig_file:\n            with open(sig_file, 'rb') as f:\n                sig_data = f.read()\n            files.append(('gpg_signature', os.path.basename(sig_file),\n                         sig_data))\n            shutil.rmtree(os.path.dirname(sig_file))\n        request = self.encode_request(d.items(), files)\n        return self.send_request(request)\n\n    def upload_documentation(self, metadata, doc_dir):  # pragma: no cover\n        \"\"\"\n        Upload documentation to the index.\n\n        :param metadata: A :class:`Metadata` instance defining at least a name\n                         and version number for the documentation to be\n                         uploaded.\n        :param doc_dir: The pathname of the directory which contains the\n                        documentation. This should be the directory that\n                        contains the ``index.html`` for the documentation.\n        :return: The HTTP response received from PyPI upon submission of the\n                request.\n        \"\"\"\n        self.check_credentials()\n        if not os.path.isdir(doc_dir):\n            raise DistlibException('not a directory: %r' % doc_dir)\n        fn = os.path.join(doc_dir, 'index.html')\n        if not os.path.exists(fn):\n            raise DistlibException('not found: %r' % fn)\n        metadata.validate()\n        name, version = metadata.name, metadata.version\n        zip_data = zip_dir(doc_dir).getvalue()\n        fields = [(':action', 'doc_upload'),\n                  ('name', name), ('version', version)]\n        files = [('content', name, zip_data)]\n        request = self.encode_request(fields, files)\n        return self.send_request(request)\n\n    def get_verify_command(self, signature_filename, data_filename,\n                           keystore=None):\n        \"\"\"\n        Return a suitable command for verifying a file.\n\n        :param signature_filename: The pathname to the file containing the\n                                   signature.\n        :param data_filename: The pathname to the file containing the\n                              signed data.\n        :param keystore: The path to a directory which contains the keys\n                         used in verification. If not specified, the\n                         instance's ``gpg_home`` attribute is used instead.\n        :return: The verifying command as a list suitable to be\n                 passed to :class:`subprocess.Popen`.\n        \"\"\"\n        cmd = [self.gpg, '--status-fd', '2', '--no-tty']\n        if keystore is None:\n            keystore = self.gpg_home\n        if keystore:\n            cmd.extend(['--homedir', keystore])\n        cmd.extend(['--verify', signature_filename, data_filename])\n        logger.debug('invoking: %s', ' '.join(cmd))\n        return cmd\n\n    def verify_signature(self, signature_filename, data_filename,\n                         keystore=None):\n        \"\"\"\n        Verify a signature for a file.\n\n        :param signature_filename: The pathname to the file containing the\n                                   signature.\n        :param data_filename: The pathname to the file containing the\n                              signed data.\n        :param keystore: The path to a directory which contains the keys\n                         used in verification. If not specified, the\n                         instance's ``gpg_home`` attribute is used instead.\n        :return: True if the signature was verified, else False.\n        \"\"\"\n        if not self.gpg:\n            raise DistlibException('verification unavailable because gpg '\n                                   'unavailable')\n        cmd = self.get_verify_command(signature_filename, data_filename,\n                                      keystore)\n        rc, stdout, stderr = self.run_command(cmd)\n        if rc not in (0, 1):\n            raise DistlibException('verify command failed with error code %s' % rc)\n        return rc == 0\n\n    def download_file(self, url, destfile, digest=None, reporthook=None):\n        \"\"\"\n        This is a convenience method for downloading a file from an URL.\n        Normally, this will be a file from the index, though currently\n        no check is made for this (i.e. a file can be downloaded from\n        anywhere).\n\n        The method is just like the :func:`urlretrieve` function in the\n        standard library, except that it allows digest computation to be\n        done during download and checking that the downloaded data\n        matched any expected value.\n\n        :param url: The URL of the file to be downloaded (assumed to be\n                    available via an HTTP GET request).\n        :param destfile: The pathname where the downloaded file is to be\n                         saved.\n        :param digest: If specified, this must be a (hasher, value)\n                       tuple, where hasher is the algorithm used (e.g.\n                       ``'md5'``) and ``value`` is the expected value.\n        :param reporthook: The same as for :func:`urlretrieve` in the\n                           standard library.\n        \"\"\"\n        if digest is None:\n            digester = None\n            logger.debug('No digest specified')\n        else:\n            if isinstance(digest, (list, tuple)):\n                hasher, digest = digest\n            else:\n                hasher = 'md5'\n            digester = getattr(hashlib, hasher)()\n            logger.debug('Digest specified: %s' % digest)\n        # The following code is equivalent to urlretrieve.\n        # We need to do it this way so that we can compute the\n        # digest of the file as we go.\n        with open(destfile, 'wb') as dfp:\n            # addinfourl is not a context manager on 2.x\n            # so we have to use try/finally\n            sfp = self.send_request(Request(url))\n            try:\n                headers = sfp.info()\n                blocksize = 8192\n                size = -1\n                read = 0\n                blocknum = 0\n                if \"content-length\" in headers:\n                    size = int(headers[\"Content-Length\"])\n                if reporthook:\n                    reporthook(blocknum, blocksize, size)\n                while True:\n                    block = sfp.read(blocksize)\n                    if not block:\n                        break\n                    read += len(block)\n                    dfp.write(block)\n                    if digester:\n                        digester.update(block)\n                    blocknum += 1\n                    if reporthook:\n                        reporthook(blocknum, blocksize, size)\n            finally:\n                sfp.close()\n\n        # check that we got the whole file, if we can\n        if size >= 0 and read < size:\n            raise DistlibException(\n                'retrieval incomplete: got only %d out of %d bytes'\n                % (read, size))\n        # if we have a digest, it must match.\n        if digester:\n            actual = digester.hexdigest()\n            if digest != actual:\n                raise DistlibException('%s digest mismatch for %s: expected '\n                                       '%s, got %s' % (hasher, destfile,\n                                                       digest, actual))\n            logger.debug('Digest verified: %s', digest)\n\n    def send_request(self, req):\n        \"\"\"\n        Send a standard library :class:`Request` to PyPI and return its\n        response.\n\n        :param req: The request to send.\n        :return: The HTTP response from PyPI (a standard library HTTPResponse).\n        \"\"\"\n        handlers = []\n        if self.password_handler:\n            handlers.append(self.password_handler)\n        if self.ssl_verifier:\n            handlers.append(self.ssl_verifier)\n        opener = build_opener(*handlers)\n        return opener.open(req)\n\n    def encode_request(self, fields, files):\n        \"\"\"\n        Encode fields and files for posting to an HTTP server.\n\n        :param fields: The fields to send as a list of (fieldname, value)\n                       tuples.\n        :param files: The files to send as a list of (fieldname, filename,\n                      file_bytes) tuple.\n        \"\"\"\n        # Adapted from packaging, which in turn was adapted from\n        # http://code.activestate.com/recipes/146306\n\n        parts = []\n        boundary = self.boundary\n        for k, values in fields:\n            if not isinstance(values, (list, tuple)):\n                values = [values]\n\n            for v in values:\n                parts.extend((\n                    b'--' + boundary,\n                    ('Content-Disposition: form-data; name=\"%s\"' %\n                     k).encode('utf-8'),\n                    b'',\n                    v.encode('utf-8')))\n        for key, filename, value in files:\n            parts.extend((\n                b'--' + boundary,\n                ('Content-Disposition: form-data; name=\"%s\"; filename=\"%s\"' %\n                 (key, filename)).encode('utf-8'),\n                b'',\n                value))\n\n        parts.extend((b'--' + boundary + b'--', b''))\n\n        body = b'\\r\\n'.join(parts)\n        ct = b'multipart/form-data; boundary=' + boundary\n        headers = {\n            'Content-type': ct,\n            'Content-length': str(len(body))\n        }\n        return Request(self.url, body, headers)\n\n    def search(self, terms, operator=None):  # pragma: no cover\n        if isinstance(terms, string_types):\n            terms = {'name': terms}\n        rpc_proxy = ServerProxy(self.url, timeout=3.0)\n        try:\n            return rpc_proxy.search(terms, operator or 'and')\n        finally:\n            rpc_proxy('close')()\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_vendor/distlib/locators.py","size":51026,"sha1":"9ea4315030ddc90bd677b818eadce2466c2e9f05","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"# -*- coding: utf-8 -*-\n#\n# Copyright (C) 2012-2023 Vinay Sajip.\n# Licensed to the Python Software Foundation under a contributor agreement.\n# See LICENSE.txt and CONTRIBUTORS.txt.\n#\n\nimport gzip\nfrom io import BytesIO\nimport json\nimport logging\nimport os\nimport posixpath\nimport re\ntry:\n    import threading\nexcept ImportError:  # pragma: no cover\n    import dummy_threading as threading\nimport zlib\n\nfrom . import DistlibException\nfrom .compat import (urljoin, urlparse, urlunparse, url2pathname, pathname2url, queue, quote, unescape, build_opener,\n                     HTTPRedirectHandler as BaseRedirectHandler, text_type, Request, HTTPError, URLError)\nfrom .database import Distribution, DistributionPath, make_dist\nfrom .metadata import Metadata, MetadataInvalidError\nfrom .util import (cached_property, ensure_slash, split_filename, get_project_data, parse_requirement,\n                   parse_name_and_version, ServerProxy, normalize_name)\nfrom .version import get_scheme, UnsupportedVersionError\nfrom .wheel import Wheel, is_compatible\n\nlogger = logging.getLogger(__name__)\n\nHASHER_HASH = re.compile(r'^(\\w+)=([a-f0-9]+)')\nCHARSET = re.compile(r';\\s*charset\\s*=\\s*(.*)\\s*$', re.I)\nHTML_CONTENT_TYPE = re.compile('text/html|application/x(ht)?ml')\nDEFAULT_INDEX = 'https://pypi.org/pypi'\n\n\ndef get_all_distribution_names(url=None):\n    \"\"\"\n    Return all distribution names known by an index.\n    :param url: The URL of the index.\n    :return: A list of all known distribution names.\n    \"\"\"\n    if url is None:\n        url = DEFAULT_INDEX\n    client = ServerProxy(url, timeout=3.0)\n    try:\n        return client.list_packages()\n    finally:\n        client('close')()\n\n\nclass RedirectHandler(BaseRedirectHandler):\n    \"\"\"\n    A class to work around a bug in some Python 3.2.x releases.\n    \"\"\"\n\n    # There's a bug in the base version for some 3.2.x\n    # (e.g. 3.2.2 on Ubuntu Oneiric). If a Location header\n    # returns e.g. /abc, it bails because it says the scheme ''\n    # is bogus, when actually it should use the request's\n    # URL for the scheme. See Python issue #13696.\n    def http_error_302(self, req, fp, code, msg, headers):\n        # Some servers (incorrectly) return multiple Location headers\n        # (so probably same goes for URI).  Use first header.\n        newurl = None\n        for key in ('location', 'uri'):\n            if key in headers:\n                newurl = headers[key]\n                break\n        if newurl is None:  # pragma: no cover\n            return\n        urlparts = urlparse(newurl)\n        if urlparts.scheme == '':\n            newurl = urljoin(req.get_full_url(), newurl)\n            if hasattr(headers, 'replace_header'):\n                headers.replace_header(key, newurl)\n            else:\n                headers[key] = newurl\n        return BaseRedirectHandler.http_error_302(self, req, fp, code, msg, headers)\n\n    http_error_301 = http_error_303 = http_error_307 = http_error_302\n\n\nclass Locator(object):\n    \"\"\"\n    A base class for locators - things that locate distributions.\n    \"\"\"\n    source_extensions = ('.tar.gz', '.tar.bz2', '.tar', '.zip', '.tgz', '.tbz')\n    binary_extensions = ('.egg', '.exe', '.whl')\n    excluded_extensions = ('.pdf', )\n\n    # A list of tags indicating which wheels you want to match. The default\n    # value of None matches against the tags compatible with the running\n    # Python. If you want to match other values, set wheel_tags on a locator\n    # instance to a list of tuples (pyver, abi, arch) which you want to match.\n    wheel_tags = None\n\n    downloadable_extensions = source_extensions + ('.whl', )\n\n    def __init__(self, scheme='default'):\n        \"\"\"\n        Initialise an instance.\n        :param scheme: Because locators look for most recent versions, they\n                       need to know the version scheme to use. This specifies\n                       the current PEP-recommended scheme - use ``'legacy'``\n                       if you need to support existing distributions on PyPI.\n        \"\"\"\n        self._cache = {}\n        self.scheme = scheme\n        # Because of bugs in some of the handlers on some of the platforms,\n        # we use our own opener rather than just using urlopen.\n        self.opener = build_opener(RedirectHandler())\n        # If get_project() is called from locate(), the matcher instance\n        # is set from the requirement passed to locate(). See issue #18 for\n        # why this can be useful to know.\n        self.matcher = None\n        self.errors = queue.Queue()\n\n    def get_errors(self):\n        \"\"\"\n        Return any errors which have occurred.\n        \"\"\"\n        result = []\n        while not self.errors.empty():  # pragma: no cover\n            try:\n                e = self.errors.get(False)\n                result.append(e)\n            except self.errors.Empty:\n                continue\n            self.errors.task_done()\n        return result\n\n    def clear_errors(self):\n        \"\"\"\n        Clear any errors which may have been logged.\n        \"\"\"\n        # Just get the errors and throw them away\n        self.get_errors()\n\n    def clear_cache(self):\n        self._cache.clear()\n\n    def _get_scheme(self):\n        return self._scheme\n\n    def _set_scheme(self, value):\n        self._scheme = value\n\n    scheme = property(_get_scheme, _set_scheme)\n\n    def _get_project(self, name):\n        \"\"\"\n        For a given project, get a dictionary mapping available versions to Distribution\n        instances.\n\n        This should be implemented in subclasses.\n\n        If called from a locate() request, self.matcher will be set to a\n        matcher for the requirement to satisfy, otherwise it will be None.\n        \"\"\"\n        raise NotImplementedError('Please implement in the subclass')\n\n    def get_distribution_names(self):\n        \"\"\"\n        Return all the distribution names known to this locator.\n        \"\"\"\n        raise NotImplementedError('Please implement in the subclass')\n\n    def get_project(self, name):\n        \"\"\"\n        For a given project, get a dictionary mapping available versions to Distribution\n        instances.\n\n        This calls _get_project to do all the work, and just implements a caching layer on top.\n        \"\"\"\n        if self._cache is None:  # pragma: no cover\n            result = self._get_project(name)\n        elif name in self._cache:\n            result = self._cache[name]\n        else:\n            self.clear_errors()\n            result = self._get_project(name)\n            self._cache[name] = result\n        return result\n\n    def score_url(self, url):\n        \"\"\"\n        Give an url a score which can be used to choose preferred URLs\n        for a given project release.\n        \"\"\"\n        t = urlparse(url)\n        basename = posixpath.basename(t.path)\n        compatible = True\n        is_wheel = basename.endswith('.whl')\n        is_downloadable = basename.endswith(self.downloadable_extensions)\n        if is_wheel:\n            compatible = is_compatible(Wheel(basename), self.wheel_tags)\n        return (t.scheme == 'https', 'pypi.org' in t.netloc, is_downloadable, is_wheel, compatible, basename)\n\n    def prefer_url(self, url1, url2):\n        \"\"\"\n        Choose one of two URLs where both are candidates for distribution\n        archives for the same version of a distribution (for example,\n        .tar.gz vs. zip).\n\n        The current implementation favours https:// URLs over http://, archives\n        from PyPI over those from other locations, wheel compatibility (if a\n        wheel) and then the archive name.\n        \"\"\"\n        result = url2\n        if url1:\n            s1 = self.score_url(url1)\n            s2 = self.score_url(url2)\n            if s1 > s2:\n                result = url1\n            if result != url2:\n                logger.debug('Not replacing %r with %r', url1, url2)\n            else:\n                logger.debug('Replacing %r with %r', url1, url2)\n        return result\n\n    def split_filename(self, filename, project_name):\n        \"\"\"\n        Attempt to split a filename in project name, version and Python version.\n        \"\"\"\n        return split_filename(filename, project_name)\n\n    def convert_url_to_download_info(self, url, project_name):\n        \"\"\"\n        See if a URL is a candidate for a download URL for a project (the URL\n        has typically been scraped from an HTML page).\n\n        If it is, a dictionary is returned with keys \"name\", \"version\",\n        \"filename\" and \"url\"; otherwise, None is returned.\n        \"\"\"\n\n        def same_project(name1, name2):\n            return normalize_name(name1) == normalize_name(name2)\n\n        result = None\n        scheme, netloc, path, params, query, frag = urlparse(url)\n        if frag.lower().startswith('egg='):  # pragma: no cover\n            logger.debug('%s: version hint in fragment: %r', project_name, frag)\n        m = HASHER_HASH.match(frag)\n        if m:\n            algo, digest = m.groups()\n        else:\n            algo, digest = None, None\n        origpath = path\n        if path and path[-1] == '/':  # pragma: no cover\n            path = path[:-1]\n        if path.endswith('.whl'):\n            try:\n                wheel = Wheel(path)\n                if not is_compatible(wheel, self.wheel_tags):\n                    logger.debug('Wheel not compatible: %s', path)\n                else:\n                    if project_name is None:\n                        include = True\n                    else:\n                        include = same_project(wheel.name, project_name)\n                    if include:\n                        result = {\n                            'name': wheel.name,\n                            'version': wheel.version,\n                            'filename': wheel.filename,\n                            'url': urlunparse((scheme, netloc, origpath, params, query, '')),\n                            'python-version': ', '.join(['.'.join(list(v[2:])) for v in wheel.pyver]),\n                        }\n            except Exception:  # pragma: no cover\n                logger.warning('invalid path for wheel: %s', path)\n        elif not path.endswith(self.downloadable_extensions):  # pragma: no cover\n            logger.debug('Not downloadable: %s', path)\n        else:  # downloadable extension\n            path = filename = posixpath.basename(path)\n            for ext in self.downloadable_extensions:\n                if path.endswith(ext):\n                    path = path[:-len(ext)]\n                    t = self.split_filename(path, project_name)\n                    if not t:  # pragma: no cover\n                        logger.debug('No match for project/version: %s', path)\n                    else:\n                        name, version, pyver = t\n                        if not project_name or same_project(project_name, name):\n                            result = {\n                                'name': name,\n                                'version': version,\n                                'filename': filename,\n                                'url': urlunparse((scheme, netloc, origpath, params, query, '')),\n                            }\n                            if pyver:  # pragma: no cover\n                                result['python-version'] = pyver\n                    break\n        if result and algo:\n            result['%s_digest' % algo] = digest\n        return result\n\n    def _get_digest(self, info):\n        \"\"\"\n        Get a digest from a dictionary by looking at a \"digests\" dictionary\n        or keys of the form 'algo_digest'.\n\n        Returns a 2-tuple (algo, digest) if found, else None. Currently\n        looks only for SHA256, then MD5.\n        \"\"\"\n        result = None\n        if 'digests' in info:\n            digests = info['digests']\n            for algo in ('sha256', 'md5'):\n                if algo in digests:\n                    result = (algo, digests[algo])\n                    break\n        if not result:\n            for algo in ('sha256', 'md5'):\n                key = '%s_digest' % algo\n                if key in info:\n                    result = (algo, info[key])\n                    break\n        return result\n\n    def _update_version_data(self, result, info):\n        \"\"\"\n        Update a result dictionary (the final result from _get_project) with a\n        dictionary for a specific version, which typically holds information\n        gleaned from a filename or URL for an archive for the distribution.\n        \"\"\"\n        name = info.pop('name')\n        version = info.pop('version')\n        if version in result:\n            dist = result[version]\n            md = dist.metadata\n        else:\n            dist = make_dist(name, version, scheme=self.scheme)\n            md = dist.metadata\n        dist.digest = digest = self._get_digest(info)\n        url = info['url']\n        result['digests'][url] = digest\n        if md.source_url != info['url']:\n            md.source_url = self.prefer_url(md.source_url, url)\n            result['urls'].setdefault(version, set()).add(url)\n        dist.locator = self\n        result[version] = dist\n\n    def locate(self, requirement, prereleases=False):\n        \"\"\"\n        Find the most recent distribution which matches the given\n        requirement.\n\n        :param requirement: A requirement of the form 'foo (1.0)' or perhaps\n                            'foo (>= 1.0, < 2.0, != 1.3)'\n        :param prereleases: If ``True``, allow pre-release versions\n                            to be located. Otherwise, pre-release versions\n                            are not returned.\n        :return: A :class:`Distribution` instance, or ``None`` if no such\n                 distribution could be located.\n        \"\"\"\n        result = None\n        r = parse_requirement(requirement)\n        if r is None:  # pragma: no cover\n            raise DistlibException('Not a valid requirement: %r' % requirement)\n        scheme = get_scheme(self.scheme)\n        self.matcher = matcher = scheme.matcher(r.requirement)\n        logger.debug('matcher: %s (%s)', matcher, type(matcher).__name__)\n        versions = self.get_project(r.name)\n        if len(versions) > 2:  # urls and digests keys are present\n            # sometimes, versions are invalid\n            slist = []\n            vcls = matcher.version_class\n            for k in versions:\n                if k in ('urls', 'digests'):\n                    continue\n                try:\n                    if not matcher.match(k):\n                        pass  # logger.debug('%s did not match %r', matcher, k)\n                    else:\n                        if prereleases or not vcls(k).is_prerelease:\n                            slist.append(k)\n                except Exception:  # pragma: no cover\n                    logger.warning('error matching %s with %r', matcher, k)\n                    pass  # slist.append(k)\n            if len(slist) > 1:\n                slist = sorted(slist, key=scheme.key)\n            if slist:\n                logger.debug('sorted list: %s', slist)\n                version = slist[-1]\n                result = versions[version]\n        if result:\n            if r.extras:\n                result.extras = r.extras\n            result.download_urls = versions.get('urls', {}).get(version, set())\n            d = {}\n            sd = versions.get('digests', {})\n            for url in result.download_urls:\n                if url in sd:  # pragma: no cover\n                    d[url] = sd[url]\n            result.digests = d\n        self.matcher = None\n        return result\n\n\nclass PyPIRPCLocator(Locator):\n    \"\"\"\n    This locator uses XML-RPC to locate distributions. It therefore\n    cannot be used with simple mirrors (that only mirror file content).\n    \"\"\"\n\n    def __init__(self, url, **kwargs):\n        \"\"\"\n        Initialise an instance.\n\n        :param url: The URL to use for XML-RPC.\n        :param kwargs: Passed to the superclass constructor.\n        \"\"\"\n        super(PyPIRPCLocator, self).__init__(**kwargs)\n        self.base_url = url\n        self.client = ServerProxy(url, timeout=3.0)\n\n    def get_distribution_names(self):\n        \"\"\"\n        Return all the distribution names known to this locator.\n        \"\"\"\n        return set(self.client.list_packages())\n\n    def _get_project(self, name):\n        result = {'urls': {}, 'digests': {}}\n        versions = self.client.package_releases(name, True)\n        for v in versions:\n            urls = self.client.release_urls(name, v)\n            data = self.client.release_data(name, v)\n            metadata = Metadata(scheme=self.scheme)\n            metadata.name = data['name']\n            metadata.version = data['version']\n            metadata.license = data.get('license')\n            metadata.keywords = data.get('keywords', [])\n            metadata.summary = data.get('summary')\n            dist = Distribution(metadata)\n            if urls:\n                info = urls[0]\n                metadata.source_url = info['url']\n                dist.digest = self._get_digest(info)\n                dist.locator = self\n                result[v] = dist\n                for info in urls:\n                    url = info['url']\n                    digest = self._get_digest(info)\n                    result['urls'].setdefault(v, set()).add(url)\n                    result['digests'][url] = digest\n        return result\n\n\nclass PyPIJSONLocator(Locator):\n    \"\"\"\n    This locator uses PyPI's JSON interface. It's very limited in functionality\n    and probably not worth using.\n    \"\"\"\n\n    def __init__(self, url, **kwargs):\n        super(PyPIJSONLocator, self).__init__(**kwargs)\n        self.base_url = ensure_slash(url)\n\n    def get_distribution_names(self):\n        \"\"\"\n        Return all the distribution names known to this locator.\n        \"\"\"\n        raise NotImplementedError('Not available from this locator')\n\n    def _get_project(self, name):\n        result = {'urls': {}, 'digests': {}}\n        url = urljoin(self.base_url, '%s/json' % quote(name))\n        try:\n            resp = self.opener.open(url)\n            data = resp.read().decode()  # for now\n            d = json.loads(data)\n            md = Metadata(scheme=self.scheme)\n            data = d['info']\n            md.name = data['name']\n            md.version = data['version']\n            md.license = data.get('license')\n            md.keywords = data.get('keywords', [])\n            md.summary = data.get('summary')\n            dist = Distribution(md)\n            dist.locator = self\n            # urls = d['urls']\n            result[md.version] = dist\n            for info in d['urls']:\n                url = info['url']\n                dist.download_urls.add(url)\n                dist.digests[url] = self._get_digest(info)\n                result['urls'].setdefault(md.version, set()).add(url)\n                result['digests'][url] = self._get_digest(info)\n            # Now get other releases\n            for version, infos in d['releases'].items():\n                if version == md.version:\n                    continue  # already done\n                omd = Metadata(scheme=self.scheme)\n                omd.name = md.name\n                omd.version = version\n                odist = Distribution(omd)\n                odist.locator = self\n                result[version] = odist\n                for info in infos:\n                    url = info['url']\n                    odist.download_urls.add(url)\n                    odist.digests[url] = self._get_digest(info)\n                    result['urls'].setdefault(version, set()).add(url)\n                    result['digests'][url] = self._get_digest(info)\n\n\n#            for info in urls:\n#                md.source_url = info['url']\n#                dist.digest = self._get_digest(info)\n#                dist.locator = self\n#                for info in urls:\n#                    url = info['url']\n#                    result['urls'].setdefault(md.version, set()).add(url)\n#                    result['digests'][url] = self._get_digest(info)\n        except Exception as e:\n            self.errors.put(text_type(e))\n            logger.exception('JSON fetch failed: %s', e)\n        return result\n\n\nclass Page(object):\n    \"\"\"\n    This class represents a scraped HTML page.\n    \"\"\"\n    # The following slightly hairy-looking regex just looks for the contents of\n    # an anchor link, which has an attribute \"href\" either immediately preceded\n    # or immediately followed by a \"rel\" attribute. The attribute values can be\n    # declared with double quotes, single quotes or no quotes - which leads to\n    # the length of the expression.\n    _href = re.compile(\n        \"\"\"\n(rel\\\\s*=\\\\s*(?:\"(?P<rel1>[^\"]*)\"|'(?P<rel2>[^']*)'|(?P<rel3>[^>\\\\s\\n]*))\\\\s+)?\nhref\\\\s*=\\\\s*(?:\"(?P<url1>[^\"]*)\"|'(?P<url2>[^']*)'|(?P<url3>[^>\\\\s\\n]*))\n(\\\\s+rel\\\\s*=\\\\s*(?:\"(?P<rel4>[^\"]*)\"|'(?P<rel5>[^']*)'|(?P<rel6>[^>\\\\s\\n]*)))?\n\"\"\", re.I | re.S | re.X)\n    _base = re.compile(r\"\"\"<base\\s+href\\s*=\\s*['\"]?([^'\">]+)\"\"\", re.I | re.S)\n\n    def __init__(self, data, url):\n        \"\"\"\n        Initialise an instance with the Unicode page contents and the URL they\n        came from.\n        \"\"\"\n        self.data = data\n        self.base_url = self.url = url\n        m = self._base.search(self.data)\n        if m:\n            self.base_url = m.group(1)\n\n    _clean_re = re.compile(r'[^a-z0-9$&+,/:;=?@.#%_\\\\|-]', re.I)\n\n    @cached_property\n    def links(self):\n        \"\"\"\n        Return the URLs of all the links on a page together with information\n        about their \"rel\" attribute, for determining which ones to treat as\n        downloads and which ones to queue for further scraping.\n        \"\"\"\n\n        def clean(url):\n            \"Tidy up an URL.\"\n            scheme, netloc, path, params, query, frag = urlparse(url)\n            return urlunparse((scheme, netloc, quote(path), params, query, frag))\n\n        result = set()\n        for match in self._href.finditer(self.data):\n            d = match.groupdict('')\n            rel = (d['rel1'] or d['rel2'] or d['rel3'] or d['rel4'] or d['rel5'] or d['rel6'])\n            url = d['url1'] or d['url2'] or d['url3']\n            url = urljoin(self.base_url, url)\n            url = unescape(url)\n            url = self._clean_re.sub(lambda m: '%%%2x' % ord(m.group(0)), url)\n            result.add((url, rel))\n        # We sort the result, hoping to bring the most recent versions\n        # to the front\n        result = sorted(result, key=lambda t: t[0], reverse=True)\n        return result\n\n\nclass SimpleScrapingLocator(Locator):\n    \"\"\"\n    A locator which scrapes HTML pages to locate downloads for a distribution.\n    This runs multiple threads to do the I/O; performance is at least as good\n    as pip's PackageFinder, which works in an analogous fashion.\n    \"\"\"\n\n    # These are used to deal with various Content-Encoding schemes.\n    decoders = {\n        'deflate': zlib.decompress,\n        'gzip': lambda b: gzip.GzipFile(fileobj=BytesIO(b)).read(),\n        'none': lambda b: b,\n    }\n\n    def __init__(self, url, timeout=None, num_workers=10, **kwargs):\n        \"\"\"\n        Initialise an instance.\n        :param url: The root URL to use for scraping.\n        :param timeout: The timeout, in seconds, to be applied to requests.\n                        This defaults to ``None`` (no timeout specified).\n        :param num_workers: The number of worker threads you want to do I/O,\n                            This defaults to 10.\n        :param kwargs: Passed to the superclass.\n        \"\"\"\n        super(SimpleScrapingLocator, self).__init__(**kwargs)\n        self.base_url = ensure_slash(url)\n        self.timeout = timeout\n        self._page_cache = {}\n        self._seen = set()\n        self._to_fetch = queue.Queue()\n        self._bad_hosts = set()\n        self.skip_externals = False\n        self.num_workers = num_workers\n        self._lock = threading.RLock()\n        # See issue #45: we need to be resilient when the locator is used\n        # in a thread, e.g. with concurrent.futures. We can't use self._lock\n        # as it is for coordinating our internal threads - the ones created\n        # in _prepare_threads.\n        self._gplock = threading.RLock()\n        self.platform_check = False  # See issue #112\n\n    def _prepare_threads(self):\n        \"\"\"\n        Threads are created only when get_project is called, and terminate\n        before it returns. They are there primarily to parallelise I/O (i.e.\n        fetching web pages).\n        \"\"\"\n        self._threads = []\n        for i in range(self.num_workers):\n            t = threading.Thread(target=self._fetch)\n            t.daemon = True\n            t.start()\n            self._threads.append(t)\n\n    def _wait_threads(self):\n        \"\"\"\n        Tell all the threads to terminate (by sending a sentinel value) and\n        wait for them to do so.\n        \"\"\"\n        # Note that you need two loops, since you can't say which\n        # thread will get each sentinel\n        for t in self._threads:\n            self._to_fetch.put(None)  # sentinel\n        for t in self._threads:\n            t.join()\n        self._threads = []\n\n    def _get_project(self, name):\n        result = {'urls': {}, 'digests': {}}\n        with self._gplock:\n            self.result = result\n            self.project_name = name\n            url = urljoin(self.base_url, '%s/' % quote(name))\n            self._seen.clear()\n            self._page_cache.clear()\n            self._prepare_threads()\n            try:\n                logger.debug('Queueing %s', url)\n                self._to_fetch.put(url)\n                self._to_fetch.join()\n            finally:\n                self._wait_threads()\n            del self.result\n        return result\n\n    platform_dependent = re.compile(r'\\b(linux_(i\\d86|x86_64|arm\\w+)|'\n                                    r'win(32|_amd64)|macosx_?\\d+)\\b', re.I)\n\n    def _is_platform_dependent(self, url):\n        \"\"\"\n        Does an URL refer to a platform-specific download?\n        \"\"\"\n        return self.platform_dependent.search(url)\n\n    def _process_download(self, url):\n        \"\"\"\n        See if an URL is a suitable download for a project.\n\n        If it is, register information in the result dictionary (for\n        _get_project) about the specific version it's for.\n\n        Note that the return value isn't actually used other than as a boolean\n        value.\n        \"\"\"\n        if self.platform_check and self._is_platform_dependent(url):\n            info = None\n        else:\n            info = self.convert_url_to_download_info(url, self.project_name)\n        logger.debug('process_download: %s -> %s', url, info)\n        if info:\n            with self._lock:  # needed because self.result is shared\n                self._update_version_data(self.result, info)\n        return info\n\n    def _should_queue(self, link, referrer, rel):\n        \"\"\"\n        Determine whether a link URL from a referring page and with a\n        particular \"rel\" attribute should be queued for scraping.\n        \"\"\"\n        scheme, netloc, path, _, _, _ = urlparse(link)\n        if path.endswith(self.source_extensions + self.binary_extensions + self.excluded_extensions):\n            result = False\n        elif self.skip_externals and not link.startswith(self.base_url):\n            result = False\n        elif not referrer.startswith(self.base_url):\n            result = False\n        elif rel not in ('homepage', 'download'):\n            result = False\n        elif scheme not in ('http', 'https', 'ftp'):\n            result = False\n        elif self._is_platform_dependent(link):\n            result = False\n        else:\n            host = netloc.split(':', 1)[0]\n            if host.lower() == 'localhost':\n                result = False\n            else:\n                result = True\n        logger.debug('should_queue: %s (%s) from %s -> %s', link, rel, referrer, result)\n        return result\n\n    def _fetch(self):\n        \"\"\"\n        Get a URL to fetch from the work queue, get the HTML page, examine its\n        links for download candidates and candidates for further scraping.\n\n        This is a handy method to run in a thread.\n        \"\"\"\n        while True:\n            url = self._to_fetch.get()\n            try:\n                if url:\n                    page = self.get_page(url)\n                    if page is None:  # e.g. after an error\n                        continue\n                    for link, rel in page.links:\n                        if link not in self._seen:\n                            try:\n                                self._seen.add(link)\n                                if (not self._process_download(link) and self._should_queue(link, url, rel)):\n                                    logger.debug('Queueing %s from %s', link, url)\n                                    self._to_fetch.put(link)\n                            except MetadataInvalidError:  # e.g. invalid versions\n                                pass\n            except Exception as e:  # pragma: no cover\n                self.errors.put(text_type(e))\n            finally:\n                # always do this, to avoid hangs :-)\n                self._to_fetch.task_done()\n            if not url:\n                # logger.debug('Sentinel seen, quitting.')\n                break\n\n    def get_page(self, url):\n        \"\"\"\n        Get the HTML for an URL, possibly from an in-memory cache.\n\n        XXX TODO Note: this cache is never actually cleared. It's assumed that\n        the data won't get stale over the lifetime of a locator instance (not\n        necessarily true for the default_locator).\n        \"\"\"\n        # http://peak.telecommunity.com/DevCenter/EasyInstall#package-index-api\n        scheme, netloc, path, _, _, _ = urlparse(url)\n        if scheme == 'file' and os.path.isdir(url2pathname(path)):\n            url = urljoin(ensure_slash(url), 'index.html')\n\n        if url in self._page_cache:\n            result = self._page_cache[url]\n            logger.debug('Returning %s from cache: %s', url, result)\n        else:\n            host = netloc.split(':', 1)[0]\n            result = None\n            if host in self._bad_hosts:\n                logger.debug('Skipping %s due to bad host %s', url, host)\n            else:\n                req = Request(url, headers={'Accept-encoding': 'identity'})\n                try:\n                    logger.debug('Fetching %s', url)\n                    resp = self.opener.open(req, timeout=self.timeout)\n                    logger.debug('Fetched %s', url)\n                    headers = resp.info()\n                    content_type = headers.get('Content-Type', '')\n                    if HTML_CONTENT_TYPE.match(content_type):\n                        final_url = resp.geturl()\n                        data = resp.read()\n                        encoding = headers.get('Content-Encoding')\n                        if encoding:\n                            decoder = self.decoders[encoding]  # fail if not found\n                            data = decoder(data)\n                        encoding = 'utf-8'\n                        m = CHARSET.search(content_type)\n                        if m:\n                            encoding = m.group(1)\n                        try:\n                            data = data.decode(encoding)\n                        except UnicodeError:  # pragma: no cover\n                            data = data.decode('latin-1')  # fallback\n                        result = Page(data, final_url)\n                        self._page_cache[final_url] = result\n                except HTTPError as e:\n                    if e.code != 404:\n                        logger.exception('Fetch failed: %s: %s', url, e)\n                except URLError as e:  # pragma: no cover\n                    logger.exception('Fetch failed: %s: %s', url, e)\n                    with self._lock:\n                        self._bad_hosts.add(host)\n                except Exception as e:  # pragma: no cover\n                    logger.exception('Fetch failed: %s: %s', url, e)\n                finally:\n                    self._page_cache[url] = result  # even if None (failure)\n        return result\n\n    _distname_re = re.compile('<a href=[^>]*>([^<]+)<')\n\n    def get_distribution_names(self):\n        \"\"\"\n        Return all the distribution names known to this locator.\n        \"\"\"\n        result = set()\n        page = self.get_page(self.base_url)\n        if not page:\n            raise DistlibException('Unable to get %s' % self.base_url)\n        for match in self._distname_re.finditer(page.data):\n            result.add(match.group(1))\n        return result\n\n\nclass DirectoryLocator(Locator):\n    \"\"\"\n    This class locates distributions in a directory tree.\n    \"\"\"\n\n    def __init__(self, path, **kwargs):\n        \"\"\"\n        Initialise an instance.\n        :param path: The root of the directory tree to search.\n        :param kwargs: Passed to the superclass constructor,\n                       except for:\n                       * recursive - if True (the default), subdirectories are\n                         recursed into. If False, only the top-level directory\n                         is searched,\n        \"\"\"\n        self.recursive = kwargs.pop('recursive', True)\n        super(DirectoryLocator, self).__init__(**kwargs)\n        path = os.path.abspath(path)\n        if not os.path.isdir(path):  # pragma: no cover\n            raise DistlibException('Not a directory: %r' % path)\n        self.base_dir = path\n\n    def should_include(self, filename, parent):\n        \"\"\"\n        Should a filename be considered as a candidate for a distribution\n        archive? As well as the filename, the directory which contains it\n        is provided, though not used by the current implementation.\n        \"\"\"\n        return filename.endswith(self.downloadable_extensions)\n\n    def _get_project(self, name):\n        result = {'urls': {}, 'digests': {}}\n        for root, dirs, files in os.walk(self.base_dir):\n            for fn in files:\n                if self.should_include(fn, root):\n                    fn = os.path.join(root, fn)\n                    url = urlunparse(('file', '', pathname2url(os.path.abspath(fn)), '', '', ''))\n                    info = self.convert_url_to_download_info(url, name)\n                    if info:\n                        self._update_version_data(result, info)\n            if not self.recursive:\n                break\n        return result\n\n    def get_distribution_names(self):\n        \"\"\"\n        Return all the distribution names known to this locator.\n        \"\"\"\n        result = set()\n        for root, dirs, files in os.walk(self.base_dir):\n            for fn in files:\n                if self.should_include(fn, root):\n                    fn = os.path.join(root, fn)\n                    url = urlunparse(('file', '', pathname2url(os.path.abspath(fn)), '', '', ''))\n                    info = self.convert_url_to_download_info(url, None)\n                    if info:\n                        result.add(info['name'])\n            if not self.recursive:\n                break\n        return result\n\n\nclass JSONLocator(Locator):\n    \"\"\"\n    This locator uses special extended metadata (not available on PyPI) and is\n    the basis of performant dependency resolution in distlib. Other locators\n    require archive downloads before dependencies can be determined! As you\n    might imagine, that can be slow.\n    \"\"\"\n\n    def get_distribution_names(self):\n        \"\"\"\n        Return all the distribution names known to this locator.\n        \"\"\"\n        raise NotImplementedError('Not available from this locator')\n\n    def _get_project(self, name):\n        result = {'urls': {}, 'digests': {}}\n        data = get_project_data(name)\n        if data:\n            for info in data.get('files', []):\n                if info['ptype'] != 'sdist' or info['pyversion'] != 'source':\n                    continue\n                # We don't store summary in project metadata as it makes\n                # the data bigger for no benefit during dependency\n                # resolution\n                dist = make_dist(data['name'],\n                                 info['version'],\n                                 summary=data.get('summary', 'Placeholder for summary'),\n                                 scheme=self.scheme)\n                md = dist.metadata\n                md.source_url = info['url']\n                # TODO SHA256 digest\n                if 'digest' in info and info['digest']:\n                    dist.digest = ('md5', info['digest'])\n                md.dependencies = info.get('requirements', {})\n                dist.exports = info.get('exports', {})\n                result[dist.version] = dist\n                result['urls'].setdefault(dist.version, set()).add(info['url'])\n        return result\n\n\nclass DistPathLocator(Locator):\n    \"\"\"\n    This locator finds installed distributions in a path. It can be useful for\n    adding to an :class:`AggregatingLocator`.\n    \"\"\"\n\n    def __init__(self, distpath, **kwargs):\n        \"\"\"\n        Initialise an instance.\n\n        :param distpath: A :class:`DistributionPath` instance to search.\n        \"\"\"\n        super(DistPathLocator, self).__init__(**kwargs)\n        assert isinstance(distpath, DistributionPath)\n        self.distpath = distpath\n\n    def _get_project(self, name):\n        dist = self.distpath.get_distribution(name)\n        if dist is None:\n            result = {'urls': {}, 'digests': {}}\n        else:\n            result = {\n                dist.version: dist,\n                'urls': {\n                    dist.version: set([dist.source_url])\n                },\n                'digests': {\n                    dist.version: set([None])\n                }\n            }\n        return result\n\n\nclass AggregatingLocator(Locator):\n    \"\"\"\n    This class allows you to chain and/or merge a list of locators.\n    \"\"\"\n\n    def __init__(self, *locators, **kwargs):\n        \"\"\"\n        Initialise an instance.\n\n        :param locators: The list of locators to search.\n        :param kwargs: Passed to the superclass constructor,\n                       except for:\n                       * merge - if False (the default), the first successful\n                         search from any of the locators is returned. If True,\n                         the results from all locators are merged (this can be\n                         slow).\n        \"\"\"\n        self.merge = kwargs.pop('merge', False)\n        self.locators = locators\n        super(AggregatingLocator, self).__init__(**kwargs)\n\n    def clear_cache(self):\n        super(AggregatingLocator, self).clear_cache()\n        for locator in self.locators:\n            locator.clear_cache()\n\n    def _set_scheme(self, value):\n        self._scheme = value\n        for locator in self.locators:\n            locator.scheme = value\n\n    scheme = property(Locator.scheme.fget, _set_scheme)\n\n    def _get_project(self, name):\n        result = {}\n        for locator in self.locators:\n            d = locator.get_project(name)\n            if d:\n                if self.merge:\n                    files = result.get('urls', {})\n                    digests = result.get('digests', {})\n                    # next line could overwrite result['urls'], result['digests']\n                    result.update(d)\n                    df = result.get('urls')\n                    if files and df:\n                        for k, v in files.items():\n                            if k in df:\n                                df[k] |= v\n                            else:\n                                df[k] = v\n                    dd = result.get('digests')\n                    if digests and dd:\n                        dd.update(digests)\n                else:\n                    # See issue #18. If any dists are found and we're looking\n                    # for specific constraints, we only return something if\n                    # a match is found. For example, if a DirectoryLocator\n                    # returns just foo (1.0) while we're looking for\n                    # foo (>= 2.0), we'll pretend there was nothing there so\n                    # that subsequent locators can be queried. Otherwise we\n                    # would just return foo (1.0) which would then lead to a\n                    # failure to find foo (>= 2.0), because other locators\n                    # weren't searched. Note that this only matters when\n                    # merge=False.\n                    if self.matcher is None:\n                        found = True\n                    else:\n                        found = False\n                        for k in d:\n                            if self.matcher.match(k):\n                                found = True\n                                break\n                    if found:\n                        result = d\n                        break\n        return result\n\n    def get_distribution_names(self):\n        \"\"\"\n        Return all the distribution names known to this locator.\n        \"\"\"\n        result = set()\n        for locator in self.locators:\n            try:\n                result |= locator.get_distribution_names()\n            except NotImplementedError:\n                pass\n        return result\n\n\n# We use a legacy scheme simply because most of the dists on PyPI use legacy\n# versions which don't conform to PEP 440.\ndefault_locator = AggregatingLocator(\n    # JSONLocator(), # don't use as PEP 426 is withdrawn\n    SimpleScrapingLocator('https://pypi.org/simple/', timeout=3.0),\n    scheme='legacy')\n\nlocate = default_locator.locate\n\n\nclass DependencyFinder(object):\n    \"\"\"\n    Locate dependencies for distributions.\n    \"\"\"\n\n    def __init__(self, locator=None):\n        \"\"\"\n        Initialise an instance, using the specified locator\n        to locate distributions.\n        \"\"\"\n        self.locator = locator or default_locator\n        self.scheme = get_scheme(self.locator.scheme)\n\n    def add_distribution(self, dist):\n        \"\"\"\n        Add a distribution to the finder. This will update internal information\n        about who provides what.\n        :param dist: The distribution to add.\n        \"\"\"\n        logger.debug('adding distribution %s', dist)\n        name = dist.key\n        self.dists_by_name[name] = dist\n        self.dists[(name, dist.version)] = dist\n        for p in dist.provides:\n            name, version = parse_name_and_version(p)\n            logger.debug('Add to provided: %s, %s, %s', name, version, dist)\n            self.provided.setdefault(name, set()).add((version, dist))\n\n    def remove_distribution(self, dist):\n        \"\"\"\n        Remove a distribution from the finder. This will update internal\n        information about who provides what.\n        :param dist: The distribution to remove.\n        \"\"\"\n        logger.debug('removing distribution %s', dist)\n        name = dist.key\n        del self.dists_by_name[name]\n        del self.dists[(name, dist.version)]\n        for p in dist.provides:\n            name, version = parse_name_and_version(p)\n            logger.debug('Remove from provided: %s, %s, %s', name, version, dist)\n            s = self.provided[name]\n            s.remove((version, dist))\n            if not s:\n                del self.provided[name]\n\n    def get_matcher(self, reqt):\n        \"\"\"\n        Get a version matcher for a requirement.\n        :param reqt: The requirement\n        :type reqt: str\n        :return: A version matcher (an instance of\n                 :class:`distlib.version.Matcher`).\n        \"\"\"\n        try:\n            matcher = self.scheme.matcher(reqt)\n        except UnsupportedVersionError:  # pragma: no cover\n            # XXX compat-mode if cannot read the version\n            name = reqt.split()[0]\n            matcher = self.scheme.matcher(name)\n        return matcher\n\n    def find_providers(self, reqt):\n        \"\"\"\n        Find the distributions which can fulfill a requirement.\n\n        :param reqt: The requirement.\n         :type reqt: str\n        :return: A set of distribution which can fulfill the requirement.\n        \"\"\"\n        matcher = self.get_matcher(reqt)\n        name = matcher.key  # case-insensitive\n        result = set()\n        provided = self.provided\n        if name in provided:\n            for version, provider in provided[name]:\n                try:\n                    match = matcher.match(version)\n                except UnsupportedVersionError:\n                    match = False\n\n                if match:\n                    result.add(provider)\n                    break\n        return result\n\n    def try_to_replace(self, provider, other, problems):\n        \"\"\"\n        Attempt to replace one provider with another. This is typically used\n        when resolving dependencies from multiple sources, e.g. A requires\n        (B >= 1.0) while C requires (B >= 1.1).\n\n        For successful replacement, ``provider`` must meet all the requirements\n        which ``other`` fulfills.\n\n        :param provider: The provider we are trying to replace with.\n        :param other: The provider we're trying to replace.\n        :param problems: If False is returned, this will contain what\n                         problems prevented replacement. This is currently\n                         a tuple of the literal string 'cantreplace',\n                         ``provider``, ``other``  and the set of requirements\n                         that ``provider`` couldn't fulfill.\n        :return: True if we can replace ``other`` with ``provider``, else\n                 False.\n        \"\"\"\n        rlist = self.reqts[other]\n        unmatched = set()\n        for s in rlist:\n            matcher = self.get_matcher(s)\n            if not matcher.match(provider.version):\n                unmatched.add(s)\n        if unmatched:\n            # can't replace other with provider\n            problems.add(('cantreplace', provider, other, frozenset(unmatched)))\n            result = False\n        else:\n            # can replace other with provider\n            self.remove_distribution(other)\n            del self.reqts[other]\n            for s in rlist:\n                self.reqts.setdefault(provider, set()).add(s)\n            self.add_distribution(provider)\n            result = True\n        return result\n\n    def find(self, requirement, meta_extras=None, prereleases=False):\n        \"\"\"\n        Find a distribution and all distributions it depends on.\n\n        :param requirement: The requirement specifying the distribution to\n                            find, or a Distribution instance.\n        :param meta_extras: A list of meta extras such as :test:, :build: and\n                            so on.\n        :param prereleases: If ``True``, allow pre-release versions to be\n                            returned - otherwise, don't return prereleases\n                            unless they're all that's available.\n\n        Return a set of :class:`Distribution` instances and a set of\n        problems.\n\n        The distributions returned should be such that they have the\n        :attr:`required` attribute set to ``True`` if they were\n        from the ``requirement`` passed to ``find()``, and they have the\n        :attr:`build_time_dependency` attribute set to ``True`` unless they\n        are post-installation dependencies of the ``requirement``.\n\n        The problems should be a tuple consisting of the string\n        ``'unsatisfied'`` and the requirement which couldn't be satisfied\n        by any distribution known to the locator.\n        \"\"\"\n\n        self.provided = {}\n        self.dists = {}\n        self.dists_by_name = {}\n        self.reqts = {}\n\n        meta_extras = set(meta_extras or [])\n        if ':*:' in meta_extras:\n            meta_extras.remove(':*:')\n            # :meta: and :run: are implicitly included\n            meta_extras |= set([':test:', ':build:', ':dev:'])\n\n        if isinstance(requirement, Distribution):\n            dist = odist = requirement\n            logger.debug('passed %s as requirement', odist)\n        else:\n            dist = odist = self.locator.locate(requirement, prereleases=prereleases)\n            if dist is None:\n                raise DistlibException('Unable to locate %r' % requirement)\n            logger.debug('located %s', odist)\n        dist.requested = True\n        problems = set()\n        todo = set([dist])\n        install_dists = set([odist])\n        while todo:\n            dist = todo.pop()\n            name = dist.key  # case-insensitive\n            if name not in self.dists_by_name:\n                self.add_distribution(dist)\n            else:\n                # import pdb; pdb.set_trace()\n                other = self.dists_by_name[name]\n                if other != dist:\n                    self.try_to_replace(dist, other, problems)\n\n            ireqts = dist.run_requires | dist.meta_requires\n            sreqts = dist.build_requires\n            ereqts = set()\n            if meta_extras and dist in install_dists:\n                for key in ('test', 'build', 'dev'):\n                    e = ':%s:' % key\n                    if e in meta_extras:\n                        ereqts |= getattr(dist, '%s_requires' % key)\n            all_reqts = ireqts | sreqts | ereqts\n            for r in all_reqts:\n                providers = self.find_providers(r)\n                if not providers:\n                    logger.debug('No providers found for %r', r)\n                    provider = self.locator.locate(r, prereleases=prereleases)\n                    # If no provider is found and we didn't consider\n                    # prereleases, consider them now.\n                    if provider is None and not prereleases:\n                        provider = self.locator.locate(r, prereleases=True)\n                    if provider is None:\n                        logger.debug('Cannot satisfy %r', r)\n                        problems.add(('unsatisfied', r))\n                    else:\n                        n, v = provider.key, provider.version\n                        if (n, v) not in self.dists:\n                            todo.add(provider)\n                        providers.add(provider)\n                        if r in ireqts and dist in install_dists:\n                            install_dists.add(provider)\n                            logger.debug('Adding %s to install_dists', provider.name_and_version)\n                for p in providers:\n                    name = p.key\n                    if name not in self.dists_by_name:\n                        self.reqts.setdefault(p, set()).add(r)\n                    else:\n                        other = self.dists_by_name[name]\n                        if other != p:\n                            # see if other can be replaced by p\n                            self.try_to_replace(p, other, problems)\n\n        dists = set(self.dists.values())\n        for dist in dists:\n            dist.build_time_dependency = dist not in install_dists\n            if dist.build_time_dependency:\n                logger.debug('%s is a build-time dependency only.', dist.name_and_version)\n        logger.debug('find done for %s', odist)\n        return dists, problems\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_vendor/distlib/manifest.py","size":14168,"sha1":"4f3923e9575c2d64530fd413da556e1d84e74883","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"# -*- coding: utf-8 -*-\n#\n# Copyright (C) 2012-2023 Python Software Foundation.\n# See LICENSE.txt and CONTRIBUTORS.txt.\n#\n\"\"\"\nClass representing the list of files in a distribution.\n\nEquivalent to distutils.filelist, but fixes some problems.\n\"\"\"\nimport fnmatch\nimport logging\nimport os\nimport re\nimport sys\n\nfrom . import DistlibException\nfrom .compat import fsdecode\nfrom .util import convert_path\n\n\n__all__ = ['Manifest']\n\nlogger = logging.getLogger(__name__)\n\n# a \\ followed by some spaces + EOL\n_COLLAPSE_PATTERN = re.compile('\\\\\\\\w*\\n', re.M)\n_COMMENTED_LINE = re.compile('#.*?(?=\\n)|\\n(?=$)', re.M | re.S)\n\n#\n# Due to the different results returned by fnmatch.translate, we need\n# to do slightly different processing for Python 2.7 and 3.2 ... this needed\n# to be brought in for Python 3.6 onwards.\n#\n_PYTHON_VERSION = sys.version_info[:2]\n\n\nclass Manifest(object):\n    \"\"\"\n    A list of files built by exploring the filesystem and filtered by applying various\n    patterns to what we find there.\n    \"\"\"\n\n    def __init__(self, base=None):\n        \"\"\"\n        Initialise an instance.\n\n        :param base: The base directory to explore under.\n        \"\"\"\n        self.base = os.path.abspath(os.path.normpath(base or os.getcwd()))\n        self.prefix = self.base + os.sep\n        self.allfiles = None\n        self.files = set()\n\n    #\n    # Public API\n    #\n\n    def findall(self):\n        \"\"\"Find all files under the base and set ``allfiles`` to the absolute\n        pathnames of files found.\n        \"\"\"\n        from stat import S_ISREG, S_ISDIR, S_ISLNK\n\n        self.allfiles = allfiles = []\n        root = self.base\n        stack = [root]\n        pop = stack.pop\n        push = stack.append\n\n        while stack:\n            root = pop()\n            names = os.listdir(root)\n\n            for name in names:\n                fullname = os.path.join(root, name)\n\n                # Avoid excess stat calls -- just one will do, thank you!\n                stat = os.stat(fullname)\n                mode = stat.st_mode\n                if S_ISREG(mode):\n                    allfiles.append(fsdecode(fullname))\n                elif S_ISDIR(mode) and not S_ISLNK(mode):\n                    push(fullname)\n\n    def add(self, item):\n        \"\"\"\n        Add a file to the manifest.\n\n        :param item: The pathname to add. This can be relative to the base.\n        \"\"\"\n        if not item.startswith(self.prefix):\n            item = os.path.join(self.base, item)\n        self.files.add(os.path.normpath(item))\n\n    def add_many(self, items):\n        \"\"\"\n        Add a list of files to the manifest.\n\n        :param items: The pathnames to add. These can be relative to the base.\n        \"\"\"\n        for item in items:\n            self.add(item)\n\n    def sorted(self, wantdirs=False):\n        \"\"\"\n        Return sorted files in directory order\n        \"\"\"\n\n        def add_dir(dirs, d):\n            dirs.add(d)\n            logger.debug('add_dir added %s', d)\n            if d != self.base:\n                parent, _ = os.path.split(d)\n                assert parent not in ('', '/')\n                add_dir(dirs, parent)\n\n        result = set(self.files)    # make a copy!\n        if wantdirs:\n            dirs = set()\n            for f in result:\n                add_dir(dirs, os.path.dirname(f))\n            result |= dirs\n        return [os.path.join(*path_tuple) for path_tuple in\n                sorted(os.path.split(path) for path in result)]\n\n    def clear(self):\n        \"\"\"Clear all collected files.\"\"\"\n        self.files = set()\n        self.allfiles = []\n\n    def process_directive(self, directive):\n        \"\"\"\n        Process a directive which either adds some files from ``allfiles`` to\n        ``files``, or removes some files from ``files``.\n\n        :param directive: The directive to process. This should be in a format\n                     compatible with distutils ``MANIFEST.in`` files:\n\n                     http://docs.python.org/distutils/sourcedist.html#commands\n        \"\"\"\n        # Parse the line: split it up, make sure the right number of words\n        # is there, and return the relevant words.  'action' is always\n        # defined: it's the first word of the line.  Which of the other\n        # three are defined depends on the action; it'll be either\n        # patterns, (dir and patterns), or (dirpattern).\n        action, patterns, thedir, dirpattern = self._parse_directive(directive)\n\n        # OK, now we know that the action is valid and we have the\n        # right number of words on the line for that action -- so we\n        # can proceed with minimal error-checking.\n        if action == 'include':\n            for pattern in patterns:\n                if not self._include_pattern(pattern, anchor=True):\n                    logger.warning('no files found matching %r', pattern)\n\n        elif action == 'exclude':\n            for pattern in patterns:\n                self._exclude_pattern(pattern, anchor=True)\n\n        elif action == 'global-include':\n            for pattern in patterns:\n                if not self._include_pattern(pattern, anchor=False):\n                    logger.warning('no files found matching %r '\n                                   'anywhere in distribution', pattern)\n\n        elif action == 'global-exclude':\n            for pattern in patterns:\n                self._exclude_pattern(pattern, anchor=False)\n\n        elif action == 'recursive-include':\n            for pattern in patterns:\n                if not self._include_pattern(pattern, prefix=thedir):\n                    logger.warning('no files found matching %r '\n                                   'under directory %r', pattern, thedir)\n\n        elif action == 'recursive-exclude':\n            for pattern in patterns:\n                self._exclude_pattern(pattern, prefix=thedir)\n\n        elif action == 'graft':\n            if not self._include_pattern(None, prefix=dirpattern):\n                logger.warning('no directories found matching %r',\n                               dirpattern)\n\n        elif action == 'prune':\n            if not self._exclude_pattern(None, prefix=dirpattern):\n                logger.warning('no previously-included directories found '\n                               'matching %r', dirpattern)\n        else:   # pragma: no cover\n            # This should never happen, as it should be caught in\n            # _parse_template_line\n            raise DistlibException(\n                'invalid action %r' % action)\n\n    #\n    # Private API\n    #\n\n    def _parse_directive(self, directive):\n        \"\"\"\n        Validate a directive.\n        :param directive: The directive to validate.\n        :return: A tuple of action, patterns, thedir, dir_patterns\n        \"\"\"\n        words = directive.split()\n        if len(words) == 1 and words[0] not in ('include', 'exclude',\n                                                'global-include',\n                                                'global-exclude',\n                                                'recursive-include',\n                                                'recursive-exclude',\n                                                'graft', 'prune'):\n            # no action given, let's use the default 'include'\n            words.insert(0, 'include')\n\n        action = words[0]\n        patterns = thedir = dir_pattern = None\n\n        if action in ('include', 'exclude',\n                      'global-include', 'global-exclude'):\n            if len(words) < 2:\n                raise DistlibException(\n                    '%r expects <pattern1> <pattern2> ...' % action)\n\n            patterns = [convert_path(word) for word in words[1:]]\n\n        elif action in ('recursive-include', 'recursive-exclude'):\n            if len(words) < 3:\n                raise DistlibException(\n                    '%r expects <dir> <pattern1> <pattern2> ...' % action)\n\n            thedir = convert_path(words[1])\n            patterns = [convert_path(word) for word in words[2:]]\n\n        elif action in ('graft', 'prune'):\n            if len(words) != 2:\n                raise DistlibException(\n                    '%r expects a single <dir_pattern>' % action)\n\n            dir_pattern = convert_path(words[1])\n\n        else:\n            raise DistlibException('unknown action %r' % action)\n\n        return action, patterns, thedir, dir_pattern\n\n    def _include_pattern(self, pattern, anchor=True, prefix=None,\n                         is_regex=False):\n        \"\"\"Select strings (presumably filenames) from 'self.files' that\n        match 'pattern', a Unix-style wildcard (glob) pattern.\n\n        Patterns are not quite the same as implemented by the 'fnmatch'\n        module: '*' and '?'  match non-special characters, where \"special\"\n        is platform-dependent: slash on Unix; colon, slash, and backslash on\n        DOS/Windows; and colon on Mac OS.\n\n        If 'anchor' is true (the default), then the pattern match is more\n        stringent: \"*.py\" will match \"foo.py\" but not \"foo/bar.py\".  If\n        'anchor' is false, both of these will match.\n\n        If 'prefix' is supplied, then only filenames starting with 'prefix'\n        (itself a pattern) and ending with 'pattern', with anything in between\n        them, will match.  'anchor' is ignored in this case.\n\n        If 'is_regex' is true, 'anchor' and 'prefix' are ignored, and\n        'pattern' is assumed to be either a string containing a regex or a\n        regex object -- no translation is done, the regex is just compiled\n        and used as-is.\n\n        Selected strings will be added to self.files.\n\n        Return True if files are found.\n        \"\"\"\n        # XXX docstring lying about what the special chars are?\n        found = False\n        pattern_re = self._translate_pattern(pattern, anchor, prefix, is_regex)\n\n        # delayed loading of allfiles list\n        if self.allfiles is None:\n            self.findall()\n\n        for name in self.allfiles:\n            if pattern_re.search(name):\n                self.files.add(name)\n                found = True\n        return found\n\n    def _exclude_pattern(self, pattern, anchor=True, prefix=None,\n                         is_regex=False):\n        \"\"\"Remove strings (presumably filenames) from 'files' that match\n        'pattern'.\n\n        Other parameters are the same as for 'include_pattern()', above.\n        The list 'self.files' is modified in place. Return True if files are\n        found.\n\n        This API is public to allow e.g. exclusion of SCM subdirs, e.g. when\n        packaging source distributions\n        \"\"\"\n        found = False\n        pattern_re = self._translate_pattern(pattern, anchor, prefix, is_regex)\n        for f in list(self.files):\n            if pattern_re.search(f):\n                self.files.remove(f)\n                found = True\n        return found\n\n    def _translate_pattern(self, pattern, anchor=True, prefix=None,\n                           is_regex=False):\n        \"\"\"Translate a shell-like wildcard pattern to a compiled regular\n        expression.\n\n        Return the compiled regex.  If 'is_regex' true,\n        then 'pattern' is directly compiled to a regex (if it's a string)\n        or just returned as-is (assumes it's a regex object).\n        \"\"\"\n        if is_regex:\n            if isinstance(pattern, str):\n                return re.compile(pattern)\n            else:\n                return pattern\n\n        if _PYTHON_VERSION > (3, 2):\n            # ditch start and end characters\n            start, _, end = self._glob_to_re('_').partition('_')\n\n        if pattern:\n            pattern_re = self._glob_to_re(pattern)\n            if _PYTHON_VERSION > (3, 2):\n                assert pattern_re.startswith(start) and pattern_re.endswith(end)\n        else:\n            pattern_re = ''\n\n        base = re.escape(os.path.join(self.base, ''))\n        if prefix is not None:\n            # ditch end of pattern character\n            if _PYTHON_VERSION <= (3, 2):\n                empty_pattern = self._glob_to_re('')\n                prefix_re = self._glob_to_re(prefix)[:-len(empty_pattern)]\n            else:\n                prefix_re = self._glob_to_re(prefix)\n                assert prefix_re.startswith(start) and prefix_re.endswith(end)\n                prefix_re = prefix_re[len(start): len(prefix_re) - len(end)]\n            sep = os.sep\n            if os.sep == '\\\\':\n                sep = r'\\\\'\n            if _PYTHON_VERSION <= (3, 2):\n                pattern_re = '^' + base + sep.join((prefix_re,\n                                                    '.*' + pattern_re))\n            else:\n                pattern_re = pattern_re[len(start): len(pattern_re) - len(end)]\n                pattern_re = r'%s%s%s%s.*%s%s' % (start, base, prefix_re, sep,\n                                                  pattern_re, end)\n        else:  # no prefix -- respect anchor flag\n            if anchor:\n                if _PYTHON_VERSION <= (3, 2):\n                    pattern_re = '^' + base + pattern_re\n                else:\n                    pattern_re = r'%s%s%s' % (start, base, pattern_re[len(start):])\n\n        return re.compile(pattern_re)\n\n    def _glob_to_re(self, pattern):\n        \"\"\"Translate a shell-like glob pattern to a regular expression.\n\n        Return a string containing the regex.  Differs from\n        'fnmatch.translate()' in that '*' does not match \"special characters\"\n        (which are platform-specific).\n        \"\"\"\n        pattern_re = fnmatch.translate(pattern)\n\n        # '?' and '*' in the glob pattern become '.' and '.*' in the RE, which\n        # IMHO is wrong -- '?' and '*' aren't supposed to match slash in Unix,\n        # and by extension they shouldn't match such \"special characters\" under\n        # any OS.  So change all non-escaped dots in the RE to match any\n        # character except the special characters (currently: just os.sep).\n        sep = os.sep\n        if os.sep == '\\\\':\n            # we're using a regex to manipulate a regex, so we need\n            # to escape the backslash twice\n            sep = r'\\\\\\\\'\n        escaped = r'\\1[^%s]' % sep\n        pattern_re = re.sub(r'((?<!\\\\)(\\\\\\\\)*)\\.', escaped, pattern_re)\n        return pattern_re\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_vendor/distlib/markers.py","size":5164,"sha1":"f4856e2646544139485564a7c20f5af60cbc3521","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"# -*- coding: utf-8 -*-\n#\n# Copyright (C) 2012-2023 Vinay Sajip.\n# Licensed to the Python Software Foundation under a contributor agreement.\n# See LICENSE.txt and CONTRIBUTORS.txt.\n#\n\"\"\"\nParser for the environment markers micro-language defined in PEP 508.\n\"\"\"\n\n# Note: In PEP 345, the micro-language was Python compatible, so the ast\n# module could be used to parse it. However, PEP 508 introduced operators such\n# as ~= and === which aren't in Python, necessitating a different approach.\n\nimport os\nimport re\nimport sys\nimport platform\n\nfrom .compat import string_types\nfrom .util import in_venv, parse_marker\nfrom .version import LegacyVersion as LV\n\n__all__ = ['interpret']\n\n_VERSION_PATTERN = re.compile(r'((\\d+(\\.\\d+)*\\w*)|\\'(\\d+(\\.\\d+)*\\w*)\\'|\\\"(\\d+(\\.\\d+)*\\w*)\\\")')\n_VERSION_MARKERS = {'python_version', 'python_full_version'}\n\n\ndef _is_version_marker(s):\n    return isinstance(s, string_types) and s in _VERSION_MARKERS\n\n\ndef _is_literal(o):\n    if not isinstance(o, string_types) or not o:\n        return False\n    return o[0] in '\\'\"'\n\n\ndef _get_versions(s):\n    return {LV(m.groups()[0]) for m in _VERSION_PATTERN.finditer(s)}\n\n\nclass Evaluator(object):\n    \"\"\"\n    This class is used to evaluate marker expressions.\n    \"\"\"\n\n    operations = {\n        '==': lambda x, y: x == y,\n        '===': lambda x, y: x == y,\n        '~=': lambda x, y: x == y or x > y,\n        '!=': lambda x, y: x != y,\n        '<': lambda x, y: x < y,\n        '<=': lambda x, y: x == y or x < y,\n        '>': lambda x, y: x > y,\n        '>=': lambda x, y: x == y or x > y,\n        'and': lambda x, y: x and y,\n        'or': lambda x, y: x or y,\n        'in': lambda x, y: x in y,\n        'not in': lambda x, y: x not in y,\n    }\n\n    def evaluate(self, expr, context):\n        \"\"\"\n        Evaluate a marker expression returned by the :func:`parse_requirement`\n        function in the specified context.\n        \"\"\"\n        if isinstance(expr, string_types):\n            if expr[0] in '\\'\"':\n                result = expr[1:-1]\n            else:\n                if expr not in context:\n                    raise SyntaxError('unknown variable: %s' % expr)\n                result = context[expr]\n        else:\n            assert isinstance(expr, dict)\n            op = expr['op']\n            if op not in self.operations:\n                raise NotImplementedError('op not implemented: %s' % op)\n            elhs = expr['lhs']\n            erhs = expr['rhs']\n            if _is_literal(expr['lhs']) and _is_literal(expr['rhs']):\n                raise SyntaxError('invalid comparison: %s %s %s' % (elhs, op, erhs))\n\n            lhs = self.evaluate(elhs, context)\n            rhs = self.evaluate(erhs, context)\n            if ((_is_version_marker(elhs) or _is_version_marker(erhs)) and\n                    op in ('<', '<=', '>', '>=', '===', '==', '!=', '~=')):\n                lhs = LV(lhs)\n                rhs = LV(rhs)\n            elif _is_version_marker(elhs) and op in ('in', 'not in'):\n                lhs = LV(lhs)\n                rhs = _get_versions(rhs)\n            result = self.operations[op](lhs, rhs)\n        return result\n\n\n_DIGITS = re.compile(r'\\d+\\.\\d+')\n\n\ndef default_context():\n\n    def format_full_version(info):\n        version = '%s.%s.%s' % (info.major, info.minor, info.micro)\n        kind = info.releaselevel\n        if kind != 'final':\n            version += kind[0] + str(info.serial)\n        return version\n\n    if hasattr(sys, 'implementation'):\n        implementation_version = format_full_version(sys.implementation.version)\n        implementation_name = sys.implementation.name\n    else:\n        implementation_version = '0'\n        implementation_name = ''\n\n    ppv = platform.python_version()\n    m = _DIGITS.match(ppv)\n    pv = m.group(0)\n    result = {\n        'implementation_name': implementation_name,\n        'implementation_version': implementation_version,\n        'os_name': os.name,\n        'platform_machine': platform.machine(),\n        'platform_python_implementation': platform.python_implementation(),\n        'platform_release': platform.release(),\n        'platform_system': platform.system(),\n        'platform_version': platform.version(),\n        'platform_in_venv': str(in_venv()),\n        'python_full_version': ppv,\n        'python_version': pv,\n        'sys_platform': sys.platform,\n    }\n    return result\n\n\nDEFAULT_CONTEXT = default_context()\ndel default_context\n\nevaluator = Evaluator()\n\n\ndef interpret(marker, execution_context=None):\n    \"\"\"\n    Interpret a marker and return a result depending on environment.\n\n    :param marker: The marker to interpret.\n    :type marker: str\n    :param execution_context: The context used for name lookup.\n    :type execution_context: mapping\n    \"\"\"\n    try:\n        expr, rest = parse_marker(marker)\n    except Exception as e:\n        raise SyntaxError('Unable to interpret marker syntax: %s: %s' % (marker, e))\n    if rest and rest[0] != '#':\n        raise SyntaxError('unexpected trailing data in marker: %s: %s' % (marker, rest))\n    context = dict(DEFAULT_CONTEXT)\n    if execution_context:\n        context.update(execution_context)\n    return evaluator.evaluate(expr, context)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_vendor/distlib/metadata.py","size":38724,"sha1":"d80f0eff18d0f7a74d42b9cba0f7c64b02787a35","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"# -*- coding: utf-8 -*-\n#\n# Copyright (C) 2012 The Python Software Foundation.\n# See LICENSE.txt and CONTRIBUTORS.txt.\n#\n\"\"\"Implementation of the Metadata for Python packages PEPs.\n\nSupports all metadata formats (1.0, 1.1, 1.2, 1.3/2.1 and 2.2).\n\"\"\"\nfrom __future__ import unicode_literals\n\nimport codecs\nfrom email import message_from_file\nimport json\nimport logging\nimport re\n\nfrom . import DistlibException, __version__\nfrom .compat import StringIO, string_types, text_type\nfrom .markers import interpret\nfrom .util import extract_by_key, get_extras\nfrom .version import get_scheme, PEP440_VERSION_RE\n\nlogger = logging.getLogger(__name__)\n\n\nclass MetadataMissingError(DistlibException):\n    \"\"\"A required metadata is missing\"\"\"\n\n\nclass MetadataConflictError(DistlibException):\n    \"\"\"Attempt to read or write metadata fields that are conflictual.\"\"\"\n\n\nclass MetadataUnrecognizedVersionError(DistlibException):\n    \"\"\"Unknown metadata version number.\"\"\"\n\n\nclass MetadataInvalidError(DistlibException):\n    \"\"\"A metadata value is invalid\"\"\"\n\n\n# public API of this module\n__all__ = ['Metadata', 'PKG_INFO_ENCODING', 'PKG_INFO_PREFERRED_VERSION']\n\n# Encoding used for the PKG-INFO files\nPKG_INFO_ENCODING = 'utf-8'\n\n# preferred version. Hopefully will be changed\n# to 1.2 once PEP 345 is supported everywhere\nPKG_INFO_PREFERRED_VERSION = '1.1'\n\n_LINE_PREFIX_1_2 = re.compile('\\n       \\\\|')\n_LINE_PREFIX_PRE_1_2 = re.compile('\\n        ')\n_241_FIELDS = ('Metadata-Version', 'Name', 'Version', 'Platform', 'Summary', 'Description', 'Keywords', 'Home-page',\n               'Author', 'Author-email', 'License')\n\n_314_FIELDS = ('Metadata-Version', 'Name', 'Version', 'Platform', 'Supported-Platform', 'Summary', 'Description',\n               'Keywords', 'Home-page', 'Author', 'Author-email', 'License', 'Classifier', 'Download-URL', 'Obsoletes',\n               'Provides', 'Requires')\n\n_314_MARKERS = ('Obsoletes', 'Provides', 'Requires', 'Classifier', 'Download-URL')\n\n_345_FIELDS = ('Metadata-Version', 'Name', 'Version', 'Platform', 'Supported-Platform', 'Summary', 'Description',\n               'Keywords', 'Home-page', 'Author', 'Author-email', 'Maintainer', 'Maintainer-email', 'License',\n               'Classifier', 'Download-URL', 'Obsoletes-Dist', 'Project-URL', 'Provides-Dist', 'Requires-Dist',\n               'Requires-Python', 'Requires-External')\n\n_345_MARKERS = ('Provides-Dist', 'Requires-Dist', 'Requires-Python', 'Obsoletes-Dist', 'Requires-External',\n                'Maintainer', 'Maintainer-email', 'Project-URL')\n\n_426_FIELDS = ('Metadata-Version', 'Name', 'Version', 'Platform', 'Supported-Platform', 'Summary', 'Description',\n               'Keywords', 'Home-page', 'Author', 'Author-email', 'Maintainer', 'Maintainer-email', 'License',\n               'Classifier', 'Download-URL', 'Obsoletes-Dist', 'Project-URL', 'Provides-Dist', 'Requires-Dist',\n               'Requires-Python', 'Requires-External', 'Private-Version', 'Obsoleted-By', 'Setup-Requires-Dist',\n               'Extension', 'Provides-Extra')\n\n_426_MARKERS = ('Private-Version', 'Provides-Extra', 'Obsoleted-By', 'Setup-Requires-Dist', 'Extension')\n\n# See issue #106: Sometimes 'Requires' and 'Provides' occur wrongly in\n# the metadata. Include them in the tuple literal below to allow them\n# (for now).\n# Ditto for Obsoletes - see issue #140.\n_566_FIELDS = _426_FIELDS + ('Description-Content-Type', 'Requires', 'Provides', 'Obsoletes')\n\n_566_MARKERS = ('Description-Content-Type', )\n\n_643_MARKERS = ('Dynamic', 'License-File')\n\n_643_FIELDS = _566_FIELDS + _643_MARKERS\n\n_ALL_FIELDS = set()\n_ALL_FIELDS.update(_241_FIELDS)\n_ALL_FIELDS.update(_314_FIELDS)\n_ALL_FIELDS.update(_345_FIELDS)\n_ALL_FIELDS.update(_426_FIELDS)\n_ALL_FIELDS.update(_566_FIELDS)\n_ALL_FIELDS.update(_643_FIELDS)\n\nEXTRA_RE = re.compile(r'''extra\\s*==\\s*(\"([^\"]+)\"|'([^']+)')''')\n\n\ndef _version2fieldlist(version):\n    if version == '1.0':\n        return _241_FIELDS\n    elif version == '1.1':\n        return _314_FIELDS\n    elif version == '1.2':\n        return _345_FIELDS\n    elif version in ('1.3', '2.1'):\n        # avoid adding field names if already there\n        return _345_FIELDS + tuple(f for f in _566_FIELDS if f not in _345_FIELDS)\n    elif version == '2.0':\n        raise ValueError('Metadata 2.0 is withdrawn and not supported')\n        # return _426_FIELDS\n    elif version == '2.2':\n        return _643_FIELDS\n    raise MetadataUnrecognizedVersionError(version)\n\n\ndef _best_version(fields):\n    \"\"\"Detect the best version depending on the fields used.\"\"\"\n\n    def _has_marker(keys, markers):\n        return any(marker in keys for marker in markers)\n\n    keys = [key for key, value in fields.items() if value not in ([], 'UNKNOWN', None)]\n    possible_versions = ['1.0', '1.1', '1.2', '1.3', '2.1', '2.2']  # 2.0 removed\n\n    # first let's try to see if a field is not part of one of the version\n    for key in keys:\n        if key not in _241_FIELDS and '1.0' in possible_versions:\n            possible_versions.remove('1.0')\n            logger.debug('Removed 1.0 due to %s', key)\n        if key not in _314_FIELDS and '1.1' in possible_versions:\n            possible_versions.remove('1.1')\n            logger.debug('Removed 1.1 due to %s', key)\n        if key not in _345_FIELDS and '1.2' in possible_versions:\n            possible_versions.remove('1.2')\n            logger.debug('Removed 1.2 due to %s', key)\n        if key not in _566_FIELDS and '1.3' in possible_versions:\n            possible_versions.remove('1.3')\n            logger.debug('Removed 1.3 due to %s', key)\n        if key not in _566_FIELDS and '2.1' in possible_versions:\n            if key != 'Description':  # In 2.1, description allowed after headers\n                possible_versions.remove('2.1')\n                logger.debug('Removed 2.1 due to %s', key)\n        if key not in _643_FIELDS and '2.2' in possible_versions:\n            possible_versions.remove('2.2')\n            logger.debug('Removed 2.2 due to %s', key)\n        # if key not in _426_FIELDS and '2.0' in possible_versions:\n        # possible_versions.remove('2.0')\n        # logger.debug('Removed 2.0 due to %s', key)\n\n    # possible_version contains qualified versions\n    if len(possible_versions) == 1:\n        return possible_versions[0]  # found !\n    elif len(possible_versions) == 0:\n        logger.debug('Out of options - unknown metadata set: %s', fields)\n        raise MetadataConflictError('Unknown metadata set')\n\n    # let's see if one unique marker is found\n    is_1_1 = '1.1' in possible_versions and _has_marker(keys, _314_MARKERS)\n    is_1_2 = '1.2' in possible_versions and _has_marker(keys, _345_MARKERS)\n    is_2_1 = '2.1' in possible_versions and _has_marker(keys, _566_MARKERS)\n    # is_2_0 = '2.0' in possible_versions and _has_marker(keys, _426_MARKERS)\n    is_2_2 = '2.2' in possible_versions and _has_marker(keys, _643_MARKERS)\n    if int(is_1_1) + int(is_1_2) + int(is_2_1) + int(is_2_2) > 1:\n        raise MetadataConflictError('You used incompatible 1.1/1.2/2.1/2.2 fields')\n\n    # we have the choice, 1.0, or 1.2, 2.1 or 2.2\n    #   - 1.0 has a broken Summary field but works with all tools\n    #   - 1.1 is to avoid\n    #   - 1.2 fixes Summary but has little adoption\n    #   - 2.1 adds more features\n    #   - 2.2 is the latest\n    if not is_1_1 and not is_1_2 and not is_2_1 and not is_2_2:\n        # we couldn't find any specific marker\n        if PKG_INFO_PREFERRED_VERSION in possible_versions:\n            return PKG_INFO_PREFERRED_VERSION\n    if is_1_1:\n        return '1.1'\n    if is_1_2:\n        return '1.2'\n    if is_2_1:\n        return '2.1'\n    # if is_2_2:\n    # return '2.2'\n\n    return '2.2'\n\n\n# This follows the rules about transforming keys as described in\n# https://www.python.org/dev/peps/pep-0566/#id17\n_ATTR2FIELD = {name.lower().replace(\"-\", \"_\"): name for name in _ALL_FIELDS}\n_FIELD2ATTR = {field: attr for attr, field in _ATTR2FIELD.items()}\n\n_PREDICATE_FIELDS = ('Requires-Dist', 'Obsoletes-Dist', 'Provides-Dist')\n_VERSIONS_FIELDS = ('Requires-Python', )\n_VERSION_FIELDS = ('Version', )\n_LISTFIELDS = ('Platform', 'Classifier', 'Obsoletes', 'Requires', 'Provides', 'Obsoletes-Dist', 'Provides-Dist',\n               'Requires-Dist', 'Requires-External', 'Project-URL', 'Supported-Platform', 'Setup-Requires-Dist',\n               'Provides-Extra', 'Extension', 'License-File')\n_LISTTUPLEFIELDS = ('Project-URL', )\n\n_ELEMENTSFIELD = ('Keywords', )\n\n_UNICODEFIELDS = ('Author', 'Maintainer', 'Summary', 'Description')\n\n_MISSING = object()\n\n_FILESAFE = re.compile('[^A-Za-z0-9.]+')\n\n\ndef _get_name_and_version(name, version, for_filename=False):\n    \"\"\"Return the distribution name with version.\n\n    If for_filename is true, return a filename-escaped form.\"\"\"\n    if for_filename:\n        # For both name and version any runs of non-alphanumeric or '.'\n        # characters are replaced with a single '-'.  Additionally any\n        # spaces in the version string become '.'\n        name = _FILESAFE.sub('-', name)\n        version = _FILESAFE.sub('-', version.replace(' ', '.'))\n    return '%s-%s' % (name, version)\n\n\nclass LegacyMetadata(object):\n    \"\"\"The legacy metadata of a release.\n\n    Supports versions 1.0, 1.1, 1.2, 2.0 and 1.3/2.1 (auto-detected). You can\n    instantiate the class with one of these arguments (or none):\n    - *path*, the path to a metadata file\n    - *fileobj* give a file-like object with metadata as content\n    - *mapping* is a dict-like object\n    - *scheme* is a version scheme name\n    \"\"\"\n\n    # TODO document the mapping API and UNKNOWN default key\n\n    def __init__(self, path=None, fileobj=None, mapping=None, scheme='default'):\n        if [path, fileobj, mapping].count(None) < 2:\n            raise TypeError('path, fileobj and mapping are exclusive')\n        self._fields = {}\n        self.requires_files = []\n        self._dependencies = None\n        self.scheme = scheme\n        if path is not None:\n            self.read(path)\n        elif fileobj is not None:\n            self.read_file(fileobj)\n        elif mapping is not None:\n            self.update(mapping)\n            self.set_metadata_version()\n\n    def set_metadata_version(self):\n        self._fields['Metadata-Version'] = _best_version(self._fields)\n\n    def _write_field(self, fileobj, name, value):\n        fileobj.write('%s: %s\\n' % (name, value))\n\n    def __getitem__(self, name):\n        return self.get(name)\n\n    def __setitem__(self, name, value):\n        return self.set(name, value)\n\n    def __delitem__(self, name):\n        field_name = self._convert_name(name)\n        try:\n            del self._fields[field_name]\n        except KeyError:\n            raise KeyError(name)\n\n    def __contains__(self, name):\n        return (name in self._fields or self._convert_name(name) in self._fields)\n\n    def _convert_name(self, name):\n        if name in _ALL_FIELDS:\n            return name\n        name = name.replace('-', '_').lower()\n        return _ATTR2FIELD.get(name, name)\n\n    def _default_value(self, name):\n        if name in _LISTFIELDS or name in _ELEMENTSFIELD:\n            return []\n        return 'UNKNOWN'\n\n    def _remove_line_prefix(self, value):\n        if self.metadata_version in ('1.0', '1.1'):\n            return _LINE_PREFIX_PRE_1_2.sub('\\n', value)\n        else:\n            return _LINE_PREFIX_1_2.sub('\\n', value)\n\n    def __getattr__(self, name):\n        if name in _ATTR2FIELD:\n            return self[name]\n        raise AttributeError(name)\n\n    #\n    # Public API\n    #\n\n    def get_fullname(self, filesafe=False):\n        \"\"\"\n        Return the distribution name with version.\n\n        If filesafe is true, return a filename-escaped form.\n        \"\"\"\n        return _get_name_and_version(self['Name'], self['Version'], filesafe)\n\n    def is_field(self, name):\n        \"\"\"return True if name is a valid metadata key\"\"\"\n        name = self._convert_name(name)\n        return name in _ALL_FIELDS\n\n    def is_multi_field(self, name):\n        name = self._convert_name(name)\n        return name in _LISTFIELDS\n\n    def read(self, filepath):\n        \"\"\"Read the metadata values from a file path.\"\"\"\n        fp = codecs.open(filepath, 'r', encoding='utf-8')\n        try:\n            self.read_file(fp)\n        finally:\n            fp.close()\n\n    def read_file(self, fileob):\n        \"\"\"Read the metadata values from a file object.\"\"\"\n        msg = message_from_file(fileob)\n        self._fields['Metadata-Version'] = msg['metadata-version']\n\n        # When reading, get all the fields we can\n        for field in _ALL_FIELDS:\n            if field not in msg:\n                continue\n            if field in _LISTFIELDS:\n                # we can have multiple lines\n                values = msg.get_all(field)\n                if field in _LISTTUPLEFIELDS and values is not None:\n                    values = [tuple(value.split(',')) for value in values]\n                self.set(field, values)\n            else:\n                # single line\n                value = msg[field]\n                if value is not None and value != 'UNKNOWN':\n                    self.set(field, value)\n\n        # PEP 566 specifies that the body be used for the description, if\n        # available\n        body = msg.get_payload()\n        self[\"Description\"] = body if body else self[\"Description\"]\n        # logger.debug('Attempting to set metadata for %s', self)\n        # self.set_metadata_version()\n\n    def write(self, filepath, skip_unknown=False):\n        \"\"\"Write the metadata fields to filepath.\"\"\"\n        fp = codecs.open(filepath, 'w', encoding='utf-8')\n        try:\n            self.write_file(fp, skip_unknown)\n        finally:\n            fp.close()\n\n    def write_file(self, fileobject, skip_unknown=False):\n        \"\"\"Write the PKG-INFO format data to a file object.\"\"\"\n        self.set_metadata_version()\n\n        for field in _version2fieldlist(self['Metadata-Version']):\n            values = self.get(field)\n            if skip_unknown and values in ('UNKNOWN', [], ['UNKNOWN']):\n                continue\n            if field in _ELEMENTSFIELD:\n                self._write_field(fileobject, field, ','.join(values))\n                continue\n            if field not in _LISTFIELDS:\n                if field == 'Description':\n                    if self.metadata_version in ('1.0', '1.1'):\n                        values = values.replace('\\n', '\\n        ')\n                    else:\n                        values = values.replace('\\n', '\\n       |')\n                values = [values]\n\n            if field in _LISTTUPLEFIELDS:\n                values = [','.join(value) for value in values]\n\n            for value in values:\n                self._write_field(fileobject, field, value)\n\n    def update(self, other=None, **kwargs):\n        \"\"\"Set metadata values from the given iterable `other` and kwargs.\n\n        Behavior is like `dict.update`: If `other` has a ``keys`` method,\n        they are looped over and ``self[key]`` is assigned ``other[key]``.\n        Else, ``other`` is an iterable of ``(key, value)`` iterables.\n\n        Keys that don't match a metadata field or that have an empty value are\n        dropped.\n        \"\"\"\n\n        def _set(key, value):\n            if key in _ATTR2FIELD and value:\n                self.set(self._convert_name(key), value)\n\n        if not other:\n            # other is None or empty container\n            pass\n        elif hasattr(other, 'keys'):\n            for k in other.keys():\n                _set(k, other[k])\n        else:\n            for k, v in other:\n                _set(k, v)\n\n        if kwargs:\n            for k, v in kwargs.items():\n                _set(k, v)\n\n    def set(self, name, value):\n        \"\"\"Control then set a metadata field.\"\"\"\n        name = self._convert_name(name)\n\n        if ((name in _ELEMENTSFIELD or name == 'Platform') and not isinstance(value, (list, tuple))):\n            if isinstance(value, string_types):\n                value = [v.strip() for v in value.split(',')]\n            else:\n                value = []\n        elif (name in _LISTFIELDS and not isinstance(value, (list, tuple))):\n            if isinstance(value, string_types):\n                value = [value]\n            else:\n                value = []\n\n        if logger.isEnabledFor(logging.WARNING):\n            project_name = self['Name']\n\n            scheme = get_scheme(self.scheme)\n            if name in _PREDICATE_FIELDS and value is not None:\n                for v in value:\n                    # check that the values are valid\n                    if not scheme.is_valid_matcher(v.split(';')[0]):\n                        logger.warning(\"'%s': '%s' is not valid (field '%s')\", project_name, v, name)\n            # FIXME this rejects UNKNOWN, is that right?\n            elif name in _VERSIONS_FIELDS and value is not None:\n                if not scheme.is_valid_constraint_list(value):\n                    logger.warning(\"'%s': '%s' is not a valid version (field '%s')\", project_name, value, name)\n            elif name in _VERSION_FIELDS and value is not None:\n                if not scheme.is_valid_version(value):\n                    logger.warning(\"'%s': '%s' is not a valid version (field '%s')\", project_name, value, name)\n\n        if name in _UNICODEFIELDS:\n            if name == 'Description':\n                value = self._remove_line_prefix(value)\n\n        self._fields[name] = value\n\n    def get(self, name, default=_MISSING):\n        \"\"\"Get a metadata field.\"\"\"\n        name = self._convert_name(name)\n        if name not in self._fields:\n            if default is _MISSING:\n                default = self._default_value(name)\n            return default\n        if name in _UNICODEFIELDS:\n            value = self._fields[name]\n            return value\n        elif name in _LISTFIELDS:\n            value = self._fields[name]\n            if value is None:\n                return []\n            res = []\n            for val in value:\n                if name not in _LISTTUPLEFIELDS:\n                    res.append(val)\n                else:\n                    # That's for Project-URL\n                    res.append((val[0], val[1]))\n            return res\n\n        elif name in _ELEMENTSFIELD:\n            value = self._fields[name]\n            if isinstance(value, string_types):\n                return value.split(',')\n        return self._fields[name]\n\n    def check(self, strict=False):\n        \"\"\"Check if the metadata is compliant. If strict is True then raise if\n        no Name or Version are provided\"\"\"\n        self.set_metadata_version()\n\n        # XXX should check the versions (if the file was loaded)\n        missing, warnings = [], []\n\n        for attr in ('Name', 'Version'):  # required by PEP 345\n            if attr not in self:\n                missing.append(attr)\n\n        if strict and missing != []:\n            msg = 'missing required metadata: %s' % ', '.join(missing)\n            raise MetadataMissingError(msg)\n\n        for attr in ('Home-page', 'Author'):\n            if attr not in self:\n                missing.append(attr)\n\n        # checking metadata 1.2 (XXX needs to check 1.1, 1.0)\n        if self['Metadata-Version'] != '1.2':\n            return missing, warnings\n\n        scheme = get_scheme(self.scheme)\n\n        def are_valid_constraints(value):\n            for v in value:\n                if not scheme.is_valid_matcher(v.split(';')[0]):\n                    return False\n            return True\n\n        for fields, controller in ((_PREDICATE_FIELDS, are_valid_constraints),\n                                   (_VERSIONS_FIELDS, scheme.is_valid_constraint_list), (_VERSION_FIELDS,\n                                                                                         scheme.is_valid_version)):\n            for field in fields:\n                value = self.get(field, None)\n                if value is not None and not controller(value):\n                    warnings.append(\"Wrong value for '%s': %s\" % (field, value))\n\n        return missing, warnings\n\n    def todict(self, skip_missing=False):\n        \"\"\"Return fields as a dict.\n\n        Field names will be converted to use the underscore-lowercase style\n        instead of hyphen-mixed case (i.e. home_page instead of Home-page).\n        This is as per https://www.python.org/dev/peps/pep-0566/#id17.\n        \"\"\"\n        self.set_metadata_version()\n\n        fields = _version2fieldlist(self['Metadata-Version'])\n\n        data = {}\n\n        for field_name in fields:\n            if not skip_missing or field_name in self._fields:\n                key = _FIELD2ATTR[field_name]\n                if key != 'project_url':\n                    data[key] = self[field_name]\n                else:\n                    data[key] = [','.join(u) for u in self[field_name]]\n\n        return data\n\n    def add_requirements(self, requirements):\n        if self['Metadata-Version'] == '1.1':\n            # we can't have 1.1 metadata *and* Setuptools requires\n            for field in ('Obsoletes', 'Requires', 'Provides'):\n                if field in self:\n                    del self[field]\n        self['Requires-Dist'] += requirements\n\n    # Mapping API\n    # TODO could add iter* variants\n\n    def keys(self):\n        return list(_version2fieldlist(self['Metadata-Version']))\n\n    def __iter__(self):\n        for key in self.keys():\n            yield key\n\n    def values(self):\n        return [self[key] for key in self.keys()]\n\n    def items(self):\n        return [(key, self[key]) for key in self.keys()]\n\n    def __repr__(self):\n        return '<%s %s %s>' % (self.__class__.__name__, self.name, self.version)\n\n\nMETADATA_FILENAME = 'pydist.json'\nWHEEL_METADATA_FILENAME = 'metadata.json'\nLEGACY_METADATA_FILENAME = 'METADATA'\n\n\nclass Metadata(object):\n    \"\"\"\n    The metadata of a release. This implementation uses 2.1\n    metadata where possible. If not possible, it wraps a LegacyMetadata\n    instance which handles the key-value metadata format.\n    \"\"\"\n\n    METADATA_VERSION_MATCHER = re.compile(r'^\\d+(\\.\\d+)*$')\n\n    NAME_MATCHER = re.compile('^[0-9A-Z]([0-9A-Z_.-]*[0-9A-Z])?$', re.I)\n\n    FIELDNAME_MATCHER = re.compile('^[A-Z]([0-9A-Z-]*[0-9A-Z])?$', re.I)\n\n    VERSION_MATCHER = PEP440_VERSION_RE\n\n    SUMMARY_MATCHER = re.compile('.{1,2047}')\n\n    METADATA_VERSION = '2.0'\n\n    GENERATOR = 'distlib (%s)' % __version__\n\n    MANDATORY_KEYS = {\n        'name': (),\n        'version': (),\n        'summary': ('legacy', ),\n    }\n\n    INDEX_KEYS = ('name version license summary description author '\n                  'author_email keywords platform home_page classifiers '\n                  'download_url')\n\n    DEPENDENCY_KEYS = ('extras run_requires test_requires build_requires '\n                       'dev_requires provides meta_requires obsoleted_by '\n                       'supports_environments')\n\n    SYNTAX_VALIDATORS = {\n        'metadata_version': (METADATA_VERSION_MATCHER, ()),\n        'name': (NAME_MATCHER, ('legacy', )),\n        'version': (VERSION_MATCHER, ('legacy', )),\n        'summary': (SUMMARY_MATCHER, ('legacy', )),\n        'dynamic': (FIELDNAME_MATCHER, ('legacy', )),\n    }\n\n    __slots__ = ('_legacy', '_data', 'scheme')\n\n    def __init__(self, path=None, fileobj=None, mapping=None, scheme='default'):\n        if [path, fileobj, mapping].count(None) < 2:\n            raise TypeError('path, fileobj and mapping are exclusive')\n        self._legacy = None\n        self._data = None\n        self.scheme = scheme\n        # import pdb; pdb.set_trace()\n        if mapping is not None:\n            try:\n                self._validate_mapping(mapping, scheme)\n                self._data = mapping\n            except MetadataUnrecognizedVersionError:\n                self._legacy = LegacyMetadata(mapping=mapping, scheme=scheme)\n                self.validate()\n        else:\n            data = None\n            if path:\n                with open(path, 'rb') as f:\n                    data = f.read()\n            elif fileobj:\n                data = fileobj.read()\n            if data is None:\n                # Initialised with no args - to be added\n                self._data = {\n                    'metadata_version': self.METADATA_VERSION,\n                    'generator': self.GENERATOR,\n                }\n            else:\n                if not isinstance(data, text_type):\n                    data = data.decode('utf-8')\n                try:\n                    self._data = json.loads(data)\n                    self._validate_mapping(self._data, scheme)\n                except ValueError:\n                    # Note: MetadataUnrecognizedVersionError does not\n                    # inherit from ValueError (it's a DistlibException,\n                    # which should not inherit from ValueError).\n                    # The ValueError comes from the json.load - if that\n                    # succeeds and we get a validation error, we want\n                    # that to propagate\n                    self._legacy = LegacyMetadata(fileobj=StringIO(data), scheme=scheme)\n                    self.validate()\n\n    common_keys = set(('name', 'version', 'license', 'keywords', 'summary'))\n\n    none_list = (None, list)\n    none_dict = (None, dict)\n\n    mapped_keys = {\n        'run_requires': ('Requires-Dist', list),\n        'build_requires': ('Setup-Requires-Dist', list),\n        'dev_requires': none_list,\n        'test_requires': none_list,\n        'meta_requires': none_list,\n        'extras': ('Provides-Extra', list),\n        'modules': none_list,\n        'namespaces': none_list,\n        'exports': none_dict,\n        'commands': none_dict,\n        'classifiers': ('Classifier', list),\n        'source_url': ('Download-URL', None),\n        'metadata_version': ('Metadata-Version', None),\n    }\n\n    del none_list, none_dict\n\n    def __getattribute__(self, key):\n        common = object.__getattribute__(self, 'common_keys')\n        mapped = object.__getattribute__(self, 'mapped_keys')\n        if key in mapped:\n            lk, maker = mapped[key]\n            if self._legacy:\n                if lk is None:\n                    result = None if maker is None else maker()\n                else:\n                    result = self._legacy.get(lk)\n            else:\n                value = None if maker is None else maker()\n                if key not in ('commands', 'exports', 'modules', 'namespaces', 'classifiers'):\n                    result = self._data.get(key, value)\n                else:\n                    # special cases for PEP 459\n                    sentinel = object()\n                    result = sentinel\n                    d = self._data.get('extensions')\n                    if d:\n                        if key == 'commands':\n                            result = d.get('python.commands', value)\n                        elif key == 'classifiers':\n                            d = d.get('python.details')\n                            if d:\n                                result = d.get(key, value)\n                        else:\n                            d = d.get('python.exports')\n                            if not d:\n                                d = self._data.get('python.exports')\n                            if d:\n                                result = d.get(key, value)\n                    if result is sentinel:\n                        result = value\n        elif key not in common:\n            result = object.__getattribute__(self, key)\n        elif self._legacy:\n            result = self._legacy.get(key)\n        else:\n            result = self._data.get(key)\n        return result\n\n    def _validate_value(self, key, value, scheme=None):\n        if key in self.SYNTAX_VALIDATORS:\n            pattern, exclusions = self.SYNTAX_VALIDATORS[key]\n            if (scheme or self.scheme) not in exclusions:\n                m = pattern.match(value)\n                if not m:\n                    raise MetadataInvalidError(\"'%s' is an invalid value for \"\n                                               \"the '%s' property\" % (value, key))\n\n    def __setattr__(self, key, value):\n        self._validate_value(key, value)\n        common = object.__getattribute__(self, 'common_keys')\n        mapped = object.__getattribute__(self, 'mapped_keys')\n        if key in mapped:\n            lk, _ = mapped[key]\n            if self._legacy:\n                if lk is None:\n                    raise NotImplementedError\n                self._legacy[lk] = value\n            elif key not in ('commands', 'exports', 'modules', 'namespaces', 'classifiers'):\n                self._data[key] = value\n            else:\n                # special cases for PEP 459\n                d = self._data.setdefault('extensions', {})\n                if key == 'commands':\n                    d['python.commands'] = value\n                elif key == 'classifiers':\n                    d = d.setdefault('python.details', {})\n                    d[key] = value\n                else:\n                    d = d.setdefault('python.exports', {})\n                    d[key] = value\n        elif key not in common:\n            object.__setattr__(self, key, value)\n        else:\n            if key == 'keywords':\n                if isinstance(value, string_types):\n                    value = value.strip()\n                    if value:\n                        value = value.split()\n                    else:\n                        value = []\n            if self._legacy:\n                self._legacy[key] = value\n            else:\n                self._data[key] = value\n\n    @property\n    def name_and_version(self):\n        return _get_name_and_version(self.name, self.version, True)\n\n    @property\n    def provides(self):\n        if self._legacy:\n            result = self._legacy['Provides-Dist']\n        else:\n            result = self._data.setdefault('provides', [])\n        s = '%s (%s)' % (self.name, self.version)\n        if s not in result:\n            result.append(s)\n        return result\n\n    @provides.setter\n    def provides(self, value):\n        if self._legacy:\n            self._legacy['Provides-Dist'] = value\n        else:\n            self._data['provides'] = value\n\n    def get_requirements(self, reqts, extras=None, env=None):\n        \"\"\"\n        Base method to get dependencies, given a set of extras\n        to satisfy and an optional environment context.\n        :param reqts: A list of sometimes-wanted dependencies,\n                      perhaps dependent on extras and environment.\n        :param extras: A list of optional components being requested.\n        :param env: An optional environment for marker evaluation.\n        \"\"\"\n        if self._legacy:\n            result = reqts\n        else:\n            result = []\n            extras = get_extras(extras or [], self.extras)\n            for d in reqts:\n                if 'extra' not in d and 'environment' not in d:\n                    # unconditional\n                    include = True\n                else:\n                    if 'extra' not in d:\n                        # Not extra-dependent - only environment-dependent\n                        include = True\n                    else:\n                        include = d.get('extra') in extras\n                    if include:\n                        # Not excluded because of extras, check environment\n                        marker = d.get('environment')\n                        if marker:\n                            include = interpret(marker, env)\n                if include:\n                    result.extend(d['requires'])\n            for key in ('build', 'dev', 'test'):\n                e = ':%s:' % key\n                if e in extras:\n                    extras.remove(e)\n                    # A recursive call, but it should terminate since 'test'\n                    # has been removed from the extras\n                    reqts = self._data.get('%s_requires' % key, [])\n                    result.extend(self.get_requirements(reqts, extras=extras, env=env))\n        return result\n\n    @property\n    def dictionary(self):\n        if self._legacy:\n            return self._from_legacy()\n        return self._data\n\n    @property\n    def dependencies(self):\n        if self._legacy:\n            raise NotImplementedError\n        else:\n            return extract_by_key(self._data, self.DEPENDENCY_KEYS)\n\n    @dependencies.setter\n    def dependencies(self, value):\n        if self._legacy:\n            raise NotImplementedError\n        else:\n            self._data.update(value)\n\n    def _validate_mapping(self, mapping, scheme):\n        if mapping.get('metadata_version') != self.METADATA_VERSION:\n            raise MetadataUnrecognizedVersionError()\n        missing = []\n        for key, exclusions in self.MANDATORY_KEYS.items():\n            if key not in mapping:\n                if scheme not in exclusions:\n                    missing.append(key)\n        if missing:\n            msg = 'Missing metadata items: %s' % ', '.join(missing)\n            raise MetadataMissingError(msg)\n        for k, v in mapping.items():\n            self._validate_value(k, v, scheme)\n\n    def validate(self):\n        if self._legacy:\n            missing, warnings = self._legacy.check(True)\n            if missing or warnings:\n                logger.warning('Metadata: missing: %s, warnings: %s', missing, warnings)\n        else:\n            self._validate_mapping(self._data, self.scheme)\n\n    def todict(self):\n        if self._legacy:\n            return self._legacy.todict(True)\n        else:\n            result = extract_by_key(self._data, self.INDEX_KEYS)\n            return result\n\n    def _from_legacy(self):\n        assert self._legacy and not self._data\n        result = {\n            'metadata_version': self.METADATA_VERSION,\n            'generator': self.GENERATOR,\n        }\n        lmd = self._legacy.todict(True)  # skip missing ones\n        for k in ('name', 'version', 'license', 'summary', 'description', 'classifier'):\n            if k in lmd:\n                if k == 'classifier':\n                    nk = 'classifiers'\n                else:\n                    nk = k\n                result[nk] = lmd[k]\n        kw = lmd.get('Keywords', [])\n        if kw == ['']:\n            kw = []\n        result['keywords'] = kw\n        keys = (('requires_dist', 'run_requires'), ('setup_requires_dist', 'build_requires'))\n        for ok, nk in keys:\n            if ok in lmd and lmd[ok]:\n                result[nk] = [{'requires': lmd[ok]}]\n        result['provides'] = self.provides\n        # author = {}\n        # maintainer = {}\n        return result\n\n    LEGACY_MAPPING = {\n        'name': 'Name',\n        'version': 'Version',\n        ('extensions', 'python.details', 'license'): 'License',\n        'summary': 'Summary',\n        'description': 'Description',\n        ('extensions', 'python.project', 'project_urls', 'Home'): 'Home-page',\n        ('extensions', 'python.project', 'contacts', 0, 'name'): 'Author',\n        ('extensions', 'python.project', 'contacts', 0, 'email'): 'Author-email',\n        'source_url': 'Download-URL',\n        ('extensions', 'python.details', 'classifiers'): 'Classifier',\n    }\n\n    def _to_legacy(self):\n\n        def process_entries(entries):\n            reqts = set()\n            for e in entries:\n                extra = e.get('extra')\n                env = e.get('environment')\n                rlist = e['requires']\n                for r in rlist:\n                    if not env and not extra:\n                        reqts.add(r)\n                    else:\n                        marker = ''\n                        if extra:\n                            marker = 'extra == \"%s\"' % extra\n                        if env:\n                            if marker:\n                                marker = '(%s) and %s' % (env, marker)\n                            else:\n                                marker = env\n                        reqts.add(';'.join((r, marker)))\n            return reqts\n\n        assert self._data and not self._legacy\n        result = LegacyMetadata()\n        nmd = self._data\n        # import pdb; pdb.set_trace()\n        for nk, ok in self.LEGACY_MAPPING.items():\n            if not isinstance(nk, tuple):\n                if nk in nmd:\n                    result[ok] = nmd[nk]\n            else:\n                d = nmd\n                found = True\n                for k in nk:\n                    try:\n                        d = d[k]\n                    except (KeyError, IndexError):\n                        found = False\n                        break\n                if found:\n                    result[ok] = d\n        r1 = process_entries(self.run_requires + self.meta_requires)\n        r2 = process_entries(self.build_requires + self.dev_requires)\n        if self.extras:\n            result['Provides-Extra'] = sorted(self.extras)\n        result['Requires-Dist'] = sorted(r1)\n        result['Setup-Requires-Dist'] = sorted(r2)\n        # TODO: any other fields wanted\n        return result\n\n    def write(self, path=None, fileobj=None, legacy=False, skip_unknown=True):\n        if [path, fileobj].count(None) != 1:\n            raise ValueError('Exactly one of path and fileobj is needed')\n        self.validate()\n        if legacy:\n            if self._legacy:\n                legacy_md = self._legacy\n            else:\n                legacy_md = self._to_legacy()\n            if path:\n                legacy_md.write(path, skip_unknown=skip_unknown)\n            else:\n                legacy_md.write_file(fileobj, skip_unknown=skip_unknown)\n        else:\n            if self._legacy:\n                d = self._from_legacy()\n            else:\n                d = self._data\n            if fileobj:\n                json.dump(d, fileobj, ensure_ascii=True, indent=2, sort_keys=True)\n            else:\n                with codecs.open(path, 'w', 'utf-8') as f:\n                    json.dump(d, f, ensure_ascii=True, indent=2, sort_keys=True)\n\n    def add_requirements(self, requirements):\n        if self._legacy:\n            self._legacy.add_requirements(requirements)\n        else:\n            run_requires = self._data.setdefault('run_requires', [])\n            always = None\n            for entry in run_requires:\n                if 'environment' not in entry and 'extra' not in entry:\n                    always = entry\n                    break\n            if always is None:\n                always = {'requires': requirements}\n                run_requires.insert(0, always)\n            else:\n                rset = set(always['requires']) | set(requirements)\n                always['requires'] = sorted(rset)\n\n    def __repr__(self):\n        name = self.name or '(no name)'\n        version = self.version or 'no version'\n        return '<%s %s %s (%s)>' % (self.__class__.__name__, self.metadata_version, name, version)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_vendor/distlib/resources.py","size":10820,"sha1":"cb59892b325396652ff2998bfe12cf124959f6ca","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"# -*- coding: utf-8 -*-\n#\n# Copyright (C) 2013-2017 Vinay Sajip.\n# Licensed to the Python Software Foundation under a contributor agreement.\n# See LICENSE.txt and CONTRIBUTORS.txt.\n#\nfrom __future__ import unicode_literals\n\nimport bisect\nimport io\nimport logging\nimport os\nimport pkgutil\nimport sys\nimport types\nimport zipimport\n\nfrom . import DistlibException\nfrom .util import cached_property, get_cache_base, Cache\n\nlogger = logging.getLogger(__name__)\n\n\ncache = None    # created when needed\n\n\nclass ResourceCache(Cache):\n    def __init__(self, base=None):\n        if base is None:\n            # Use native string to avoid issues on 2.x: see Python #20140.\n            base = os.path.join(get_cache_base(), str('resource-cache'))\n        super(ResourceCache, self).__init__(base)\n\n    def is_stale(self, resource, path):\n        \"\"\"\n        Is the cache stale for the given resource?\n\n        :param resource: The :class:`Resource` being cached.\n        :param path: The path of the resource in the cache.\n        :return: True if the cache is stale.\n        \"\"\"\n        # Cache invalidation is a hard problem :-)\n        return True\n\n    def get(self, resource):\n        \"\"\"\n        Get a resource into the cache,\n\n        :param resource: A :class:`Resource` instance.\n        :return: The pathname of the resource in the cache.\n        \"\"\"\n        prefix, path = resource.finder.get_cache_info(resource)\n        if prefix is None:\n            result = path\n        else:\n            result = os.path.join(self.base, self.prefix_to_dir(prefix), path)\n            dirname = os.path.dirname(result)\n            if not os.path.isdir(dirname):\n                os.makedirs(dirname)\n            if not os.path.exists(result):\n                stale = True\n            else:\n                stale = self.is_stale(resource, path)\n            if stale:\n                # write the bytes of the resource to the cache location\n                with open(result, 'wb') as f:\n                    f.write(resource.bytes)\n        return result\n\n\nclass ResourceBase(object):\n    def __init__(self, finder, name):\n        self.finder = finder\n        self.name = name\n\n\nclass Resource(ResourceBase):\n    \"\"\"\n    A class representing an in-package resource, such as a data file. This is\n    not normally instantiated by user code, but rather by a\n    :class:`ResourceFinder` which manages the resource.\n    \"\"\"\n    is_container = False        # Backwards compatibility\n\n    def as_stream(self):\n        \"\"\"\n        Get the resource as a stream.\n\n        This is not a property to make it obvious that it returns a new stream\n        each time.\n        \"\"\"\n        return self.finder.get_stream(self)\n\n    @cached_property\n    def file_path(self):\n        global cache\n        if cache is None:\n            cache = ResourceCache()\n        return cache.get(self)\n\n    @cached_property\n    def bytes(self):\n        return self.finder.get_bytes(self)\n\n    @cached_property\n    def size(self):\n        return self.finder.get_size(self)\n\n\nclass ResourceContainer(ResourceBase):\n    is_container = True     # Backwards compatibility\n\n    @cached_property\n    def resources(self):\n        return self.finder.get_resources(self)\n\n\nclass ResourceFinder(object):\n    \"\"\"\n    Resource finder for file system resources.\n    \"\"\"\n\n    if sys.platform.startswith('java'):\n        skipped_extensions = ('.pyc', '.pyo', '.class')\n    else:\n        skipped_extensions = ('.pyc', '.pyo')\n\n    def __init__(self, module):\n        self.module = module\n        self.loader = getattr(module, '__loader__', None)\n        self.base = os.path.dirname(getattr(module, '__file__', ''))\n\n    def _adjust_path(self, path):\n        return os.path.realpath(path)\n\n    def _make_path(self, resource_name):\n        # Issue #50: need to preserve type of path on Python 2.x\n        # like os.path._get_sep\n        if isinstance(resource_name, bytes):    # should only happen on 2.x\n            sep = b'/'\n        else:\n            sep = '/'\n        parts = resource_name.split(sep)\n        parts.insert(0, self.base)\n        result = os.path.join(*parts)\n        return self._adjust_path(result)\n\n    def _find(self, path):\n        return os.path.exists(path)\n\n    def get_cache_info(self, resource):\n        return None, resource.path\n\n    def find(self, resource_name):\n        path = self._make_path(resource_name)\n        if not self._find(path):\n            result = None\n        else:\n            if self._is_directory(path):\n                result = ResourceContainer(self, resource_name)\n            else:\n                result = Resource(self, resource_name)\n            result.path = path\n        return result\n\n    def get_stream(self, resource):\n        return open(resource.path, 'rb')\n\n    def get_bytes(self, resource):\n        with open(resource.path, 'rb') as f:\n            return f.read()\n\n    def get_size(self, resource):\n        return os.path.getsize(resource.path)\n\n    def get_resources(self, resource):\n        def allowed(f):\n            return (f != '__pycache__' and not\n                    f.endswith(self.skipped_extensions))\n        return set([f for f in os.listdir(resource.path) if allowed(f)])\n\n    def is_container(self, resource):\n        return self._is_directory(resource.path)\n\n    _is_directory = staticmethod(os.path.isdir)\n\n    def iterator(self, resource_name):\n        resource = self.find(resource_name)\n        if resource is not None:\n            todo = [resource]\n            while todo:\n                resource = todo.pop(0)\n                yield resource\n                if resource.is_container:\n                    rname = resource.name\n                    for name in resource.resources:\n                        if not rname:\n                            new_name = name\n                        else:\n                            new_name = '/'.join([rname, name])\n                        child = self.find(new_name)\n                        if child.is_container:\n                            todo.append(child)\n                        else:\n                            yield child\n\n\nclass ZipResourceFinder(ResourceFinder):\n    \"\"\"\n    Resource finder for resources in .zip files.\n    \"\"\"\n    def __init__(self, module):\n        super(ZipResourceFinder, self).__init__(module)\n        archive = self.loader.archive\n        self.prefix_len = 1 + len(archive)\n        # PyPy doesn't have a _files attr on zipimporter, and you can't set one\n        if hasattr(self.loader, '_files'):\n            self._files = self.loader._files\n        else:\n            self._files = zipimport._zip_directory_cache[archive]\n        self.index = sorted(self._files)\n\n    def _adjust_path(self, path):\n        return path\n\n    def _find(self, path):\n        path = path[self.prefix_len:]\n        if path in self._files:\n            result = True\n        else:\n            if path and path[-1] != os.sep:\n                path = path + os.sep\n            i = bisect.bisect(self.index, path)\n            try:\n                result = self.index[i].startswith(path)\n            except IndexError:\n                result = False\n        if not result:\n            logger.debug('_find failed: %r %r', path, self.loader.prefix)\n        else:\n            logger.debug('_find worked: %r %r', path, self.loader.prefix)\n        return result\n\n    def get_cache_info(self, resource):\n        prefix = self.loader.archive\n        path = resource.path[1 + len(prefix):]\n        return prefix, path\n\n    def get_bytes(self, resource):\n        return self.loader.get_data(resource.path)\n\n    def get_stream(self, resource):\n        return io.BytesIO(self.get_bytes(resource))\n\n    def get_size(self, resource):\n        path = resource.path[self.prefix_len:]\n        return self._files[path][3]\n\n    def get_resources(self, resource):\n        path = resource.path[self.prefix_len:]\n        if path and path[-1] != os.sep:\n            path += os.sep\n        plen = len(path)\n        result = set()\n        i = bisect.bisect(self.index, path)\n        while i < len(self.index):\n            if not self.index[i].startswith(path):\n                break\n            s = self.index[i][plen:]\n            result.add(s.split(os.sep, 1)[0])   # only immediate children\n            i += 1\n        return result\n\n    def _is_directory(self, path):\n        path = path[self.prefix_len:]\n        if path and path[-1] != os.sep:\n            path += os.sep\n        i = bisect.bisect(self.index, path)\n        try:\n            result = self.index[i].startswith(path)\n        except IndexError:\n            result = False\n        return result\n\n\n_finder_registry = {\n    type(None): ResourceFinder,\n    zipimport.zipimporter: ZipResourceFinder\n}\n\ntry:\n    # In Python 3.6, _frozen_importlib -> _frozen_importlib_external\n    try:\n        import _frozen_importlib_external as _fi\n    except ImportError:\n        import _frozen_importlib as _fi\n    _finder_registry[_fi.SourceFileLoader] = ResourceFinder\n    _finder_registry[_fi.FileFinder] = ResourceFinder\n    # See issue #146\n    _finder_registry[_fi.SourcelessFileLoader] = ResourceFinder\n    del _fi\nexcept (ImportError, AttributeError):\n    pass\n\n\ndef register_finder(loader, finder_maker):\n    _finder_registry[type(loader)] = finder_maker\n\n\n_finder_cache = {}\n\n\ndef finder(package):\n    \"\"\"\n    Return a resource finder for a package.\n    :param package: The name of the package.\n    :return: A :class:`ResourceFinder` instance for the package.\n    \"\"\"\n    if package in _finder_cache:\n        result = _finder_cache[package]\n    else:\n        if package not in sys.modules:\n            __import__(package)\n        module = sys.modules[package]\n        path = getattr(module, '__path__', None)\n        if path is None:\n            raise DistlibException('You cannot get a finder for a module, '\n                                   'only for a package')\n        loader = getattr(module, '__loader__', None)\n        finder_maker = _finder_registry.get(type(loader))\n        if finder_maker is None:\n            raise DistlibException('Unable to locate finder for %r' % package)\n        result = finder_maker(module)\n        _finder_cache[package] = result\n    return result\n\n\n_dummy_module = types.ModuleType(str('__dummy__'))\n\n\ndef finder_for_path(path):\n    \"\"\"\n    Return a resource finder for a path, which should represent a container.\n\n    :param path: The path.\n    :return: A :class:`ResourceFinder` instance for the path.\n    \"\"\"\n    result = None\n    # calls any path hooks, gets importer into cache\n    pkgutil.get_importer(path)\n    loader = sys.path_importer_cache.get(path)\n    finder = _finder_registry.get(type(loader))\n    if finder:\n        module = _dummy_module\n        module.__file__ = os.path.join(path, '')\n        module.__loader__ = loader\n        result = finder(module)\n    return result\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_vendor/distlib/scripts.py","size":18608,"sha1":"cb4d608970a16cf38e176b7e1d7b56dcc64458af","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"# -*- coding: utf-8 -*-\n#\n# Copyright (C) 2013-2023 Vinay Sajip.\n# Licensed to the Python Software Foundation under a contributor agreement.\n# See LICENSE.txt and CONTRIBUTORS.txt.\n#\nfrom io import BytesIO\nimport logging\nimport os\nimport re\nimport struct\nimport sys\nimport time\nfrom zipfile import ZipInfo\n\nfrom .compat import sysconfig, detect_encoding, ZipFile\nfrom .resources import finder\nfrom .util import (FileOperator, get_export_entry, convert_path, get_executable, get_platform, in_venv)\n\nlogger = logging.getLogger(__name__)\n\n_DEFAULT_MANIFEST = '''\n<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?>\n<assembly xmlns=\"urn:schemas-microsoft-com:asm.v1\" manifestVersion=\"1.0\">\n <assemblyIdentity version=\"1.0.0.0\"\n processorArchitecture=\"X86\"\n name=\"%s\"\n type=\"win32\"/>\n\n <!-- Identify the application security requirements. -->\n <trustInfo xmlns=\"urn:schemas-microsoft-com:asm.v3\">\n <security>\n <requestedPrivileges>\n <requestedExecutionLevel level=\"asInvoker\" uiAccess=\"false\"/>\n </requestedPrivileges>\n </security>\n </trustInfo>\n</assembly>'''.strip()\n\n# check if Python is called on the first line with this expression\nFIRST_LINE_RE = re.compile(b'^#!.*pythonw?[0-9.]*([ \\t].*)?$')\nSCRIPT_TEMPLATE = r'''# -*- coding: utf-8 -*-\nimport re\nimport sys\nfrom %(module)s import %(import_name)s\nif __name__ == '__main__':\n    sys.argv[0] = re.sub(r'(-script\\.pyw|\\.exe)?$', '', sys.argv[0])\n    sys.exit(%(func)s())\n'''\n\n# Pre-fetch the contents of all executable wrapper stubs.\n# This is to address https://github.com/pypa/pip/issues/12666.\n# When updating pip, we rename the old pip in place before installing the\n# new version. If we try to fetch a wrapper *after* that rename, the finder\n# machinery will be confused as the package is no longer available at the\n# location where it was imported from. So we load everything into memory in\n# advance.\n\nif os.name == 'nt' or (os.name == 'java' and os._name == 'nt'):\n    # Issue 31: don't hardcode an absolute package name, but\n    # determine it relative to the current package\n    DISTLIB_PACKAGE = __name__.rsplit('.', 1)[0]\n\n    WRAPPERS = {\n        r.name: r.bytes\n        for r in finder(DISTLIB_PACKAGE).iterator(\"\")\n        if r.name.endswith(\".exe\")\n    }\n\n\ndef enquote_executable(executable):\n    if ' ' in executable:\n        # make sure we quote only the executable in case of env\n        # for example /usr/bin/env \"/dir with spaces/bin/jython\"\n        # instead of \"/usr/bin/env /dir with spaces/bin/jython\"\n        # otherwise whole\n        if executable.startswith('/usr/bin/env '):\n            env, _executable = executable.split(' ', 1)\n            if ' ' in _executable and not _executable.startswith('\"'):\n                executable = '%s \"%s\"' % (env, _executable)\n        else:\n            if not executable.startswith('\"'):\n                executable = '\"%s\"' % executable\n    return executable\n\n\n# Keep the old name around (for now), as there is at least one project using it!\n_enquote_executable = enquote_executable\n\n\nclass ScriptMaker(object):\n    \"\"\"\n    A class to copy or create scripts from source scripts or callable\n    specifications.\n    \"\"\"\n    script_template = SCRIPT_TEMPLATE\n\n    executable = None  # for shebangs\n\n    def __init__(self, source_dir, target_dir, add_launchers=True, dry_run=False, fileop=None):\n        self.source_dir = source_dir\n        self.target_dir = target_dir\n        self.add_launchers = add_launchers\n        self.force = False\n        self.clobber = False\n        # It only makes sense to set mode bits on POSIX.\n        self.set_mode = (os.name == 'posix') or (os.name == 'java' and os._name == 'posix')\n        self.variants = set(('', 'X.Y'))\n        self._fileop = fileop or FileOperator(dry_run)\n\n        self._is_nt = os.name == 'nt' or (os.name == 'java' and os._name == 'nt')\n        self.version_info = sys.version_info\n\n    def _get_alternate_executable(self, executable, options):\n        if options.get('gui', False) and self._is_nt:  # pragma: no cover\n            dn, fn = os.path.split(executable)\n            fn = fn.replace('python', 'pythonw')\n            executable = os.path.join(dn, fn)\n        return executable\n\n    if sys.platform.startswith('java'):  # pragma: no cover\n\n        def _is_shell(self, executable):\n            \"\"\"\n            Determine if the specified executable is a script\n            (contains a #! line)\n            \"\"\"\n            try:\n                with open(executable) as fp:\n                    return fp.read(2) == '#!'\n            except (OSError, IOError):\n                logger.warning('Failed to open %s', executable)\n                return False\n\n        def _fix_jython_executable(self, executable):\n            if self._is_shell(executable):\n                # Workaround for Jython is not needed on Linux systems.\n                import java\n\n                if java.lang.System.getProperty('os.name') == 'Linux':\n                    return executable\n            elif executable.lower().endswith('jython.exe'):\n                # Use wrapper exe for Jython on Windows\n                return executable\n            return '/usr/bin/env %s' % executable\n\n    def _build_shebang(self, executable, post_interp):\n        \"\"\"\n        Build a shebang line. In the simple case (on Windows, or a shebang line\n        which is not too long or contains spaces) use a simple formulation for\n        the shebang. Otherwise, use /bin/sh as the executable, with a contrived\n        shebang which allows the script to run either under Python or sh, using\n        suitable quoting. Thanks to Harald Nordgren for his input.\n\n        See also: http://www.in-ulm.de/~mascheck/various/shebang/#length\n                  https://hg.mozilla.org/mozilla-central/file/tip/mach\n        \"\"\"\n        if os.name != 'posix':\n            simple_shebang = True\n        elif getattr(sys, \"cross_compiling\", False):\n            # In a cross-compiling environment, the shebang will likely be a\n            # script; this *must* be invoked with the \"safe\" version of the\n            # shebang, or else using os.exec() to run the entry script will\n            # fail, raising \"OSError 8 [Errno 8] Exec format error\".\n            simple_shebang = False\n        else:\n            # Add 3 for '#!' prefix and newline suffix.\n            shebang_length = len(executable) + len(post_interp) + 3\n            if sys.platform == 'darwin':\n                max_shebang_length = 512\n            else:\n                max_shebang_length = 127\n            simple_shebang = ((b' ' not in executable) and (shebang_length <= max_shebang_length))\n\n        if simple_shebang:\n            result = b'#!' + executable + post_interp + b'\\n'\n        else:\n            result = b'#!/bin/sh\\n'\n            result += b\"'''exec' \" + executable + post_interp + b' \"$0\" \"$@\"\\n'\n            result += b\"' '''\\n\"\n        return result\n\n    def _get_shebang(self, encoding, post_interp=b'', options=None):\n        enquote = True\n        if self.executable:\n            executable = self.executable\n            enquote = False  # assume this will be taken care of\n        elif not sysconfig.is_python_build():\n            executable = get_executable()\n        elif in_venv():  # pragma: no cover\n            executable = os.path.join(sysconfig.get_path('scripts'), 'python%s' % sysconfig.get_config_var('EXE'))\n        else:  # pragma: no cover\n            if os.name == 'nt':\n                # for Python builds from source on Windows, no Python executables with\n                # a version suffix are created, so we use python.exe\n                executable = os.path.join(sysconfig.get_config_var('BINDIR'),\n                                          'python%s' % (sysconfig.get_config_var('EXE')))\n            else:\n                executable = os.path.join(\n                    sysconfig.get_config_var('BINDIR'),\n                    'python%s%s' % (sysconfig.get_config_var('VERSION'), sysconfig.get_config_var('EXE')))\n        if options:\n            executable = self._get_alternate_executable(executable, options)\n\n        if sys.platform.startswith('java'):  # pragma: no cover\n            executable = self._fix_jython_executable(executable)\n\n        # Normalise case for Windows - COMMENTED OUT\n        # executable = os.path.normcase(executable)\n        # N.B. The normalising operation above has been commented out: See\n        # issue #124. Although paths in Windows are generally case-insensitive,\n        # they aren't always. For example, a path containing a  (which is a\n        # LATIN CAPITAL LETTER SHARP S - U+1E9E) is normcased to  (which is a\n        # LATIN SMALL LETTER SHARP S' - U+00DF). The two are not considered by\n        # Windows as equivalent in path names.\n\n        # If the user didn't specify an executable, it may be necessary to\n        # cater for executable paths with spaces (not uncommon on Windows)\n        if enquote:\n            executable = enquote_executable(executable)\n        # Issue #51: don't use fsencode, since we later try to\n        # check that the shebang is decodable using utf-8.\n        executable = executable.encode('utf-8')\n        # in case of IronPython, play safe and enable frames support\n        if (sys.platform == 'cli' and '-X:Frames' not in post_interp and\n                '-X:FullFrames' not in post_interp):  # pragma: no cover\n            post_interp += b' -X:Frames'\n        shebang = self._build_shebang(executable, post_interp)\n        # Python parser starts to read a script using UTF-8 until\n        # it gets a #coding:xxx cookie. The shebang has to be the\n        # first line of a file, the #coding:xxx cookie cannot be\n        # written before. So the shebang has to be decodable from\n        # UTF-8.\n        try:\n            shebang.decode('utf-8')\n        except UnicodeDecodeError:  # pragma: no cover\n            raise ValueError('The shebang (%r) is not decodable from utf-8' % shebang)\n        # If the script is encoded to a custom encoding (use a\n        # #coding:xxx cookie), the shebang has to be decodable from\n        # the script encoding too.\n        if encoding != 'utf-8':\n            try:\n                shebang.decode(encoding)\n            except UnicodeDecodeError:  # pragma: no cover\n                raise ValueError('The shebang (%r) is not decodable '\n                                 'from the script encoding (%r)' % (shebang, encoding))\n        return shebang\n\n    def _get_script_text(self, entry):\n        return self.script_template % dict(\n            module=entry.prefix, import_name=entry.suffix.split('.')[0], func=entry.suffix)\n\n    manifest = _DEFAULT_MANIFEST\n\n    def get_manifest(self, exename):\n        base = os.path.basename(exename)\n        return self.manifest % base\n\n    def _write_script(self, names, shebang, script_bytes, filenames, ext):\n        use_launcher = self.add_launchers and self._is_nt\n        if not use_launcher:\n            script_bytes = shebang + script_bytes\n        else:  # pragma: no cover\n            if ext == 'py':\n                launcher = self._get_launcher('t')\n            else:\n                launcher = self._get_launcher('w')\n            stream = BytesIO()\n            with ZipFile(stream, 'w') as zf:\n                source_date_epoch = os.environ.get('SOURCE_DATE_EPOCH')\n                if source_date_epoch:\n                    date_time = time.gmtime(int(source_date_epoch))[:6]\n                    zinfo = ZipInfo(filename='__main__.py', date_time=date_time)\n                    zf.writestr(zinfo, script_bytes)\n                else:\n                    zf.writestr('__main__.py', script_bytes)\n            zip_data = stream.getvalue()\n            script_bytes = launcher + shebang + zip_data\n        for name in names:\n            outname = os.path.join(self.target_dir, name)\n            if use_launcher:  # pragma: no cover\n                n, e = os.path.splitext(outname)\n                if e.startswith('.py'):\n                    outname = n\n                outname = '%s.exe' % outname\n                try:\n                    self._fileop.write_binary_file(outname, script_bytes)\n                except Exception:\n                    # Failed writing an executable - it might be in use.\n                    logger.warning('Failed to write executable - trying to '\n                                   'use .deleteme logic')\n                    dfname = '%s.deleteme' % outname\n                    if os.path.exists(dfname):\n                        os.remove(dfname)  # Not allowed to fail here\n                    os.rename(outname, dfname)  # nor here\n                    self._fileop.write_binary_file(outname, script_bytes)\n                    logger.debug('Able to replace executable using '\n                                 '.deleteme logic')\n                    try:\n                        os.remove(dfname)\n                    except Exception:\n                        pass  # still in use - ignore error\n            else:\n                if self._is_nt and not outname.endswith('.' + ext):  # pragma: no cover\n                    outname = '%s.%s' % (outname, ext)\n                if os.path.exists(outname) and not self.clobber:\n                    logger.warning('Skipping existing file %s', outname)\n                    continue\n                self._fileop.write_binary_file(outname, script_bytes)\n                if self.set_mode:\n                    self._fileop.set_executable_mode([outname])\n            filenames.append(outname)\n\n    variant_separator = '-'\n\n    def get_script_filenames(self, name):\n        result = set()\n        if '' in self.variants:\n            result.add(name)\n        if 'X' in self.variants:\n            result.add('%s%s' % (name, self.version_info[0]))\n        if 'X.Y' in self.variants:\n            result.add('%s%s%s.%s' % (name, self.variant_separator, self.version_info[0], self.version_info[1]))\n        return result\n\n    def _make_script(self, entry, filenames, options=None):\n        post_interp = b''\n        if options:\n            args = options.get('interpreter_args', [])\n            if args:\n                args = ' %s' % ' '.join(args)\n                post_interp = args.encode('utf-8')\n        shebang = self._get_shebang('utf-8', post_interp, options=options)\n        script = self._get_script_text(entry).encode('utf-8')\n        scriptnames = self.get_script_filenames(entry.name)\n        if options and options.get('gui', False):\n            ext = 'pyw'\n        else:\n            ext = 'py'\n        self._write_script(scriptnames, shebang, script, filenames, ext)\n\n    def _copy_script(self, script, filenames):\n        adjust = False\n        script = os.path.join(self.source_dir, convert_path(script))\n        outname = os.path.join(self.target_dir, os.path.basename(script))\n        if not self.force and not self._fileop.newer(script, outname):\n            logger.debug('not copying %s (up-to-date)', script)\n            return\n\n        # Always open the file, but ignore failures in dry-run mode --\n        # that way, we'll get accurate feedback if we can read the\n        # script.\n        try:\n            f = open(script, 'rb')\n        except IOError:  # pragma: no cover\n            if not self.dry_run:\n                raise\n            f = None\n        else:\n            first_line = f.readline()\n            if not first_line:  # pragma: no cover\n                logger.warning('%s is an empty file (skipping)', script)\n                return\n\n            match = FIRST_LINE_RE.match(first_line.replace(b'\\r\\n', b'\\n'))\n            if match:\n                adjust = True\n                post_interp = match.group(1) or b''\n\n        if not adjust:\n            if f:\n                f.close()\n            self._fileop.copy_file(script, outname)\n            if self.set_mode:\n                self._fileop.set_executable_mode([outname])\n            filenames.append(outname)\n        else:\n            logger.info('copying and adjusting %s -> %s', script, self.target_dir)\n            if not self._fileop.dry_run:\n                encoding, lines = detect_encoding(f.readline)\n                f.seek(0)\n                shebang = self._get_shebang(encoding, post_interp)\n                if b'pythonw' in first_line:  # pragma: no cover\n                    ext = 'pyw'\n                else:\n                    ext = 'py'\n                n = os.path.basename(outname)\n                self._write_script([n], shebang, f.read(), filenames, ext)\n            if f:\n                f.close()\n\n    @property\n    def dry_run(self):\n        return self._fileop.dry_run\n\n    @dry_run.setter\n    def dry_run(self, value):\n        self._fileop.dry_run = value\n\n    if os.name == 'nt' or (os.name == 'java' and os._name == 'nt'):  # pragma: no cover\n        # Executable launcher support.\n        # Launchers are from https://bitbucket.org/vinay.sajip/simple_launcher/\n\n        def _get_launcher(self, kind):\n            if struct.calcsize('P') == 8:  # 64-bit\n                bits = '64'\n            else:\n                bits = '32'\n            platform_suffix = '-arm' if get_platform() == 'win-arm64' else ''\n            name = '%s%s%s.exe' % (kind, bits, platform_suffix)\n            if name not in WRAPPERS:\n                msg = ('Unable to find resource %s in package %s' %\n                       (name, DISTLIB_PACKAGE))\n                raise ValueError(msg)\n            return WRAPPERS[name]\n\n    # Public API follows\n\n    def make(self, specification, options=None):\n        \"\"\"\n        Make a script.\n\n        :param specification: The specification, which is either a valid export\n                              entry specification (to make a script from a\n                              callable) or a filename (to make a script by\n                              copying from a source location).\n        :param options: A dictionary of options controlling script generation.\n        :return: A list of all absolute pathnames written to.\n        \"\"\"\n        filenames = []\n        entry = get_export_entry(specification)\n        if entry is None:\n            self._copy_script(specification, filenames)\n        else:\n            self._make_script(entry, filenames, options=options)\n        return filenames\n\n    def make_multiple(self, specifications, options=None):\n        \"\"\"\n        Take a list of specifications and make scripts from them,\n        :param specifications: A list of specifications.\n        :return: A list of all absolute pathnames written to,\n        \"\"\"\n        filenames = []\n        for specification in specifications:\n            filenames.extend(self.make(specification, options))\n        return filenames\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_vendor/distlib/util.py","size":66682,"sha1":"9e4e2a442c8a2e4349aef70572e7f783c9ab013c","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"#\n# Copyright (C) 2012-2023 The Python Software Foundation.\n# See LICENSE.txt and CONTRIBUTORS.txt.\n#\nimport codecs\nfrom collections import deque\nimport contextlib\nimport csv\nfrom glob import iglob as std_iglob\nimport io\nimport json\nimport logging\nimport os\nimport py_compile\nimport re\nimport socket\ntry:\n    import ssl\nexcept ImportError:  # pragma: no cover\n    ssl = None\nimport subprocess\nimport sys\nimport tarfile\nimport tempfile\nimport textwrap\n\ntry:\n    import threading\nexcept ImportError:  # pragma: no cover\n    import dummy_threading as threading\nimport time\n\nfrom . import DistlibException\nfrom .compat import (string_types, text_type, shutil, raw_input, StringIO, cache_from_source, urlopen, urljoin, httplib,\n                     xmlrpclib, HTTPHandler, BaseConfigurator, valid_ident, Container, configparser, URLError, ZipFile,\n                     fsdecode, unquote, urlparse)\n\nlogger = logging.getLogger(__name__)\n\n#\n# Requirement parsing code as per PEP 508\n#\n\nIDENTIFIER = re.compile(r'^([\\w\\.-]+)\\s*')\nVERSION_IDENTIFIER = re.compile(r'^([\\w\\.*+-]+)\\s*')\nCOMPARE_OP = re.compile(r'^(<=?|>=?|={2,3}|[~!]=)\\s*')\nMARKER_OP = re.compile(r'^((<=?)|(>=?)|={2,3}|[~!]=|in|not\\s+in)\\s*')\nOR = re.compile(r'^or\\b\\s*')\nAND = re.compile(r'^and\\b\\s*')\nNON_SPACE = re.compile(r'(\\S+)\\s*')\nSTRING_CHUNK = re.compile(r'([\\s\\w\\.{}()*+#:;,/?!~`@$%^&=|<>\\[\\]-]+)')\n\n\ndef parse_marker(marker_string):\n    \"\"\"\n    Parse a marker string and return a dictionary containing a marker expression.\n\n    The dictionary will contain keys \"op\", \"lhs\" and \"rhs\" for non-terminals in\n    the expression grammar, or strings. A string contained in quotes is to be\n    interpreted as a literal string, and a string not contained in quotes is a\n    variable (such as os_name).\n    \"\"\"\n\n    def marker_var(remaining):\n        # either identifier, or literal string\n        m = IDENTIFIER.match(remaining)\n        if m:\n            result = m.groups()[0]\n            remaining = remaining[m.end():]\n        elif not remaining:\n            raise SyntaxError('unexpected end of input')\n        else:\n            q = remaining[0]\n            if q not in '\\'\"':\n                raise SyntaxError('invalid expression: %s' % remaining)\n            oq = '\\'\"'.replace(q, '')\n            remaining = remaining[1:]\n            parts = [q]\n            while remaining:\n                # either a string chunk, or oq, or q to terminate\n                if remaining[0] == q:\n                    break\n                elif remaining[0] == oq:\n                    parts.append(oq)\n                    remaining = remaining[1:]\n                else:\n                    m = STRING_CHUNK.match(remaining)\n                    if not m:\n                        raise SyntaxError('error in string literal: %s' % remaining)\n                    parts.append(m.groups()[0])\n                    remaining = remaining[m.end():]\n            else:\n                s = ''.join(parts)\n                raise SyntaxError('unterminated string: %s' % s)\n            parts.append(q)\n            result = ''.join(parts)\n            remaining = remaining[1:].lstrip()  # skip past closing quote\n        return result, remaining\n\n    def marker_expr(remaining):\n        if remaining and remaining[0] == '(':\n            result, remaining = marker(remaining[1:].lstrip())\n            if remaining[0] != ')':\n                raise SyntaxError('unterminated parenthesis: %s' % remaining)\n            remaining = remaining[1:].lstrip()\n        else:\n            lhs, remaining = marker_var(remaining)\n            while remaining:\n                m = MARKER_OP.match(remaining)\n                if not m:\n                    break\n                op = m.groups()[0]\n                remaining = remaining[m.end():]\n                rhs, remaining = marker_var(remaining)\n                lhs = {'op': op, 'lhs': lhs, 'rhs': rhs}\n            result = lhs\n        return result, remaining\n\n    def marker_and(remaining):\n        lhs, remaining = marker_expr(remaining)\n        while remaining:\n            m = AND.match(remaining)\n            if not m:\n                break\n            remaining = remaining[m.end():]\n            rhs, remaining = marker_expr(remaining)\n            lhs = {'op': 'and', 'lhs': lhs, 'rhs': rhs}\n        return lhs, remaining\n\n    def marker(remaining):\n        lhs, remaining = marker_and(remaining)\n        while remaining:\n            m = OR.match(remaining)\n            if not m:\n                break\n            remaining = remaining[m.end():]\n            rhs, remaining = marker_and(remaining)\n            lhs = {'op': 'or', 'lhs': lhs, 'rhs': rhs}\n        return lhs, remaining\n\n    return marker(marker_string)\n\n\ndef parse_requirement(req):\n    \"\"\"\n    Parse a requirement passed in as a string. Return a Container\n    whose attributes contain the various parts of the requirement.\n    \"\"\"\n    remaining = req.strip()\n    if not remaining or remaining.startswith('#'):\n        return None\n    m = IDENTIFIER.match(remaining)\n    if not m:\n        raise SyntaxError('name expected: %s' % remaining)\n    distname = m.groups()[0]\n    remaining = remaining[m.end():]\n    extras = mark_expr = versions = uri = None\n    if remaining and remaining[0] == '[':\n        i = remaining.find(']', 1)\n        if i < 0:\n            raise SyntaxError('unterminated extra: %s' % remaining)\n        s = remaining[1:i]\n        remaining = remaining[i + 1:].lstrip()\n        extras = []\n        while s:\n            m = IDENTIFIER.match(s)\n            if not m:\n                raise SyntaxError('malformed extra: %s' % s)\n            extras.append(m.groups()[0])\n            s = s[m.end():]\n            if not s:\n                break\n            if s[0] != ',':\n                raise SyntaxError('comma expected in extras: %s' % s)\n            s = s[1:].lstrip()\n        if not extras:\n            extras = None\n    if remaining:\n        if remaining[0] == '@':\n            # it's a URI\n            remaining = remaining[1:].lstrip()\n            m = NON_SPACE.match(remaining)\n            if not m:\n                raise SyntaxError('invalid URI: %s' % remaining)\n            uri = m.groups()[0]\n            t = urlparse(uri)\n            # there are issues with Python and URL parsing, so this test\n            # is a bit crude. See bpo-20271, bpo-23505. Python doesn't\n            # always parse invalid URLs correctly - it should raise\n            # exceptions for malformed URLs\n            if not (t.scheme and t.netloc):\n                raise SyntaxError('Invalid URL: %s' % uri)\n            remaining = remaining[m.end():].lstrip()\n        else:\n\n            def get_versions(ver_remaining):\n                \"\"\"\n                Return a list of operator, version tuples if any are\n                specified, else None.\n                \"\"\"\n                m = COMPARE_OP.match(ver_remaining)\n                versions = None\n                if m:\n                    versions = []\n                    while True:\n                        op = m.groups()[0]\n                        ver_remaining = ver_remaining[m.end():]\n                        m = VERSION_IDENTIFIER.match(ver_remaining)\n                        if not m:\n                            raise SyntaxError('invalid version: %s' % ver_remaining)\n                        v = m.groups()[0]\n                        versions.append((op, v))\n                        ver_remaining = ver_remaining[m.end():]\n                        if not ver_remaining or ver_remaining[0] != ',':\n                            break\n                        ver_remaining = ver_remaining[1:].lstrip()\n                        # Some packages have a trailing comma which would break things\n                        # See issue #148\n                        if not ver_remaining:\n                            break\n                        m = COMPARE_OP.match(ver_remaining)\n                        if not m:\n                            raise SyntaxError('invalid constraint: %s' % ver_remaining)\n                    if not versions:\n                        versions = None\n                return versions, ver_remaining\n\n            if remaining[0] != '(':\n                versions, remaining = get_versions(remaining)\n            else:\n                i = remaining.find(')', 1)\n                if i < 0:\n                    raise SyntaxError('unterminated parenthesis: %s' % remaining)\n                s = remaining[1:i]\n                remaining = remaining[i + 1:].lstrip()\n                # As a special diversion from PEP 508, allow a version number\n                # a.b.c in parentheses as a synonym for ~= a.b.c (because this\n                # is allowed in earlier PEPs)\n                if COMPARE_OP.match(s):\n                    versions, _ = get_versions(s)\n                else:\n                    m = VERSION_IDENTIFIER.match(s)\n                    if not m:\n                        raise SyntaxError('invalid constraint: %s' % s)\n                    v = m.groups()[0]\n                    s = s[m.end():].lstrip()\n                    if s:\n                        raise SyntaxError('invalid constraint: %s' % s)\n                    versions = [('~=', v)]\n\n    if remaining:\n        if remaining[0] != ';':\n            raise SyntaxError('invalid requirement: %s' % remaining)\n        remaining = remaining[1:].lstrip()\n\n        mark_expr, remaining = parse_marker(remaining)\n\n    if remaining and remaining[0] != '#':\n        raise SyntaxError('unexpected trailing data: %s' % remaining)\n\n    if not versions:\n        rs = distname\n    else:\n        rs = '%s %s' % (distname, ', '.join(['%s %s' % con for con in versions]))\n    return Container(name=distname, extras=extras, constraints=versions, marker=mark_expr, url=uri, requirement=rs)\n\n\ndef get_resources_dests(resources_root, rules):\n    \"\"\"Find destinations for resources files\"\"\"\n\n    def get_rel_path(root, path):\n        # normalizes and returns a lstripped-/-separated path\n        root = root.replace(os.path.sep, '/')\n        path = path.replace(os.path.sep, '/')\n        assert path.startswith(root)\n        return path[len(root):].lstrip('/')\n\n    destinations = {}\n    for base, suffix, dest in rules:\n        prefix = os.path.join(resources_root, base)\n        for abs_base in iglob(prefix):\n            abs_glob = os.path.join(abs_base, suffix)\n            for abs_path in iglob(abs_glob):\n                resource_file = get_rel_path(resources_root, abs_path)\n                if dest is None:  # remove the entry if it was here\n                    destinations.pop(resource_file, None)\n                else:\n                    rel_path = get_rel_path(abs_base, abs_path)\n                    rel_dest = dest.replace(os.path.sep, '/').rstrip('/')\n                    destinations[resource_file] = rel_dest + '/' + rel_path\n    return destinations\n\n\ndef in_venv():\n    if hasattr(sys, 'real_prefix'):\n        # virtualenv venvs\n        result = True\n    else:\n        # PEP 405 venvs\n        result = sys.prefix != getattr(sys, 'base_prefix', sys.prefix)\n    return result\n\n\ndef get_executable():\n    # The __PYVENV_LAUNCHER__ dance is apparently no longer needed, as\n    # changes to the stub launcher mean that sys.executable always points\n    # to the stub on OS X\n    #    if sys.platform == 'darwin' and ('__PYVENV_LAUNCHER__'\n    #                                     in os.environ):\n    #        result =  os.environ['__PYVENV_LAUNCHER__']\n    #    else:\n    #        result = sys.executable\n    #    return result\n    # Avoid normcasing: see issue #143\n    # result = os.path.normcase(sys.executable)\n    result = sys.executable\n    if not isinstance(result, text_type):\n        result = fsdecode(result)\n    return result\n\n\ndef proceed(prompt, allowed_chars, error_prompt=None, default=None):\n    p = prompt\n    while True:\n        s = raw_input(p)\n        p = prompt\n        if not s and default:\n            s = default\n        if s:\n            c = s[0].lower()\n            if c in allowed_chars:\n                break\n            if error_prompt:\n                p = '%c: %s\\n%s' % (c, error_prompt, prompt)\n    return c\n\n\ndef extract_by_key(d, keys):\n    if isinstance(keys, string_types):\n        keys = keys.split()\n    result = {}\n    for key in keys:\n        if key in d:\n            result[key] = d[key]\n    return result\n\n\ndef read_exports(stream):\n    if sys.version_info[0] >= 3:\n        # needs to be a text stream\n        stream = codecs.getreader('utf-8')(stream)\n    # Try to load as JSON, falling back on legacy format\n    data = stream.read()\n    stream = StringIO(data)\n    try:\n        jdata = json.load(stream)\n        result = jdata['extensions']['python.exports']['exports']\n        for group, entries in result.items():\n            for k, v in entries.items():\n                s = '%s = %s' % (k, v)\n                entry = get_export_entry(s)\n                assert entry is not None\n                entries[k] = entry\n        return result\n    except Exception:\n        stream.seek(0, 0)\n\n    def read_stream(cp, stream):\n        if hasattr(cp, 'read_file'):\n            cp.read_file(stream)\n        else:\n            cp.readfp(stream)\n\n    cp = configparser.ConfigParser()\n    try:\n        read_stream(cp, stream)\n    except configparser.MissingSectionHeaderError:\n        stream.close()\n        data = textwrap.dedent(data)\n        stream = StringIO(data)\n        read_stream(cp, stream)\n\n    result = {}\n    for key in cp.sections():\n        result[key] = entries = {}\n        for name, value in cp.items(key):\n            s = '%s = %s' % (name, value)\n            entry = get_export_entry(s)\n            assert entry is not None\n            # entry.dist = self\n            entries[name] = entry\n    return result\n\n\ndef write_exports(exports, stream):\n    if sys.version_info[0] >= 3:\n        # needs to be a text stream\n        stream = codecs.getwriter('utf-8')(stream)\n    cp = configparser.ConfigParser()\n    for k, v in exports.items():\n        # TODO check k, v for valid values\n        cp.add_section(k)\n        for entry in v.values():\n            if entry.suffix is None:\n                s = entry.prefix\n            else:\n                s = '%s:%s' % (entry.prefix, entry.suffix)\n            if entry.flags:\n                s = '%s [%s]' % (s, ', '.join(entry.flags))\n            cp.set(k, entry.name, s)\n    cp.write(stream)\n\n\n@contextlib.contextmanager\ndef tempdir():\n    td = tempfile.mkdtemp()\n    try:\n        yield td\n    finally:\n        shutil.rmtree(td)\n\n\n@contextlib.contextmanager\ndef chdir(d):\n    cwd = os.getcwd()\n    try:\n        os.chdir(d)\n        yield\n    finally:\n        os.chdir(cwd)\n\n\n@contextlib.contextmanager\ndef socket_timeout(seconds=15):\n    cto = socket.getdefaulttimeout()\n    try:\n        socket.setdefaulttimeout(seconds)\n        yield\n    finally:\n        socket.setdefaulttimeout(cto)\n\n\nclass cached_property(object):\n\n    def __init__(self, func):\n        self.func = func\n        # for attr in ('__name__', '__module__', '__doc__'):\n        #     setattr(self, attr, getattr(func, attr, None))\n\n    def __get__(self, obj, cls=None):\n        if obj is None:\n            return self\n        value = self.func(obj)\n        object.__setattr__(obj, self.func.__name__, value)\n        # obj.__dict__[self.func.__name__] = value = self.func(obj)\n        return value\n\n\ndef convert_path(pathname):\n    \"\"\"Return 'pathname' as a name that will work on the native filesystem.\n\n    The path is split on '/' and put back together again using the current\n    directory separator.  Needed because filenames in the setup script are\n    always supplied in Unix style, and have to be converted to the local\n    convention before we can actually use them in the filesystem.  Raises\n    ValueError on non-Unix-ish systems if 'pathname' either starts or\n    ends with a slash.\n    \"\"\"\n    if os.sep == '/':\n        return pathname\n    if not pathname:\n        return pathname\n    if pathname[0] == '/':\n        raise ValueError(\"path '%s' cannot be absolute\" % pathname)\n    if pathname[-1] == '/':\n        raise ValueError(\"path '%s' cannot end with '/'\" % pathname)\n\n    paths = pathname.split('/')\n    while os.curdir in paths:\n        paths.remove(os.curdir)\n    if not paths:\n        return os.curdir\n    return os.path.join(*paths)\n\n\nclass FileOperator(object):\n\n    def __init__(self, dry_run=False):\n        self.dry_run = dry_run\n        self.ensured = set()\n        self._init_record()\n\n    def _init_record(self):\n        self.record = False\n        self.files_written = set()\n        self.dirs_created = set()\n\n    def record_as_written(self, path):\n        if self.record:\n            self.files_written.add(path)\n\n    def newer(self, source, target):\n        \"\"\"Tell if the target is newer than the source.\n\n        Returns true if 'source' exists and is more recently modified than\n        'target', or if 'source' exists and 'target' doesn't.\n\n        Returns false if both exist and 'target' is the same age or younger\n        than 'source'. Raise PackagingFileError if 'source' does not exist.\n\n        Note that this test is not very accurate: files created in the same\n        second will have the same \"age\".\n        \"\"\"\n        if not os.path.exists(source):\n            raise DistlibException(\"file '%r' does not exist\" % os.path.abspath(source))\n        if not os.path.exists(target):\n            return True\n\n        return os.stat(source).st_mtime > os.stat(target).st_mtime\n\n    def copy_file(self, infile, outfile, check=True):\n        \"\"\"Copy a file respecting dry-run and force flags.\n        \"\"\"\n        self.ensure_dir(os.path.dirname(outfile))\n        logger.info('Copying %s to %s', infile, outfile)\n        if not self.dry_run:\n            msg = None\n            if check:\n                if os.path.islink(outfile):\n                    msg = '%s is a symlink' % outfile\n                elif os.path.exists(outfile) and not os.path.isfile(outfile):\n                    msg = '%s is a non-regular file' % outfile\n            if msg:\n                raise ValueError(msg + ' which would be overwritten')\n            shutil.copyfile(infile, outfile)\n        self.record_as_written(outfile)\n\n    def copy_stream(self, instream, outfile, encoding=None):\n        assert not os.path.isdir(outfile)\n        self.ensure_dir(os.path.dirname(outfile))\n        logger.info('Copying stream %s to %s', instream, outfile)\n        if not self.dry_run:\n            if encoding is None:\n                outstream = open(outfile, 'wb')\n            else:\n                outstream = codecs.open(outfile, 'w', encoding=encoding)\n            try:\n                shutil.copyfileobj(instream, outstream)\n            finally:\n                outstream.close()\n        self.record_as_written(outfile)\n\n    def write_binary_file(self, path, data):\n        self.ensure_dir(os.path.dirname(path))\n        if not self.dry_run:\n            if os.path.exists(path):\n                os.remove(path)\n            with open(path, 'wb') as f:\n                f.write(data)\n        self.record_as_written(path)\n\n    def write_text_file(self, path, data, encoding):\n        self.write_binary_file(path, data.encode(encoding))\n\n    def set_mode(self, bits, mask, files):\n        if os.name == 'posix' or (os.name == 'java' and os._name == 'posix'):\n            # Set the executable bits (owner, group, and world) on\n            # all the files specified.\n            for f in files:\n                if self.dry_run:\n                    logger.info(\"changing mode of %s\", f)\n                else:\n                    mode = (os.stat(f).st_mode | bits) & mask\n                    logger.info(\"changing mode of %s to %o\", f, mode)\n                    os.chmod(f, mode)\n\n    set_executable_mode = lambda s, f: s.set_mode(0o555, 0o7777, f)\n\n    def ensure_dir(self, path):\n        path = os.path.abspath(path)\n        if path not in self.ensured and not os.path.exists(path):\n            self.ensured.add(path)\n            d, f = os.path.split(path)\n            self.ensure_dir(d)\n            logger.info('Creating %s' % path)\n            if not self.dry_run:\n                os.mkdir(path)\n            if self.record:\n                self.dirs_created.add(path)\n\n    def byte_compile(self, path, optimize=False, force=False, prefix=None, hashed_invalidation=False):\n        dpath = cache_from_source(path, not optimize)\n        logger.info('Byte-compiling %s to %s', path, dpath)\n        if not self.dry_run:\n            if force or self.newer(path, dpath):\n                if not prefix:\n                    diagpath = None\n                else:\n                    assert path.startswith(prefix)\n                    diagpath = path[len(prefix):]\n            compile_kwargs = {}\n            if hashed_invalidation and hasattr(py_compile, 'PycInvalidationMode'):\n                if not isinstance(hashed_invalidation, py_compile.PycInvalidationMode):\n                    hashed_invalidation = py_compile.PycInvalidationMode.CHECKED_HASH\n                compile_kwargs['invalidation_mode'] = hashed_invalidation\n            py_compile.compile(path, dpath, diagpath, True, **compile_kwargs)  # raise error\n        self.record_as_written(dpath)\n        return dpath\n\n    def ensure_removed(self, path):\n        if os.path.exists(path):\n            if os.path.isdir(path) and not os.path.islink(path):\n                logger.debug('Removing directory tree at %s', path)\n                if not self.dry_run:\n                    shutil.rmtree(path)\n                if self.record:\n                    if path in self.dirs_created:\n                        self.dirs_created.remove(path)\n            else:\n                if os.path.islink(path):\n                    s = 'link'\n                else:\n                    s = 'file'\n                logger.debug('Removing %s %s', s, path)\n                if not self.dry_run:\n                    os.remove(path)\n                if self.record:\n                    if path in self.files_written:\n                        self.files_written.remove(path)\n\n    def is_writable(self, path):\n        result = False\n        while not result:\n            if os.path.exists(path):\n                result = os.access(path, os.W_OK)\n                break\n            parent = os.path.dirname(path)\n            if parent == path:\n                break\n            path = parent\n        return result\n\n    def commit(self):\n        \"\"\"\n        Commit recorded changes, turn off recording, return\n        changes.\n        \"\"\"\n        assert self.record\n        result = self.files_written, self.dirs_created\n        self._init_record()\n        return result\n\n    def rollback(self):\n        if not self.dry_run:\n            for f in list(self.files_written):\n                if os.path.exists(f):\n                    os.remove(f)\n            # dirs should all be empty now, except perhaps for\n            # __pycache__ subdirs\n            # reverse so that subdirs appear before their parents\n            dirs = sorted(self.dirs_created, reverse=True)\n            for d in dirs:\n                flist = os.listdir(d)\n                if flist:\n                    assert flist == ['__pycache__']\n                    sd = os.path.join(d, flist[0])\n                    os.rmdir(sd)\n                os.rmdir(d)  # should fail if non-empty\n        self._init_record()\n\n\ndef resolve(module_name, dotted_path):\n    if module_name in sys.modules:\n        mod = sys.modules[module_name]\n    else:\n        mod = __import__(module_name)\n    if dotted_path is None:\n        result = mod\n    else:\n        parts = dotted_path.split('.')\n        result = getattr(mod, parts.pop(0))\n        for p in parts:\n            result = getattr(result, p)\n    return result\n\n\nclass ExportEntry(object):\n\n    def __init__(self, name, prefix, suffix, flags):\n        self.name = name\n        self.prefix = prefix\n        self.suffix = suffix\n        self.flags = flags\n\n    @cached_property\n    def value(self):\n        return resolve(self.prefix, self.suffix)\n\n    def __repr__(self):  # pragma: no cover\n        return '<ExportEntry %s = %s:%s %s>' % (self.name, self.prefix, self.suffix, self.flags)\n\n    def __eq__(self, other):\n        if not isinstance(other, ExportEntry):\n            result = False\n        else:\n            result = (self.name == other.name and self.prefix == other.prefix and self.suffix == other.suffix and\n                      self.flags == other.flags)\n        return result\n\n    __hash__ = object.__hash__\n\n\nENTRY_RE = re.compile(\n    r'''(?P<name>([^\\[]\\S*))\n                      \\s*=\\s*(?P<callable>(\\w+)([:\\.]\\w+)*)\n                      \\s*(\\[\\s*(?P<flags>[\\w-]+(=\\w+)?(,\\s*\\w+(=\\w+)?)*)\\s*\\])?\n                      ''', re.VERBOSE)\n\n\ndef get_export_entry(specification):\n    m = ENTRY_RE.search(specification)\n    if not m:\n        result = None\n        if '[' in specification or ']' in specification:\n            raise DistlibException(\"Invalid specification \"\n                                   \"'%s'\" % specification)\n    else:\n        d = m.groupdict()\n        name = d['name']\n        path = d['callable']\n        colons = path.count(':')\n        if colons == 0:\n            prefix, suffix = path, None\n        else:\n            if colons != 1:\n                raise DistlibException(\"Invalid specification \"\n                                       \"'%s'\" % specification)\n            prefix, suffix = path.split(':')\n        flags = d['flags']\n        if flags is None:\n            if '[' in specification or ']' in specification:\n                raise DistlibException(\"Invalid specification \"\n                                       \"'%s'\" % specification)\n            flags = []\n        else:\n            flags = [f.strip() for f in flags.split(',')]\n        result = ExportEntry(name, prefix, suffix, flags)\n    return result\n\n\ndef get_cache_base(suffix=None):\n    \"\"\"\n    Return the default base location for distlib caches. If the directory does\n    not exist, it is created. Use the suffix provided for the base directory,\n    and default to '.distlib' if it isn't provided.\n\n    On Windows, if LOCALAPPDATA is defined in the environment, then it is\n    assumed to be a directory, and will be the parent directory of the result.\n    On POSIX, and on Windows if LOCALAPPDATA is not defined, the user's home\n    directory - using os.expanduser('~') - will be the parent directory of\n    the result.\n\n    The result is just the directory '.distlib' in the parent directory as\n    determined above, or with the name specified with ``suffix``.\n    \"\"\"\n    if suffix is None:\n        suffix = '.distlib'\n    if os.name == 'nt' and 'LOCALAPPDATA' in os.environ:\n        result = os.path.expandvars('$localappdata')\n    else:\n        # Assume posix, or old Windows\n        result = os.path.expanduser('~')\n    # we use 'isdir' instead of 'exists', because we want to\n    # fail if there's a file with that name\n    if os.path.isdir(result):\n        usable = os.access(result, os.W_OK)\n        if not usable:\n            logger.warning('Directory exists but is not writable: %s', result)\n    else:\n        try:\n            os.makedirs(result)\n            usable = True\n        except OSError:\n            logger.warning('Unable to create %s', result, exc_info=True)\n            usable = False\n    if not usable:\n        result = tempfile.mkdtemp()\n        logger.warning('Default location unusable, using %s', result)\n    return os.path.join(result, suffix)\n\n\ndef path_to_cache_dir(path, use_abspath=True):\n    \"\"\"\n    Convert an absolute path to a directory name for use in a cache.\n\n    The algorithm used is:\n\n    #. On Windows, any ``':'`` in the drive is replaced with ``'---'``.\n    #. Any occurrence of ``os.sep`` is replaced with ``'--'``.\n    #. ``'.cache'`` is appended.\n    \"\"\"\n    d, p = os.path.splitdrive(os.path.abspath(path) if use_abspath else path)\n    if d:\n        d = d.replace(':', '---')\n    p = p.replace(os.sep, '--')\n    return d + p + '.cache'\n\n\ndef ensure_slash(s):\n    if not s.endswith('/'):\n        return s + '/'\n    return s\n\n\ndef parse_credentials(netloc):\n    username = password = None\n    if '@' in netloc:\n        prefix, netloc = netloc.rsplit('@', 1)\n        if ':' not in prefix:\n            username = prefix\n        else:\n            username, password = prefix.split(':', 1)\n    if username:\n        username = unquote(username)\n    if password:\n        password = unquote(password)\n    return username, password, netloc\n\n\ndef get_process_umask():\n    result = os.umask(0o22)\n    os.umask(result)\n    return result\n\n\ndef is_string_sequence(seq):\n    result = True\n    i = None\n    for i, s in enumerate(seq):\n        if not isinstance(s, string_types):\n            result = False\n            break\n    assert i is not None\n    return result\n\n\nPROJECT_NAME_AND_VERSION = re.compile('([a-z0-9_]+([.-][a-z_][a-z0-9_]*)*)-'\n                                      '([a-z0-9_.+-]+)', re.I)\nPYTHON_VERSION = re.compile(r'-py(\\d\\.?\\d?)')\n\n\ndef split_filename(filename, project_name=None):\n    \"\"\"\n    Extract name, version, python version from a filename (no extension)\n\n    Return name, version, pyver or None\n    \"\"\"\n    result = None\n    pyver = None\n    filename = unquote(filename).replace(' ', '-')\n    m = PYTHON_VERSION.search(filename)\n    if m:\n        pyver = m.group(1)\n        filename = filename[:m.start()]\n    if project_name and len(filename) > len(project_name) + 1:\n        m = re.match(re.escape(project_name) + r'\\b', filename)\n        if m:\n            n = m.end()\n            result = filename[:n], filename[n + 1:], pyver\n    if result is None:\n        m = PROJECT_NAME_AND_VERSION.match(filename)\n        if m:\n            result = m.group(1), m.group(3), pyver\n    return result\n\n\n# Allow spaces in name because of legacy dists like \"Twisted Core\"\nNAME_VERSION_RE = re.compile(r'(?P<name>[\\w .-]+)\\s*'\n                             r'\\(\\s*(?P<ver>[^\\s)]+)\\)$')\n\n\ndef parse_name_and_version(p):\n    \"\"\"\n    A utility method used to get name and version from a string.\n\n    From e.g. a Provides-Dist value.\n\n    :param p: A value in a form 'foo (1.0)'\n    :return: The name and version as a tuple.\n    \"\"\"\n    m = NAME_VERSION_RE.match(p)\n    if not m:\n        raise DistlibException('Ill-formed name/version string: \\'%s\\'' % p)\n    d = m.groupdict()\n    return d['name'].strip().lower(), d['ver']\n\n\ndef get_extras(requested, available):\n    result = set()\n    requested = set(requested or [])\n    available = set(available or [])\n    if '*' in requested:\n        requested.remove('*')\n        result |= available\n    for r in requested:\n        if r == '-':\n            result.add(r)\n        elif r.startswith('-'):\n            unwanted = r[1:]\n            if unwanted not in available:\n                logger.warning('undeclared extra: %s' % unwanted)\n            if unwanted in result:\n                result.remove(unwanted)\n        else:\n            if r not in available:\n                logger.warning('undeclared extra: %s' % r)\n            result.add(r)\n    return result\n\n\n#\n# Extended metadata functionality\n#\n\n\ndef _get_external_data(url):\n    result = {}\n    try:\n        # urlopen might fail if it runs into redirections,\n        # because of Python issue #13696. Fixed in locators\n        # using a custom redirect handler.\n        resp = urlopen(url)\n        headers = resp.info()\n        ct = headers.get('Content-Type')\n        if not ct.startswith('application/json'):\n            logger.debug('Unexpected response for JSON request: %s', ct)\n        else:\n            reader = codecs.getreader('utf-8')(resp)\n            # data = reader.read().decode('utf-8')\n            # result = json.loads(data)\n            result = json.load(reader)\n    except Exception as e:\n        logger.exception('Failed to get external data for %s: %s', url, e)\n    return result\n\n\n_external_data_base_url = 'https://www.red-dove.com/pypi/projects/'\n\n\ndef get_project_data(name):\n    url = '%s/%s/project.json' % (name[0].upper(), name)\n    url = urljoin(_external_data_base_url, url)\n    result = _get_external_data(url)\n    return result\n\n\ndef get_package_data(name, version):\n    url = '%s/%s/package-%s.json' % (name[0].upper(), name, version)\n    url = urljoin(_external_data_base_url, url)\n    return _get_external_data(url)\n\n\nclass Cache(object):\n    \"\"\"\n    A class implementing a cache for resources that need to live in the file system\n    e.g. shared libraries. This class was moved from resources to here because it\n    could be used by other modules, e.g. the wheel module.\n    \"\"\"\n\n    def __init__(self, base):\n        \"\"\"\n        Initialise an instance.\n\n        :param base: The base directory where the cache should be located.\n        \"\"\"\n        # we use 'isdir' instead of 'exists', because we want to\n        # fail if there's a file with that name\n        if not os.path.isdir(base):  # pragma: no cover\n            os.makedirs(base)\n        if (os.stat(base).st_mode & 0o77) != 0:\n            logger.warning('Directory \\'%s\\' is not private', base)\n        self.base = os.path.abspath(os.path.normpath(base))\n\n    def prefix_to_dir(self, prefix, use_abspath=True):\n        \"\"\"\n        Converts a resource prefix to a directory name in the cache.\n        \"\"\"\n        return path_to_cache_dir(prefix, use_abspath=use_abspath)\n\n    def clear(self):\n        \"\"\"\n        Clear the cache.\n        \"\"\"\n        not_removed = []\n        for fn in os.listdir(self.base):\n            fn = os.path.join(self.base, fn)\n            try:\n                if os.path.islink(fn) or os.path.isfile(fn):\n                    os.remove(fn)\n                elif os.path.isdir(fn):\n                    shutil.rmtree(fn)\n            except Exception:\n                not_removed.append(fn)\n        return not_removed\n\n\nclass EventMixin(object):\n    \"\"\"\n    A very simple publish/subscribe system.\n    \"\"\"\n\n    def __init__(self):\n        self._subscribers = {}\n\n    def add(self, event, subscriber, append=True):\n        \"\"\"\n        Add a subscriber for an event.\n\n        :param event: The name of an event.\n        :param subscriber: The subscriber to be added (and called when the\n                           event is published).\n        :param append: Whether to append or prepend the subscriber to an\n                       existing subscriber list for the event.\n        \"\"\"\n        subs = self._subscribers\n        if event not in subs:\n            subs[event] = deque([subscriber])\n        else:\n            sq = subs[event]\n            if append:\n                sq.append(subscriber)\n            else:\n                sq.appendleft(subscriber)\n\n    def remove(self, event, subscriber):\n        \"\"\"\n        Remove a subscriber for an event.\n\n        :param event: The name of an event.\n        :param subscriber: The subscriber to be removed.\n        \"\"\"\n        subs = self._subscribers\n        if event not in subs:\n            raise ValueError('No subscribers: %r' % event)\n        subs[event].remove(subscriber)\n\n    def get_subscribers(self, event):\n        \"\"\"\n        Return an iterator for the subscribers for an event.\n        :param event: The event to return subscribers for.\n        \"\"\"\n        return iter(self._subscribers.get(event, ()))\n\n    def publish(self, event, *args, **kwargs):\n        \"\"\"\n        Publish a event and return a list of values returned by its\n        subscribers.\n\n        :param event: The event to publish.\n        :param args: The positional arguments to pass to the event's\n                     subscribers.\n        :param kwargs: The keyword arguments to pass to the event's\n                       subscribers.\n        \"\"\"\n        result = []\n        for subscriber in self.get_subscribers(event):\n            try:\n                value = subscriber(event, *args, **kwargs)\n            except Exception:\n                logger.exception('Exception during event publication')\n                value = None\n            result.append(value)\n        logger.debug('publish %s: args = %s, kwargs = %s, result = %s', event, args, kwargs, result)\n        return result\n\n\n#\n# Simple sequencing\n#\nclass Sequencer(object):\n\n    def __init__(self):\n        self._preds = {}\n        self._succs = {}\n        self._nodes = set()  # nodes with no preds/succs\n\n    def add_node(self, node):\n        self._nodes.add(node)\n\n    def remove_node(self, node, edges=False):\n        if node in self._nodes:\n            self._nodes.remove(node)\n        if edges:\n            for p in set(self._preds.get(node, ())):\n                self.remove(p, node)\n            for s in set(self._succs.get(node, ())):\n                self.remove(node, s)\n            # Remove empties\n            for k, v in list(self._preds.items()):\n                if not v:\n                    del self._preds[k]\n            for k, v in list(self._succs.items()):\n                if not v:\n                    del self._succs[k]\n\n    def add(self, pred, succ):\n        assert pred != succ\n        self._preds.setdefault(succ, set()).add(pred)\n        self._succs.setdefault(pred, set()).add(succ)\n\n    def remove(self, pred, succ):\n        assert pred != succ\n        try:\n            preds = self._preds[succ]\n            succs = self._succs[pred]\n        except KeyError:  # pragma: no cover\n            raise ValueError('%r not a successor of anything' % succ)\n        try:\n            preds.remove(pred)\n            succs.remove(succ)\n        except KeyError:  # pragma: no cover\n            raise ValueError('%r not a successor of %r' % (succ, pred))\n\n    def is_step(self, step):\n        return (step in self._preds or step in self._succs or step in self._nodes)\n\n    def get_steps(self, final):\n        if not self.is_step(final):\n            raise ValueError('Unknown: %r' % final)\n        result = []\n        todo = []\n        seen = set()\n        todo.append(final)\n        while todo:\n            step = todo.pop(0)\n            if step in seen:\n                # if a step was already seen,\n                # move it to the end (so it will appear earlier\n                # when reversed on return) ... but not for the\n                # final step, as that would be confusing for\n                # users\n                if step != final:\n                    result.remove(step)\n                    result.append(step)\n            else:\n                seen.add(step)\n                result.append(step)\n                preds = self._preds.get(step, ())\n                todo.extend(preds)\n        return reversed(result)\n\n    @property\n    def strong_connections(self):\n        # http://en.wikipedia.org/wiki/Tarjan%27s_strongly_connected_components_algorithm\n        index_counter = [0]\n        stack = []\n        lowlinks = {}\n        index = {}\n        result = []\n\n        graph = self._succs\n\n        def strongconnect(node):\n            # set the depth index for this node to the smallest unused index\n            index[node] = index_counter[0]\n            lowlinks[node] = index_counter[0]\n            index_counter[0] += 1\n            stack.append(node)\n\n            # Consider successors\n            try:\n                successors = graph[node]\n            except Exception:\n                successors = []\n            for successor in successors:\n                if successor not in lowlinks:\n                    # Successor has not yet been visited\n                    strongconnect(successor)\n                    lowlinks[node] = min(lowlinks[node], lowlinks[successor])\n                elif successor in stack:\n                    # the successor is in the stack and hence in the current\n                    # strongly connected component (SCC)\n                    lowlinks[node] = min(lowlinks[node], index[successor])\n\n            # If `node` is a root node, pop the stack and generate an SCC\n            if lowlinks[node] == index[node]:\n                connected_component = []\n\n                while True:\n                    successor = stack.pop()\n                    connected_component.append(successor)\n                    if successor == node:\n                        break\n                component = tuple(connected_component)\n                # storing the result\n                result.append(component)\n\n        for node in graph:\n            if node not in lowlinks:\n                strongconnect(node)\n\n        return result\n\n    @property\n    def dot(self):\n        result = ['digraph G {']\n        for succ in self._preds:\n            preds = self._preds[succ]\n            for pred in preds:\n                result.append('  %s -> %s;' % (pred, succ))\n        for node in self._nodes:\n            result.append('  %s;' % node)\n        result.append('}')\n        return '\\n'.join(result)\n\n\n#\n# Unarchiving functionality for zip, tar, tgz, tbz, whl\n#\n\nARCHIVE_EXTENSIONS = ('.tar.gz', '.tar.bz2', '.tar', '.zip', '.tgz', '.tbz', '.whl')\n\n\ndef unarchive(archive_filename, dest_dir, format=None, check=True):\n\n    def check_path(path):\n        if not isinstance(path, text_type):\n            path = path.decode('utf-8')\n        p = os.path.abspath(os.path.join(dest_dir, path))\n        if not p.startswith(dest_dir) or p[plen] != os.sep:\n            raise ValueError('path outside destination: %r' % p)\n\n    dest_dir = os.path.abspath(dest_dir)\n    plen = len(dest_dir)\n    archive = None\n    if format is None:\n        if archive_filename.endswith(('.zip', '.whl')):\n            format = 'zip'\n        elif archive_filename.endswith(('.tar.gz', '.tgz')):\n            format = 'tgz'\n            mode = 'r:gz'\n        elif archive_filename.endswith(('.tar.bz2', '.tbz')):\n            format = 'tbz'\n            mode = 'r:bz2'\n        elif archive_filename.endswith('.tar'):\n            format = 'tar'\n            mode = 'r'\n        else:  # pragma: no cover\n            raise ValueError('Unknown format for %r' % archive_filename)\n    try:\n        if format == 'zip':\n            archive = ZipFile(archive_filename, 'r')\n            if check:\n                names = archive.namelist()\n                for name in names:\n                    check_path(name)\n        else:\n            archive = tarfile.open(archive_filename, mode)\n            if check:\n                names = archive.getnames()\n                for name in names:\n                    check_path(name)\n        if format != 'zip' and sys.version_info[0] < 3:\n            # See Python issue 17153. If the dest path contains Unicode,\n            # tarfile extraction fails on Python 2.x if a member path name\n            # contains non-ASCII characters - it leads to an implicit\n            # bytes -> unicode conversion using ASCII to decode.\n            for tarinfo in archive.getmembers():\n                if not isinstance(tarinfo.name, text_type):\n                    tarinfo.name = tarinfo.name.decode('utf-8')\n\n        # Limit extraction of dangerous items, if this Python\n        # allows it easily. If not, just trust the input.\n        # See: https://docs.python.org/3/library/tarfile.html#extraction-filters\n        def extraction_filter(member, path):\n            \"\"\"Run tarfile.tar_filter, but raise the expected ValueError\"\"\"\n            # This is only called if the current Python has tarfile filters\n            try:\n                return tarfile.tar_filter(member, path)\n            except tarfile.FilterError as exc:\n                raise ValueError(str(exc))\n\n        archive.extraction_filter = extraction_filter\n\n        archive.extractall(dest_dir)\n\n    finally:\n        if archive:\n            archive.close()\n\n\ndef zip_dir(directory):\n    \"\"\"zip a directory tree into a BytesIO object\"\"\"\n    result = io.BytesIO()\n    dlen = len(directory)\n    with ZipFile(result, \"w\") as zf:\n        for root, dirs, files in os.walk(directory):\n            for name in files:\n                full = os.path.join(root, name)\n                rel = root[dlen:]\n                dest = os.path.join(rel, name)\n                zf.write(full, dest)\n    return result\n\n\n#\n# Simple progress bar\n#\n\nUNITS = ('', 'K', 'M', 'G', 'T', 'P')\n\n\nclass Progress(object):\n    unknown = 'UNKNOWN'\n\n    def __init__(self, minval=0, maxval=100):\n        assert maxval is None or maxval >= minval\n        self.min = self.cur = minval\n        self.max = maxval\n        self.started = None\n        self.elapsed = 0\n        self.done = False\n\n    def update(self, curval):\n        assert self.min <= curval\n        assert self.max is None or curval <= self.max\n        self.cur = curval\n        now = time.time()\n        if self.started is None:\n            self.started = now\n        else:\n            self.elapsed = now - self.started\n\n    def increment(self, incr):\n        assert incr >= 0\n        self.update(self.cur + incr)\n\n    def start(self):\n        self.update(self.min)\n        return self\n\n    def stop(self):\n        if self.max is not None:\n            self.update(self.max)\n        self.done = True\n\n    @property\n    def maximum(self):\n        return self.unknown if self.max is None else self.max\n\n    @property\n    def percentage(self):\n        if self.done:\n            result = '100 %'\n        elif self.max is None:\n            result = ' ?? %'\n        else:\n            v = 100.0 * (self.cur - self.min) / (self.max - self.min)\n            result = '%3d %%' % v\n        return result\n\n    def format_duration(self, duration):\n        if (duration <= 0) and self.max is None or self.cur == self.min:\n            result = '??:??:??'\n        # elif duration < 1:\n        #     result = '--:--:--'\n        else:\n            result = time.strftime('%H:%M:%S', time.gmtime(duration))\n        return result\n\n    @property\n    def ETA(self):\n        if self.done:\n            prefix = 'Done'\n            t = self.elapsed\n            # import pdb; pdb.set_trace()\n        else:\n            prefix = 'ETA '\n            if self.max is None:\n                t = -1\n            elif self.elapsed == 0 or (self.cur == self.min):\n                t = 0\n            else:\n                # import pdb; pdb.set_trace()\n                t = float(self.max - self.min)\n                t /= self.cur - self.min\n                t = (t - 1) * self.elapsed\n        return '%s: %s' % (prefix, self.format_duration(t))\n\n    @property\n    def speed(self):\n        if self.elapsed == 0:\n            result = 0.0\n        else:\n            result = (self.cur - self.min) / self.elapsed\n        for unit in UNITS:\n            if result < 1000:\n                break\n            result /= 1000.0\n        return '%d %sB/s' % (result, unit)\n\n\n#\n# Glob functionality\n#\n\nRICH_GLOB = re.compile(r'\\{([^}]*)\\}')\n_CHECK_RECURSIVE_GLOB = re.compile(r'[^/\\\\,{]\\*\\*|\\*\\*[^/\\\\,}]')\n_CHECK_MISMATCH_SET = re.compile(r'^[^{]*\\}|\\{[^}]*$')\n\n\ndef iglob(path_glob):\n    \"\"\"Extended globbing function that supports ** and {opt1,opt2,opt3}.\"\"\"\n    if _CHECK_RECURSIVE_GLOB.search(path_glob):\n        msg = \"\"\"invalid glob %r: recursive glob \"**\" must be used alone\"\"\"\n        raise ValueError(msg % path_glob)\n    if _CHECK_MISMATCH_SET.search(path_glob):\n        msg = \"\"\"invalid glob %r: mismatching set marker '{' or '}'\"\"\"\n        raise ValueError(msg % path_glob)\n    return _iglob(path_glob)\n\n\ndef _iglob(path_glob):\n    rich_path_glob = RICH_GLOB.split(path_glob, 1)\n    if len(rich_path_glob) > 1:\n        assert len(rich_path_glob) == 3, rich_path_glob\n        prefix, set, suffix = rich_path_glob\n        for item in set.split(','):\n            for path in _iglob(''.join((prefix, item, suffix))):\n                yield path\n    else:\n        if '**' not in path_glob:\n            for item in std_iglob(path_glob):\n                yield item\n        else:\n            prefix, radical = path_glob.split('**', 1)\n            if prefix == '':\n                prefix = '.'\n            if radical == '':\n                radical = '*'\n            else:\n                # we support both\n                radical = radical.lstrip('/')\n                radical = radical.lstrip('\\\\')\n            for path, dir, files in os.walk(prefix):\n                path = os.path.normpath(path)\n                for fn in _iglob(os.path.join(path, radical)):\n                    yield fn\n\n\nif ssl:\n    from .compat import (HTTPSHandler as BaseHTTPSHandler, match_hostname, CertificateError)\n\n    #\n    # HTTPSConnection which verifies certificates/matches domains\n    #\n\n    class HTTPSConnection(httplib.HTTPSConnection):\n        ca_certs = None  # set this to the path to the certs file (.pem)\n        check_domain = True  # only used if ca_certs is not None\n\n        # noinspection PyPropertyAccess\n        def connect(self):\n            sock = socket.create_connection((self.host, self.port), self.timeout)\n            if getattr(self, '_tunnel_host', False):\n                self.sock = sock\n                self._tunnel()\n\n            context = ssl.SSLContext(ssl.PROTOCOL_SSLv23)\n            if hasattr(ssl, 'OP_NO_SSLv2'):\n                context.options |= ssl.OP_NO_SSLv2\n            if getattr(self, 'cert_file', None):\n                context.load_cert_chain(self.cert_file, self.key_file)\n            kwargs = {}\n            if self.ca_certs:\n                context.verify_mode = ssl.CERT_REQUIRED\n                context.load_verify_locations(cafile=self.ca_certs)\n                if getattr(ssl, 'HAS_SNI', False):\n                    kwargs['server_hostname'] = self.host\n\n            self.sock = context.wrap_socket(sock, **kwargs)\n            if self.ca_certs and self.check_domain:\n                try:\n                    match_hostname(self.sock.getpeercert(), self.host)\n                    logger.debug('Host verified: %s', self.host)\n                except CertificateError:  # pragma: no cover\n                    self.sock.shutdown(socket.SHUT_RDWR)\n                    self.sock.close()\n                    raise\n\n    class HTTPSHandler(BaseHTTPSHandler):\n\n        def __init__(self, ca_certs, check_domain=True):\n            BaseHTTPSHandler.__init__(self)\n            self.ca_certs = ca_certs\n            self.check_domain = check_domain\n\n        def _conn_maker(self, *args, **kwargs):\n            \"\"\"\n            This is called to create a connection instance. Normally you'd\n            pass a connection class to do_open, but it doesn't actually check for\n            a class, and just expects a callable. As long as we behave just as a\n            constructor would have, we should be OK. If it ever changes so that\n            we *must* pass a class, we'll create an UnsafeHTTPSConnection class\n            which just sets check_domain to False in the class definition, and\n            choose which one to pass to do_open.\n            \"\"\"\n            result = HTTPSConnection(*args, **kwargs)\n            if self.ca_certs:\n                result.ca_certs = self.ca_certs\n                result.check_domain = self.check_domain\n            return result\n\n        def https_open(self, req):\n            try:\n                return self.do_open(self._conn_maker, req)\n            except URLError as e:\n                if 'certificate verify failed' in str(e.reason):\n                    raise CertificateError('Unable to verify server certificate '\n                                           'for %s' % req.host)\n                else:\n                    raise\n\n    #\n    # To prevent against mixing HTTP traffic with HTTPS (examples: A Man-In-The-\n    # Middle proxy using HTTP listens on port 443, or an index mistakenly serves\n    # HTML containing a http://xyz link when it should be https://xyz),\n    # you can use the following handler class, which does not allow HTTP traffic.\n    #\n    # It works by inheriting from HTTPHandler - so build_opener won't add a\n    # handler for HTTP itself.\n    #\n    class HTTPSOnlyHandler(HTTPSHandler, HTTPHandler):\n\n        def http_open(self, req):\n            raise URLError('Unexpected HTTP request on what should be a secure '\n                           'connection: %s' % req)\n\n\n#\n# XML-RPC with timeouts\n#\nclass Transport(xmlrpclib.Transport):\n\n    def __init__(self, timeout, use_datetime=0):\n        self.timeout = timeout\n        xmlrpclib.Transport.__init__(self, use_datetime)\n\n    def make_connection(self, host):\n        h, eh, x509 = self.get_host_info(host)\n        if not self._connection or host != self._connection[0]:\n            self._extra_headers = eh\n            self._connection = host, httplib.HTTPConnection(h)\n        return self._connection[1]\n\n\nif ssl:\n\n    class SafeTransport(xmlrpclib.SafeTransport):\n\n        def __init__(self, timeout, use_datetime=0):\n            self.timeout = timeout\n            xmlrpclib.SafeTransport.__init__(self, use_datetime)\n\n        def make_connection(self, host):\n            h, eh, kwargs = self.get_host_info(host)\n            if not kwargs:\n                kwargs = {}\n            kwargs['timeout'] = self.timeout\n            if not self._connection or host != self._connection[0]:\n                self._extra_headers = eh\n                self._connection = host, httplib.HTTPSConnection(h, None, **kwargs)\n            return self._connection[1]\n\n\nclass ServerProxy(xmlrpclib.ServerProxy):\n\n    def __init__(self, uri, **kwargs):\n        self.timeout = timeout = kwargs.pop('timeout', None)\n        # The above classes only come into play if a timeout\n        # is specified\n        if timeout is not None:\n            # scheme = splittype(uri)  # deprecated as of Python 3.8\n            scheme = urlparse(uri)[0]\n            use_datetime = kwargs.get('use_datetime', 0)\n            if scheme == 'https':\n                tcls = SafeTransport\n            else:\n                tcls = Transport\n            kwargs['transport'] = t = tcls(timeout, use_datetime=use_datetime)\n            self.transport = t\n        xmlrpclib.ServerProxy.__init__(self, uri, **kwargs)\n\n\n#\n# CSV functionality. This is provided because on 2.x, the csv module can't\n# handle Unicode. However, we need to deal with Unicode in e.g. RECORD files.\n#\n\n\ndef _csv_open(fn, mode, **kwargs):\n    if sys.version_info[0] < 3:\n        mode += 'b'\n    else:\n        kwargs['newline'] = ''\n        # Python 3 determines encoding from locale. Force 'utf-8'\n        # file encoding to match other forced utf-8 encoding\n        kwargs['encoding'] = 'utf-8'\n    return open(fn, mode, **kwargs)\n\n\nclass CSVBase(object):\n    defaults = {\n        'delimiter': str(','),  # The strs are used because we need native\n        'quotechar': str('\"'),  # str in the csv API (2.x won't take\n        'lineterminator': str('\\n')  # Unicode)\n    }\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, *exc_info):\n        self.stream.close()\n\n\nclass CSVReader(CSVBase):\n\n    def __init__(self, **kwargs):\n        if 'stream' in kwargs:\n            stream = kwargs['stream']\n            if sys.version_info[0] >= 3:\n                # needs to be a text stream\n                stream = codecs.getreader('utf-8')(stream)\n            self.stream = stream\n        else:\n            self.stream = _csv_open(kwargs['path'], 'r')\n        self.reader = csv.reader(self.stream, **self.defaults)\n\n    def __iter__(self):\n        return self\n\n    def next(self):\n        result = next(self.reader)\n        if sys.version_info[0] < 3:\n            for i, item in enumerate(result):\n                if not isinstance(item, text_type):\n                    result[i] = item.decode('utf-8')\n        return result\n\n    __next__ = next\n\n\nclass CSVWriter(CSVBase):\n\n    def __init__(self, fn, **kwargs):\n        self.stream = _csv_open(fn, 'w')\n        self.writer = csv.writer(self.stream, **self.defaults)\n\n    def writerow(self, row):\n        if sys.version_info[0] < 3:\n            r = []\n            for item in row:\n                if isinstance(item, text_type):\n                    item = item.encode('utf-8')\n                r.append(item)\n            row = r\n        self.writer.writerow(row)\n\n\n#\n#   Configurator functionality\n#\n\n\nclass Configurator(BaseConfigurator):\n\n    value_converters = dict(BaseConfigurator.value_converters)\n    value_converters['inc'] = 'inc_convert'\n\n    def __init__(self, config, base=None):\n        super(Configurator, self).__init__(config)\n        self.base = base or os.getcwd()\n\n    def configure_custom(self, config):\n\n        def convert(o):\n            if isinstance(o, (list, tuple)):\n                result = type(o)([convert(i) for i in o])\n            elif isinstance(o, dict):\n                if '()' in o:\n                    result = self.configure_custom(o)\n                else:\n                    result = {}\n                    for k in o:\n                        result[k] = convert(o[k])\n            else:\n                result = self.convert(o)\n            return result\n\n        c = config.pop('()')\n        if not callable(c):\n            c = self.resolve(c)\n        props = config.pop('.', None)\n        # Check for valid identifiers\n        args = config.pop('[]', ())\n        if args:\n            args = tuple([convert(o) for o in args])\n        items = [(k, convert(config[k])) for k in config if valid_ident(k)]\n        kwargs = dict(items)\n        result = c(*args, **kwargs)\n        if props:\n            for n, v in props.items():\n                setattr(result, n, convert(v))\n        return result\n\n    def __getitem__(self, key):\n        result = self.config[key]\n        if isinstance(result, dict) and '()' in result:\n            self.config[key] = result = self.configure_custom(result)\n        return result\n\n    def inc_convert(self, value):\n        \"\"\"Default converter for the inc:// protocol.\"\"\"\n        if not os.path.isabs(value):\n            value = os.path.join(self.base, value)\n        with codecs.open(value, 'r', encoding='utf-8') as f:\n            result = json.load(f)\n        return result\n\n\nclass SubprocessMixin(object):\n    \"\"\"\n    Mixin for running subprocesses and capturing their output\n    \"\"\"\n\n    def __init__(self, verbose=False, progress=None):\n        self.verbose = verbose\n        self.progress = progress\n\n    def reader(self, stream, context):\n        \"\"\"\n        Read lines from a subprocess' output stream and either pass to a progress\n        callable (if specified) or write progress information to sys.stderr.\n        \"\"\"\n        progress = self.progress\n        verbose = self.verbose\n        while True:\n            s = stream.readline()\n            if not s:\n                break\n            if progress is not None:\n                progress(s, context)\n            else:\n                if not verbose:\n                    sys.stderr.write('.')\n                else:\n                    sys.stderr.write(s.decode('utf-8'))\n                sys.stderr.flush()\n        stream.close()\n\n    def run_command(self, cmd, **kwargs):\n        p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, **kwargs)\n        t1 = threading.Thread(target=self.reader, args=(p.stdout, 'stdout'))\n        t1.start()\n        t2 = threading.Thread(target=self.reader, args=(p.stderr, 'stderr'))\n        t2.start()\n        p.wait()\n        t1.join()\n        t2.join()\n        if self.progress is not None:\n            self.progress('done.', 'main')\n        elif self.verbose:\n            sys.stderr.write('done.\\n')\n        return p\n\n\ndef normalize_name(name):\n    \"\"\"Normalize a python package name a la PEP 503\"\"\"\n    # https://www.python.org/dev/peps/pep-0503/#normalized-names\n    return re.sub('[-_.]+', '-', name).lower()\n\n\n# def _get_pypirc_command():\n# \"\"\"\n# Get the distutils command for interacting with PyPI configurations.\n# :return: the command.\n# \"\"\"\n# from distutils.core import Distribution\n# from distutils.config import PyPIRCCommand\n# d = Distribution()\n# return PyPIRCCommand(d)\n\n\nclass PyPIRCFile(object):\n\n    DEFAULT_REPOSITORY = 'https://upload.pypi.org/legacy/'\n    DEFAULT_REALM = 'pypi'\n\n    def __init__(self, fn=None, url=None):\n        if fn is None:\n            fn = os.path.join(os.path.expanduser('~'), '.pypirc')\n        self.filename = fn\n        self.url = url\n\n    def read(self):\n        result = {}\n\n        if os.path.exists(self.filename):\n            repository = self.url or self.DEFAULT_REPOSITORY\n\n            config = configparser.RawConfigParser()\n            config.read(self.filename)\n            sections = config.sections()\n            if 'distutils' in sections:\n                # let's get the list of servers\n                index_servers = config.get('distutils', 'index-servers')\n                _servers = [server.strip() for server in index_servers.split('\\n') if server.strip() != '']\n                if _servers == []:\n                    # nothing set, let's try to get the default pypi\n                    if 'pypi' in sections:\n                        _servers = ['pypi']\n                else:\n                    for server in _servers:\n                        result = {'server': server}\n                        result['username'] = config.get(server, 'username')\n\n                        # optional params\n                        for key, default in (('repository', self.DEFAULT_REPOSITORY), ('realm', self.DEFAULT_REALM),\n                                             ('password', None)):\n                            if config.has_option(server, key):\n                                result[key] = config.get(server, key)\n                            else:\n                                result[key] = default\n\n                        # work around people having \"repository\" for the \"pypi\"\n                        # section of their config set to the HTTP (rather than\n                        # HTTPS) URL\n                        if (server == 'pypi' and repository in (self.DEFAULT_REPOSITORY, 'pypi')):\n                            result['repository'] = self.DEFAULT_REPOSITORY\n                        elif (result['server'] != repository and result['repository'] != repository):\n                            result = {}\n            elif 'server-login' in sections:\n                # old format\n                server = 'server-login'\n                if config.has_option(server, 'repository'):\n                    repository = config.get(server, 'repository')\n                else:\n                    repository = self.DEFAULT_REPOSITORY\n                result = {\n                    'username': config.get(server, 'username'),\n                    'password': config.get(server, 'password'),\n                    'repository': repository,\n                    'server': server,\n                    'realm': self.DEFAULT_REALM\n                }\n        return result\n\n    def update(self, username, password):\n        # import pdb; pdb.set_trace()\n        config = configparser.RawConfigParser()\n        fn = self.filename\n        config.read(fn)\n        if not config.has_section('pypi'):\n            config.add_section('pypi')\n        config.set('pypi', 'username', username)\n        config.set('pypi', 'password', password)\n        with open(fn, 'w') as f:\n            config.write(f)\n\n\ndef _load_pypirc(index):\n    \"\"\"\n    Read the PyPI access configuration as supported by distutils.\n    \"\"\"\n    return PyPIRCFile(url=index.url).read()\n\n\ndef _store_pypirc(index):\n    PyPIRCFile().update(index.username, index.password)\n\n\n#\n# get_platform()/get_host_platform() copied from Python 3.10.a0 source, with some minor\n# tweaks\n#\n\n\ndef get_host_platform():\n    \"\"\"Return a string that identifies the current platform.  This is used mainly to\n    distinguish platform-specific build directories and platform-specific built\n    distributions.  Typically includes the OS name and version and the\n    architecture (as supplied by 'os.uname()'), although the exact information\n    included depends on the OS; eg. on Linux, the kernel version isn't\n    particularly important.\n\n    Examples of returned values:\n       linux-i586\n       linux-alpha (?)\n       solaris-2.6-sun4u\n\n    Windows will return one of:\n       win-amd64 (64bit Windows on AMD64 (aka x86_64, Intel64, EM64T, etc)\n       win32 (all others - specifically, sys.platform is returned)\n\n    For other non-POSIX platforms, currently just returns 'sys.platform'.\n\n    \"\"\"\n    if os.name == 'nt':\n        if 'amd64' in sys.version.lower():\n            return 'win-amd64'\n        if '(arm)' in sys.version.lower():\n            return 'win-arm32'\n        if '(arm64)' in sys.version.lower():\n            return 'win-arm64'\n        return sys.platform\n\n    # Set for cross builds explicitly\n    if \"_PYTHON_HOST_PLATFORM\" in os.environ:\n        return os.environ[\"_PYTHON_HOST_PLATFORM\"]\n\n    if os.name != 'posix' or not hasattr(os, 'uname'):\n        # XXX what about the architecture? NT is Intel or Alpha,\n        # Mac OS is M68k or PPC, etc.\n        return sys.platform\n\n    # Try to distinguish various flavours of Unix\n\n    (osname, host, release, version, machine) = os.uname()\n\n    # Convert the OS name to lowercase, remove '/' characters, and translate\n    # spaces (for \"Power Macintosh\")\n    osname = osname.lower().replace('/', '')\n    machine = machine.replace(' ', '_').replace('/', '-')\n\n    if osname[:5] == 'linux':\n        # At least on Linux/Intel, 'machine' is the processor --\n        # i386, etc.\n        # XXX what about Alpha, SPARC, etc?\n        return \"%s-%s\" % (osname, machine)\n\n    elif osname[:5] == 'sunos':\n        if release[0] >= '5':  # SunOS 5 == Solaris 2\n            osname = 'solaris'\n            release = '%d.%s' % (int(release[0]) - 3, release[2:])\n            # We can't use 'platform.architecture()[0]' because a\n            # bootstrap problem. We use a dict to get an error\n            # if some suspicious happens.\n            bitness = {2147483647: '32bit', 9223372036854775807: '64bit'}\n            machine += '.%s' % bitness[sys.maxsize]\n        # fall through to standard osname-release-machine representation\n    elif osname[:3] == 'aix':\n        from _aix_support import aix_platform\n        return aix_platform()\n    elif osname[:6] == 'cygwin':\n        osname = 'cygwin'\n        rel_re = re.compile(r'[\\d.]+', re.ASCII)\n        m = rel_re.match(release)\n        if m:\n            release = m.group()\n    elif osname[:6] == 'darwin':\n        import _osx_support\n        try:\n            from distutils import sysconfig\n        except ImportError:\n            import sysconfig\n        osname, release, machine = _osx_support.get_platform_osx(sysconfig.get_config_vars(), osname, release, machine)\n\n    return '%s-%s-%s' % (osname, release, machine)\n\n\n_TARGET_TO_PLAT = {\n    'x86': 'win32',\n    'x64': 'win-amd64',\n    'arm': 'win-arm32',\n}\n\n\ndef get_platform():\n    if os.name != 'nt':\n        return get_host_platform()\n    cross_compilation_target = os.environ.get('VSCMD_ARG_TGT_ARCH')\n    if cross_compilation_target not in _TARGET_TO_PLAT:\n        return get_host_platform()\n    return _TARGET_TO_PLAT[cross_compilation_target]\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_vendor/distlib/version.py","size":23727,"sha1":"0f65f2c7f70b133ef379fa2a3b2985ff81ad9159","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"# -*- coding: utf-8 -*-\n#\n# Copyright (C) 2012-2023 The Python Software Foundation.\n# See LICENSE.txt and CONTRIBUTORS.txt.\n#\n\"\"\"\nImplementation of a flexible versioning scheme providing support for PEP-440,\nsetuptools-compatible and semantic versioning.\n\"\"\"\n\nimport logging\nimport re\n\nfrom .compat import string_types\nfrom .util import parse_requirement\n\n__all__ = ['NormalizedVersion', 'NormalizedMatcher',\n           'LegacyVersion', 'LegacyMatcher',\n           'SemanticVersion', 'SemanticMatcher',\n           'UnsupportedVersionError', 'get_scheme']\n\nlogger = logging.getLogger(__name__)\n\n\nclass UnsupportedVersionError(ValueError):\n    \"\"\"This is an unsupported version.\"\"\"\n    pass\n\n\nclass Version(object):\n    def __init__(self, s):\n        self._string = s = s.strip()\n        self._parts = parts = self.parse(s)\n        assert isinstance(parts, tuple)\n        assert len(parts) > 0\n\n    def parse(self, s):\n        raise NotImplementedError('please implement in a subclass')\n\n    def _check_compatible(self, other):\n        if type(self) != type(other):\n            raise TypeError('cannot compare %r and %r' % (self, other))\n\n    def __eq__(self, other):\n        self._check_compatible(other)\n        return self._parts == other._parts\n\n    def __ne__(self, other):\n        return not self.__eq__(other)\n\n    def __lt__(self, other):\n        self._check_compatible(other)\n        return self._parts < other._parts\n\n    def __gt__(self, other):\n        return not (self.__lt__(other) or self.__eq__(other))\n\n    def __le__(self, other):\n        return self.__lt__(other) or self.__eq__(other)\n\n    def __ge__(self, other):\n        return self.__gt__(other) or self.__eq__(other)\n\n    # See http://docs.python.org/reference/datamodel#object.__hash__\n    def __hash__(self):\n        return hash(self._parts)\n\n    def __repr__(self):\n        return \"%s('%s')\" % (self.__class__.__name__, self._string)\n\n    def __str__(self):\n        return self._string\n\n    @property\n    def is_prerelease(self):\n        raise NotImplementedError('Please implement in subclasses.')\n\n\nclass Matcher(object):\n    version_class = None\n\n    # value is either a callable or the name of a method\n    _operators = {\n        '<': lambda v, c, p: v < c,\n        '>': lambda v, c, p: v > c,\n        '<=': lambda v, c, p: v == c or v < c,\n        '>=': lambda v, c, p: v == c or v > c,\n        '==': lambda v, c, p: v == c,\n        '===': lambda v, c, p: v == c,\n        # by default, compatible => >=.\n        '~=': lambda v, c, p: v == c or v > c,\n        '!=': lambda v, c, p: v != c,\n    }\n\n    # this is a method only to support alternative implementations\n    # via overriding\n    def parse_requirement(self, s):\n        return parse_requirement(s)\n\n    def __init__(self, s):\n        if self.version_class is None:\n            raise ValueError('Please specify a version class')\n        self._string = s = s.strip()\n        r = self.parse_requirement(s)\n        if not r:\n            raise ValueError('Not valid: %r' % s)\n        self.name = r.name\n        self.key = self.name.lower()    # for case-insensitive comparisons\n        clist = []\n        if r.constraints:\n            # import pdb; pdb.set_trace()\n            for op, s in r.constraints:\n                if s.endswith('.*'):\n                    if op not in ('==', '!='):\n                        raise ValueError('\\'.*\\' not allowed for '\n                                         '%r constraints' % op)\n                    # Could be a partial version (e.g. for '2.*') which\n                    # won't parse as a version, so keep it as a string\n                    vn, prefix = s[:-2], True\n                    # Just to check that vn is a valid version\n                    self.version_class(vn)\n                else:\n                    # Should parse as a version, so we can create an\n                    # instance for the comparison\n                    vn, prefix = self.version_class(s), False\n                clist.append((op, vn, prefix))\n        self._parts = tuple(clist)\n\n    def match(self, version):\n        \"\"\"\n        Check if the provided version matches the constraints.\n\n        :param version: The version to match against this instance.\n        :type version: String or :class:`Version` instance.\n        \"\"\"\n        if isinstance(version, string_types):\n            version = self.version_class(version)\n        for operator, constraint, prefix in self._parts:\n            f = self._operators.get(operator)\n            if isinstance(f, string_types):\n                f = getattr(self, f)\n            if not f:\n                msg = ('%r not implemented '\n                       'for %s' % (operator, self.__class__.__name__))\n                raise NotImplementedError(msg)\n            if not f(version, constraint, prefix):\n                return False\n        return True\n\n    @property\n    def exact_version(self):\n        result = None\n        if len(self._parts) == 1 and self._parts[0][0] in ('==', '==='):\n            result = self._parts[0][1]\n        return result\n\n    def _check_compatible(self, other):\n        if type(self) != type(other) or self.name != other.name:\n            raise TypeError('cannot compare %s and %s' % (self, other))\n\n    def __eq__(self, other):\n        self._check_compatible(other)\n        return self.key == other.key and self._parts == other._parts\n\n    def __ne__(self, other):\n        return not self.__eq__(other)\n\n    # See http://docs.python.org/reference/datamodel#object.__hash__\n    def __hash__(self):\n        return hash(self.key) + hash(self._parts)\n\n    def __repr__(self):\n        return \"%s(%r)\" % (self.__class__.__name__, self._string)\n\n    def __str__(self):\n        return self._string\n\n\nPEP440_VERSION_RE = re.compile(r'^v?(\\d+!)?(\\d+(\\.\\d+)*)((a|alpha|b|beta|c|rc|pre|preview)(\\d+)?)?'\n                               r'(\\.(post|r|rev)(\\d+)?)?([._-]?(dev)(\\d+)?)?'\n                               r'(\\+([a-zA-Z\\d]+(\\.[a-zA-Z\\d]+)?))?$', re.I)\n\n\ndef _pep_440_key(s):\n    s = s.strip()\n    m = PEP440_VERSION_RE.match(s)\n    if not m:\n        raise UnsupportedVersionError('Not a valid version: %s' % s)\n    groups = m.groups()\n    nums = tuple(int(v) for v in groups[1].split('.'))\n    while len(nums) > 1 and nums[-1] == 0:\n        nums = nums[:-1]\n\n    if not groups[0]:\n        epoch = 0\n    else:\n        epoch = int(groups[0][:-1])\n    pre = groups[4:6]\n    post = groups[7:9]\n    dev = groups[10:12]\n    local = groups[13]\n    if pre == (None, None):\n        pre = ()\n    else:\n        if pre[1] is None:\n            pre = pre[0], 0\n        else:\n            pre = pre[0], int(pre[1])\n    if post == (None, None):\n        post = ()\n    else:\n        if post[1] is None:\n            post = post[0], 0\n        else:\n            post = post[0], int(post[1])\n    if dev == (None, None):\n        dev = ()\n    else:\n        if dev[1] is None:\n            dev = dev[0], 0\n        else:\n            dev = dev[0], int(dev[1])\n    if local is None:\n        local = ()\n    else:\n        parts = []\n        for part in local.split('.'):\n            # to ensure that numeric compares as > lexicographic, avoid\n            # comparing them directly, but encode a tuple which ensures\n            # correct sorting\n            if part.isdigit():\n                part = (1, int(part))\n            else:\n                part = (0, part)\n            parts.append(part)\n        local = tuple(parts)\n    if not pre:\n        # either before pre-release, or final release and after\n        if not post and dev:\n            # before pre-release\n            pre = ('a', -1)     # to sort before a0\n        else:\n            pre = ('z',)        # to sort after all pre-releases\n    # now look at the state of post and dev.\n    if not post:\n        post = ('_',)   # sort before 'a'\n    if not dev:\n        dev = ('final',)\n\n    return epoch, nums, pre, post, dev, local\n\n\n_normalized_key = _pep_440_key\n\n\nclass NormalizedVersion(Version):\n    \"\"\"A rational version.\n\n    Good:\n        1.2         # equivalent to \"1.2.0\"\n        1.2.0\n        1.2a1\n        1.2.3a2\n        1.2.3b1\n        1.2.3c1\n        1.2.3.4\n        TODO: fill this out\n\n    Bad:\n        1           # minimum two numbers\n        1.2a        # release level must have a release serial\n        1.2.3b\n    \"\"\"\n    def parse(self, s):\n        result = _normalized_key(s)\n        # _normalized_key loses trailing zeroes in the release\n        # clause, since that's needed to ensure that X.Y == X.Y.0 == X.Y.0.0\n        # However, PEP 440 prefix matching needs it: for example,\n        # (~= 1.4.5.0) matches differently to (~= 1.4.5.0.0).\n        m = PEP440_VERSION_RE.match(s)      # must succeed\n        groups = m.groups()\n        self._release_clause = tuple(int(v) for v in groups[1].split('.'))\n        return result\n\n    PREREL_TAGS = set(['a', 'b', 'c', 'rc', 'dev'])\n\n    @property\n    def is_prerelease(self):\n        return any(t[0] in self.PREREL_TAGS for t in self._parts if t)\n\n\ndef _match_prefix(x, y):\n    x = str(x)\n    y = str(y)\n    if x == y:\n        return True\n    if not x.startswith(y):\n        return False\n    n = len(y)\n    return x[n] == '.'\n\n\nclass NormalizedMatcher(Matcher):\n    version_class = NormalizedVersion\n\n    # value is either a callable or the name of a method\n    _operators = {\n        '~=': '_match_compatible',\n        '<': '_match_lt',\n        '>': '_match_gt',\n        '<=': '_match_le',\n        '>=': '_match_ge',\n        '==': '_match_eq',\n        '===': '_match_arbitrary',\n        '!=': '_match_ne',\n    }\n\n    def _adjust_local(self, version, constraint, prefix):\n        if prefix:\n            strip_local = '+' not in constraint and version._parts[-1]\n        else:\n            # both constraint and version are\n            # NormalizedVersion instances.\n            # If constraint does not have a local component,\n            # ensure the version doesn't, either.\n            strip_local = not constraint._parts[-1] and version._parts[-1]\n        if strip_local:\n            s = version._string.split('+', 1)[0]\n            version = self.version_class(s)\n        return version, constraint\n\n    def _match_lt(self, version, constraint, prefix):\n        version, constraint = self._adjust_local(version, constraint, prefix)\n        if version >= constraint:\n            return False\n        release_clause = constraint._release_clause\n        pfx = '.'.join([str(i) for i in release_clause])\n        return not _match_prefix(version, pfx)\n\n    def _match_gt(self, version, constraint, prefix):\n        version, constraint = self._adjust_local(version, constraint, prefix)\n        if version <= constraint:\n            return False\n        release_clause = constraint._release_clause\n        pfx = '.'.join([str(i) for i in release_clause])\n        return not _match_prefix(version, pfx)\n\n    def _match_le(self, version, constraint, prefix):\n        version, constraint = self._adjust_local(version, constraint, prefix)\n        return version <= constraint\n\n    def _match_ge(self, version, constraint, prefix):\n        version, constraint = self._adjust_local(version, constraint, prefix)\n        return version >= constraint\n\n    def _match_eq(self, version, constraint, prefix):\n        version, constraint = self._adjust_local(version, constraint, prefix)\n        if not prefix:\n            result = (version == constraint)\n        else:\n            result = _match_prefix(version, constraint)\n        return result\n\n    def _match_arbitrary(self, version, constraint, prefix):\n        return str(version) == str(constraint)\n\n    def _match_ne(self, version, constraint, prefix):\n        version, constraint = self._adjust_local(version, constraint, prefix)\n        if not prefix:\n            result = (version != constraint)\n        else:\n            result = not _match_prefix(version, constraint)\n        return result\n\n    def _match_compatible(self, version, constraint, prefix):\n        version, constraint = self._adjust_local(version, constraint, prefix)\n        if version == constraint:\n            return True\n        if version < constraint:\n            return False\n#        if not prefix:\n#            return True\n        release_clause = constraint._release_clause\n        if len(release_clause) > 1:\n            release_clause = release_clause[:-1]\n        pfx = '.'.join([str(i) for i in release_clause])\n        return _match_prefix(version, pfx)\n\n\n_REPLACEMENTS = (\n    (re.compile('[.+-]$'), ''),                     # remove trailing puncts\n    (re.compile(r'^[.](\\d)'), r'0.\\1'),             # .N -> 0.N at start\n    (re.compile('^[.-]'), ''),                      # remove leading puncts\n    (re.compile(r'^\\((.*)\\)$'), r'\\1'),             # remove parentheses\n    (re.compile(r'^v(ersion)?\\s*(\\d+)'), r'\\2'),    # remove leading v(ersion)\n    (re.compile(r'^r(ev)?\\s*(\\d+)'), r'\\2'),        # remove leading v(ersion)\n    (re.compile('[.]{2,}'), '.'),                   # multiple runs of '.'\n    (re.compile(r'\\b(alfa|apha)\\b'), 'alpha'),      # misspelt alpha\n    (re.compile(r'\\b(pre-alpha|prealpha)\\b'),\n        'pre.alpha'),                               # standardise\n    (re.compile(r'\\(beta\\)$'), 'beta'),             # remove parentheses\n)\n\n_SUFFIX_REPLACEMENTS = (\n    (re.compile('^[:~._+-]+'), ''),                   # remove leading puncts\n    (re.compile('[,*\")([\\\\]]'), ''),                  # remove unwanted chars\n    (re.compile('[~:+_ -]'), '.'),                    # replace illegal chars\n    (re.compile('[.]{2,}'), '.'),                   # multiple runs of '.'\n    (re.compile(r'\\.$'), ''),                       # trailing '.'\n)\n\n_NUMERIC_PREFIX = re.compile(r'(\\d+(\\.\\d+)*)')\n\n\ndef _suggest_semantic_version(s):\n    \"\"\"\n    Try to suggest a semantic form for a version for which\n    _suggest_normalized_version couldn't come up with anything.\n    \"\"\"\n    result = s.strip().lower()\n    for pat, repl in _REPLACEMENTS:\n        result = pat.sub(repl, result)\n    if not result:\n        result = '0.0.0'\n\n    # Now look for numeric prefix, and separate it out from\n    # the rest.\n    # import pdb; pdb.set_trace()\n    m = _NUMERIC_PREFIX.match(result)\n    if not m:\n        prefix = '0.0.0'\n        suffix = result\n    else:\n        prefix = m.groups()[0].split('.')\n        prefix = [int(i) for i in prefix]\n        while len(prefix) < 3:\n            prefix.append(0)\n        if len(prefix) == 3:\n            suffix = result[m.end():]\n        else:\n            suffix = '.'.join([str(i) for i in prefix[3:]]) + result[m.end():]\n            prefix = prefix[:3]\n        prefix = '.'.join([str(i) for i in prefix])\n        suffix = suffix.strip()\n    if suffix:\n        # import pdb; pdb.set_trace()\n        # massage the suffix.\n        for pat, repl in _SUFFIX_REPLACEMENTS:\n            suffix = pat.sub(repl, suffix)\n\n    if not suffix:\n        result = prefix\n    else:\n        sep = '-' if 'dev' in suffix else '+'\n        result = prefix + sep + suffix\n    if not is_semver(result):\n        result = None\n    return result\n\n\ndef _suggest_normalized_version(s):\n    \"\"\"Suggest a normalized version close to the given version string.\n\n    If you have a version string that isn't rational (i.e. NormalizedVersion\n    doesn't like it) then you might be able to get an equivalent (or close)\n    rational version from this function.\n\n    This does a number of simple normalizations to the given string, based\n    on observation of versions currently in use on PyPI. Given a dump of\n    those version during PyCon 2009, 4287 of them:\n    - 2312 (53.93%) match NormalizedVersion without change\n      with the automatic suggestion\n    - 3474 (81.04%) match when using this suggestion method\n\n    @param s {str} An irrational version string.\n    @returns A rational version string, or None, if couldn't determine one.\n    \"\"\"\n    try:\n        _normalized_key(s)\n        return s   # already rational\n    except UnsupportedVersionError:\n        pass\n\n    rs = s.lower()\n\n    # part of this could use maketrans\n    for orig, repl in (('-alpha', 'a'), ('-beta', 'b'), ('alpha', 'a'),\n                       ('beta', 'b'), ('rc', 'c'), ('-final', ''),\n                       ('-pre', 'c'),\n                       ('-release', ''), ('.release', ''), ('-stable', ''),\n                       ('+', '.'), ('_', '.'), (' ', ''), ('.final', ''),\n                       ('final', '')):\n        rs = rs.replace(orig, repl)\n\n    # if something ends with dev or pre, we add a 0\n    rs = re.sub(r\"pre$\", r\"pre0\", rs)\n    rs = re.sub(r\"dev$\", r\"dev0\", rs)\n\n    # if we have something like \"b-2\" or \"a.2\" at the end of the\n    # version, that is probably beta, alpha, etc\n    # let's remove the dash or dot\n    rs = re.sub(r\"([abc]|rc)[\\-\\.](\\d+)$\", r\"\\1\\2\", rs)\n\n    # 1.0-dev-r371 -> 1.0.dev371\n    # 0.1-dev-r79 -> 0.1.dev79\n    rs = re.sub(r\"[\\-\\.](dev)[\\-\\.]?r?(\\d+)$\", r\".\\1\\2\", rs)\n\n    # Clean: 2.0.a.3, 2.0.b1, 0.9.0~c1\n    rs = re.sub(r\"[.~]?([abc])\\.?\", r\"\\1\", rs)\n\n    # Clean: v0.3, v1.0\n    if rs.startswith('v'):\n        rs = rs[1:]\n\n    # Clean leading '0's on numbers.\n    # TODO: unintended side-effect on, e.g., \"2003.05.09\"\n    # PyPI stats: 77 (~2%) better\n    rs = re.sub(r\"\\b0+(\\d+)(?!\\d)\", r\"\\1\", rs)\n\n    # Clean a/b/c with no version. E.g. \"1.0a\" -> \"1.0a0\". Setuptools infers\n    # zero.\n    # PyPI stats: 245 (7.56%) better\n    rs = re.sub(r\"(\\d+[abc])$\", r\"\\g<1>0\", rs)\n\n    # the 'dev-rNNN' tag is a dev tag\n    rs = re.sub(r\"\\.?(dev-r|dev\\.r)\\.?(\\d+)$\", r\".dev\\2\", rs)\n\n    # clean the - when used as a pre delimiter\n    rs = re.sub(r\"-(a|b|c)(\\d+)$\", r\"\\1\\2\", rs)\n\n    # a terminal \"dev\" or \"devel\" can be changed into \".dev0\"\n    rs = re.sub(r\"[\\.\\-](dev|devel)$\", r\".dev0\", rs)\n\n    # a terminal \"dev\" can be changed into \".dev0\"\n    rs = re.sub(r\"(?![\\.\\-])dev$\", r\".dev0\", rs)\n\n    # a terminal \"final\" or \"stable\" can be removed\n    rs = re.sub(r\"(final|stable)$\", \"\", rs)\n\n    # The 'r' and the '-' tags are post release tags\n    #   0.4a1.r10       ->  0.4a1.post10\n    #   0.9.33-17222    ->  0.9.33.post17222\n    #   0.9.33-r17222   ->  0.9.33.post17222\n    rs = re.sub(r\"\\.?(r|-|-r)\\.?(\\d+)$\", r\".post\\2\", rs)\n\n    # Clean 'r' instead of 'dev' usage:\n    #   0.9.33+r17222   ->  0.9.33.dev17222\n    #   1.0dev123       ->  1.0.dev123\n    #   1.0.git123      ->  1.0.dev123\n    #   1.0.bzr123      ->  1.0.dev123\n    #   0.1a0dev.123    ->  0.1a0.dev123\n    # PyPI stats:  ~150 (~4%) better\n    rs = re.sub(r\"\\.?(dev|git|bzr)\\.?(\\d+)$\", r\".dev\\2\", rs)\n\n    # Clean '.pre' (normalized from '-pre' above) instead of 'c' usage:\n    #   0.2.pre1        ->  0.2c1\n    #   0.2-c1         ->  0.2c1\n    #   1.0preview123   ->  1.0c123\n    # PyPI stats: ~21 (0.62%) better\n    rs = re.sub(r\"\\.?(pre|preview|-c)(\\d+)$\", r\"c\\g<2>\", rs)\n\n    # Tcl/Tk uses \"px\" for their post release markers\n    rs = re.sub(r\"p(\\d+)$\", r\".post\\1\", rs)\n\n    try:\n        _normalized_key(rs)\n    except UnsupportedVersionError:\n        rs = None\n    return rs\n\n#\n#   Legacy version processing (distribute-compatible)\n#\n\n\n_VERSION_PART = re.compile(r'([a-z]+|\\d+|[\\.-])', re.I)\n_VERSION_REPLACE = {\n    'pre': 'c',\n    'preview': 'c',\n    '-': 'final-',\n    'rc': 'c',\n    'dev': '@',\n    '': None,\n    '.': None,\n}\n\n\ndef _legacy_key(s):\n    def get_parts(s):\n        result = []\n        for p in _VERSION_PART.split(s.lower()):\n            p = _VERSION_REPLACE.get(p, p)\n            if p:\n                if '0' <= p[:1] <= '9':\n                    p = p.zfill(8)\n                else:\n                    p = '*' + p\n                result.append(p)\n        result.append('*final')\n        return result\n\n    result = []\n    for p in get_parts(s):\n        if p.startswith('*'):\n            if p < '*final':\n                while result and result[-1] == '*final-':\n                    result.pop()\n            while result and result[-1] == '00000000':\n                result.pop()\n        result.append(p)\n    return tuple(result)\n\n\nclass LegacyVersion(Version):\n    def parse(self, s):\n        return _legacy_key(s)\n\n    @property\n    def is_prerelease(self):\n        result = False\n        for x in self._parts:\n            if (isinstance(x, string_types) and x.startswith('*') and x < '*final'):\n                result = True\n                break\n        return result\n\n\nclass LegacyMatcher(Matcher):\n    version_class = LegacyVersion\n\n    _operators = dict(Matcher._operators)\n    _operators['~='] = '_match_compatible'\n\n    numeric_re = re.compile(r'^(\\d+(\\.\\d+)*)')\n\n    def _match_compatible(self, version, constraint, prefix):\n        if version < constraint:\n            return False\n        m = self.numeric_re.match(str(constraint))\n        if not m:\n            logger.warning('Cannot compute compatible match for version %s '\n                           ' and constraint %s', version, constraint)\n            return True\n        s = m.groups()[0]\n        if '.' in s:\n            s = s.rsplit('.', 1)[0]\n        return _match_prefix(version, s)\n\n#\n#   Semantic versioning\n#\n\n\n_SEMVER_RE = re.compile(r'^(\\d+)\\.(\\d+)\\.(\\d+)'\n                        r'(-[a-z0-9]+(\\.[a-z0-9-]+)*)?'\n                        r'(\\+[a-z0-9]+(\\.[a-z0-9-]+)*)?$', re.I)\n\n\ndef is_semver(s):\n    return _SEMVER_RE.match(s)\n\n\ndef _semantic_key(s):\n    def make_tuple(s, absent):\n        if s is None:\n            result = (absent,)\n        else:\n            parts = s[1:].split('.')\n            # We can't compare ints and strings on Python 3, so fudge it\n            # by zero-filling numeric values so simulate a numeric comparison\n            result = tuple([p.zfill(8) if p.isdigit() else p for p in parts])\n        return result\n\n    m = is_semver(s)\n    if not m:\n        raise UnsupportedVersionError(s)\n    groups = m.groups()\n    major, minor, patch = [int(i) for i in groups[:3]]\n    # choose the '|' and '*' so that versions sort correctly\n    pre, build = make_tuple(groups[3], '|'), make_tuple(groups[5], '*')\n    return (major, minor, patch), pre, build\n\n\nclass SemanticVersion(Version):\n    def parse(self, s):\n        return _semantic_key(s)\n\n    @property\n    def is_prerelease(self):\n        return self._parts[1][0] != '|'\n\n\nclass SemanticMatcher(Matcher):\n    version_class = SemanticVersion\n\n\nclass VersionScheme(object):\n    def __init__(self, key, matcher, suggester=None):\n        self.key = key\n        self.matcher = matcher\n        self.suggester = suggester\n\n    def is_valid_version(self, s):\n        try:\n            self.matcher.version_class(s)\n            result = True\n        except UnsupportedVersionError:\n            result = False\n        return result\n\n    def is_valid_matcher(self, s):\n        try:\n            self.matcher(s)\n            result = True\n        except UnsupportedVersionError:\n            result = False\n        return result\n\n    def is_valid_constraint_list(self, s):\n        \"\"\"\n        Used for processing some metadata fields\n        \"\"\"\n        # See issue #140. Be tolerant of a single trailing comma.\n        if s.endswith(','):\n            s = s[:-1]\n        return self.is_valid_matcher('dummy_name (%s)' % s)\n\n    def suggest(self, s):\n        if self.suggester is None:\n            result = None\n        else:\n            result = self.suggester(s)\n        return result\n\n\n_SCHEMES = {\n    'normalized': VersionScheme(_normalized_key, NormalizedMatcher,\n                                _suggest_normalized_version),\n    'legacy': VersionScheme(_legacy_key, LegacyMatcher, lambda self, s: s),\n    'semantic': VersionScheme(_semantic_key, SemanticMatcher,\n                              _suggest_semantic_version),\n}\n\n_SCHEMES['default'] = _SCHEMES['normalized']\n\n\ndef get_scheme(name):\n    if name not in _SCHEMES:\n        raise ValueError('unknown scheme name: %r' % name)\n    return _SCHEMES[name]\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_vendor/distlib/wheel.py","size":43979,"sha1":"1fef539f07accf5e289df3164b7de8fd7f4503bb","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"# -*- coding: utf-8 -*-\n#\n# Copyright (C) 2013-2023 Vinay Sajip.\n# Licensed to the Python Software Foundation under a contributor agreement.\n# See LICENSE.txt and CONTRIBUTORS.txt.\n#\nfrom __future__ import unicode_literals\n\nimport base64\nimport codecs\nimport datetime\nfrom email import message_from_file\nimport hashlib\nimport json\nimport logging\nimport os\nimport posixpath\nimport re\nimport shutil\nimport sys\nimport tempfile\nimport zipfile\n\nfrom . import __version__, DistlibException\nfrom .compat import sysconfig, ZipFile, fsdecode, text_type, filter\nfrom .database import InstalledDistribution\nfrom .metadata import Metadata, WHEEL_METADATA_FILENAME, LEGACY_METADATA_FILENAME\nfrom .util import (FileOperator, convert_path, CSVReader, CSVWriter, Cache, cached_property, get_cache_base,\n                   read_exports, tempdir, get_platform)\nfrom .version import NormalizedVersion, UnsupportedVersionError\n\nlogger = logging.getLogger(__name__)\n\ncache = None  # created when needed\n\nif hasattr(sys, 'pypy_version_info'):  # pragma: no cover\n    IMP_PREFIX = 'pp'\nelif sys.platform.startswith('java'):  # pragma: no cover\n    IMP_PREFIX = 'jy'\nelif sys.platform == 'cli':  # pragma: no cover\n    IMP_PREFIX = 'ip'\nelse:\n    IMP_PREFIX = 'cp'\n\nVER_SUFFIX = sysconfig.get_config_var('py_version_nodot')\nif not VER_SUFFIX:  # pragma: no cover\n    VER_SUFFIX = '%s%s' % sys.version_info[:2]\nPYVER = 'py' + VER_SUFFIX\nIMPVER = IMP_PREFIX + VER_SUFFIX\n\nARCH = get_platform().replace('-', '_').replace('.', '_')\n\nABI = sysconfig.get_config_var('SOABI')\nif ABI and ABI.startswith('cpython-'):\n    ABI = ABI.replace('cpython-', 'cp').split('-')[0]\nelse:\n\n    def _derive_abi():\n        parts = ['cp', VER_SUFFIX]\n        if sysconfig.get_config_var('Py_DEBUG'):\n            parts.append('d')\n        if IMP_PREFIX == 'cp':\n            vi = sys.version_info[:2]\n            if vi < (3, 8):\n                wpm = sysconfig.get_config_var('WITH_PYMALLOC')\n                if wpm is None:\n                    wpm = True\n                if wpm:\n                    parts.append('m')\n                if vi < (3, 3):\n                    us = sysconfig.get_config_var('Py_UNICODE_SIZE')\n                    if us == 4 or (us is None and sys.maxunicode == 0x10FFFF):\n                        parts.append('u')\n        return ''.join(parts)\n\n    ABI = _derive_abi()\n    del _derive_abi\n\nFILENAME_RE = re.compile(\n    r'''\n(?P<nm>[^-]+)\n-(?P<vn>\\d+[^-]*)\n(-(?P<bn>\\d+[^-]*))?\n-(?P<py>\\w+\\d+(\\.\\w+\\d+)*)\n-(?P<bi>\\w+)\n-(?P<ar>\\w+(\\.\\w+)*)\n\\.whl$\n''', re.IGNORECASE | re.VERBOSE)\n\nNAME_VERSION_RE = re.compile(r'''\n(?P<nm>[^-]+)\n-(?P<vn>\\d+[^-]*)\n(-(?P<bn>\\d+[^-]*))?$\n''', re.IGNORECASE | re.VERBOSE)\n\nSHEBANG_RE = re.compile(br'\\s*#![^\\r\\n]*')\nSHEBANG_DETAIL_RE = re.compile(br'^(\\s*#!(\"[^\"]+\"|\\S+))\\s+(.*)$')\nSHEBANG_PYTHON = b'#!python'\nSHEBANG_PYTHONW = b'#!pythonw'\n\nif os.sep == '/':\n    to_posix = lambda o: o\nelse:\n    to_posix = lambda o: o.replace(os.sep, '/')\n\nif sys.version_info[0] < 3:\n    import imp\nelse:\n    imp = None\n    import importlib.machinery\n    import importlib.util\n\n\ndef _get_suffixes():\n    if imp:\n        return [s[0] for s in imp.get_suffixes()]\n    else:\n        return importlib.machinery.EXTENSION_SUFFIXES\n\n\ndef _load_dynamic(name, path):\n    # https://docs.python.org/3/library/importlib.html#importing-a-source-file-directly\n    if imp:\n        return imp.load_dynamic(name, path)\n    else:\n        spec = importlib.util.spec_from_file_location(name, path)\n        module = importlib.util.module_from_spec(spec)\n        sys.modules[name] = module\n        spec.loader.exec_module(module)\n        return module\n\n\nclass Mounter(object):\n\n    def __init__(self):\n        self.impure_wheels = {}\n        self.libs = {}\n\n    def add(self, pathname, extensions):\n        self.impure_wheels[pathname] = extensions\n        self.libs.update(extensions)\n\n    def remove(self, pathname):\n        extensions = self.impure_wheels.pop(pathname)\n        for k, v in extensions:\n            if k in self.libs:\n                del self.libs[k]\n\n    def find_module(self, fullname, path=None):\n        if fullname in self.libs:\n            result = self\n        else:\n            result = None\n        return result\n\n    def load_module(self, fullname):\n        if fullname in sys.modules:\n            result = sys.modules[fullname]\n        else:\n            if fullname not in self.libs:\n                raise ImportError('unable to find extension for %s' % fullname)\n            result = _load_dynamic(fullname, self.libs[fullname])\n            result.__loader__ = self\n            parts = fullname.rsplit('.', 1)\n            if len(parts) > 1:\n                result.__package__ = parts[0]\n        return result\n\n\n_hook = Mounter()\n\n\nclass Wheel(object):\n    \"\"\"\n    Class to build and install from Wheel files (PEP 427).\n    \"\"\"\n\n    wheel_version = (1, 1)\n    hash_kind = 'sha256'\n\n    def __init__(self, filename=None, sign=False, verify=False):\n        \"\"\"\n        Initialise an instance using a (valid) filename.\n        \"\"\"\n        self.sign = sign\n        self.should_verify = verify\n        self.buildver = ''\n        self.pyver = [PYVER]\n        self.abi = ['none']\n        self.arch = ['any']\n        self.dirname = os.getcwd()\n        if filename is None:\n            self.name = 'dummy'\n            self.version = '0.1'\n            self._filename = self.filename\n        else:\n            m = NAME_VERSION_RE.match(filename)\n            if m:\n                info = m.groupdict('')\n                self.name = info['nm']\n                # Reinstate the local version separator\n                self.version = info['vn'].replace('_', '-')\n                self.buildver = info['bn']\n                self._filename = self.filename\n            else:\n                dirname, filename = os.path.split(filename)\n                m = FILENAME_RE.match(filename)\n                if not m:\n                    raise DistlibException('Invalid name or '\n                                           'filename: %r' % filename)\n                if dirname:\n                    self.dirname = os.path.abspath(dirname)\n                self._filename = filename\n                info = m.groupdict('')\n                self.name = info['nm']\n                self.version = info['vn']\n                self.buildver = info['bn']\n                self.pyver = info['py'].split('.')\n                self.abi = info['bi'].split('.')\n                self.arch = info['ar'].split('.')\n\n    @property\n    def filename(self):\n        \"\"\"\n        Build and return a filename from the various components.\n        \"\"\"\n        if self.buildver:\n            buildver = '-' + self.buildver\n        else:\n            buildver = ''\n        pyver = '.'.join(self.pyver)\n        abi = '.'.join(self.abi)\n        arch = '.'.join(self.arch)\n        # replace - with _ as a local version separator\n        version = self.version.replace('-', '_')\n        return '%s-%s%s-%s-%s-%s.whl' % (self.name, version, buildver, pyver, abi, arch)\n\n    @property\n    def exists(self):\n        path = os.path.join(self.dirname, self.filename)\n        return os.path.isfile(path)\n\n    @property\n    def tags(self):\n        for pyver in self.pyver:\n            for abi in self.abi:\n                for arch in self.arch:\n                    yield pyver, abi, arch\n\n    @cached_property\n    def metadata(self):\n        pathname = os.path.join(self.dirname, self.filename)\n        name_ver = '%s-%s' % (self.name, self.version)\n        info_dir = '%s.dist-info' % name_ver\n        wrapper = codecs.getreader('utf-8')\n        with ZipFile(pathname, 'r') as zf:\n            self.get_wheel_metadata(zf)\n            # wv = wheel_metadata['Wheel-Version'].split('.', 1)\n            # file_version = tuple([int(i) for i in wv])\n            # if file_version < (1, 1):\n            # fns = [WHEEL_METADATA_FILENAME, METADATA_FILENAME,\n            # LEGACY_METADATA_FILENAME]\n            # else:\n            # fns = [WHEEL_METADATA_FILENAME, METADATA_FILENAME]\n            fns = [WHEEL_METADATA_FILENAME, LEGACY_METADATA_FILENAME]\n            result = None\n            for fn in fns:\n                try:\n                    metadata_filename = posixpath.join(info_dir, fn)\n                    with zf.open(metadata_filename) as bf:\n                        wf = wrapper(bf)\n                        result = Metadata(fileobj=wf)\n                        if result:\n                            break\n                except KeyError:\n                    pass\n            if not result:\n                raise ValueError('Invalid wheel, because metadata is '\n                                 'missing: looked in %s' % ', '.join(fns))\n        return result\n\n    def get_wheel_metadata(self, zf):\n        name_ver = '%s-%s' % (self.name, self.version)\n        info_dir = '%s.dist-info' % name_ver\n        metadata_filename = posixpath.join(info_dir, 'WHEEL')\n        with zf.open(metadata_filename) as bf:\n            wf = codecs.getreader('utf-8')(bf)\n            message = message_from_file(wf)\n        return dict(message)\n\n    @cached_property\n    def info(self):\n        pathname = os.path.join(self.dirname, self.filename)\n        with ZipFile(pathname, 'r') as zf:\n            result = self.get_wheel_metadata(zf)\n        return result\n\n    def process_shebang(self, data):\n        m = SHEBANG_RE.match(data)\n        if m:\n            end = m.end()\n            shebang, data_after_shebang = data[:end], data[end:]\n            # Preserve any arguments after the interpreter\n            if b'pythonw' in shebang.lower():\n                shebang_python = SHEBANG_PYTHONW\n            else:\n                shebang_python = SHEBANG_PYTHON\n            m = SHEBANG_DETAIL_RE.match(shebang)\n            if m:\n                args = b' ' + m.groups()[-1]\n            else:\n                args = b''\n            shebang = shebang_python + args\n            data = shebang + data_after_shebang\n        else:\n            cr = data.find(b'\\r')\n            lf = data.find(b'\\n')\n            if cr < 0 or cr > lf:\n                term = b'\\n'\n            else:\n                if data[cr:cr + 2] == b'\\r\\n':\n                    term = b'\\r\\n'\n                else:\n                    term = b'\\r'\n            data = SHEBANG_PYTHON + term + data\n        return data\n\n    def get_hash(self, data, hash_kind=None):\n        if hash_kind is None:\n            hash_kind = self.hash_kind\n        try:\n            hasher = getattr(hashlib, hash_kind)\n        except AttributeError:\n            raise DistlibException('Unsupported hash algorithm: %r' % hash_kind)\n        result = hasher(data).digest()\n        result = base64.urlsafe_b64encode(result).rstrip(b'=').decode('ascii')\n        return hash_kind, result\n\n    def write_record(self, records, record_path, archive_record_path):\n        records = list(records)  # make a copy, as mutated\n        records.append((archive_record_path, '', ''))\n        with CSVWriter(record_path) as writer:\n            for row in records:\n                writer.writerow(row)\n\n    def write_records(self, info, libdir, archive_paths):\n        records = []\n        distinfo, info_dir = info\n        # hasher = getattr(hashlib, self.hash_kind)\n        for ap, p in archive_paths:\n            with open(p, 'rb') as f:\n                data = f.read()\n            digest = '%s=%s' % self.get_hash(data)\n            size = os.path.getsize(p)\n            records.append((ap, digest, size))\n\n        p = os.path.join(distinfo, 'RECORD')\n        ap = to_posix(os.path.join(info_dir, 'RECORD'))\n        self.write_record(records, p, ap)\n        archive_paths.append((ap, p))\n\n    def build_zip(self, pathname, archive_paths):\n        with ZipFile(pathname, 'w', zipfile.ZIP_DEFLATED) as zf:\n            for ap, p in archive_paths:\n                logger.debug('Wrote %s to %s in wheel', p, ap)\n                zf.write(p, ap)\n\n    def build(self, paths, tags=None, wheel_version=None):\n        \"\"\"\n        Build a wheel from files in specified paths, and use any specified tags\n        when determining the name of the wheel.\n        \"\"\"\n        if tags is None:\n            tags = {}\n\n        libkey = list(filter(lambda o: o in paths, ('purelib', 'platlib')))[0]\n        if libkey == 'platlib':\n            is_pure = 'false'\n            default_pyver = [IMPVER]\n            default_abi = [ABI]\n            default_arch = [ARCH]\n        else:\n            is_pure = 'true'\n            default_pyver = [PYVER]\n            default_abi = ['none']\n            default_arch = ['any']\n\n        self.pyver = tags.get('pyver', default_pyver)\n        self.abi = tags.get('abi', default_abi)\n        self.arch = tags.get('arch', default_arch)\n\n        libdir = paths[libkey]\n\n        name_ver = '%s-%s' % (self.name, self.version)\n        data_dir = '%s.data' % name_ver\n        info_dir = '%s.dist-info' % name_ver\n\n        archive_paths = []\n\n        # First, stuff which is not in site-packages\n        for key in ('data', 'headers', 'scripts'):\n            if key not in paths:\n                continue\n            path = paths[key]\n            if os.path.isdir(path):\n                for root, dirs, files in os.walk(path):\n                    for fn in files:\n                        p = fsdecode(os.path.join(root, fn))\n                        rp = os.path.relpath(p, path)\n                        ap = to_posix(os.path.join(data_dir, key, rp))\n                        archive_paths.append((ap, p))\n                        if key == 'scripts' and not p.endswith('.exe'):\n                            with open(p, 'rb') as f:\n                                data = f.read()\n                            data = self.process_shebang(data)\n                            with open(p, 'wb') as f:\n                                f.write(data)\n\n        # Now, stuff which is in site-packages, other than the\n        # distinfo stuff.\n        path = libdir\n        distinfo = None\n        for root, dirs, files in os.walk(path):\n            if root == path:\n                # At the top level only, save distinfo for later\n                # and skip it for now\n                for i, dn in enumerate(dirs):\n                    dn = fsdecode(dn)\n                    if dn.endswith('.dist-info'):\n                        distinfo = os.path.join(root, dn)\n                        del dirs[i]\n                        break\n                assert distinfo, '.dist-info directory expected, not found'\n\n            for fn in files:\n                # comment out next suite to leave .pyc files in\n                if fsdecode(fn).endswith(('.pyc', '.pyo')):\n                    continue\n                p = os.path.join(root, fn)\n                rp = to_posix(os.path.relpath(p, path))\n                archive_paths.append((rp, p))\n\n        # Now distinfo. Assumed to be flat, i.e. os.listdir is enough.\n        files = os.listdir(distinfo)\n        for fn in files:\n            if fn not in ('RECORD', 'INSTALLER', 'SHARED', 'WHEEL'):\n                p = fsdecode(os.path.join(distinfo, fn))\n                ap = to_posix(os.path.join(info_dir, fn))\n                archive_paths.append((ap, p))\n\n        wheel_metadata = [\n            'Wheel-Version: %d.%d' % (wheel_version or self.wheel_version),\n            'Generator: distlib %s' % __version__,\n            'Root-Is-Purelib: %s' % is_pure,\n        ]\n        for pyver, abi, arch in self.tags:\n            wheel_metadata.append('Tag: %s-%s-%s' % (pyver, abi, arch))\n        p = os.path.join(distinfo, 'WHEEL')\n        with open(p, 'w') as f:\n            f.write('\\n'.join(wheel_metadata))\n        ap = to_posix(os.path.join(info_dir, 'WHEEL'))\n        archive_paths.append((ap, p))\n\n        # sort the entries by archive path. Not needed by any spec, but it\n        # keeps the archive listing and RECORD tidier than they would otherwise\n        # be. Use the number of path segments to keep directory entries together,\n        # and keep the dist-info stuff at the end.\n        def sorter(t):\n            ap = t[0]\n            n = ap.count('/')\n            if '.dist-info' in ap:\n                n += 10000\n            return (n, ap)\n\n        archive_paths = sorted(archive_paths, key=sorter)\n\n        # Now, at last, RECORD.\n        # Paths in here are archive paths - nothing else makes sense.\n        self.write_records((distinfo, info_dir), libdir, archive_paths)\n        # Now, ready to build the zip file\n        pathname = os.path.join(self.dirname, self.filename)\n        self.build_zip(pathname, archive_paths)\n        return pathname\n\n    def skip_entry(self, arcname):\n        \"\"\"\n        Determine whether an archive entry should be skipped when verifying\n        or installing.\n        \"\"\"\n        # The signature file won't be in RECORD,\n        # and we  don't currently don't do anything with it\n        # We also skip directories, as they won't be in RECORD\n        # either. See:\n        #\n        # https://github.com/pypa/wheel/issues/294\n        # https://github.com/pypa/wheel/issues/287\n        # https://github.com/pypa/wheel/pull/289\n        #\n        return arcname.endswith(('/', '/RECORD.jws'))\n\n    def install(self, paths, maker, **kwargs):\n        \"\"\"\n        Install a wheel to the specified paths. If kwarg ``warner`` is\n        specified, it should be a callable, which will be called with two\n        tuples indicating the wheel version of this software and the wheel\n        version in the file, if there is a discrepancy in the versions.\n        This can be used to issue any warnings to raise any exceptions.\n        If kwarg ``lib_only`` is True, only the purelib/platlib files are\n        installed, and the headers, scripts, data and dist-info metadata are\n        not written. If kwarg ``bytecode_hashed_invalidation`` is True, written\n        bytecode will try to use file-hash based invalidation (PEP-552) on\n        supported interpreter versions (CPython 3.7+).\n\n        The return value is a :class:`InstalledDistribution` instance unless\n        ``options.lib_only`` is True, in which case the return value is ``None``.\n        \"\"\"\n\n        dry_run = maker.dry_run\n        warner = kwargs.get('warner')\n        lib_only = kwargs.get('lib_only', False)\n        bc_hashed_invalidation = kwargs.get('bytecode_hashed_invalidation', False)\n\n        pathname = os.path.join(self.dirname, self.filename)\n        name_ver = '%s-%s' % (self.name, self.version)\n        data_dir = '%s.data' % name_ver\n        info_dir = '%s.dist-info' % name_ver\n\n        metadata_name = posixpath.join(info_dir, LEGACY_METADATA_FILENAME)\n        wheel_metadata_name = posixpath.join(info_dir, 'WHEEL')\n        record_name = posixpath.join(info_dir, 'RECORD')\n\n        wrapper = codecs.getreader('utf-8')\n\n        with ZipFile(pathname, 'r') as zf:\n            with zf.open(wheel_metadata_name) as bwf:\n                wf = wrapper(bwf)\n                message = message_from_file(wf)\n            wv = message['Wheel-Version'].split('.', 1)\n            file_version = tuple([int(i) for i in wv])\n            if (file_version != self.wheel_version) and warner:\n                warner(self.wheel_version, file_version)\n\n            if message['Root-Is-Purelib'] == 'true':\n                libdir = paths['purelib']\n            else:\n                libdir = paths['platlib']\n\n            records = {}\n            with zf.open(record_name) as bf:\n                with CSVReader(stream=bf) as reader:\n                    for row in reader:\n                        p = row[0]\n                        records[p] = row\n\n            data_pfx = posixpath.join(data_dir, '')\n            info_pfx = posixpath.join(info_dir, '')\n            script_pfx = posixpath.join(data_dir, 'scripts', '')\n\n            # make a new instance rather than a copy of maker's,\n            # as we mutate it\n            fileop = FileOperator(dry_run=dry_run)\n            fileop.record = True  # so we can rollback if needed\n\n            bc = not sys.dont_write_bytecode  # Double negatives. Lovely!\n\n            outfiles = []  # for RECORD writing\n\n            # for script copying/shebang processing\n            workdir = tempfile.mkdtemp()\n            # set target dir later\n            # we default add_launchers to False, as the\n            # Python Launcher should be used instead\n            maker.source_dir = workdir\n            maker.target_dir = None\n            try:\n                for zinfo in zf.infolist():\n                    arcname = zinfo.filename\n                    if isinstance(arcname, text_type):\n                        u_arcname = arcname\n                    else:\n                        u_arcname = arcname.decode('utf-8')\n                    if self.skip_entry(u_arcname):\n                        continue\n                    row = records[u_arcname]\n                    if row[2] and str(zinfo.file_size) != row[2]:\n                        raise DistlibException('size mismatch for '\n                                               '%s' % u_arcname)\n                    if row[1]:\n                        kind, value = row[1].split('=', 1)\n                        with zf.open(arcname) as bf:\n                            data = bf.read()\n                        _, digest = self.get_hash(data, kind)\n                        if digest != value:\n                            raise DistlibException('digest mismatch for '\n                                                   '%s' % arcname)\n\n                    if lib_only and u_arcname.startswith((info_pfx, data_pfx)):\n                        logger.debug('lib_only: skipping %s', u_arcname)\n                        continue\n                    is_script = (u_arcname.startswith(script_pfx) and not u_arcname.endswith('.exe'))\n\n                    if u_arcname.startswith(data_pfx):\n                        _, where, rp = u_arcname.split('/', 2)\n                        outfile = os.path.join(paths[where], convert_path(rp))\n                    else:\n                        # meant for site-packages.\n                        if u_arcname in (wheel_metadata_name, record_name):\n                            continue\n                        outfile = os.path.join(libdir, convert_path(u_arcname))\n                    if not is_script:\n                        with zf.open(arcname) as bf:\n                            fileop.copy_stream(bf, outfile)\n                        # Issue #147: permission bits aren't preserved. Using\n                        # zf.extract(zinfo, libdir) should have worked, but didn't,\n                        # see https://www.thetopsites.net/article/53834422.shtml\n                        # So ... manually preserve permission bits as given in zinfo\n                        if os.name == 'posix':\n                            # just set the normal permission bits\n                            os.chmod(outfile, (zinfo.external_attr >> 16) & 0x1FF)\n                        outfiles.append(outfile)\n                        # Double check the digest of the written file\n                        if not dry_run and row[1]:\n                            with open(outfile, 'rb') as bf:\n                                data = bf.read()\n                                _, newdigest = self.get_hash(data, kind)\n                                if newdigest != digest:\n                                    raise DistlibException('digest mismatch '\n                                                           'on write for '\n                                                           '%s' % outfile)\n                        if bc and outfile.endswith('.py'):\n                            try:\n                                pyc = fileop.byte_compile(outfile, hashed_invalidation=bc_hashed_invalidation)\n                                outfiles.append(pyc)\n                            except Exception:\n                                # Don't give up if byte-compilation fails,\n                                # but log it and perhaps warn the user\n                                logger.warning('Byte-compilation failed', exc_info=True)\n                    else:\n                        fn = os.path.basename(convert_path(arcname))\n                        workname = os.path.join(workdir, fn)\n                        with zf.open(arcname) as bf:\n                            fileop.copy_stream(bf, workname)\n\n                        dn, fn = os.path.split(outfile)\n                        maker.target_dir = dn\n                        filenames = maker.make(fn)\n                        fileop.set_executable_mode(filenames)\n                        outfiles.extend(filenames)\n\n                if lib_only:\n                    logger.debug('lib_only: returning None')\n                    dist = None\n                else:\n                    # Generate scripts\n\n                    # Try to get pydist.json so we can see if there are\n                    # any commands to generate. If this fails (e.g. because\n                    # of a legacy wheel), log a warning but don't give up.\n                    commands = None\n                    file_version = self.info['Wheel-Version']\n                    if file_version == '1.0':\n                        # Use legacy info\n                        ep = posixpath.join(info_dir, 'entry_points.txt')\n                        try:\n                            with zf.open(ep) as bwf:\n                                epdata = read_exports(bwf)\n                            commands = {}\n                            for key in ('console', 'gui'):\n                                k = '%s_scripts' % key\n                                if k in epdata:\n                                    commands['wrap_%s' % key] = d = {}\n                                    for v in epdata[k].values():\n                                        s = '%s:%s' % (v.prefix, v.suffix)\n                                        if v.flags:\n                                            s += ' [%s]' % ','.join(v.flags)\n                                        d[v.name] = s\n                        except Exception:\n                            logger.warning('Unable to read legacy script '\n                                           'metadata, so cannot generate '\n                                           'scripts')\n                    else:\n                        try:\n                            with zf.open(metadata_name) as bwf:\n                                wf = wrapper(bwf)\n                                commands = json.load(wf).get('extensions')\n                                if commands:\n                                    commands = commands.get('python.commands')\n                        except Exception:\n                            logger.warning('Unable to read JSON metadata, so '\n                                           'cannot generate scripts')\n                    if commands:\n                        console_scripts = commands.get('wrap_console', {})\n                        gui_scripts = commands.get('wrap_gui', {})\n                        if console_scripts or gui_scripts:\n                            script_dir = paths.get('scripts', '')\n                            if not os.path.isdir(script_dir):\n                                raise ValueError('Valid script path not '\n                                                 'specified')\n                            maker.target_dir = script_dir\n                            for k, v in console_scripts.items():\n                                script = '%s = %s' % (k, v)\n                                filenames = maker.make(script)\n                                fileop.set_executable_mode(filenames)\n\n                            if gui_scripts:\n                                options = {'gui': True}\n                                for k, v in gui_scripts.items():\n                                    script = '%s = %s' % (k, v)\n                                    filenames = maker.make(script, options)\n                                    fileop.set_executable_mode(filenames)\n\n                    p = os.path.join(libdir, info_dir)\n                    dist = InstalledDistribution(p)\n\n                    # Write SHARED\n                    paths = dict(paths)  # don't change passed in dict\n                    del paths['purelib']\n                    del paths['platlib']\n                    paths['lib'] = libdir\n                    p = dist.write_shared_locations(paths, dry_run)\n                    if p:\n                        outfiles.append(p)\n\n                    # Write RECORD\n                    dist.write_installed_files(outfiles, paths['prefix'], dry_run)\n                return dist\n            except Exception:  # pragma: no cover\n                logger.exception('installation failed.')\n                fileop.rollback()\n                raise\n            finally:\n                shutil.rmtree(workdir)\n\n    def _get_dylib_cache(self):\n        global cache\n        if cache is None:\n            # Use native string to avoid issues on 2.x: see Python #20140.\n            base = os.path.join(get_cache_base(), str('dylib-cache'), '%s.%s' % sys.version_info[:2])\n            cache = Cache(base)\n        return cache\n\n    def _get_extensions(self):\n        pathname = os.path.join(self.dirname, self.filename)\n        name_ver = '%s-%s' % (self.name, self.version)\n        info_dir = '%s.dist-info' % name_ver\n        arcname = posixpath.join(info_dir, 'EXTENSIONS')\n        wrapper = codecs.getreader('utf-8')\n        result = []\n        with ZipFile(pathname, 'r') as zf:\n            try:\n                with zf.open(arcname) as bf:\n                    wf = wrapper(bf)\n                    extensions = json.load(wf)\n                    cache = self._get_dylib_cache()\n                    prefix = cache.prefix_to_dir(self.filename, use_abspath=False)\n                    cache_base = os.path.join(cache.base, prefix)\n                    if not os.path.isdir(cache_base):\n                        os.makedirs(cache_base)\n                    for name, relpath in extensions.items():\n                        dest = os.path.join(cache_base, convert_path(relpath))\n                        if not os.path.exists(dest):\n                            extract = True\n                        else:\n                            file_time = os.stat(dest).st_mtime\n                            file_time = datetime.datetime.fromtimestamp(file_time)\n                            info = zf.getinfo(relpath)\n                            wheel_time = datetime.datetime(*info.date_time)\n                            extract = wheel_time > file_time\n                        if extract:\n                            zf.extract(relpath, cache_base)\n                        result.append((name, dest))\n            except KeyError:\n                pass\n        return result\n\n    def is_compatible(self):\n        \"\"\"\n        Determine if a wheel is compatible with the running system.\n        \"\"\"\n        return is_compatible(self)\n\n    def is_mountable(self):\n        \"\"\"\n        Determine if a wheel is asserted as mountable by its metadata.\n        \"\"\"\n        return True  # for now - metadata details TBD\n\n    def mount(self, append=False):\n        pathname = os.path.abspath(os.path.join(self.dirname, self.filename))\n        if not self.is_compatible():\n            msg = 'Wheel %s not compatible with this Python.' % pathname\n            raise DistlibException(msg)\n        if not self.is_mountable():\n            msg = 'Wheel %s is marked as not mountable.' % pathname\n            raise DistlibException(msg)\n        if pathname in sys.path:\n            logger.debug('%s already in path', pathname)\n        else:\n            if append:\n                sys.path.append(pathname)\n            else:\n                sys.path.insert(0, pathname)\n            extensions = self._get_extensions()\n            if extensions:\n                if _hook not in sys.meta_path:\n                    sys.meta_path.append(_hook)\n                _hook.add(pathname, extensions)\n\n    def unmount(self):\n        pathname = os.path.abspath(os.path.join(self.dirname, self.filename))\n        if pathname not in sys.path:\n            logger.debug('%s not in path', pathname)\n        else:\n            sys.path.remove(pathname)\n            if pathname in _hook.impure_wheels:\n                _hook.remove(pathname)\n            if not _hook.impure_wheels:\n                if _hook in sys.meta_path:\n                    sys.meta_path.remove(_hook)\n\n    def verify(self):\n        pathname = os.path.join(self.dirname, self.filename)\n        name_ver = '%s-%s' % (self.name, self.version)\n        # data_dir = '%s.data' % name_ver\n        info_dir = '%s.dist-info' % name_ver\n\n        # metadata_name = posixpath.join(info_dir, LEGACY_METADATA_FILENAME)\n        wheel_metadata_name = posixpath.join(info_dir, 'WHEEL')\n        record_name = posixpath.join(info_dir, 'RECORD')\n\n        wrapper = codecs.getreader('utf-8')\n\n        with ZipFile(pathname, 'r') as zf:\n            with zf.open(wheel_metadata_name) as bwf:\n                wf = wrapper(bwf)\n                message_from_file(wf)\n            # wv = message['Wheel-Version'].split('.', 1)\n            # file_version = tuple([int(i) for i in wv])\n            # TODO version verification\n\n            records = {}\n            with zf.open(record_name) as bf:\n                with CSVReader(stream=bf) as reader:\n                    for row in reader:\n                        p = row[0]\n                        records[p] = row\n\n            for zinfo in zf.infolist():\n                arcname = zinfo.filename\n                if isinstance(arcname, text_type):\n                    u_arcname = arcname\n                else:\n                    u_arcname = arcname.decode('utf-8')\n                # See issue #115: some wheels have .. in their entries, but\n                # in the filename ... e.g. __main__..py ! So the check is\n                # updated to look for .. in the directory portions\n                p = u_arcname.split('/')\n                if '..' in p:\n                    raise DistlibException('invalid entry in '\n                                           'wheel: %r' % u_arcname)\n\n                if self.skip_entry(u_arcname):\n                    continue\n                row = records[u_arcname]\n                if row[2] and str(zinfo.file_size) != row[2]:\n                    raise DistlibException('size mismatch for '\n                                           '%s' % u_arcname)\n                if row[1]:\n                    kind, value = row[1].split('=', 1)\n                    with zf.open(arcname) as bf:\n                        data = bf.read()\n                    _, digest = self.get_hash(data, kind)\n                    if digest != value:\n                        raise DistlibException('digest mismatch for '\n                                               '%s' % arcname)\n\n    def update(self, modifier, dest_dir=None, **kwargs):\n        \"\"\"\n        Update the contents of a wheel in a generic way. The modifier should\n        be a callable which expects a dictionary argument: its keys are\n        archive-entry paths, and its values are absolute filesystem paths\n        where the contents the corresponding archive entries can be found. The\n        modifier is free to change the contents of the files pointed to, add\n        new entries and remove entries, before returning. This method will\n        extract the entire contents of the wheel to a temporary location, call\n        the modifier, and then use the passed (and possibly updated)\n        dictionary to write a new wheel. If ``dest_dir`` is specified, the new\n        wheel is written there -- otherwise, the original wheel is overwritten.\n\n        The modifier should return True if it updated the wheel, else False.\n        This method returns the same value the modifier returns.\n        \"\"\"\n\n        def get_version(path_map, info_dir):\n            version = path = None\n            key = '%s/%s' % (info_dir, LEGACY_METADATA_FILENAME)\n            if key not in path_map:\n                key = '%s/PKG-INFO' % info_dir\n            if key in path_map:\n                path = path_map[key]\n                version = Metadata(path=path).version\n            return version, path\n\n        def update_version(version, path):\n            updated = None\n            try:\n                NormalizedVersion(version)\n                i = version.find('-')\n                if i < 0:\n                    updated = '%s+1' % version\n                else:\n                    parts = [int(s) for s in version[i + 1:].split('.')]\n                    parts[-1] += 1\n                    updated = '%s+%s' % (version[:i], '.'.join(str(i) for i in parts))\n            except UnsupportedVersionError:\n                logger.debug('Cannot update non-compliant (PEP-440) '\n                             'version %r', version)\n            if updated:\n                md = Metadata(path=path)\n                md.version = updated\n                legacy = path.endswith(LEGACY_METADATA_FILENAME)\n                md.write(path=path, legacy=legacy)\n                logger.debug('Version updated from %r to %r', version, updated)\n\n        pathname = os.path.join(self.dirname, self.filename)\n        name_ver = '%s-%s' % (self.name, self.version)\n        info_dir = '%s.dist-info' % name_ver\n        record_name = posixpath.join(info_dir, 'RECORD')\n        with tempdir() as workdir:\n            with ZipFile(pathname, 'r') as zf:\n                path_map = {}\n                for zinfo in zf.infolist():\n                    arcname = zinfo.filename\n                    if isinstance(arcname, text_type):\n                        u_arcname = arcname\n                    else:\n                        u_arcname = arcname.decode('utf-8')\n                    if u_arcname == record_name:\n                        continue\n                    if '..' in u_arcname:\n                        raise DistlibException('invalid entry in '\n                                               'wheel: %r' % u_arcname)\n                    zf.extract(zinfo, workdir)\n                    path = os.path.join(workdir, convert_path(u_arcname))\n                    path_map[u_arcname] = path\n\n            # Remember the version.\n            original_version, _ = get_version(path_map, info_dir)\n            # Files extracted. Call the modifier.\n            modified = modifier(path_map, **kwargs)\n            if modified:\n                # Something changed - need to build a new wheel.\n                current_version, path = get_version(path_map, info_dir)\n                if current_version and (current_version == original_version):\n                    # Add or update local version to signify changes.\n                    update_version(current_version, path)\n                # Decide where the new wheel goes.\n                if dest_dir is None:\n                    fd, newpath = tempfile.mkstemp(suffix='.whl', prefix='wheel-update-', dir=workdir)\n                    os.close(fd)\n                else:\n                    if not os.path.isdir(dest_dir):\n                        raise DistlibException('Not a directory: %r' % dest_dir)\n                    newpath = os.path.join(dest_dir, self.filename)\n                archive_paths = list(path_map.items())\n                distinfo = os.path.join(workdir, info_dir)\n                info = distinfo, info_dir\n                self.write_records(info, workdir, archive_paths)\n                self.build_zip(newpath, archive_paths)\n                if dest_dir is None:\n                    shutil.copyfile(newpath, pathname)\n        return modified\n\n\ndef _get_glibc_version():\n    import platform\n    ver = platform.libc_ver()\n    result = []\n    if ver[0] == 'glibc':\n        for s in ver[1].split('.'):\n            result.append(int(s) if s.isdigit() else 0)\n        result = tuple(result)\n    return result\n\n\ndef compatible_tags():\n    \"\"\"\n    Return (pyver, abi, arch) tuples compatible with this Python.\n    \"\"\"\n    class _Version:\n        def __init__(self, major, minor):\n            self.major = major\n            self.major_minor = (major, minor)\n            self.string = ''.join((str(major), str(minor)))\n\n        def __str__(self):\n            return self.string\n\n\n    versions = [\n        _Version(sys.version_info.major, minor_version)\n        for minor_version in range(sys.version_info.minor, -1, -1)\n    ]\n    abis = []\n    for suffix in _get_suffixes():\n        if suffix.startswith('.abi'):\n            abis.append(suffix.split('.', 2)[1])\n    abis.sort()\n    if ABI != 'none':\n        abis.insert(0, ABI)\n    abis.append('none')\n    result = []\n\n    arches = [ARCH]\n    if sys.platform == 'darwin':\n        m = re.match(r'(\\w+)_(\\d+)_(\\d+)_(\\w+)$', ARCH)\n        if m:\n            name, major, minor, arch = m.groups()\n            minor = int(minor)\n            matches = [arch]\n            if arch in ('i386', 'ppc'):\n                matches.append('fat')\n            if arch in ('i386', 'ppc', 'x86_64'):\n                matches.append('fat3')\n            if arch in ('ppc64', 'x86_64'):\n                matches.append('fat64')\n            if arch in ('i386', 'x86_64'):\n                matches.append('intel')\n            if arch in ('i386', 'x86_64', 'intel', 'ppc', 'ppc64'):\n                matches.append('universal')\n            while minor >= 0:\n                for match in matches:\n                    s = '%s_%s_%s_%s' % (name, major, minor, match)\n                    if s != ARCH:  # already there\n                        arches.append(s)\n                minor -= 1\n\n    # Most specific - our Python version, ABI and arch\n    for i, version_object in enumerate(versions):\n        version = str(version_object)\n        add_abis = []\n\n        if i == 0:\n            add_abis = abis\n\n        if IMP_PREFIX == 'cp' and version_object.major_minor >= (3, 2):\n            limited_api_abi = 'abi' + str(version_object.major)\n            if limited_api_abi not in add_abis:\n                add_abis.append(limited_api_abi)\n\n        for abi in add_abis:\n            for arch in arches:\n                result.append((''.join((IMP_PREFIX, version)), abi, arch))\n                # manylinux\n                if abi != 'none' and sys.platform.startswith('linux'):\n                    arch = arch.replace('linux_', '')\n                    parts = _get_glibc_version()\n                    if len(parts) == 2:\n                        if parts >= (2, 5):\n                            result.append((''.join((IMP_PREFIX, version)), abi, 'manylinux1_%s' % arch))\n                        if parts >= (2, 12):\n                            result.append((''.join((IMP_PREFIX, version)), abi, 'manylinux2010_%s' % arch))\n                        if parts >= (2, 17):\n                            result.append((''.join((IMP_PREFIX, version)), abi, 'manylinux2014_%s' % arch))\n                        result.append((''.join(\n                            (IMP_PREFIX, version)), abi, 'manylinux_%s_%s_%s' % (parts[0], parts[1], arch)))\n\n    # where no ABI / arch dependency, but IMP_PREFIX dependency\n    for i, version_object in enumerate(versions):\n        version = str(version_object)\n        result.append((''.join((IMP_PREFIX, version)), 'none', 'any'))\n        if i == 0:\n            result.append((''.join((IMP_PREFIX, version[0])), 'none', 'any'))\n\n    # no IMP_PREFIX, ABI or arch dependency\n    for i, version_object in enumerate(versions):\n        version = str(version_object)\n        result.append((''.join(('py', version)), 'none', 'any'))\n        if i == 0:\n            result.append((''.join(('py', version[0])), 'none', 'any'))\n\n    return set(result)\n\n\nCOMPATIBLE_TAGS = compatible_tags()\n\ndel compatible_tags\n\n\ndef is_compatible(wheel, tags=None):\n    if not isinstance(wheel, Wheel):\n        wheel = Wheel(wheel)  # assume it's a filename\n    result = False\n    if tags is None:\n        tags = COMPATIBLE_TAGS\n    for ver, abi, arch in tags:\n        if ver in wheel.pyver and abi in wheel.abi and arch in wheel.arch:\n            result = True\n            break\n    return result\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_vendor/distro/__init__.py","size":981,"sha1":"4a736116da5e08dd8ec668e9768acf14ead0e823","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"from .distro import (\n    NORMALIZED_DISTRO_ID,\n    NORMALIZED_LSB_ID,\n    NORMALIZED_OS_ID,\n    LinuxDistribution,\n    __version__,\n    build_number,\n    codename,\n    distro_release_attr,\n    distro_release_info,\n    id,\n    info,\n    like,\n    linux_distribution,\n    lsb_release_attr,\n    lsb_release_info,\n    major_version,\n    minor_version,\n    name,\n    os_release_attr,\n    os_release_info,\n    uname_attr,\n    uname_info,\n    version,\n    version_parts,\n)\n\n__all__ = [\n    \"NORMALIZED_DISTRO_ID\",\n    \"NORMALIZED_LSB_ID\",\n    \"NORMALIZED_OS_ID\",\n    \"LinuxDistribution\",\n    \"build_number\",\n    \"codename\",\n    \"distro_release_attr\",\n    \"distro_release_info\",\n    \"id\",\n    \"info\",\n    \"like\",\n    \"linux_distribution\",\n    \"lsb_release_attr\",\n    \"lsb_release_info\",\n    \"major_version\",\n    \"minor_version\",\n    \"name\",\n    \"os_release_attr\",\n    \"os_release_info\",\n    \"uname_attr\",\n    \"uname_info\",\n    \"version\",\n    \"version_parts\",\n]\n\n__version__ = __version__\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_vendor/distro/__main__.py","size":64,"sha1":"be9d6fcd0debf92ebea7d4c5c0331f9482ba0c29","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"from .distro import main\n\nif __name__ == \"__main__\":\n    main()\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_vendor/distro/distro.py","size":49430,"sha1":"3d81b2572ba6ceeedf490abfb393fe13a02a1602","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"#!/usr/bin/env python\n# Copyright 2015-2021 Nir Cohen\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nThe ``distro`` package (``distro`` stands for Linux Distribution) provides\ninformation about the Linux distribution it runs on, such as a reliable\nmachine-readable distro ID, or version information.\n\nIt is the recommended replacement for Python's original\n:py:func:`platform.linux_distribution` function, but it provides much more\nfunctionality. An alternative implementation became necessary because Python\n3.5 deprecated this function, and Python 3.8 removed it altogether. Its\npredecessor function :py:func:`platform.dist` was already deprecated since\nPython 2.6 and removed in Python 3.8. Still, there are many cases in which\naccess to OS distribution information is needed. See `Python issue 1322\n<https://bugs.python.org/issue1322>`_ for more information.\n\"\"\"\n\nimport argparse\nimport json\nimport logging\nimport os\nimport re\nimport shlex\nimport subprocess\nimport sys\nimport warnings\nfrom typing import (\n    Any,\n    Callable,\n    Dict,\n    Iterable,\n    Optional,\n    Sequence,\n    TextIO,\n    Tuple,\n    Type,\n)\n\ntry:\n    from typing import TypedDict\nexcept ImportError:\n    # Python 3.7\n    TypedDict = dict\n\n__version__ = \"1.9.0\"\n\n\nclass VersionDict(TypedDict):\n    major: str\n    minor: str\n    build_number: str\n\n\nclass InfoDict(TypedDict):\n    id: str\n    version: str\n    version_parts: VersionDict\n    like: str\n    codename: str\n\n\n_UNIXCONFDIR = os.environ.get(\"UNIXCONFDIR\", \"/etc\")\n_UNIXUSRLIBDIR = os.environ.get(\"UNIXUSRLIBDIR\", \"/usr/lib\")\n_OS_RELEASE_BASENAME = \"os-release\"\n\n#: Translation table for normalizing the \"ID\" attribute defined in os-release\n#: files, for use by the :func:`distro.id` method.\n#:\n#: * Key: Value as defined in the os-release file, translated to lower case,\n#:   with blanks translated to underscores.\n#:\n#: * Value: Normalized value.\nNORMALIZED_OS_ID = {\n    \"ol\": \"oracle\",  # Oracle Linux\n    \"opensuse-leap\": \"opensuse\",  # Newer versions of OpenSuSE report as opensuse-leap\n}\n\n#: Translation table for normalizing the \"Distributor ID\" attribute returned by\n#: the lsb_release command, for use by the :func:`distro.id` method.\n#:\n#: * Key: Value as returned by the lsb_release command, translated to lower\n#:   case, with blanks translated to underscores.\n#:\n#: * Value: Normalized value.\nNORMALIZED_LSB_ID = {\n    \"enterpriseenterpriseas\": \"oracle\",  # Oracle Enterprise Linux 4\n    \"enterpriseenterpriseserver\": \"oracle\",  # Oracle Linux 5\n    \"redhatenterpriseworkstation\": \"rhel\",  # RHEL 6, 7 Workstation\n    \"redhatenterpriseserver\": \"rhel\",  # RHEL 6, 7 Server\n    \"redhatenterprisecomputenode\": \"rhel\",  # RHEL 6 ComputeNode\n}\n\n#: Translation table for normalizing the distro ID derived from the file name\n#: of distro release files, for use by the :func:`distro.id` method.\n#:\n#: * Key: Value as derived from the file name of a distro release file,\n#:   translated to lower case, with blanks translated to underscores.\n#:\n#: * Value: Normalized value.\nNORMALIZED_DISTRO_ID = {\n    \"redhat\": \"rhel\",  # RHEL 6.x, 7.x\n}\n\n# Pattern for content of distro release file (reversed)\n_DISTRO_RELEASE_CONTENT_REVERSED_PATTERN = re.compile(\n    r\"(?:[^)]*\\)(.*)\\()? *(?:STL )?([\\d.+\\-a-z]*\\d) *(?:esaeler *)?(.+)\"\n)\n\n# Pattern for base file name of distro release file\n_DISTRO_RELEASE_BASENAME_PATTERN = re.compile(r\"(\\w+)[-_](release|version)$\")\n\n# Base file names to be looked up for if _UNIXCONFDIR is not readable.\n_DISTRO_RELEASE_BASENAMES = [\n    \"SuSE-release\",\n    \"altlinux-release\",\n    \"arch-release\",\n    \"base-release\",\n    \"centos-release\",\n    \"fedora-release\",\n    \"gentoo-release\",\n    \"mageia-release\",\n    \"mandrake-release\",\n    \"mandriva-release\",\n    \"mandrivalinux-release\",\n    \"manjaro-release\",\n    \"oracle-release\",\n    \"redhat-release\",\n    \"rocky-release\",\n    \"sl-release\",\n    \"slackware-version\",\n]\n\n# Base file names to be ignored when searching for distro release file\n_DISTRO_RELEASE_IGNORE_BASENAMES = (\n    \"debian_version\",\n    \"lsb-release\",\n    \"oem-release\",\n    _OS_RELEASE_BASENAME,\n    \"system-release\",\n    \"plesk-release\",\n    \"iredmail-release\",\n    \"board-release\",\n    \"ec2_version\",\n)\n\n\ndef linux_distribution(full_distribution_name: bool = True) -> Tuple[str, str, str]:\n    \"\"\"\n    .. deprecated:: 1.6.0\n\n        :func:`distro.linux_distribution()` is deprecated. It should only be\n        used as a compatibility shim with Python's\n        :py:func:`platform.linux_distribution()`. Please use :func:`distro.id`,\n        :func:`distro.version` and :func:`distro.name` instead.\n\n    Return information about the current OS distribution as a tuple\n    ``(id_name, version, codename)`` with items as follows:\n\n    * ``id_name``:  If *full_distribution_name* is false, the result of\n      :func:`distro.id`. Otherwise, the result of :func:`distro.name`.\n\n    * ``version``:  The result of :func:`distro.version`.\n\n    * ``codename``:  The extra item (usually in parentheses) after the\n      os-release version number, or the result of :func:`distro.codename`.\n\n    The interface of this function is compatible with the original\n    :py:func:`platform.linux_distribution` function, supporting a subset of\n    its parameters.\n\n    The data it returns may not exactly be the same, because it uses more data\n    sources than the original function, and that may lead to different data if\n    the OS distribution is not consistent across multiple data sources it\n    provides (there are indeed such distributions ...).\n\n    Another reason for differences is the fact that the :func:`distro.id`\n    method normalizes the distro ID string to a reliable machine-readable value\n    for a number of popular OS distributions.\n    \"\"\"\n    warnings.warn(\n        \"distro.linux_distribution() is deprecated. It should only be used as a \"\n        \"compatibility shim with Python's platform.linux_distribution(). Please use \"\n        \"distro.id(), distro.version() and distro.name() instead.\",\n        DeprecationWarning,\n        stacklevel=2,\n    )\n    return _distro.linux_distribution(full_distribution_name)\n\n\ndef id() -> str:\n    \"\"\"\n    Return the distro ID of the current distribution, as a\n    machine-readable string.\n\n    For a number of OS distributions, the returned distro ID value is\n    *reliable*, in the sense that it is documented and that it does not change\n    across releases of the distribution.\n\n    This package maintains the following reliable distro ID values:\n\n    ==============  =========================================\n    Distro ID       Distribution\n    ==============  =========================================\n    \"ubuntu\"        Ubuntu\n    \"debian\"        Debian\n    \"rhel\"          RedHat Enterprise Linux\n    \"centos\"        CentOS\n    \"fedora\"        Fedora\n    \"sles\"          SUSE Linux Enterprise Server\n    \"opensuse\"      openSUSE\n    \"amzn\"          Amazon Linux\n    \"arch\"          Arch Linux\n    \"buildroot\"     Buildroot\n    \"cloudlinux\"    CloudLinux OS\n    \"exherbo\"       Exherbo Linux\n    \"gentoo\"        GenToo Linux\n    \"ibm_powerkvm\"  IBM PowerKVM\n    \"kvmibm\"        KVM for IBM z Systems\n    \"linuxmint\"     Linux Mint\n    \"mageia\"        Mageia\n    \"mandriva\"      Mandriva Linux\n    \"parallels\"     Parallels\n    \"pidora\"        Pidora\n    \"raspbian\"      Raspbian\n    \"oracle\"        Oracle Linux (and Oracle Enterprise Linux)\n    \"scientific\"    Scientific Linux\n    \"slackware\"     Slackware\n    \"xenserver\"     XenServer\n    \"openbsd\"       OpenBSD\n    \"netbsd\"        NetBSD\n    \"freebsd\"       FreeBSD\n    \"midnightbsd\"   MidnightBSD\n    \"rocky\"         Rocky Linux\n    \"aix\"           AIX\n    \"guix\"          Guix System\n    \"altlinux\"      ALT Linux\n    ==============  =========================================\n\n    If you have a need to get distros for reliable IDs added into this set,\n    or if you find that the :func:`distro.id` function returns a different\n    distro ID for one of the listed distros, please create an issue in the\n    `distro issue tracker`_.\n\n    **Lookup hierarchy and transformations:**\n\n    First, the ID is obtained from the following sources, in the specified\n    order. The first available and non-empty value is used:\n\n    * the value of the \"ID\" attribute of the os-release file,\n\n    * the value of the \"Distributor ID\" attribute returned by the lsb_release\n      command,\n\n    * the first part of the file name of the distro release file,\n\n    The so determined ID value then passes the following transformations,\n    before it is returned by this method:\n\n    * it is translated to lower case,\n\n    * blanks (which should not be there anyway) are translated to underscores,\n\n    * a normalization of the ID is performed, based upon\n      `normalization tables`_. The purpose of this normalization is to ensure\n      that the ID is as reliable as possible, even across incompatible changes\n      in the OS distributions. A common reason for an incompatible change is\n      the addition of an os-release file, or the addition of the lsb_release\n      command, with ID values that differ from what was previously determined\n      from the distro release file name.\n    \"\"\"\n    return _distro.id()\n\n\ndef name(pretty: bool = False) -> str:\n    \"\"\"\n    Return the name of the current OS distribution, as a human-readable\n    string.\n\n    If *pretty* is false, the name is returned without version or codename.\n    (e.g. \"CentOS Linux\")\n\n    If *pretty* is true, the version and codename are appended.\n    (e.g. \"CentOS Linux 7.1.1503 (Core)\")\n\n    **Lookup hierarchy:**\n\n    The name is obtained from the following sources, in the specified order.\n    The first available and non-empty value is used:\n\n    * If *pretty* is false:\n\n      - the value of the \"NAME\" attribute of the os-release file,\n\n      - the value of the \"Distributor ID\" attribute returned by the lsb_release\n        command,\n\n      - the value of the \"<name>\" field of the distro release file.\n\n    * If *pretty* is true:\n\n      - the value of the \"PRETTY_NAME\" attribute of the os-release file,\n\n      - the value of the \"Description\" attribute returned by the lsb_release\n        command,\n\n      - the value of the \"<name>\" field of the distro release file, appended\n        with the value of the pretty version (\"<version_id>\" and \"<codename>\"\n        fields) of the distro release file, if available.\n    \"\"\"\n    return _distro.name(pretty)\n\n\ndef version(pretty: bool = False, best: bool = False) -> str:\n    \"\"\"\n    Return the version of the current OS distribution, as a human-readable\n    string.\n\n    If *pretty* is false, the version is returned without codename (e.g.\n    \"7.0\").\n\n    If *pretty* is true, the codename in parenthesis is appended, if the\n    codename is non-empty (e.g. \"7.0 (Maipo)\").\n\n    Some distributions provide version numbers with different precisions in\n    the different sources of distribution information. Examining the different\n    sources in a fixed priority order does not always yield the most precise\n    version (e.g. for Debian 8.2, or CentOS 7.1).\n\n    Some other distributions may not provide this kind of information. In these\n    cases, an empty string would be returned. This behavior can be observed\n    with rolling releases distributions (e.g. Arch Linux).\n\n    The *best* parameter can be used to control the approach for the returned\n    version:\n\n    If *best* is false, the first non-empty version number in priority order of\n    the examined sources is returned.\n\n    If *best* is true, the most precise version number out of all examined\n    sources is returned.\n\n    **Lookup hierarchy:**\n\n    In all cases, the version number is obtained from the following sources.\n    If *best* is false, this order represents the priority order:\n\n    * the value of the \"VERSION_ID\" attribute of the os-release file,\n    * the value of the \"Release\" attribute returned by the lsb_release\n      command,\n    * the version number parsed from the \"<version_id>\" field of the first line\n      of the distro release file,\n    * the version number parsed from the \"PRETTY_NAME\" attribute of the\n      os-release file, if it follows the format of the distro release files.\n    * the version number parsed from the \"Description\" attribute returned by\n      the lsb_release command, if it follows the format of the distro release\n      files.\n    \"\"\"\n    return _distro.version(pretty, best)\n\n\ndef version_parts(best: bool = False) -> Tuple[str, str, str]:\n    \"\"\"\n    Return the version of the current OS distribution as a tuple\n    ``(major, minor, build_number)`` with items as follows:\n\n    * ``major``:  The result of :func:`distro.major_version`.\n\n    * ``minor``:  The result of :func:`distro.minor_version`.\n\n    * ``build_number``:  The result of :func:`distro.build_number`.\n\n    For a description of the *best* parameter, see the :func:`distro.version`\n    method.\n    \"\"\"\n    return _distro.version_parts(best)\n\n\ndef major_version(best: bool = False) -> str:\n    \"\"\"\n    Return the major version of the current OS distribution, as a string,\n    if provided.\n    Otherwise, the empty string is returned. The major version is the first\n    part of the dot-separated version string.\n\n    For a description of the *best* parameter, see the :func:`distro.version`\n    method.\n    \"\"\"\n    return _distro.major_version(best)\n\n\ndef minor_version(best: bool = False) -> str:\n    \"\"\"\n    Return the minor version of the current OS distribution, as a string,\n    if provided.\n    Otherwise, the empty string is returned. The minor version is the second\n    part of the dot-separated version string.\n\n    For a description of the *best* parameter, see the :func:`distro.version`\n    method.\n    \"\"\"\n    return _distro.minor_version(best)\n\n\ndef build_number(best: bool = False) -> str:\n    \"\"\"\n    Return the build number of the current OS distribution, as a string,\n    if provided.\n    Otherwise, the empty string is returned. The build number is the third part\n    of the dot-separated version string.\n\n    For a description of the *best* parameter, see the :func:`distro.version`\n    method.\n    \"\"\"\n    return _distro.build_number(best)\n\n\ndef like() -> str:\n    \"\"\"\n    Return a space-separated list of distro IDs of distributions that are\n    closely related to the current OS distribution in regards to packaging\n    and programming interfaces, for example distributions the current\n    distribution is a derivative from.\n\n    **Lookup hierarchy:**\n\n    This information item is only provided by the os-release file.\n    For details, see the description of the \"ID_LIKE\" attribute in the\n    `os-release man page\n    <http://www.freedesktop.org/software/systemd/man/os-release.html>`_.\n    \"\"\"\n    return _distro.like()\n\n\ndef codename() -> str:\n    \"\"\"\n    Return the codename for the release of the current OS distribution,\n    as a string.\n\n    If the distribution does not have a codename, an empty string is returned.\n\n    Note that the returned codename is not always really a codename. For\n    example, openSUSE returns \"x86_64\". This function does not handle such\n    cases in any special way and just returns the string it finds, if any.\n\n    **Lookup hierarchy:**\n\n    * the codename within the \"VERSION\" attribute of the os-release file, if\n      provided,\n\n    * the value of the \"Codename\" attribute returned by the lsb_release\n      command,\n\n    * the value of the \"<codename>\" field of the distro release file.\n    \"\"\"\n    return _distro.codename()\n\n\ndef info(pretty: bool = False, best: bool = False) -> InfoDict:\n    \"\"\"\n    Return certain machine-readable information items about the current OS\n    distribution in a dictionary, as shown in the following example:\n\n    .. sourcecode:: python\n\n        {\n            'id': 'rhel',\n            'version': '7.0',\n            'version_parts': {\n                'major': '7',\n                'minor': '0',\n                'build_number': ''\n            },\n            'like': 'fedora',\n            'codename': 'Maipo'\n        }\n\n    The dictionary structure and keys are always the same, regardless of which\n    information items are available in the underlying data sources. The values\n    for the various keys are as follows:\n\n    * ``id``:  The result of :func:`distro.id`.\n\n    * ``version``:  The result of :func:`distro.version`.\n\n    * ``version_parts -> major``:  The result of :func:`distro.major_version`.\n\n    * ``version_parts -> minor``:  The result of :func:`distro.minor_version`.\n\n    * ``version_parts -> build_number``:  The result of\n      :func:`distro.build_number`.\n\n    * ``like``:  The result of :func:`distro.like`.\n\n    * ``codename``:  The result of :func:`distro.codename`.\n\n    For a description of the *pretty* and *best* parameters, see the\n    :func:`distro.version` method.\n    \"\"\"\n    return _distro.info(pretty, best)\n\n\ndef os_release_info() -> Dict[str, str]:\n    \"\"\"\n    Return a dictionary containing key-value pairs for the information items\n    from the os-release file data source of the current OS distribution.\n\n    See `os-release file`_ for details about these information items.\n    \"\"\"\n    return _distro.os_release_info()\n\n\ndef lsb_release_info() -> Dict[str, str]:\n    \"\"\"\n    Return a dictionary containing key-value pairs for the information items\n    from the lsb_release command data source of the current OS distribution.\n\n    See `lsb_release command output`_ for details about these information\n    items.\n    \"\"\"\n    return _distro.lsb_release_info()\n\n\ndef distro_release_info() -> Dict[str, str]:\n    \"\"\"\n    Return a dictionary containing key-value pairs for the information items\n    from the distro release file data source of the current OS distribution.\n\n    See `distro release file`_ for details about these information items.\n    \"\"\"\n    return _distro.distro_release_info()\n\n\ndef uname_info() -> Dict[str, str]:\n    \"\"\"\n    Return a dictionary containing key-value pairs for the information items\n    from the distro release file data source of the current OS distribution.\n    \"\"\"\n    return _distro.uname_info()\n\n\ndef os_release_attr(attribute: str) -> str:\n    \"\"\"\n    Return a single named information item from the os-release file data source\n    of the current OS distribution.\n\n    Parameters:\n\n    * ``attribute`` (string): Key of the information item.\n\n    Returns:\n\n    * (string): Value of the information item, if the item exists.\n      The empty string, if the item does not exist.\n\n    See `os-release file`_ for details about these information items.\n    \"\"\"\n    return _distro.os_release_attr(attribute)\n\n\ndef lsb_release_attr(attribute: str) -> str:\n    \"\"\"\n    Return a single named information item from the lsb_release command output\n    data source of the current OS distribution.\n\n    Parameters:\n\n    * ``attribute`` (string): Key of the information item.\n\n    Returns:\n\n    * (string): Value of the information item, if the item exists.\n      The empty string, if the item does not exist.\n\n    See `lsb_release command output`_ for details about these information\n    items.\n    \"\"\"\n    return _distro.lsb_release_attr(attribute)\n\n\ndef distro_release_attr(attribute: str) -> str:\n    \"\"\"\n    Return a single named information item from the distro release file\n    data source of the current OS distribution.\n\n    Parameters:\n\n    * ``attribute`` (string): Key of the information item.\n\n    Returns:\n\n    * (string): Value of the information item, if the item exists.\n      The empty string, if the item does not exist.\n\n    See `distro release file`_ for details about these information items.\n    \"\"\"\n    return _distro.distro_release_attr(attribute)\n\n\ndef uname_attr(attribute: str) -> str:\n    \"\"\"\n    Return a single named information item from the distro release file\n    data source of the current OS distribution.\n\n    Parameters:\n\n    * ``attribute`` (string): Key of the information item.\n\n    Returns:\n\n    * (string): Value of the information item, if the item exists.\n                The empty string, if the item does not exist.\n    \"\"\"\n    return _distro.uname_attr(attribute)\n\n\ntry:\n    from functools import cached_property\nexcept ImportError:\n    # Python < 3.8\n    class cached_property:  # type: ignore\n        \"\"\"A version of @property which caches the value.  On access, it calls the\n        underlying function and sets the value in `__dict__` so future accesses\n        will not re-call the property.\n        \"\"\"\n\n        def __init__(self, f: Callable[[Any], Any]) -> None:\n            self._fname = f.__name__\n            self._f = f\n\n        def __get__(self, obj: Any, owner: Type[Any]) -> Any:\n            assert obj is not None, f\"call {self._fname} on an instance\"\n            ret = obj.__dict__[self._fname] = self._f(obj)\n            return ret\n\n\nclass LinuxDistribution:\n    \"\"\"\n    Provides information about a OS distribution.\n\n    This package creates a private module-global instance of this class with\n    default initialization arguments, that is used by the\n    `consolidated accessor functions`_ and `single source accessor functions`_.\n    By using default initialization arguments, that module-global instance\n    returns data about the current OS distribution (i.e. the distro this\n    package runs on).\n\n    Normally, it is not necessary to create additional instances of this class.\n    However, in situations where control is needed over the exact data sources\n    that are used, instances of this class can be created with a specific\n    distro release file, or a specific os-release file, or without invoking the\n    lsb_release command.\n    \"\"\"\n\n    def __init__(\n        self,\n        include_lsb: Optional[bool] = None,\n        os_release_file: str = \"\",\n        distro_release_file: str = \"\",\n        include_uname: Optional[bool] = None,\n        root_dir: Optional[str] = None,\n        include_oslevel: Optional[bool] = None,\n    ) -> None:\n        \"\"\"\n        The initialization method of this class gathers information from the\n        available data sources, and stores that in private instance attributes.\n        Subsequent access to the information items uses these private instance\n        attributes, so that the data sources are read only once.\n\n        Parameters:\n\n        * ``include_lsb`` (bool): Controls whether the\n          `lsb_release command output`_ is included as a data source.\n\n          If the lsb_release command is not available in the program execution\n          path, the data source for the lsb_release command will be empty.\n\n        * ``os_release_file`` (string): The path name of the\n          `os-release file`_ that is to be used as a data source.\n\n          An empty string (the default) will cause the default path name to\n          be used (see `os-release file`_ for details).\n\n          If the specified or defaulted os-release file does not exist, the\n          data source for the os-release file will be empty.\n\n        * ``distro_release_file`` (string): The path name of the\n          `distro release file`_ that is to be used as a data source.\n\n          An empty string (the default) will cause a default search algorithm\n          to be used (see `distro release file`_ for details).\n\n          If the specified distro release file does not exist, or if no default\n          distro release file can be found, the data source for the distro\n          release file will be empty.\n\n        * ``include_uname`` (bool): Controls whether uname command output is\n          included as a data source. If the uname command is not available in\n          the program execution path the data source for the uname command will\n          be empty.\n\n        * ``root_dir`` (string): The absolute path to the root directory to use\n          to find distro-related information files. Note that ``include_*``\n          parameters must not be enabled in combination with ``root_dir``.\n\n        * ``include_oslevel`` (bool): Controls whether (AIX) oslevel command\n          output is included as a data source. If the oslevel command is not\n          available in the program execution path the data source will be\n          empty.\n\n        Public instance attributes:\n\n        * ``os_release_file`` (string): The path name of the\n          `os-release file`_ that is actually used as a data source. The\n          empty string if no distro release file is used as a data source.\n\n        * ``distro_release_file`` (string): The path name of the\n          `distro release file`_ that is actually used as a data source. The\n          empty string if no distro release file is used as a data source.\n\n        * ``include_lsb`` (bool): The result of the ``include_lsb`` parameter.\n          This controls whether the lsb information will be loaded.\n\n        * ``include_uname`` (bool): The result of the ``include_uname``\n          parameter. This controls whether the uname information will\n          be loaded.\n\n        * ``include_oslevel`` (bool): The result of the ``include_oslevel``\n          parameter. This controls whether (AIX) oslevel information will be\n          loaded.\n\n        * ``root_dir`` (string): The result of the ``root_dir`` parameter.\n          The absolute path to the root directory to use to find distro-related\n          information files.\n\n        Raises:\n\n        * :py:exc:`ValueError`: Initialization parameters combination is not\n           supported.\n\n        * :py:exc:`OSError`: Some I/O issue with an os-release file or distro\n          release file.\n\n        * :py:exc:`UnicodeError`: A data source has unexpected characters or\n          uses an unexpected encoding.\n        \"\"\"\n        self.root_dir = root_dir\n        self.etc_dir = os.path.join(root_dir, \"etc\") if root_dir else _UNIXCONFDIR\n        self.usr_lib_dir = (\n            os.path.join(root_dir, \"usr/lib\") if root_dir else _UNIXUSRLIBDIR\n        )\n\n        if os_release_file:\n            self.os_release_file = os_release_file\n        else:\n            etc_dir_os_release_file = os.path.join(self.etc_dir, _OS_RELEASE_BASENAME)\n            usr_lib_os_release_file = os.path.join(\n                self.usr_lib_dir, _OS_RELEASE_BASENAME\n            )\n\n            # NOTE: The idea is to respect order **and** have it set\n            #       at all times for API backwards compatibility.\n            if os.path.isfile(etc_dir_os_release_file) or not os.path.isfile(\n                usr_lib_os_release_file\n            ):\n                self.os_release_file = etc_dir_os_release_file\n            else:\n                self.os_release_file = usr_lib_os_release_file\n\n        self.distro_release_file = distro_release_file or \"\"  # updated later\n\n        is_root_dir_defined = root_dir is not None\n        if is_root_dir_defined and (include_lsb or include_uname or include_oslevel):\n            raise ValueError(\n                \"Including subprocess data sources from specific root_dir is disallowed\"\n                \" to prevent false information\"\n            )\n        self.include_lsb = (\n            include_lsb if include_lsb is not None else not is_root_dir_defined\n        )\n        self.include_uname = (\n            include_uname if include_uname is not None else not is_root_dir_defined\n        )\n        self.include_oslevel = (\n            include_oslevel if include_oslevel is not None else not is_root_dir_defined\n        )\n\n    def __repr__(self) -> str:\n        \"\"\"Return repr of all info\"\"\"\n        return (\n            \"LinuxDistribution(\"\n            \"os_release_file={self.os_release_file!r}, \"\n            \"distro_release_file={self.distro_release_file!r}, \"\n            \"include_lsb={self.include_lsb!r}, \"\n            \"include_uname={self.include_uname!r}, \"\n            \"include_oslevel={self.include_oslevel!r}, \"\n            \"root_dir={self.root_dir!r}, \"\n            \"_os_release_info={self._os_release_info!r}, \"\n            \"_lsb_release_info={self._lsb_release_info!r}, \"\n            \"_distro_release_info={self._distro_release_info!r}, \"\n            \"_uname_info={self._uname_info!r}, \"\n            \"_oslevel_info={self._oslevel_info!r})\".format(self=self)\n        )\n\n    def linux_distribution(\n        self, full_distribution_name: bool = True\n    ) -> Tuple[str, str, str]:\n        \"\"\"\n        Return information about the OS distribution that is compatible\n        with Python's :func:`platform.linux_distribution`, supporting a subset\n        of its parameters.\n\n        For details, see :func:`distro.linux_distribution`.\n        \"\"\"\n        return (\n            self.name() if full_distribution_name else self.id(),\n            self.version(),\n            self._os_release_info.get(\"release_codename\") or self.codename(),\n        )\n\n    def id(self) -> str:\n        \"\"\"Return the distro ID of the OS distribution, as a string.\n\n        For details, see :func:`distro.id`.\n        \"\"\"\n\n        def normalize(distro_id: str, table: Dict[str, str]) -> str:\n            distro_id = distro_id.lower().replace(\" \", \"_\")\n            return table.get(distro_id, distro_id)\n\n        distro_id = self.os_release_attr(\"id\")\n        if distro_id:\n            return normalize(distro_id, NORMALIZED_OS_ID)\n\n        distro_id = self.lsb_release_attr(\"distributor_id\")\n        if distro_id:\n            return normalize(distro_id, NORMALIZED_LSB_ID)\n\n        distro_id = self.distro_release_attr(\"id\")\n        if distro_id:\n            return normalize(distro_id, NORMALIZED_DISTRO_ID)\n\n        distro_id = self.uname_attr(\"id\")\n        if distro_id:\n            return normalize(distro_id, NORMALIZED_DISTRO_ID)\n\n        return \"\"\n\n    def name(self, pretty: bool = False) -> str:\n        \"\"\"\n        Return the name of the OS distribution, as a string.\n\n        For details, see :func:`distro.name`.\n        \"\"\"\n        name = (\n            self.os_release_attr(\"name\")\n            or self.lsb_release_attr(\"distributor_id\")\n            or self.distro_release_attr(\"name\")\n            or self.uname_attr(\"name\")\n        )\n        if pretty:\n            name = self.os_release_attr(\"pretty_name\") or self.lsb_release_attr(\n                \"description\"\n            )\n            if not name:\n                name = self.distro_release_attr(\"name\") or self.uname_attr(\"name\")\n                version = self.version(pretty=True)\n                if version:\n                    name = f\"{name} {version}\"\n        return name or \"\"\n\n    def version(self, pretty: bool = False, best: bool = False) -> str:\n        \"\"\"\n        Return the version of the OS distribution, as a string.\n\n        For details, see :func:`distro.version`.\n        \"\"\"\n        versions = [\n            self.os_release_attr(\"version_id\"),\n            self.lsb_release_attr(\"release\"),\n            self.distro_release_attr(\"version_id\"),\n            self._parse_distro_release_content(self.os_release_attr(\"pretty_name\")).get(\n                \"version_id\", \"\"\n            ),\n            self._parse_distro_release_content(\n                self.lsb_release_attr(\"description\")\n            ).get(\"version_id\", \"\"),\n            self.uname_attr(\"release\"),\n        ]\n        if self.uname_attr(\"id\").startswith(\"aix\"):\n            # On AIX platforms, prefer oslevel command output.\n            versions.insert(0, self.oslevel_info())\n        elif self.id() == \"debian\" or \"debian\" in self.like().split():\n            # On Debian-like, add debian_version file content to candidates list.\n            versions.append(self._debian_version)\n        version = \"\"\n        if best:\n            # This algorithm uses the last version in priority order that has\n            # the best precision. If the versions are not in conflict, that\n            # does not matter; otherwise, using the last one instead of the\n            # first one might be considered a surprise.\n            for v in versions:\n                if v.count(\".\") > version.count(\".\") or version == \"\":\n                    version = v\n        else:\n            for v in versions:\n                if v != \"\":\n                    version = v\n                    break\n        if pretty and version and self.codename():\n            version = f\"{version} ({self.codename()})\"\n        return version\n\n    def version_parts(self, best: bool = False) -> Tuple[str, str, str]:\n        \"\"\"\n        Return the version of the OS distribution, as a tuple of version\n        numbers.\n\n        For details, see :func:`distro.version_parts`.\n        \"\"\"\n        version_str = self.version(best=best)\n        if version_str:\n            version_regex = re.compile(r\"(\\d+)\\.?(\\d+)?\\.?(\\d+)?\")\n            matches = version_regex.match(version_str)\n            if matches:\n                major, minor, build_number = matches.groups()\n                return major, minor or \"\", build_number or \"\"\n        return \"\", \"\", \"\"\n\n    def major_version(self, best: bool = False) -> str:\n        \"\"\"\n        Return the major version number of the current distribution.\n\n        For details, see :func:`distro.major_version`.\n        \"\"\"\n        return self.version_parts(best)[0]\n\n    def minor_version(self, best: bool = False) -> str:\n        \"\"\"\n        Return the minor version number of the current distribution.\n\n        For details, see :func:`distro.minor_version`.\n        \"\"\"\n        return self.version_parts(best)[1]\n\n    def build_number(self, best: bool = False) -> str:\n        \"\"\"\n        Return the build number of the current distribution.\n\n        For details, see :func:`distro.build_number`.\n        \"\"\"\n        return self.version_parts(best)[2]\n\n    def like(self) -> str:\n        \"\"\"\n        Return the IDs of distributions that are like the OS distribution.\n\n        For details, see :func:`distro.like`.\n        \"\"\"\n        return self.os_release_attr(\"id_like\") or \"\"\n\n    def codename(self) -> str:\n        \"\"\"\n        Return the codename of the OS distribution.\n\n        For details, see :func:`distro.codename`.\n        \"\"\"\n        try:\n            # Handle os_release specially since distros might purposefully set\n            # this to empty string to have no codename\n            return self._os_release_info[\"codename\"]\n        except KeyError:\n            return (\n                self.lsb_release_attr(\"codename\")\n                or self.distro_release_attr(\"codename\")\n                or \"\"\n            )\n\n    def info(self, pretty: bool = False, best: bool = False) -> InfoDict:\n        \"\"\"\n        Return certain machine-readable information about the OS\n        distribution.\n\n        For details, see :func:`distro.info`.\n        \"\"\"\n        return InfoDict(\n            id=self.id(),\n            version=self.version(pretty, best),\n            version_parts=VersionDict(\n                major=self.major_version(best),\n                minor=self.minor_version(best),\n                build_number=self.build_number(best),\n            ),\n            like=self.like(),\n            codename=self.codename(),\n        )\n\n    def os_release_info(self) -> Dict[str, str]:\n        \"\"\"\n        Return a dictionary containing key-value pairs for the information\n        items from the os-release file data source of the OS distribution.\n\n        For details, see :func:`distro.os_release_info`.\n        \"\"\"\n        return self._os_release_info\n\n    def lsb_release_info(self) -> Dict[str, str]:\n        \"\"\"\n        Return a dictionary containing key-value pairs for the information\n        items from the lsb_release command data source of the OS\n        distribution.\n\n        For details, see :func:`distro.lsb_release_info`.\n        \"\"\"\n        return self._lsb_release_info\n\n    def distro_release_info(self) -> Dict[str, str]:\n        \"\"\"\n        Return a dictionary containing key-value pairs for the information\n        items from the distro release file data source of the OS\n        distribution.\n\n        For details, see :func:`distro.distro_release_info`.\n        \"\"\"\n        return self._distro_release_info\n\n    def uname_info(self) -> Dict[str, str]:\n        \"\"\"\n        Return a dictionary containing key-value pairs for the information\n        items from the uname command data source of the OS distribution.\n\n        For details, see :func:`distro.uname_info`.\n        \"\"\"\n        return self._uname_info\n\n    def oslevel_info(self) -> str:\n        \"\"\"\n        Return AIX' oslevel command output.\n        \"\"\"\n        return self._oslevel_info\n\n    def os_release_attr(self, attribute: str) -> str:\n        \"\"\"\n        Return a single named information item from the os-release file data\n        source of the OS distribution.\n\n        For details, see :func:`distro.os_release_attr`.\n        \"\"\"\n        return self._os_release_info.get(attribute, \"\")\n\n    def lsb_release_attr(self, attribute: str) -> str:\n        \"\"\"\n        Return a single named information item from the lsb_release command\n        output data source of the OS distribution.\n\n        For details, see :func:`distro.lsb_release_attr`.\n        \"\"\"\n        return self._lsb_release_info.get(attribute, \"\")\n\n    def distro_release_attr(self, attribute: str) -> str:\n        \"\"\"\n        Return a single named information item from the distro release file\n        data source of the OS distribution.\n\n        For details, see :func:`distro.distro_release_attr`.\n        \"\"\"\n        return self._distro_release_info.get(attribute, \"\")\n\n    def uname_attr(self, attribute: str) -> str:\n        \"\"\"\n        Return a single named information item from the uname command\n        output data source of the OS distribution.\n\n        For details, see :func:`distro.uname_attr`.\n        \"\"\"\n        return self._uname_info.get(attribute, \"\")\n\n    @cached_property\n    def _os_release_info(self) -> Dict[str, str]:\n        \"\"\"\n        Get the information items from the specified os-release file.\n\n        Returns:\n            A dictionary containing all information items.\n        \"\"\"\n        if os.path.isfile(self.os_release_file):\n            with open(self.os_release_file, encoding=\"utf-8\") as release_file:\n                return self._parse_os_release_content(release_file)\n        return {}\n\n    @staticmethod\n    def _parse_os_release_content(lines: TextIO) -> Dict[str, str]:\n        \"\"\"\n        Parse the lines of an os-release file.\n\n        Parameters:\n\n        * lines: Iterable through the lines in the os-release file.\n                 Each line must be a unicode string or a UTF-8 encoded byte\n                 string.\n\n        Returns:\n            A dictionary containing all information items.\n        \"\"\"\n        props = {}\n        lexer = shlex.shlex(lines, posix=True)\n        lexer.whitespace_split = True\n\n        tokens = list(lexer)\n        for token in tokens:\n            # At this point, all shell-like parsing has been done (i.e.\n            # comments processed, quotes and backslash escape sequences\n            # processed, multi-line values assembled, trailing newlines\n            # stripped, etc.), so the tokens are now either:\n            # * variable assignments: var=value\n            # * commands or their arguments (not allowed in os-release)\n            # Ignore any tokens that are not variable assignments\n            if \"=\" in token:\n                k, v = token.split(\"=\", 1)\n                props[k.lower()] = v\n\n        if \"version\" in props:\n            # extract release codename (if any) from version attribute\n            match = re.search(r\"\\((\\D+)\\)|,\\s*(\\D+)\", props[\"version\"])\n            if match:\n                release_codename = match.group(1) or match.group(2)\n                props[\"codename\"] = props[\"release_codename\"] = release_codename\n\n        if \"version_codename\" in props:\n            # os-release added a version_codename field.  Use that in\n            # preference to anything else Note that some distros purposefully\n            # do not have code names.  They should be setting\n            # version_codename=\"\"\n            props[\"codename\"] = props[\"version_codename\"]\n        elif \"ubuntu_codename\" in props:\n            # Same as above but a non-standard field name used on older Ubuntus\n            props[\"codename\"] = props[\"ubuntu_codename\"]\n\n        return props\n\n    @cached_property\n    def _lsb_release_info(self) -> Dict[str, str]:\n        \"\"\"\n        Get the information items from the lsb_release command output.\n\n        Returns:\n            A dictionary containing all information items.\n        \"\"\"\n        if not self.include_lsb:\n            return {}\n        try:\n            cmd = (\"lsb_release\", \"-a\")\n            stdout = subprocess.check_output(cmd, stderr=subprocess.DEVNULL)\n        # Command not found or lsb_release returned error\n        except (OSError, subprocess.CalledProcessError):\n            return {}\n        content = self._to_str(stdout).splitlines()\n        return self._parse_lsb_release_content(content)\n\n    @staticmethod\n    def _parse_lsb_release_content(lines: Iterable[str]) -> Dict[str, str]:\n        \"\"\"\n        Parse the output of the lsb_release command.\n\n        Parameters:\n\n        * lines: Iterable through the lines of the lsb_release output.\n                 Each line must be a unicode string or a UTF-8 encoded byte\n                 string.\n\n        Returns:\n            A dictionary containing all information items.\n        \"\"\"\n        props = {}\n        for line in lines:\n            kv = line.strip(\"\\n\").split(\":\", 1)\n            if len(kv) != 2:\n                # Ignore lines without colon.\n                continue\n            k, v = kv\n            props.update({k.replace(\" \", \"_\").lower(): v.strip()})\n        return props\n\n    @cached_property\n    def _uname_info(self) -> Dict[str, str]:\n        if not self.include_uname:\n            return {}\n        try:\n            cmd = (\"uname\", \"-rs\")\n            stdout = subprocess.check_output(cmd, stderr=subprocess.DEVNULL)\n        except OSError:\n            return {}\n        content = self._to_str(stdout).splitlines()\n        return self._parse_uname_content(content)\n\n    @cached_property\n    def _oslevel_info(self) -> str:\n        if not self.include_oslevel:\n            return \"\"\n        try:\n            stdout = subprocess.check_output(\"oslevel\", stderr=subprocess.DEVNULL)\n        except (OSError, subprocess.CalledProcessError):\n            return \"\"\n        return self._to_str(stdout).strip()\n\n    @cached_property\n    def _debian_version(self) -> str:\n        try:\n            with open(\n                os.path.join(self.etc_dir, \"debian_version\"), encoding=\"ascii\"\n            ) as fp:\n                return fp.readline().rstrip()\n        except FileNotFoundError:\n            return \"\"\n\n    @staticmethod\n    def _parse_uname_content(lines: Sequence[str]) -> Dict[str, str]:\n        if not lines:\n            return {}\n        props = {}\n        match = re.search(r\"^([^\\s]+)\\s+([\\d\\.]+)\", lines[0].strip())\n        if match:\n            name, version = match.groups()\n\n            # This is to prevent the Linux kernel version from\n            # appearing as the 'best' version on otherwise\n            # identifiable distributions.\n            if name == \"Linux\":\n                return {}\n            props[\"id\"] = name.lower()\n            props[\"name\"] = name\n            props[\"release\"] = version\n        return props\n\n    @staticmethod\n    def _to_str(bytestring: bytes) -> str:\n        encoding = sys.getfilesystemencoding()\n        return bytestring.decode(encoding)\n\n    @cached_property\n    def _distro_release_info(self) -> Dict[str, str]:\n        \"\"\"\n        Get the information items from the specified distro release file.\n\n        Returns:\n            A dictionary containing all information items.\n        \"\"\"\n        if self.distro_release_file:\n            # If it was specified, we use it and parse what we can, even if\n            # its file name or content does not match the expected pattern.\n            distro_info = self._parse_distro_release_file(self.distro_release_file)\n            basename = os.path.basename(self.distro_release_file)\n            # The file name pattern for user-specified distro release files\n            # is somewhat more tolerant (compared to when searching for the\n            # file), because we want to use what was specified as best as\n            # possible.\n            match = _DISTRO_RELEASE_BASENAME_PATTERN.match(basename)\n        else:\n            try:\n                basenames = [\n                    basename\n                    for basename in os.listdir(self.etc_dir)\n                    if basename not in _DISTRO_RELEASE_IGNORE_BASENAMES\n                    and os.path.isfile(os.path.join(self.etc_dir, basename))\n                ]\n                # We sort for repeatability in cases where there are multiple\n                # distro specific files; e.g. CentOS, Oracle, Enterprise all\n                # containing `redhat-release` on top of their own.\n                basenames.sort()\n            except OSError:\n                # This may occur when /etc is not readable but we can't be\n                # sure about the *-release files. Check common entries of\n                # /etc for information. If they turn out to not be there the\n                # error is handled in `_parse_distro_release_file()`.\n                basenames = _DISTRO_RELEASE_BASENAMES\n            for basename in basenames:\n                match = _DISTRO_RELEASE_BASENAME_PATTERN.match(basename)\n                if match is None:\n                    continue\n                filepath = os.path.join(self.etc_dir, basename)\n                distro_info = self._parse_distro_release_file(filepath)\n                # The name is always present if the pattern matches.\n                if \"name\" not in distro_info:\n                    continue\n                self.distro_release_file = filepath\n                break\n            else:  # the loop didn't \"break\": no candidate.\n                return {}\n\n        if match is not None:\n            distro_info[\"id\"] = match.group(1)\n\n        # CloudLinux < 7: manually enrich info with proper id.\n        if \"cloudlinux\" in distro_info.get(\"name\", \"\").lower():\n            distro_info[\"id\"] = \"cloudlinux\"\n\n        return distro_info\n\n    def _parse_distro_release_file(self, filepath: str) -> Dict[str, str]:\n        \"\"\"\n        Parse a distro release file.\n\n        Parameters:\n\n        * filepath: Path name of the distro release file.\n\n        Returns:\n            A dictionary containing all information items.\n        \"\"\"\n        try:\n            with open(filepath, encoding=\"utf-8\") as fp:\n                # Only parse the first line. For instance, on SLES there\n                # are multiple lines. We don't want them...\n                return self._parse_distro_release_content(fp.readline())\n        except OSError:\n            # Ignore not being able to read a specific, seemingly version\n            # related file.\n            # See https://github.com/python-distro/distro/issues/162\n            return {}\n\n    @staticmethod\n    def _parse_distro_release_content(line: str) -> Dict[str, str]:\n        \"\"\"\n        Parse a line from a distro release file.\n\n        Parameters:\n        * line: Line from the distro release file. Must be a unicode string\n                or a UTF-8 encoded byte string.\n\n        Returns:\n            A dictionary containing all information items.\n        \"\"\"\n        matches = _DISTRO_RELEASE_CONTENT_REVERSED_PATTERN.match(line.strip()[::-1])\n        distro_info = {}\n        if matches:\n            # regexp ensures non-None\n            distro_info[\"name\"] = matches.group(3)[::-1]\n            if matches.group(2):\n                distro_info[\"version_id\"] = matches.group(2)[::-1]\n            if matches.group(1):\n                distro_info[\"codename\"] = matches.group(1)[::-1]\n        elif line:\n            distro_info[\"name\"] = line.strip()\n        return distro_info\n\n\n_distro = LinuxDistribution()\n\n\ndef main() -> None:\n    logger = logging.getLogger(__name__)\n    logger.setLevel(logging.DEBUG)\n    logger.addHandler(logging.StreamHandler(sys.stdout))\n\n    parser = argparse.ArgumentParser(description=\"OS distro info tool\")\n    parser.add_argument(\n        \"--json\", \"-j\", help=\"Output in machine readable format\", action=\"store_true\"\n    )\n\n    parser.add_argument(\n        \"--root-dir\",\n        \"-r\",\n        type=str,\n        dest=\"root_dir\",\n        help=\"Path to the root filesystem directory (defaults to /)\",\n    )\n\n    args = parser.parse_args()\n\n    if args.root_dir:\n        dist = LinuxDistribution(\n            include_lsb=False,\n            include_uname=False,\n            include_oslevel=False,\n            root_dir=args.root_dir,\n        )\n    else:\n        dist = _distro\n\n    if args.json:\n        logger.info(json.dumps(dist.info(), indent=4, sort_keys=True))\n    else:\n        logger.info(\"Name: %s\", dist.name(pretty=True))\n        distribution_version = dist.version(pretty=True)\n        logger.info(\"Version: %s\", distribution_version)\n        distribution_codename = dist.codename()\n        logger.info(\"Codename: %s\", distribution_codename)\n\n\nif __name__ == \"__main__\":\n    main()\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_vendor/idna/__init__.py","size":868,"sha1":"d4af52a5c4f468358f49fe8cf0a91586958b9f91","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"from .core import (\n    IDNABidiError,\n    IDNAError,\n    InvalidCodepoint,\n    InvalidCodepointContext,\n    alabel,\n    check_bidi,\n    check_hyphen_ok,\n    check_initial_combiner,\n    check_label,\n    check_nfc,\n    decode,\n    encode,\n    ulabel,\n    uts46_remap,\n    valid_contextj,\n    valid_contexto,\n    valid_label_length,\n    valid_string_length,\n)\nfrom .intranges import intranges_contain\nfrom .package_data import __version__\n\n__all__ = [\n    \"__version__\",\n    \"IDNABidiError\",\n    \"IDNAError\",\n    \"InvalidCodepoint\",\n    \"InvalidCodepointContext\",\n    \"alabel\",\n    \"check_bidi\",\n    \"check_hyphen_ok\",\n    \"check_initial_combiner\",\n    \"check_label\",\n    \"check_nfc\",\n    \"decode\",\n    \"encode\",\n    \"intranges_contain\",\n    \"ulabel\",\n    \"uts46_remap\",\n    \"valid_contextj\",\n    \"valid_contexto\",\n    \"valid_label_length\",\n    \"valid_string_length\",\n]\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_vendor/idna/codec.py","size":3422,"sha1":"c326ea0e90cd4ad5638ab0c33a649080c002cef3","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import codecs\nimport re\nfrom typing import Any, Optional, Tuple\n\nfrom .core import IDNAError, alabel, decode, encode, ulabel\n\n_unicode_dots_re = re.compile(\"[\\u002e\\u3002\\uff0e\\uff61]\")\n\n\nclass Codec(codecs.Codec):\n    def encode(self, data: str, errors: str = \"strict\") -> Tuple[bytes, int]:\n        if errors != \"strict\":\n            raise IDNAError('Unsupported error handling \"{}\"'.format(errors))\n\n        if not data:\n            return b\"\", 0\n\n        return encode(data), len(data)\n\n    def decode(self, data: bytes, errors: str = \"strict\") -> Tuple[str, int]:\n        if errors != \"strict\":\n            raise IDNAError('Unsupported error handling \"{}\"'.format(errors))\n\n        if not data:\n            return \"\", 0\n\n        return decode(data), len(data)\n\n\nclass IncrementalEncoder(codecs.BufferedIncrementalEncoder):\n    def _buffer_encode(self, data: str, errors: str, final: bool) -> Tuple[bytes, int]:\n        if errors != \"strict\":\n            raise IDNAError('Unsupported error handling \"{}\"'.format(errors))\n\n        if not data:\n            return b\"\", 0\n\n        labels = _unicode_dots_re.split(data)\n        trailing_dot = b\"\"\n        if labels:\n            if not labels[-1]:\n                trailing_dot = b\".\"\n                del labels[-1]\n            elif not final:\n                # Keep potentially unfinished label until the next call\n                del labels[-1]\n                if labels:\n                    trailing_dot = b\".\"\n\n        result = []\n        size = 0\n        for label in labels:\n            result.append(alabel(label))\n            if size:\n                size += 1\n            size += len(label)\n\n        # Join with U+002E\n        result_bytes = b\".\".join(result) + trailing_dot\n        size += len(trailing_dot)\n        return result_bytes, size\n\n\nclass IncrementalDecoder(codecs.BufferedIncrementalDecoder):\n    def _buffer_decode(self, data: Any, errors: str, final: bool) -> Tuple[str, int]:\n        if errors != \"strict\":\n            raise IDNAError('Unsupported error handling \"{}\"'.format(errors))\n\n        if not data:\n            return (\"\", 0)\n\n        if not isinstance(data, str):\n            data = str(data, \"ascii\")\n\n        labels = _unicode_dots_re.split(data)\n        trailing_dot = \"\"\n        if labels:\n            if not labels[-1]:\n                trailing_dot = \".\"\n                del labels[-1]\n            elif not final:\n                # Keep potentially unfinished label until the next call\n                del labels[-1]\n                if labels:\n                    trailing_dot = \".\"\n\n        result = []\n        size = 0\n        for label in labels:\n            result.append(ulabel(label))\n            if size:\n                size += 1\n            size += len(label)\n\n        result_str = \".\".join(result) + trailing_dot\n        size += len(trailing_dot)\n        return (result_str, size)\n\n\nclass StreamWriter(Codec, codecs.StreamWriter):\n    pass\n\n\nclass StreamReader(Codec, codecs.StreamReader):\n    pass\n\n\ndef search_function(name: str) -> Optional[codecs.CodecInfo]:\n    if name != \"idna2008\":\n        return None\n    return codecs.CodecInfo(\n        name=name,\n        encode=Codec().encode,\n        decode=Codec().decode,\n        incrementalencoder=IncrementalEncoder,\n        incrementaldecoder=IncrementalDecoder,\n        streamwriter=StreamWriter,\n        streamreader=StreamReader,\n    )\n\n\ncodecs.register(search_function)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_vendor/idna/compat.py","size":316,"sha1":"b422d026efa3d16aeeb49683d2cc3cd62c26bbc1","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"from typing import Any, Union\n\nfrom .core import decode, encode\n\n\ndef ToASCII(label: str) -> bytes:\n    return encode(label)\n\n\ndef ToUnicode(label: Union[bytes, bytearray]) -> str:\n    return decode(label)\n\n\ndef nameprep(s: Any) -> None:\n    raise NotImplementedError(\"IDNA 2008 does not utilise nameprep protocol\")\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/pip-25.0.1-py3-none-any/pip/_vendor/idna/core.py","size":13239,"sha1":"fdb292d5720b4cb87e753b655578c60e31767423","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import bisect\nimport re\nimport unicodedata\nfrom typing import Optional, Union\n\nfrom . import idnadata\nfrom .intranges import intranges_contain\n\n_virama_combining_class = 9\n_alabel_prefix = b\"xn--\"\n_unicode_dots_re = re.compile(\"[\\u002e\\u3002\\uff0e\\uff61]\")\n\n\nclass IDNAError(UnicodeError):\n    \"\"\"Base exception for all IDNA-encoding related problems\"\"\"\n\n    pass\n\n\nclass IDNABidiError(IDNAError):\n    \"\"\"Exception when bidirectional requirements are not satisfied\"\"\"\n\n    pass\n\n\nclass InvalidCodepoint(IDNAError):\n    \"\"\"Exception when a disallowed or unallocated codepoint is used\"\"\"\n\n    pass\n\n\nclass InvalidCodepointContext(IDNAError):\n    \"\"\"Exception when the codepoint is not valid in the context it is used\"\"\"\n\n    pass\n\n\ndef _combining_class(cp: int) -> int:\n    v = unicodedata.combining(chr(cp))\n    if v == 0:\n        if not unicodedata.name(chr(cp)):\n            raise ValueError(\"Unknown character in unicodedata\")\n    return v\n\n\ndef _is_script(cp: str, script: str) -> bool:\n    return intranges_contain(ord(cp), idnadata.scripts[script])\n\n\ndef _punycode(s: str) -> bytes:\n    return s.encode(\"punycode\")\n\n\ndef _unot(s: int) -> str:\n    return \"U+{:04X}\".format(s)\n\n\ndef valid_label_length(label: Union[bytes, str]) -> bool:\n    if len(label) > 63:\n        return False\n    return True\n\n\ndef valid_string_length(label: Union[bytes, str], trailing_dot: bool) -> bool:\n    if len(label) > (254 if trailing_dot else 253):\n        return False\n    return True\n\n\ndef check_bidi(label: str, check_ltr: bool = False) -> bool:\n    # Bidi rules should only be applied if string contains RTL characters\n    bidi_label = False\n    for idx, cp in enumerate(label, 1):\n        direction = unicodedata.bidirectional(cp)\n        if direction == \"\":\n            # String likely comes from a newer version of Unicode\n            raise IDNABidiError(\"Unknown directionality in label {} at position {}\".format(repr(label), idx))\n        if direction in [\"R\", \"AL\", \"AN\"]:\n            bidi_label = True\n    if not bidi_label and not check_ltr:\n        return True\n\n    # Bidi rule 1\n    direction = unicodedata.bidirectional(label[0])\n    if direction in [\"R\", \"AL\"]:\n        rtl = True\n    elif direction == \"L\":\n        rtl = False\n    else:\n        raise IDNABidiError(\"First codepoint in label {} must be directionality L, R or AL\".format(repr(label)))\n\n    valid_ending = False\n    number_type: Optional[str] = None\n    for idx, cp in enumerate(label, 1):\n        direction = unicodedata.bidirectional(cp)\n\n        if rtl:\n            # Bidi rule 2\n            if direction not in [\n                \"R\",\n                \"AL\",\n                \"AN\",\n                \"EN\",\n                \"ES\",\n                \"CS\",\n                \"ET\",\n                \"ON\",\n                \"BN\",\n                \"NSM\",\n            ]:\n                raise IDNABidiError(\"Invalid direction for codepoint at position {} in a right-to-left label\".format(idx))\n            # Bidi rule 3\n            if direction in [\"R\", \"AL\", \"EN\", \"AN\"]:\n                valid_ending = True\n            elif direction != \"NSM\":\n                valid_ending = False\n            # Bidi rule 4\n            if direction in [\"AN\", \"EN\"]:\n                if not number_type:\n                    number_type = direction\n                else:\n                    if number_type != direction:\n                        raise IDNABidiError(\"Can not mix numeral types in a right-to-left label\")\n        else:\n            # Bidi rule 5\n            if direction not in [\"L\", \"EN\", \"ES\", \"CS\", \"ET\", \"ON\", \"BN\", \"NSM\"]:\n                raise IDNABidiError(\"Invalid direction for codepoint at position {} in a left-to-right label\".format(idx))\n            # Bidi rule 6\n            if direction in [\"L\", \"EN\"]:\n                valid_ending = True\n            elif direction != \"NSM\":\n                valid_ending = False\n\n    if not valid_ending:\n        raise IDNABidiError(\"Label ends with illegal codepoint directionality\")\n\n    return True\n\n\ndef check_initial_combiner(label: str) -> bool:\n    if unicodedata.category(label[0])[0] == \"M\":\n        raise IDNAError(\"Label begins with an illegal combining character\")\n    return True\n\n\ndef check_hyphen_ok(label: str) -> bool:\n    if label[2:4] == \"--\":\n        raise IDNAError(\"Label has disallowed hyphens in 3rd and 4th position\")\n    if label[0] == \"-\" or label[-1] == \"-\":\n        raise IDNAError(\"Label must not start or end with a hyphen\")\n    return True\n\n\ndef check_nfc(label: str) -> None:\n    if unicodedata.normalize(\"NFC\", label) != label:\n        raise IDNAError(\"Label must be in Normalization Form C\")\n\n\ndef valid_contextj(label: str, pos: int) -> bool:\n    cp_value = ord(label[pos])\n\n    if cp_value == 0x200C:\n        if pos > 0:\n            if _combining_class(ord(label[pos - 1])) == _virama_combining_class:\n                return True\n\n        ok = False\n        for i in range(pos - 1, -1, -1):\n            joining_type = idnadata.joining_types.get(ord(label[i]))\n            if joining_type == ord(\"T\"):\n                continue\n            elif joining_type in [ord(\"L\"), ord(\"D\")]:\n                ok = True\n                break\n            else:\n                break\n\n        if not ok:\n            return False\n\n        ok = False\n        for i in range(pos + 1, len(label)):\n            joining_type = idnadata.joining_types.get(ord(label[i]))\n            if joining_type == ord(\"T\"):\n                continue\n            elif joining_type in [ord(\"R\"), ord(\"D\")]:\n                ok = True\n                break\n            else:\n                break\n        return ok\n\n    if cp_value == 0x200D:\n        if pos > 0:\n            if _combining_class(ord(label[pos - 1])) == _virama_combining_class:\n                return True\n        return False\n\n    else:\n        return False\n\n\ndef valid_contexto(label: str, pos: int, exception: bool = False) -> bool:\n    cp_value = ord(label[pos])\n\n    if cp_value == 0x00B7:\n        if 0 < pos < len(label) - 1:\n            if ord(label[pos - 1]) == 0x006C and ord(label[pos + 1]) == 0x006C:\n                return True\n        return False\n\n    elif cp_value == 0x0375:\n        if pos < len(label) - 1 and len(label) > 1:\n            return _is_script(label[pos + 1], \"Greek\")\n        return False\n\n    elif cp_value == 0x05F3 or cp_value == 0x05F4:\n        if pos > 0:\n            return _is_script(label[pos - 1], \"Hebrew\")\n        return False\n\n    elif cp_value == 0x30FB:\n        for cp in label:\n            if cp == \"\\u30fb\":\n                continue\n            if _is_script(cp, \"Hiragana\") or _is_script(cp, \"Katakana\") or _is_script(cp, \"Han\"):\n                return True\n        return False\n\n    elif 0x660 <= cp_value <= 0x669:\n        for cp in label:\n            if 0x6F0 <= ord(cp) <= 0x06F9:\n                return False\n        return True\n\n    elif 0x6F0 <= cp_value <= 0x6F9:\n        for cp in label:\n            if 0x660 <= ord(cp) <= 0x0669:\n                return False\n        return True\n\n    return False\n\n\ndef check_label(label: Union[str, bytes, bytearray]) -> None:\n    if isinstance(label, (bytes, bytearray)):\n        label = label.decode(\"utf-8\")\n    if len(label) == 0:\n        raise IDNAError(\"Empty Label\")\n\n    check_nfc(label)\n    check_hyphen_ok(label)\n    check_initial_combiner(label)\n\n    for pos, cp in enumerate(label):\n        cp_value = ord(cp)\n        if intranges_contain(cp_value, idnadata.codepoint_classes[\"PVALID\"]):\n            continue\n        elif intranges_contain(cp_value, idnadata.codepoint_classes[\"CONTEXTJ\"]):\n            try:\n                if not valid_contextj(label, pos):\n                    raise InvalidCodepointContext(\n                        \"Joiner {} not allowed at position {} in {}\".format(_unot(cp_value), pos + 1, repr(label))\n                    )\n            except ValueError:\n                raise IDNAError(\n                    \"Unknown codepoint adjacent to joiner {} at position {} in {}\".format(\n                        _unot(cp_value), pos + 1, repr(label)\n                    )\n                )\n        elif intranges_contain(cp_value, idnadata.codepoint_classes[\"CONTEXTO\"]):\n            if not valid_contexto(label, pos):\n                raise InvalidCodepointContext(\n                    \"Codepoint {} not allowed at position {} in {}\".format(_unot(cp_value), pos + 1, repr(label))\n                )\n        else:\n            raise InvalidCodepoint(\n                \"Codepoint {} at position {} of {} not allowed\".format(_unot(cp_value), pos + 1, repr(label))\n            )\n\n    check_bidi(label)\n\n\ndef alabel(label: str) -> bytes:\n    try:\n        label_bytes = label.encode(\"ascii\")\n        ulabel(label_bytes)\n        if not valid_label_length(label_bytes):\n            raise IDNAError(\"Label too long\")\n        return label_bytes\n    except UnicodeEncodeError:\n        pass\n\n    check_label(label)\n    label_bytes = _alabel_prefix + _punycode(label)\n\n    if not valid_label_length(label_bytes):\n        raise IDNAError(\"Label too long\")\n\n    return label_bytes\n\n\ndef ulabel(label: Union[str, bytes, bytearray]) -> str:\n    if not isinstance(label, (bytes, bytearray)):\n        try:\n            label_bytes = label.encode(\"ascii\")\n        except UnicodeEncodeError:\n            check_label(label)\n            return label\n    else:\n        label_bytes = label\n\n    label_bytes = label_bytes.lower()\n    if label_bytes.startswith(_alabel_prefix):\n        label_bytes = label_bytes[len(_alabel_prefix) :]\n        if not label_bytes:\n            raise IDNAError(\"Malformed A-label, no Punycode eligible content found\")\n        if label_bytes.decode(\"ascii\")[-1] == \"-\":\n            raise IDNAError(\"A-label must not end with a hyphen\")\n    else:\n        check_label(label_bytes)\n        return label_bytes.decode(\"ascii\")\n\n    try:\n        label = label_bytes.decode(\"punycode\")\n    except UnicodeError:\n        raise IDNAError(\"Invalid A-label\")\n    check_label(label)\n    return label\n\n\ndef uts46_remap(domain: str, std3_rules: bool = True, transitional: bool = False) -> str:\n    \"\"\"Re-map the characters in the string according to UTS46 processing.\"\"\"\n    from .uts46data import uts46data\n\n    output = \"\"\n\n    for pos, char in enumerate(domain):\n        code_point = ord(char)\n        try:\n            uts46row = uts46data[code_point if code_point < 256 else bisect.bisect_left(uts46data, (code_point, \"Z\")) - 1]\n            status = uts46row[1]\n            replacement: Optional[str] = None\n            if len(uts46row) == 3:\n                replacement = uts46row[2]\n            if (\n                status == \"V\"\n                or (status == \"D\" and not transitional)\n                or (status == \"3\" and not std3_rules and replacement is None)\n            ):\n                output += char\n            elif replacement is not None and (\n                status == \"M\" or (status == \"3\" and not std3_rules) or (status == \"D\" and transitional)\n            ):\n                output += replacement\n            elif status != \"I\":\n                raise IndexError()\n        except IndexError:\n            raise InvalidCodepoint(\n                \"Codepoint {} not allowed at position {} in {}\".format(_unot(code_point), pos + 1, repr(domain))\n            )\n\n    return unicodedata.normalize(\"NFC\", output)\n\n\ndef encode(\n    s: Union[str, bytes, bytearray],\n    strict: bool = False,\n    uts46: bool = False,\n    std3_rules: bool = False,\n    transitional: bool = False,\n) -> bytes:\n    if not isinstance(s, str):\n        try:\n            s = str(s, \"ascii\")\n        except UnicodeDecodeError:\n            raise IDNAError(\"should pass a unicode string to the function rather than a byte string.\")\n    if uts46:\n        s = uts46_remap(s, std3_rules, transitional)\n    trailing_dot = False\n    result = []\n    if strict:\n        labels = s.split(\".\")\n    else:\n        labels = _unicode_dots_re.split(s)\n    if not labels or labels == [\"\"]:\n        raise IDNAError(\"Empty domain\")\n    if labels[-1] == \"\":\n        del labels[-1]\n        trailing_dot = True\n    for label in labels:\n        s = alabel(label)\n        if s:\n            result.append(s)\n        else:\n            raise IDNAError(\"Empty label\")\n    if trailing_dot:\n        result.append(b\"\")\n    s = b\".\".join(result)\n    if not valid_string_length(s, trailing_dot):\n        raise IDNAError(\"Domain too long\")\n    return s\n\n\ndef decode(\n    s: Union[str, bytes, bytearray],\n    strict: bool = False,\n    uts46: bool = False,\n    std3_rules: bool = False,\n) -> str:\n    try:\n        if not isinstance(s, str):\n            s = str(s, \"ascii\")\n    except UnicodeDecodeError:\n        raise IDNAError(\"Invalid ASCII in A-label\")\n    if uts46:\n        s = uts46_remap(s, std3_rules, False)\n    trailing_dot = False\n    result = []\n    if not strict:\n        labels = _unicode_dots_re.split(s)\n    else:\n        labels = s.split(\".\")\n    if not labels or labels == [\"\"]:\n        raise IDNAError(\"Empty domain\")\n    if not labels[-1]:\n        del labels[-1]\n        trailing_dot = True\n    for label in labels:\n        s = ulabel(label)\n        if s:\n            result.append(s)\n        else:\n            raise IDNAError(\"Empty label\")\n    if trailing_dot:\n        result.append(\"\")\n    return \".\".join(result)\n"}]}