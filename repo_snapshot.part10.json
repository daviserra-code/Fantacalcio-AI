{"generated_at":"2025-08-17T20:04:18.057896Z","root":"/home/runner/workspace","git":{"head":"7b7135163eb4227ef98e4c2d7b4ab78ea306bd73","branch":"main","status":" M corrections.db\n?? app_changes.json\n?? export_changes.py\n"},"filters":{"git_range":null,"since":null,"include_ext":[".cfg",".css",".env",".htm",".html",".ini",".jinja",".jinja2",".js",".json",".md",".py",".toml",".ts",".yaml",".yml"],"exclude_dirs":[".git",".ipynb_checkpoints",".mypy_cache",".pytest_cache",".pythonlibs",".venv","__pycache__","cache","chroma_db","data/exports","node_modules","venv"],"exclude_globs":["*.bmp","*.db","*.feather","*.gif","*.gz","*.ico","*.jpeg","*.jpg","*.jsonl","*.lock","*.log","*.parquet","*.png","*.sqlite","*.sqlite3","*.tar","*.webp","*.zip"],"max_file_bytes":400000},"summary":{"file_count":876,"total_bytes":9265289},"files":[{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/config/_validate_pyproject/fastjsonschema_validations.py","size":335460,"sha1":"4479396da7b4c7c85e77f750b0213d0075cad0aa","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"# noqa\n# ruff: noqa\n# flake8: noqa\n# pylint: skip-file\n# mypy: ignore-errors\n# yapf: disable\n# pylama:skip=1\n\n\n# *** PLEASE DO NOT MODIFY DIRECTLY: Automatically generated code *** \n\n\nVERSION = \"2.20.0\"\nfrom decimal import Decimal\nimport re\nfrom .fastjsonschema_exceptions import JsonSchemaValueException\n\n\nREGEX_PATTERNS = {\n    '^.*$': re.compile('^.*$'),\n    '.+': re.compile('.+'),\n    '^.+$': re.compile('^.+$'),\n    'idn-email_re_pattern': re.compile('^[^@]+@[^@]+\\\\.[^@]+\\\\Z')\n}\n\nNoneType = type(None)\n\ndef validate(data, custom_formats={}, name_prefix=None):\n    validate_https___packaging_python_org_en_latest_specifications_declaring_build_dependencies(data, custom_formats, (name_prefix or \"data\") + \"\")\n    return data\n\ndef validate_https___packaging_python_org_en_latest_specifications_declaring_build_dependencies(data, custom_formats={}, name_prefix=None):\n    if not isinstance(data, (dict)):\n        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \" must be object\", value=data, name=\"\" + (name_prefix or \"data\") + \"\", definition={'$schema': 'http://json-schema.org/draft-07/schema#', '$id': 'https://packaging.python.org/en/latest/specifications/declaring-build-dependencies/', 'title': 'Data structure for ``pyproject.toml`` files', '$$description': ['File format containing build-time configurations for the Python ecosystem. ', ':pep:`517` initially defined a build-system independent format for source trees', 'which was complemented by :pep:`518` to provide a way of specifying dependencies ', 'for building Python projects.', 'Please notice the ``project`` table (as initially defined in  :pep:`621`) is not included', 'in this schema and should be considered separately.'], 'type': 'object', 'additionalProperties': False, 'properties': {'build-system': {'type': 'object', 'description': 'Table used to store build-related data', 'additionalProperties': False, 'properties': {'requires': {'type': 'array', '$$description': ['List of dependencies in the :pep:`508` format required to execute the build', 'system. Please notice that the resulting dependency graph', '**MUST NOT contain cycles**'], 'items': {'type': 'string'}}, 'build-backend': {'type': 'string', 'description': 'Python object that will be used to perform the build according to :pep:`517`', 'format': 'pep517-backend-reference'}, 'backend-path': {'type': 'array', '$$description': ['List of directories to be prepended to ``sys.path`` when loading the', 'back-end, and running its hooks'], 'items': {'type': 'string', '$comment': 'Should be a path (TODO: enforce it with format?)'}}}, 'required': ['requires']}, 'project': {'$schema': 'http://json-schema.org/draft-07/schema#', '$id': 'https://packaging.python.org/en/latest/specifications/pyproject-toml/', 'title': 'Package metadata stored in the ``project`` table', '$$description': ['Data structure for the **project** table inside ``pyproject.toml``', '(as initially defined in :pep:`621`)'], 'type': 'object', 'properties': {'name': {'type': 'string', 'description': 'The name (primary identifier) of the project. MUST be statically defined.', 'format': 'pep508-identifier'}, 'version': {'type': 'string', 'description': 'The version of the project as supported by :pep:`440`.', 'format': 'pep440'}, 'description': {'type': 'string', '$$description': ['The `summary description of the project', '<https://packaging.python.org/specifications/core-metadata/#summary>`_']}, 'readme': {'$$description': ['`Full/detailed description of the project in the form of a README', '<https://peps.python.org/pep-0621/#readme>`_', \"with meaning similar to the one defined in `core metadata's Description\", '<https://packaging.python.org/specifications/core-metadata/#description>`_'], 'oneOf': [{'type': 'string', '$$description': ['Relative path to a text file (UTF-8) containing the full description', 'of the project. If the file path ends in case-insensitive ``.md`` or', '``.rst`` suffixes, then the content-type is respectively', '``text/markdown`` or ``text/x-rst``']}, {'type': 'object', 'allOf': [{'anyOf': [{'properties': {'file': {'type': 'string', '$$description': ['Relative path to a text file containing the full description', 'of the project.']}}, 'required': ['file']}, {'properties': {'text': {'type': 'string', 'description': 'Full text describing the project.'}}, 'required': ['text']}]}, {'properties': {'content-type': {'type': 'string', '$$description': ['Content-type (:rfc:`1341`) of the full description', '(e.g. ``text/markdown``). The ``charset`` parameter is assumed', 'UTF-8 when not present.'], '$comment': 'TODO: add regex pattern or format?'}}, 'required': ['content-type']}]}]}, 'requires-python': {'type': 'string', 'format': 'pep508-versionspec', '$$description': ['`The Python version requirements of the project', '<https://packaging.python.org/specifications/core-metadata/#requires-python>`_.']}, 'license': {'description': '`Project license <https://peps.python.org/pep-0621/#license>`_.', 'oneOf': [{'properties': {'file': {'type': 'string', '$$description': ['Relative path to the file (UTF-8) which contains the license for the', 'project.']}}, 'required': ['file']}, {'properties': {'text': {'type': 'string', '$$description': ['The license of the project whose meaning is that of the', '`License field from the core metadata', '<https://packaging.python.org/specifications/core-metadata/#license>`_.']}}, 'required': ['text']}]}, 'authors': {'type': 'array', 'items': {'$ref': '#/definitions/author'}, '$$description': [\"The people or organizations considered to be the 'authors' of the project.\", 'The exact meaning is open to interpretation (e.g. original or primary authors,', 'current maintainers, or owners of the package).']}, 'maintainers': {'type': 'array', 'items': {'$ref': '#/definitions/author'}, '$$description': [\"The people or organizations considered to be the 'maintainers' of the project.\", 'Similarly to ``authors``, the exact meaning is open to interpretation.']}, 'keywords': {'type': 'array', 'items': {'type': 'string'}, 'description': 'List of keywords to assist searching for the distribution in a larger catalog.'}, 'classifiers': {'type': 'array', 'items': {'type': 'string', 'format': 'trove-classifier', 'description': '`PyPI classifier <https://pypi.org/classifiers/>`_.'}, '$$description': ['`Trove classifiers <https://pypi.org/classifiers/>`_', 'which apply to the project.']}, 'urls': {'type': 'object', 'description': 'URLs associated with the project in the form ``label => value``.', 'additionalProperties': False, 'patternProperties': {'^.+$': {'type': 'string', 'format': 'url'}}}, 'scripts': {'$ref': '#/definitions/entry-point-group', '$$description': ['Instruct the installer to create command-line wrappers for the given', '`entry points <https://packaging.python.org/specifications/entry-points/>`_.']}, 'gui-scripts': {'$ref': '#/definitions/entry-point-group', '$$description': ['Instruct the installer to create GUI wrappers for the given', '`entry points <https://packaging.python.org/specifications/entry-points/>`_.', 'The difference between ``scripts`` and ``gui-scripts`` is only relevant in', 'Windows.']}, 'entry-points': {'$$description': ['Instruct the installer to expose the given modules/functions via', '``entry-point`` discovery mechanism (useful for plugins).', 'More information available in the `Python packaging guide', '<https://packaging.python.org/specifications/entry-points/>`_.'], 'propertyNames': {'format': 'python-entrypoint-group'}, 'additionalProperties': False, 'patternProperties': {'^.+$': {'$ref': '#/definitions/entry-point-group'}}}, 'dependencies': {'type': 'array', 'description': 'Project (mandatory) dependencies.', 'items': {'$ref': '#/definitions/dependency'}}, 'optional-dependencies': {'type': 'object', 'description': 'Optional dependency for the project', 'propertyNames': {'format': 'pep508-identifier'}, 'additionalProperties': False, 'patternProperties': {'^.+$': {'type': 'array', 'items': {'$ref': '#/definitions/dependency'}}}}, 'dynamic': {'type': 'array', '$$description': ['Specifies which fields are intentionally unspecified and expected to be', 'dynamically provided by build tools'], 'items': {'enum': ['version', 'description', 'readme', 'requires-python', 'license', 'authors', 'maintainers', 'keywords', 'classifiers', 'urls', 'scripts', 'gui-scripts', 'entry-points', 'dependencies', 'optional-dependencies']}}}, 'required': ['name'], 'additionalProperties': False, 'if': {'not': {'required': ['dynamic'], 'properties': {'dynamic': {'contains': {'const': 'version'}, '$$description': ['version is listed in ``dynamic``']}}}, '$$comment': ['According to :pep:`621`:', '    If the core metadata specification lists a field as \"Required\", then', '    the metadata MUST specify the field statically or list it in dynamic', 'In turn, `core metadata`_ defines:', '    The required fields are: Metadata-Version, Name, Version.', '    All the other fields are optional.', 'Since ``Metadata-Version`` is defined by the build back-end, ``name`` and', '``version`` are the only mandatory information in ``pyproject.toml``.', '.. _core metadata: https://packaging.python.org/specifications/core-metadata/']}, 'then': {'required': ['version'], '$$description': ['version should be statically defined in the ``version`` field']}, 'definitions': {'author': {'$id': '#/definitions/author', 'title': 'Author or Maintainer', '$comment': 'https://peps.python.org/pep-0621/#authors-maintainers', 'type': 'object', 'additionalProperties': False, 'properties': {'name': {'type': 'string', '$$description': ['MUST be a valid email name, i.e. whatever can be put as a name, before an', 'email, in :rfc:`822`.']}, 'email': {'type': 'string', 'format': 'idn-email', 'description': 'MUST be a valid email address'}}}, 'entry-point-group': {'$id': '#/definitions/entry-point-group', 'title': 'Entry-points', 'type': 'object', '$$description': ['Entry-points are grouped together to indicate what sort of capabilities they', 'provide.', 'See the `packaging guides', '<https://packaging.python.org/specifications/entry-points/>`_', 'and `setuptools docs', '<https://setuptools.pypa.io/en/latest/userguide/entry_point.html>`_', 'for more information.'], 'propertyNames': {'format': 'python-entrypoint-name'}, 'additionalProperties': False, 'patternProperties': {'^.+$': {'type': 'string', '$$description': ['Reference to a Python object. It is either in the form', '``importable.module``, or ``importable.module:object.attr``.'], 'format': 'python-entrypoint-reference', '$comment': 'https://packaging.python.org/specifications/entry-points/'}}}, 'dependency': {'$id': '#/definitions/dependency', 'title': 'Dependency', 'type': 'string', 'description': 'Project dependency specification according to PEP 508', 'format': 'pep508'}}}, 'tool': {'type': 'object', 'properties': {'distutils': {'$schema': 'http://json-schema.org/draft-07/schema#', '$id': 'https://setuptools.pypa.io/en/latest/deprecated/distutils/configfile.html', 'title': '``tool.distutils`` table', '$$description': ['**EXPERIMENTAL** (NOT OFFICIALLY SUPPORTED): Use ``tool.distutils``', 'subtables to configure arguments for ``distutils`` commands.', 'Originally, ``distutils`` allowed developers to configure arguments for', '``setup.py`` commands via `distutils configuration files', '<https://setuptools.pypa.io/en/latest/deprecated/distutils/configfile.html>`_.', 'See also `the old Python docs <https://docs.python.org/3.11/install/>_`.'], 'type': 'object', 'properties': {'global': {'type': 'object', 'description': 'Global options applied to all ``distutils`` commands'}}, 'patternProperties': {'.+': {'type': 'object'}}, '$comment': 'TODO: Is there a practical way of making this schema more specific?'}, 'setuptools': {'$schema': 'http://json-schema.org/draft-07/schema#', '$id': 'https://setuptools.pypa.io/en/latest/userguide/pyproject_config.html', 'title': '``tool.setuptools`` table', '$$description': ['``setuptools``-specific configurations that can be set by users that require', 'customization.', 'These configurations are completely optional and probably can be skipped when', 'creating simple packages. They are equivalent to some of the `Keywords', '<https://setuptools.pypa.io/en/latest/references/keywords.html>`_', 'used by the ``setup.py`` file, and can be set via the ``tool.setuptools`` table.', 'It considers only ``setuptools`` `parameters', '<https://setuptools.pypa.io/en/latest/userguide/pyproject_config.html#setuptools-specific-configuration>`_', 'that are not covered by :pep:`621`; and intentionally excludes ``dependency_links``', 'and ``setup_requires`` (incompatible with modern workflows/standards).'], 'type': 'object', 'additionalProperties': False, 'properties': {'platforms': {'type': 'array', 'items': {'type': 'string'}}, 'provides': {'$$description': ['Package and virtual package names contained within this package', '**(not supported by pip)**'], 'type': 'array', 'items': {'type': 'string', 'format': 'pep508-identifier'}}, 'obsoletes': {'$$description': ['Packages which this package renders obsolete', '**(not supported by pip)**'], 'type': 'array', 'items': {'type': 'string', 'format': 'pep508-identifier'}}, 'zip-safe': {'$$description': ['Whether the project can be safely installed and run from a zip file.', '**OBSOLETE**: only relevant for ``pkg_resources``, ``easy_install`` and', '``setup.py install`` in the context of ``eggs`` (**DEPRECATED**).'], 'type': 'boolean'}, 'script-files': {'$$description': ['Legacy way of defining scripts (entry-points are preferred).', 'Equivalent to the ``script`` keyword in ``setup.py``', '(it was renamed to avoid confusion with entry-point based ``project.scripts``', 'defined in :pep:`621`).', '**DISCOURAGED**: generic script wrappers are tricky and may not work properly.', 'Whenever possible, please use ``project.scripts`` instead.'], 'type': 'array', 'items': {'type': 'string'}, '$comment': 'TODO: is this field deprecated/should be removed?'}, 'eager-resources': {'$$description': ['Resources that should be extracted together, if any of them is needed,', 'or if any C extensions included in the project are imported.', '**OBSOLETE**: only relevant for ``pkg_resources``, ``easy_install`` and', '``setup.py install`` in the context of ``eggs`` (**DEPRECATED**).'], 'type': 'array', 'items': {'type': 'string'}}, 'packages': {'$$description': ['Packages that should be included in the distribution.', 'It can be given either as a list of package identifiers', 'or as a ``dict``-like structure with a single key ``find``', 'which corresponds to a dynamic call to', '``setuptools.config.expand.find_packages`` function.', 'The ``find`` key is associated with a nested ``dict``-like structure that can', 'contain ``where``, ``include``, ``exclude`` and ``namespaces`` keys,', 'mimicking the keyword arguments of the associated function.'], 'oneOf': [{'title': 'Array of Python package identifiers', 'type': 'array', 'items': {'$ref': '#/definitions/package-name'}}, {'$ref': '#/definitions/find-directive'}]}, 'package-dir': {'$$description': [':class:`dict`-like structure mapping from package names to directories where their', 'code can be found.', 'The empty string (as key) means that all packages are contained inside', 'the given directory will be included in the distribution.'], 'type': 'object', 'additionalProperties': False, 'propertyNames': {'anyOf': [{'const': ''}, {'$ref': '#/definitions/package-name'}]}, 'patternProperties': {'^.*$': {'type': 'string'}}}, 'package-data': {'$$description': ['Mapping from package names to lists of glob patterns.', 'Usually this option is not needed when using ``include-package-data = true``', 'For more information on how to include data files, check ``setuptools`` `docs', '<https://setuptools.pypa.io/en/latest/userguide/datafiles.html>`_.'], 'type': 'object', 'additionalProperties': False, 'propertyNames': {'anyOf': [{'type': 'string', 'format': 'python-module-name'}, {'const': '*'}]}, 'patternProperties': {'^.*$': {'type': 'array', 'items': {'type': 'string'}}}}, 'include-package-data': {'$$description': ['Automatically include any data files inside the package directories', 'that are specified by ``MANIFEST.in``', 'For more information on how to include data files, check ``setuptools`` `docs', '<https://setuptools.pypa.io/en/latest/userguide/datafiles.html>`_.'], 'type': 'boolean'}, 'exclude-package-data': {'$$description': ['Mapping from package names to lists of glob patterns that should be excluded', 'For more information on how to include data files, check ``setuptools`` `docs', '<https://setuptools.pypa.io/en/latest/userguide/datafiles.html>`_.'], 'type': 'object', 'additionalProperties': False, 'propertyNames': {'anyOf': [{'type': 'string', 'format': 'python-module-name'}, {'const': '*'}]}, 'patternProperties': {'^.*$': {'type': 'array', 'items': {'type': 'string'}}}}, 'namespace-packages': {'type': 'array', 'items': {'type': 'string', 'format': 'python-module-name-relaxed'}, '$comment': 'https://setuptools.pypa.io/en/latest/userguide/package_discovery.html', 'description': '**DEPRECATED**: use implicit namespaces instead (:pep:`420`).'}, 'py-modules': {'description': 'Modules that setuptools will manipulate', 'type': 'array', 'items': {'type': 'string', 'format': 'python-module-name-relaxed'}, '$comment': 'TODO: clarify the relationship with ``packages``'}, 'ext-modules': {'description': 'Extension modules to be compiled by setuptools', 'type': 'array', 'items': {'$ref': '#/definitions/ext-module'}}, 'data-files': {'$$description': ['``dict``-like structure where each key represents a directory and', 'the value is a list of glob patterns that should be installed in them.', '**DISCOURAGED**: please notice this might not work as expected with wheels.', 'Whenever possible, consider using data files inside the package directories', '(or create a new namespace package that only contains data files).', 'See `data files support', '<https://setuptools.pypa.io/en/latest/userguide/datafiles.html>`_.'], 'type': 'object', 'patternProperties': {'^.*$': {'type': 'array', 'items': {'type': 'string'}}}}, 'cmdclass': {'$$description': ['Mapping of distutils-style command names to ``setuptools.Command`` subclasses', 'which in turn should be represented by strings with a qualified class name', '(i.e., \"dotted\" form with module), e.g.::\\n\\n', '    cmdclass = {mycmd = \"pkg.subpkg.module.CommandClass\"}\\n\\n', 'The command class should be a directly defined at the top-level of the', 'containing module (no class nesting).'], 'type': 'object', 'patternProperties': {'^.*$': {'type': 'string', 'format': 'python-qualified-identifier'}}}, 'license-files': {'type': 'array', 'items': {'type': 'string'}, '$$description': ['**PROVISIONAL**: list of glob patterns for all license files being distributed.', '(likely to become standard with :pep:`639`).', \"By default: ``['LICEN[CS]E*', 'COPYING*', 'NOTICE*', 'AUTHORS*']``\"], '$comment': 'TODO: revise if PEP 639 is accepted. Probably ``project.license-files``?'}, 'dynamic': {'type': 'object', 'description': 'Instructions for loading :pep:`621`-related metadata dynamically', 'additionalProperties': False, 'properties': {'version': {'$$description': ['A version dynamically loaded via either the ``attr:`` or ``file:``', 'directives. Please make sure the given file or attribute respects :pep:`440`.', 'Also ensure to set ``project.dynamic`` accordingly.'], 'oneOf': [{'$ref': '#/definitions/attr-directive'}, {'$ref': '#/definitions/file-directive'}]}, 'classifiers': {'$ref': '#/definitions/file-directive'}, 'description': {'$ref': '#/definitions/file-directive'}, 'entry-points': {'$ref': '#/definitions/file-directive'}, 'dependencies': {'$ref': '#/definitions/file-directive-for-dependencies'}, 'optional-dependencies': {'type': 'object', 'propertyNames': {'type': 'string', 'format': 'pep508-identifier'}, 'additionalProperties': False, 'patternProperties': {'.+': {'$ref': '#/definitions/file-directive-for-dependencies'}}}, 'readme': {'type': 'object', 'anyOf': [{'$ref': '#/definitions/file-directive'}, {'type': 'object', 'properties': {'content-type': {'type': 'string'}, 'file': {'$ref': '#/definitions/file-directive/properties/file'}}, 'additionalProperties': False}], 'required': ['file']}}}}, 'definitions': {'package-name': {'$id': '#/definitions/package-name', 'title': 'Valid package name', 'description': 'Valid package name (importable or :pep:`561`).', 'type': 'string', 'anyOf': [{'type': 'string', 'format': 'python-module-name-relaxed'}, {'type': 'string', 'format': 'pep561-stub-name'}]}, 'ext-module': {'$id': '#/definitions/ext-module', 'title': 'Extension module', 'description': 'Parameters to construct a :class:`setuptools.Extension` object', 'type': 'object', 'required': ['name', 'sources'], 'additionalProperties': False, 'properties': {'name': {'type': 'string', 'format': 'python-module-name-relaxed'}, 'sources': {'type': 'array', 'items': {'type': 'string'}}, 'include-dirs': {'type': 'array', 'items': {'type': 'string'}}, 'define-macros': {'type': 'array', 'items': {'type': 'array', 'items': [{'description': 'macro name', 'type': 'string'}, {'description': 'macro value', 'oneOf': [{'type': 'string'}, {'type': 'null'}]}], 'additionalItems': False}}, 'undef-macros': {'type': 'array', 'items': {'type': 'string'}}, 'library-dirs': {'type': 'array', 'items': {'type': 'string'}}, 'libraries': {'type': 'array', 'items': {'type': 'string'}}, 'runtime-library-dirs': {'type': 'array', 'items': {'type': 'string'}}, 'extra-objects': {'type': 'array', 'items': {'type': 'string'}}, 'extra-compile-args': {'type': 'array', 'items': {'type': 'string'}}, 'extra-link-args': {'type': 'array', 'items': {'type': 'string'}}, 'export-symbols': {'type': 'array', 'items': {'type': 'string'}}, 'swig-opts': {'type': 'array', 'items': {'type': 'string'}}, 'depends': {'type': 'array', 'items': {'type': 'string'}}, 'language': {'type': 'string'}, 'optional': {'type': 'boolean'}, 'py-limited-api': {'type': 'boolean'}}}, 'file-directive': {'$id': '#/definitions/file-directive', 'title': \"'file:' directive\", 'description': 'Value is read from a file (or list of files and then concatenated)', 'type': 'object', 'additionalProperties': False, 'properties': {'file': {'oneOf': [{'type': 'string'}, {'type': 'array', 'items': {'type': 'string'}}]}}, 'required': ['file']}, 'file-directive-for-dependencies': {'title': \"'file:' directive for dependencies\", 'allOf': [{'$$description': ['**BETA**: subset of the ``requirements.txt`` format', 'without ``pip`` flags and options', '(one :pep:`508`-compliant string per line,', 'lines that are blank or start with ``#`` are excluded).', 'See `dynamic metadata', '<https://setuptools.pypa.io/en/latest/userguide/pyproject_config.html#dynamic-metadata>`_.']}, {'$ref': '#/definitions/file-directive'}]}, 'attr-directive': {'title': \"'attr:' directive\", '$id': '#/definitions/attr-directive', '$$description': ['Value is read from a module attribute. Supports callables and iterables;', 'unsupported types are cast via ``str()``'], 'type': 'object', 'additionalProperties': False, 'properties': {'attr': {'type': 'string', 'format': 'python-qualified-identifier'}}, 'required': ['attr']}, 'find-directive': {'$id': '#/definitions/find-directive', 'title': \"'find:' directive\", 'type': 'object', 'additionalProperties': False, 'properties': {'find': {'type': 'object', '$$description': ['Dynamic `package discovery', '<https://setuptools.pypa.io/en/latest/userguide/package_discovery.html>`_.'], 'additionalProperties': False, 'properties': {'where': {'description': 'Directories to be searched for packages (Unix-style relative path)', 'type': 'array', 'items': {'type': 'string'}}, 'exclude': {'type': 'array', '$$description': ['Exclude packages that match the values listed in this field.', \"Can container shell-style wildcards (e.g. ``'pkg.*'``)\"], 'items': {'type': 'string'}}, 'include': {'type': 'array', '$$description': ['Restrict the found packages to just the ones listed in this field.', \"Can container shell-style wildcards (e.g. ``'pkg.*'``)\"], 'items': {'type': 'string'}}, 'namespaces': {'type': 'boolean', '$$description': ['When ``True``, directories without a ``__init__.py`` file will also', 'be scanned for :pep:`420`-style implicit namespaces']}}}}}}}}}}, 'project': {'$schema': 'http://json-schema.org/draft-07/schema#', '$id': 'https://packaging.python.org/en/latest/specifications/pyproject-toml/', 'title': 'Package metadata stored in the ``project`` table', '$$description': ['Data structure for the **project** table inside ``pyproject.toml``', '(as initially defined in :pep:`621`)'], 'type': 'object', 'properties': {'name': {'type': 'string', 'description': 'The name (primary identifier) of the project. MUST be statically defined.', 'format': 'pep508-identifier'}, 'version': {'type': 'string', 'description': 'The version of the project as supported by :pep:`440`.', 'format': 'pep440'}, 'description': {'type': 'string', '$$description': ['The `summary description of the project', '<https://packaging.python.org/specifications/core-metadata/#summary>`_']}, 'readme': {'$$description': ['`Full/detailed description of the project in the form of a README', '<https://peps.python.org/pep-0621/#readme>`_', \"with meaning similar to the one defined in `core metadata's Description\", '<https://packaging.python.org/specifications/core-metadata/#description>`_'], 'oneOf': [{'type': 'string', '$$description': ['Relative path to a text file (UTF-8) containing the full description', 'of the project. If the file path ends in case-insensitive ``.md`` or', '``.rst`` suffixes, then the content-type is respectively', '``text/markdown`` or ``text/x-rst``']}, {'type': 'object', 'allOf': [{'anyOf': [{'properties': {'file': {'type': 'string', '$$description': ['Relative path to a text file containing the full description', 'of the project.']}}, 'required': ['file']}, {'properties': {'text': {'type': 'string', 'description': 'Full text describing the project.'}}, 'required': ['text']}]}, {'properties': {'content-type': {'type': 'string', '$$description': ['Content-type (:rfc:`1341`) of the full description', '(e.g. ``text/markdown``). The ``charset`` parameter is assumed', 'UTF-8 when not present.'], '$comment': 'TODO: add regex pattern or format?'}}, 'required': ['content-type']}]}]}, 'requires-python': {'type': 'string', 'format': 'pep508-versionspec', '$$description': ['`The Python version requirements of the project', '<https://packaging.python.org/specifications/core-metadata/#requires-python>`_.']}, 'license': {'description': '`Project license <https://peps.python.org/pep-0621/#license>`_.', 'oneOf': [{'properties': {'file': {'type': 'string', '$$description': ['Relative path to the file (UTF-8) which contains the license for the', 'project.']}}, 'required': ['file']}, {'properties': {'text': {'type': 'string', '$$description': ['The license of the project whose meaning is that of the', '`License field from the core metadata', '<https://packaging.python.org/specifications/core-metadata/#license>`_.']}}, 'required': ['text']}]}, 'authors': {'type': 'array', 'items': {'$ref': '#/definitions/author'}, '$$description': [\"The people or organizations considered to be the 'authors' of the project.\", 'The exact meaning is open to interpretation (e.g. original or primary authors,', 'current maintainers, or owners of the package).']}, 'maintainers': {'type': 'array', 'items': {'$ref': '#/definitions/author'}, '$$description': [\"The people or organizations considered to be the 'maintainers' of the project.\", 'Similarly to ``authors``, the exact meaning is open to interpretation.']}, 'keywords': {'type': 'array', 'items': {'type': 'string'}, 'description': 'List of keywords to assist searching for the distribution in a larger catalog.'}, 'classifiers': {'type': 'array', 'items': {'type': 'string', 'format': 'trove-classifier', 'description': '`PyPI classifier <https://pypi.org/classifiers/>`_.'}, '$$description': ['`Trove classifiers <https://pypi.org/classifiers/>`_', 'which apply to the project.']}, 'urls': {'type': 'object', 'description': 'URLs associated with the project in the form ``label => value``.', 'additionalProperties': False, 'patternProperties': {'^.+$': {'type': 'string', 'format': 'url'}}}, 'scripts': {'$ref': '#/definitions/entry-point-group', '$$description': ['Instruct the installer to create command-line wrappers for the given', '`entry points <https://packaging.python.org/specifications/entry-points/>`_.']}, 'gui-scripts': {'$ref': '#/definitions/entry-point-group', '$$description': ['Instruct the installer to create GUI wrappers for the given', '`entry points <https://packaging.python.org/specifications/entry-points/>`_.', 'The difference between ``scripts`` and ``gui-scripts`` is only relevant in', 'Windows.']}, 'entry-points': {'$$description': ['Instruct the installer to expose the given modules/functions via', '``entry-point`` discovery mechanism (useful for plugins).', 'More information available in the `Python packaging guide', '<https://packaging.python.org/specifications/entry-points/>`_.'], 'propertyNames': {'format': 'python-entrypoint-group'}, 'additionalProperties': False, 'patternProperties': {'^.+$': {'$ref': '#/definitions/entry-point-group'}}}, 'dependencies': {'type': 'array', 'description': 'Project (mandatory) dependencies.', 'items': {'$ref': '#/definitions/dependency'}}, 'optional-dependencies': {'type': 'object', 'description': 'Optional dependency for the project', 'propertyNames': {'format': 'pep508-identifier'}, 'additionalProperties': False, 'patternProperties': {'^.+$': {'type': 'array', 'items': {'$ref': '#/definitions/dependency'}}}}, 'dynamic': {'type': 'array', '$$description': ['Specifies which fields are intentionally unspecified and expected to be', 'dynamically provided by build tools'], 'items': {'enum': ['version', 'description', 'readme', 'requires-python', 'license', 'authors', 'maintainers', 'keywords', 'classifiers', 'urls', 'scripts', 'gui-scripts', 'entry-points', 'dependencies', 'optional-dependencies']}}}, 'required': ['name'], 'additionalProperties': False, 'if': {'not': {'required': ['dynamic'], 'properties': {'dynamic': {'contains': {'const': 'version'}, '$$description': ['version is listed in ``dynamic``']}}}, '$$comment': ['According to :pep:`621`:', '    If the core metadata specification lists a field as \"Required\", then', '    the metadata MUST specify the field statically or list it in dynamic', 'In turn, `core metadata`_ defines:', '    The required fields are: Metadata-Version, Name, Version.', '    All the other fields are optional.', 'Since ``Metadata-Version`` is defined by the build back-end, ``name`` and', '``version`` are the only mandatory information in ``pyproject.toml``.', '.. _core metadata: https://packaging.python.org/specifications/core-metadata/']}, 'then': {'required': ['version'], '$$description': ['version should be statically defined in the ``version`` field']}, 'definitions': {'author': {'$id': '#/definitions/author', 'title': 'Author or Maintainer', '$comment': 'https://peps.python.org/pep-0621/#authors-maintainers', 'type': 'object', 'additionalProperties': False, 'properties': {'name': {'type': 'string', '$$description': ['MUST be a valid email name, i.e. whatever can be put as a name, before an', 'email, in :rfc:`822`.']}, 'email': {'type': 'string', 'format': 'idn-email', 'description': 'MUST be a valid email address'}}}, 'entry-point-group': {'$id': '#/definitions/entry-point-group', 'title': 'Entry-points', 'type': 'object', '$$description': ['Entry-points are grouped together to indicate what sort of capabilities they', 'provide.', 'See the `packaging guides', '<https://packaging.python.org/specifications/entry-points/>`_', 'and `setuptools docs', '<https://setuptools.pypa.io/en/latest/userguide/entry_point.html>`_', 'for more information.'], 'propertyNames': {'format': 'python-entrypoint-name'}, 'additionalProperties': False, 'patternProperties': {'^.+$': {'type': 'string', '$$description': ['Reference to a Python object. It is either in the form', '``importable.module``, or ``importable.module:object.attr``.'], 'format': 'python-entrypoint-reference', '$comment': 'https://packaging.python.org/specifications/entry-points/'}}}, 'dependency': {'$id': '#/definitions/dependency', 'title': 'Dependency', 'type': 'string', 'description': 'Project dependency specification according to PEP 508', 'format': 'pep508'}}}}, rule='type')\n    data_is_dict = isinstance(data, dict)\n    if data_is_dict:\n        data_keys = set(data.keys())\n        if \"build-system\" in data_keys:\n            data_keys.remove(\"build-system\")\n            data__buildsystem = data[\"build-system\"]\n            if not isinstance(data__buildsystem, (dict)):\n                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".build-system must be object\", value=data__buildsystem, name=\"\" + (name_prefix or \"data\") + \".build-system\", definition={'type': 'object', 'description': 'Table used to store build-related data', 'additionalProperties': False, 'properties': {'requires': {'type': 'array', '$$description': ['List of dependencies in the :pep:`508` format required to execute the build', 'system. Please notice that the resulting dependency graph', '**MUST NOT contain cycles**'], 'items': {'type': 'string'}}, 'build-backend': {'type': 'string', 'description': 'Python object that will be used to perform the build according to :pep:`517`', 'format': 'pep517-backend-reference'}, 'backend-path': {'type': 'array', '$$description': ['List of directories to be prepended to ``sys.path`` when loading the', 'back-end, and running its hooks'], 'items': {'type': 'string', '$comment': 'Should be a path (TODO: enforce it with format?)'}}}, 'required': ['requires']}, rule='type')\n            data__buildsystem_is_dict = isinstance(data__buildsystem, dict)\n            if data__buildsystem_is_dict:\n                data__buildsystem__missing_keys = set(['requires']) - data__buildsystem.keys()\n                if data__buildsystem__missing_keys:\n                    raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".build-system must contain \" + (str(sorted(data__buildsystem__missing_keys)) + \" properties\"), value=data__buildsystem, name=\"\" + (name_prefix or \"data\") + \".build-system\", definition={'type': 'object', 'description': 'Table used to store build-related data', 'additionalProperties': False, 'properties': {'requires': {'type': 'array', '$$description': ['List of dependencies in the :pep:`508` format required to execute the build', 'system. Please notice that the resulting dependency graph', '**MUST NOT contain cycles**'], 'items': {'type': 'string'}}, 'build-backend': {'type': 'string', 'description': 'Python object that will be used to perform the build according to :pep:`517`', 'format': 'pep517-backend-reference'}, 'backend-path': {'type': 'array', '$$description': ['List of directories to be prepended to ``sys.path`` when loading the', 'back-end, and running its hooks'], 'items': {'type': 'string', '$comment': 'Should be a path (TODO: enforce it with format?)'}}}, 'required': ['requires']}, rule='required')\n                data__buildsystem_keys = set(data__buildsystem.keys())\n                if \"requires\" in data__buildsystem_keys:\n                    data__buildsystem_keys.remove(\"requires\")\n                    data__buildsystem__requires = data__buildsystem[\"requires\"]\n                    if not isinstance(data__buildsystem__requires, (list, tuple)):\n                        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".build-system.requires must be array\", value=data__buildsystem__requires, name=\"\" + (name_prefix or \"data\") + \".build-system.requires\", definition={'type': 'array', '$$description': ['List of dependencies in the :pep:`508` format required to execute the build', 'system. Please notice that the resulting dependency graph', '**MUST NOT contain cycles**'], 'items': {'type': 'string'}}, rule='type')\n                    data__buildsystem__requires_is_list = isinstance(data__buildsystem__requires, (list, tuple))\n                    if data__buildsystem__requires_is_list:\n                        data__buildsystem__requires_len = len(data__buildsystem__requires)\n                        for data__buildsystem__requires_x, data__buildsystem__requires_item in enumerate(data__buildsystem__requires):\n                            if not isinstance(data__buildsystem__requires_item, (str)):\n                                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".build-system.requires[{data__buildsystem__requires_x}]\".format(**locals()) + \" must be string\", value=data__buildsystem__requires_item, name=\"\" + (name_prefix or \"data\") + \".build-system.requires[{data__buildsystem__requires_x}]\".format(**locals()) + \"\", definition={'type': 'string'}, rule='type')\n                if \"build-backend\" in data__buildsystem_keys:\n                    data__buildsystem_keys.remove(\"build-backend\")\n                    data__buildsystem__buildbackend = data__buildsystem[\"build-backend\"]\n                    if not isinstance(data__buildsystem__buildbackend, (str)):\n                        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".build-system.build-backend must be string\", value=data__buildsystem__buildbackend, name=\"\" + (name_prefix or \"data\") + \".build-system.build-backend\", definition={'type': 'string', 'description': 'Python object that will be used to perform the build according to :pep:`517`', 'format': 'pep517-backend-reference'}, rule='type')\n                    if isinstance(data__buildsystem__buildbackend, str):\n                        if not custom_formats[\"pep517-backend-reference\"](data__buildsystem__buildbackend):\n                            raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".build-system.build-backend must be pep517-backend-reference\", value=data__buildsystem__buildbackend, name=\"\" + (name_prefix or \"data\") + \".build-system.build-backend\", definition={'type': 'string', 'description': 'Python object that will be used to perform the build according to :pep:`517`', 'format': 'pep517-backend-reference'}, rule='format')\n                if \"backend-path\" in data__buildsystem_keys:\n                    data__buildsystem_keys.remove(\"backend-path\")\n                    data__buildsystem__backendpath = data__buildsystem[\"backend-path\"]\n                    if not isinstance(data__buildsystem__backendpath, (list, tuple)):\n                        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".build-system.backend-path must be array\", value=data__buildsystem__backendpath, name=\"\" + (name_prefix or \"data\") + \".build-system.backend-path\", definition={'type': 'array', '$$description': ['List of directories to be prepended to ``sys.path`` when loading the', 'back-end, and running its hooks'], 'items': {'type': 'string', '$comment': 'Should be a path (TODO: enforce it with format?)'}}, rule='type')\n                    data__buildsystem__backendpath_is_list = isinstance(data__buildsystem__backendpath, (list, tuple))\n                    if data__buildsystem__backendpath_is_list:\n                        data__buildsystem__backendpath_len = len(data__buildsystem__backendpath)\n                        for data__buildsystem__backendpath_x, data__buildsystem__backendpath_item in enumerate(data__buildsystem__backendpath):\n                            if not isinstance(data__buildsystem__backendpath_item, (str)):\n                                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".build-system.backend-path[{data__buildsystem__backendpath_x}]\".format(**locals()) + \" must be string\", value=data__buildsystem__backendpath_item, name=\"\" + (name_prefix or \"data\") + \".build-system.backend-path[{data__buildsystem__backendpath_x}]\".format(**locals()) + \"\", definition={'type': 'string', '$comment': 'Should be a path (TODO: enforce it with format?)'}, rule='type')\n                if data__buildsystem_keys:\n                    raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".build-system must not contain \"+str(data__buildsystem_keys)+\" properties\", value=data__buildsystem, name=\"\" + (name_prefix or \"data\") + \".build-system\", definition={'type': 'object', 'description': 'Table used to store build-related data', 'additionalProperties': False, 'properties': {'requires': {'type': 'array', '$$description': ['List of dependencies in the :pep:`508` format required to execute the build', 'system. Please notice that the resulting dependency graph', '**MUST NOT contain cycles**'], 'items': {'type': 'string'}}, 'build-backend': {'type': 'string', 'description': 'Python object that will be used to perform the build according to :pep:`517`', 'format': 'pep517-backend-reference'}, 'backend-path': {'type': 'array', '$$description': ['List of directories to be prepended to ``sys.path`` when loading the', 'back-end, and running its hooks'], 'items': {'type': 'string', '$comment': 'Should be a path (TODO: enforce it with format?)'}}}, 'required': ['requires']}, rule='additionalProperties')\n        if \"project\" in data_keys:\n            data_keys.remove(\"project\")\n            data__project = data[\"project\"]\n            validate_https___packaging_python_org_en_latest_specifications_pyproject_toml(data__project, custom_formats, (name_prefix or \"data\") + \".project\")\n        if \"tool\" in data_keys:\n            data_keys.remove(\"tool\")\n            data__tool = data[\"tool\"]\n            if not isinstance(data__tool, (dict)):\n                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".tool must be object\", value=data__tool, name=\"\" + (name_prefix or \"data\") + \".tool\", definition={'type': 'object', 'properties': {'distutils': {'$schema': 'http://json-schema.org/draft-07/schema#', '$id': 'https://setuptools.pypa.io/en/latest/deprecated/distutils/configfile.html', 'title': '``tool.distutils`` table', '$$description': ['**EXPERIMENTAL** (NOT OFFICIALLY SUPPORTED): Use ``tool.distutils``', 'subtables to configure arguments for ``distutils`` commands.', 'Originally, ``distutils`` allowed developers to configure arguments for', '``setup.py`` commands via `distutils configuration files', '<https://setuptools.pypa.io/en/latest/deprecated/distutils/configfile.html>`_.', 'See also `the old Python docs <https://docs.python.org/3.11/install/>_`.'], 'type': 'object', 'properties': {'global': {'type': 'object', 'description': 'Global options applied to all ``distutils`` commands'}}, 'patternProperties': {'.+': {'type': 'object'}}, '$comment': 'TODO: Is there a practical way of making this schema more specific?'}, 'setuptools': {'$schema': 'http://json-schema.org/draft-07/schema#', '$id': 'https://setuptools.pypa.io/en/latest/userguide/pyproject_config.html', 'title': '``tool.setuptools`` table', '$$description': ['``setuptools``-specific configurations that can be set by users that require', 'customization.', 'These configurations are completely optional and probably can be skipped when', 'creating simple packages. They are equivalent to some of the `Keywords', '<https://setuptools.pypa.io/en/latest/references/keywords.html>`_', 'used by the ``setup.py`` file, and can be set via the ``tool.setuptools`` table.', 'It considers only ``setuptools`` `parameters', '<https://setuptools.pypa.io/en/latest/userguide/pyproject_config.html#setuptools-specific-configuration>`_', 'that are not covered by :pep:`621`; and intentionally excludes ``dependency_links``', 'and ``setup_requires`` (incompatible with modern workflows/standards).'], 'type': 'object', 'additionalProperties': False, 'properties': {'platforms': {'type': 'array', 'items': {'type': 'string'}}, 'provides': {'$$description': ['Package and virtual package names contained within this package', '**(not supported by pip)**'], 'type': 'array', 'items': {'type': 'string', 'format': 'pep508-identifier'}}, 'obsoletes': {'$$description': ['Packages which this package renders obsolete', '**(not supported by pip)**'], 'type': 'array', 'items': {'type': 'string', 'format': 'pep508-identifier'}}, 'zip-safe': {'$$description': ['Whether the project can be safely installed and run from a zip file.', '**OBSOLETE**: only relevant for ``pkg_resources``, ``easy_install`` and', '``setup.py install`` in the context of ``eggs`` (**DEPRECATED**).'], 'type': 'boolean'}, 'script-files': {'$$description': ['Legacy way of defining scripts (entry-points are preferred).', 'Equivalent to the ``script`` keyword in ``setup.py``', '(it was renamed to avoid confusion with entry-point based ``project.scripts``', 'defined in :pep:`621`).', '**DISCOURAGED**: generic script wrappers are tricky and may not work properly.', 'Whenever possible, please use ``project.scripts`` instead.'], 'type': 'array', 'items': {'type': 'string'}, '$comment': 'TODO: is this field deprecated/should be removed?'}, 'eager-resources': {'$$description': ['Resources that should be extracted together, if any of them is needed,', 'or if any C extensions included in the project are imported.', '**OBSOLETE**: only relevant for ``pkg_resources``, ``easy_install`` and', '``setup.py install`` in the context of ``eggs`` (**DEPRECATED**).'], 'type': 'array', 'items': {'type': 'string'}}, 'packages': {'$$description': ['Packages that should be included in the distribution.', 'It can be given either as a list of package identifiers', 'or as a ``dict``-like structure with a single key ``find``', 'which corresponds to a dynamic call to', '``setuptools.config.expand.find_packages`` function.', 'The ``find`` key is associated with a nested ``dict``-like structure that can', 'contain ``where``, ``include``, ``exclude`` and ``namespaces`` keys,', 'mimicking the keyword arguments of the associated function.'], 'oneOf': [{'title': 'Array of Python package identifiers', 'type': 'array', 'items': {'$ref': '#/definitions/package-name'}}, {'$ref': '#/definitions/find-directive'}]}, 'package-dir': {'$$description': [':class:`dict`-like structure mapping from package names to directories where their', 'code can be found.', 'The empty string (as key) means that all packages are contained inside', 'the given directory will be included in the distribution.'], 'type': 'object', 'additionalProperties': False, 'propertyNames': {'anyOf': [{'const': ''}, {'$ref': '#/definitions/package-name'}]}, 'patternProperties': {'^.*$': {'type': 'string'}}}, 'package-data': {'$$description': ['Mapping from package names to lists of glob patterns.', 'Usually this option is not needed when using ``include-package-data = true``', 'For more information on how to include data files, check ``setuptools`` `docs', '<https://setuptools.pypa.io/en/latest/userguide/datafiles.html>`_.'], 'type': 'object', 'additionalProperties': False, 'propertyNames': {'anyOf': [{'type': 'string', 'format': 'python-module-name'}, {'const': '*'}]}, 'patternProperties': {'^.*$': {'type': 'array', 'items': {'type': 'string'}}}}, 'include-package-data': {'$$description': ['Automatically include any data files inside the package directories', 'that are specified by ``MANIFEST.in``', 'For more information on how to include data files, check ``setuptools`` `docs', '<https://setuptools.pypa.io/en/latest/userguide/datafiles.html>`_.'], 'type': 'boolean'}, 'exclude-package-data': {'$$description': ['Mapping from package names to lists of glob patterns that should be excluded', 'For more information on how to include data files, check ``setuptools`` `docs', '<https://setuptools.pypa.io/en/latest/userguide/datafiles.html>`_.'], 'type': 'object', 'additionalProperties': False, 'propertyNames': {'anyOf': [{'type': 'string', 'format': 'python-module-name'}, {'const': '*'}]}, 'patternProperties': {'^.*$': {'type': 'array', 'items': {'type': 'string'}}}}, 'namespace-packages': {'type': 'array', 'items': {'type': 'string', 'format': 'python-module-name-relaxed'}, '$comment': 'https://setuptools.pypa.io/en/latest/userguide/package_discovery.html', 'description': '**DEPRECATED**: use implicit namespaces instead (:pep:`420`).'}, 'py-modules': {'description': 'Modules that setuptools will manipulate', 'type': 'array', 'items': {'type': 'string', 'format': 'python-module-name-relaxed'}, '$comment': 'TODO: clarify the relationship with ``packages``'}, 'ext-modules': {'description': 'Extension modules to be compiled by setuptools', 'type': 'array', 'items': {'$ref': '#/definitions/ext-module'}}, 'data-files': {'$$description': ['``dict``-like structure where each key represents a directory and', 'the value is a list of glob patterns that should be installed in them.', '**DISCOURAGED**: please notice this might not work as expected with wheels.', 'Whenever possible, consider using data files inside the package directories', '(or create a new namespace package that only contains data files).', 'See `data files support', '<https://setuptools.pypa.io/en/latest/userguide/datafiles.html>`_.'], 'type': 'object', 'patternProperties': {'^.*$': {'type': 'array', 'items': {'type': 'string'}}}}, 'cmdclass': {'$$description': ['Mapping of distutils-style command names to ``setuptools.Command`` subclasses', 'which in turn should be represented by strings with a qualified class name', '(i.e., \"dotted\" form with module), e.g.::\\n\\n', '    cmdclass = {mycmd = \"pkg.subpkg.module.CommandClass\"}\\n\\n', 'The command class should be a directly defined at the top-level of the', 'containing module (no class nesting).'], 'type': 'object', 'patternProperties': {'^.*$': {'type': 'string', 'format': 'python-qualified-identifier'}}}, 'license-files': {'type': 'array', 'items': {'type': 'string'}, '$$description': ['**PROVISIONAL**: list of glob patterns for all license files being distributed.', '(likely to become standard with :pep:`639`).', \"By default: ``['LICEN[CS]E*', 'COPYING*', 'NOTICE*', 'AUTHORS*']``\"], '$comment': 'TODO: revise if PEP 639 is accepted. Probably ``project.license-files``?'}, 'dynamic': {'type': 'object', 'description': 'Instructions for loading :pep:`621`-related metadata dynamically', 'additionalProperties': False, 'properties': {'version': {'$$description': ['A version dynamically loaded via either the ``attr:`` or ``file:``', 'directives. Please make sure the given file or attribute respects :pep:`440`.', 'Also ensure to set ``project.dynamic`` accordingly.'], 'oneOf': [{'$ref': '#/definitions/attr-directive'}, {'$ref': '#/definitions/file-directive'}]}, 'classifiers': {'$ref': '#/definitions/file-directive'}, 'description': {'$ref': '#/definitions/file-directive'}, 'entry-points': {'$ref': '#/definitions/file-directive'}, 'dependencies': {'$ref': '#/definitions/file-directive-for-dependencies'}, 'optional-dependencies': {'type': 'object', 'propertyNames': {'type': 'string', 'format': 'pep508-identifier'}, 'additionalProperties': False, 'patternProperties': {'.+': {'$ref': '#/definitions/file-directive-for-dependencies'}}}, 'readme': {'type': 'object', 'anyOf': [{'$ref': '#/definitions/file-directive'}, {'type': 'object', 'properties': {'content-type': {'type': 'string'}, 'file': {'$ref': '#/definitions/file-directive/properties/file'}}, 'additionalProperties': False}], 'required': ['file']}}}}, 'definitions': {'package-name': {'$id': '#/definitions/package-name', 'title': 'Valid package name', 'description': 'Valid package name (importable or :pep:`561`).', 'type': 'string', 'anyOf': [{'type': 'string', 'format': 'python-module-name-relaxed'}, {'type': 'string', 'format': 'pep561-stub-name'}]}, 'ext-module': {'$id': '#/definitions/ext-module', 'title': 'Extension module', 'description': 'Parameters to construct a :class:`setuptools.Extension` object', 'type': 'object', 'required': ['name', 'sources'], 'additionalProperties': False, 'properties': {'name': {'type': 'string', 'format': 'python-module-name-relaxed'}, 'sources': {'type': 'array', 'items': {'type': 'string'}}, 'include-dirs': {'type': 'array', 'items': {'type': 'string'}}, 'define-macros': {'type': 'array', 'items': {'type': 'array', 'items': [{'description': 'macro name', 'type': 'string'}, {'description': 'macro value', 'oneOf': [{'type': 'string'}, {'type': 'null'}]}], 'additionalItems': False}}, 'undef-macros': {'type': 'array', 'items': {'type': 'string'}}, 'library-dirs': {'type': 'array', 'items': {'type': 'string'}}, 'libraries': {'type': 'array', 'items': {'type': 'string'}}, 'runtime-library-dirs': {'type': 'array', 'items': {'type': 'string'}}, 'extra-objects': {'type': 'array', 'items': {'type': 'string'}}, 'extra-compile-args': {'type': 'array', 'items': {'type': 'string'}}, 'extra-link-args': {'type': 'array', 'items': {'type': 'string'}}, 'export-symbols': {'type': 'array', 'items': {'type': 'string'}}, 'swig-opts': {'type': 'array', 'items': {'type': 'string'}}, 'depends': {'type': 'array', 'items': {'type': 'string'}}, 'language': {'type': 'string'}, 'optional': {'type': 'boolean'}, 'py-limited-api': {'type': 'boolean'}}}, 'file-directive': {'$id': '#/definitions/file-directive', 'title': \"'file:' directive\", 'description': 'Value is read from a file (or list of files and then concatenated)', 'type': 'object', 'additionalProperties': False, 'properties': {'file': {'oneOf': [{'type': 'string'}, {'type': 'array', 'items': {'type': 'string'}}]}}, 'required': ['file']}, 'file-directive-for-dependencies': {'title': \"'file:' directive for dependencies\", 'allOf': [{'$$description': ['**BETA**: subset of the ``requirements.txt`` format', 'without ``pip`` flags and options', '(one :pep:`508`-compliant string per line,', 'lines that are blank or start with ``#`` are excluded).', 'See `dynamic metadata', '<https://setuptools.pypa.io/en/latest/userguide/pyproject_config.html#dynamic-metadata>`_.']}, {'$ref': '#/definitions/file-directive'}]}, 'attr-directive': {'title': \"'attr:' directive\", '$id': '#/definitions/attr-directive', '$$description': ['Value is read from a module attribute. Supports callables and iterables;', 'unsupported types are cast via ``str()``'], 'type': 'object', 'additionalProperties': False, 'properties': {'attr': {'type': 'string', 'format': 'python-qualified-identifier'}}, 'required': ['attr']}, 'find-directive': {'$id': '#/definitions/find-directive', 'title': \"'find:' directive\", 'type': 'object', 'additionalProperties': False, 'properties': {'find': {'type': 'object', '$$description': ['Dynamic `package discovery', '<https://setuptools.pypa.io/en/latest/userguide/package_discovery.html>`_.'], 'additionalProperties': False, 'properties': {'where': {'description': 'Directories to be searched for packages (Unix-style relative path)', 'type': 'array', 'items': {'type': 'string'}}, 'exclude': {'type': 'array', '$$description': ['Exclude packages that match the values listed in this field.', \"Can container shell-style wildcards (e.g. ``'pkg.*'``)\"], 'items': {'type': 'string'}}, 'include': {'type': 'array', '$$description': ['Restrict the found packages to just the ones listed in this field.', \"Can container shell-style wildcards (e.g. ``'pkg.*'``)\"], 'items': {'type': 'string'}}, 'namespaces': {'type': 'boolean', '$$description': ['When ``True``, directories without a ``__init__.py`` file will also', 'be scanned for :pep:`420`-style implicit namespaces']}}}}}}}}}, rule='type')\n            data__tool_is_dict = isinstance(data__tool, dict)\n            if data__tool_is_dict:\n                data__tool_keys = set(data__tool.keys())\n                if \"distutils\" in data__tool_keys:\n                    data__tool_keys.remove(\"distutils\")\n                    data__tool__distutils = data__tool[\"distutils\"]\n                    validate_https___setuptools_pypa_io_en_latest_deprecated_distutils_configfile_html(data__tool__distutils, custom_formats, (name_prefix or \"data\") + \".tool.distutils\")\n                if \"setuptools\" in data__tool_keys:\n                    data__tool_keys.remove(\"setuptools\")\n                    data__tool__setuptools = data__tool[\"setuptools\"]\n                    validate_https___setuptools_pypa_io_en_latest_userguide_pyproject_config_html(data__tool__setuptools, custom_formats, (name_prefix or \"data\") + \".tool.setuptools\")\n        if data_keys:\n            raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \" must not contain \"+str(data_keys)+\" properties\", value=data, name=\"\" + (name_prefix or \"data\") + \"\", definition={'$schema': 'http://json-schema.org/draft-07/schema#', '$id': 'https://packaging.python.org/en/latest/specifications/declaring-build-dependencies/', 'title': 'Data structure for ``pyproject.toml`` files', '$$description': ['File format containing build-time configurations for the Python ecosystem. ', ':pep:`517` initially defined a build-system independent format for source trees', 'which was complemented by :pep:`518` to provide a way of specifying dependencies ', 'for building Python projects.', 'Please notice the ``project`` table (as initially defined in  :pep:`621`) is not included', 'in this schema and should be considered separately.'], 'type': 'object', 'additionalProperties': False, 'properties': {'build-system': {'type': 'object', 'description': 'Table used to store build-related data', 'additionalProperties': False, 'properties': {'requires': {'type': 'array', '$$description': ['List of dependencies in the :pep:`508` format required to execute the build', 'system. Please notice that the resulting dependency graph', '**MUST NOT contain cycles**'], 'items': {'type': 'string'}}, 'build-backend': {'type': 'string', 'description': 'Python object that will be used to perform the build according to :pep:`517`', 'format': 'pep517-backend-reference'}, 'backend-path': {'type': 'array', '$$description': ['List of directories to be prepended to ``sys.path`` when loading the', 'back-end, and running its hooks'], 'items': {'type': 'string', '$comment': 'Should be a path (TODO: enforce it with format?)'}}}, 'required': ['requires']}, 'project': {'$schema': 'http://json-schema.org/draft-07/schema#', '$id': 'https://packaging.python.org/en/latest/specifications/pyproject-toml/', 'title': 'Package metadata stored in the ``project`` table', '$$description': ['Data structure for the **project** table inside ``pyproject.toml``', '(as initially defined in :pep:`621`)'], 'type': 'object', 'properties': {'name': {'type': 'string', 'description': 'The name (primary identifier) of the project. MUST be statically defined.', 'format': 'pep508-identifier'}, 'version': {'type': 'string', 'description': 'The version of the project as supported by :pep:`440`.', 'format': 'pep440'}, 'description': {'type': 'string', '$$description': ['The `summary description of the project', '<https://packaging.python.org/specifications/core-metadata/#summary>`_']}, 'readme': {'$$description': ['`Full/detailed description of the project in the form of a README', '<https://peps.python.org/pep-0621/#readme>`_', \"with meaning similar to the one defined in `core metadata's Description\", '<https://packaging.python.org/specifications/core-metadata/#description>`_'], 'oneOf': [{'type': 'string', '$$description': ['Relative path to a text file (UTF-8) containing the full description', 'of the project. If the file path ends in case-insensitive ``.md`` or', '``.rst`` suffixes, then the content-type is respectively', '``text/markdown`` or ``text/x-rst``']}, {'type': 'object', 'allOf': [{'anyOf': [{'properties': {'file': {'type': 'string', '$$description': ['Relative path to a text file containing the full description', 'of the project.']}}, 'required': ['file']}, {'properties': {'text': {'type': 'string', 'description': 'Full text describing the project.'}}, 'required': ['text']}]}, {'properties': {'content-type': {'type': 'string', '$$description': ['Content-type (:rfc:`1341`) of the full description', '(e.g. ``text/markdown``). The ``charset`` parameter is assumed', 'UTF-8 when not present.'], '$comment': 'TODO: add regex pattern or format?'}}, 'required': ['content-type']}]}]}, 'requires-python': {'type': 'string', 'format': 'pep508-versionspec', '$$description': ['`The Python version requirements of the project', '<https://packaging.python.org/specifications/core-metadata/#requires-python>`_.']}, 'license': {'description': '`Project license <https://peps.python.org/pep-0621/#license>`_.', 'oneOf': [{'properties': {'file': {'type': 'string', '$$description': ['Relative path to the file (UTF-8) which contains the license for the', 'project.']}}, 'required': ['file']}, {'properties': {'text': {'type': 'string', '$$description': ['The license of the project whose meaning is that of the', '`License field from the core metadata', '<https://packaging.python.org/specifications/core-metadata/#license>`_.']}}, 'required': ['text']}]}, 'authors': {'type': 'array', 'items': {'$ref': '#/definitions/author'}, '$$description': [\"The people or organizations considered to be the 'authors' of the project.\", 'The exact meaning is open to interpretation (e.g. original or primary authors,', 'current maintainers, or owners of the package).']}, 'maintainers': {'type': 'array', 'items': {'$ref': '#/definitions/author'}, '$$description': [\"The people or organizations considered to be the 'maintainers' of the project.\", 'Similarly to ``authors``, the exact meaning is open to interpretation.']}, 'keywords': {'type': 'array', 'items': {'type': 'string'}, 'description': 'List of keywords to assist searching for the distribution in a larger catalog.'}, 'classifiers': {'type': 'array', 'items': {'type': 'string', 'format': 'trove-classifier', 'description': '`PyPI classifier <https://pypi.org/classifiers/>`_.'}, '$$description': ['`Trove classifiers <https://pypi.org/classifiers/>`_', 'which apply to the project.']}, 'urls': {'type': 'object', 'description': 'URLs associated with the project in the form ``label => value``.', 'additionalProperties': False, 'patternProperties': {'^.+$': {'type': 'string', 'format': 'url'}}}, 'scripts': {'$ref': '#/definitions/entry-point-group', '$$description': ['Instruct the installer to create command-line wrappers for the given', '`entry points <https://packaging.python.org/specifications/entry-points/>`_.']}, 'gui-scripts': {'$ref': '#/definitions/entry-point-group', '$$description': ['Instruct the installer to create GUI wrappers for the given', '`entry points <https://packaging.python.org/specifications/entry-points/>`_.', 'The difference between ``scripts`` and ``gui-scripts`` is only relevant in', 'Windows.']}, 'entry-points': {'$$description': ['Instruct the installer to expose the given modules/functions via', '``entry-point`` discovery mechanism (useful for plugins).', 'More information available in the `Python packaging guide', '<https://packaging.python.org/specifications/entry-points/>`_.'], 'propertyNames': {'format': 'python-entrypoint-group'}, 'additionalProperties': False, 'patternProperties': {'^.+$': {'$ref': '#/definitions/entry-point-group'}}}, 'dependencies': {'type': 'array', 'description': 'Project (mandatory) dependencies.', 'items': {'$ref': '#/definitions/dependency'}}, 'optional-dependencies': {'type': 'object', 'description': 'Optional dependency for the project', 'propertyNames': {'format': 'pep508-identifier'}, 'additionalProperties': False, 'patternProperties': {'^.+$': {'type': 'array', 'items': {'$ref': '#/definitions/dependency'}}}}, 'dynamic': {'type': 'array', '$$description': ['Specifies which fields are intentionally unspecified and expected to be', 'dynamically provided by build tools'], 'items': {'enum': ['version', 'description', 'readme', 'requires-python', 'license', 'authors', 'maintainers', 'keywords', 'classifiers', 'urls', 'scripts', 'gui-scripts', 'entry-points', 'dependencies', 'optional-dependencies']}}}, 'required': ['name'], 'additionalProperties': False, 'if': {'not': {'required': ['dynamic'], 'properties': {'dynamic': {'contains': {'const': 'version'}, '$$description': ['version is listed in ``dynamic``']}}}, '$$comment': ['According to :pep:`621`:', '    If the core metadata specification lists a field as \"Required\", then', '    the metadata MUST specify the field statically or list it in dynamic', 'In turn, `core metadata`_ defines:', '    The required fields are: Metadata-Version, Name, Version.', '    All the other fields are optional.', 'Since ``Metadata-Version`` is defined by the build back-end, ``name`` and', '``version`` are the only mandatory information in ``pyproject.toml``.', '.. _core metadata: https://packaging.python.org/specifications/core-metadata/']}, 'then': {'required': ['version'], '$$description': ['version should be statically defined in the ``version`` field']}, 'definitions': {'author': {'$id': '#/definitions/author', 'title': 'Author or Maintainer', '$comment': 'https://peps.python.org/pep-0621/#authors-maintainers', 'type': 'object', 'additionalProperties': False, 'properties': {'name': {'type': 'string', '$$description': ['MUST be a valid email name, i.e. whatever can be put as a name, before an', 'email, in :rfc:`822`.']}, 'email': {'type': 'string', 'format': 'idn-email', 'description': 'MUST be a valid email address'}}}, 'entry-point-group': {'$id': '#/definitions/entry-point-group', 'title': 'Entry-points', 'type': 'object', '$$description': ['Entry-points are grouped together to indicate what sort of capabilities they', 'provide.', 'See the `packaging guides', '<https://packaging.python.org/specifications/entry-points/>`_', 'and `setuptools docs', '<https://setuptools.pypa.io/en/latest/userguide/entry_point.html>`_', 'for more information.'], 'propertyNames': {'format': 'python-entrypoint-name'}, 'additionalProperties': False, 'patternProperties': {'^.+$': {'type': 'string', '$$description': ['Reference to a Python object. It is either in the form', '``importable.module``, or ``importable.module:object.attr``.'], 'format': 'python-entrypoint-reference', '$comment': 'https://packaging.python.org/specifications/entry-points/'}}}, 'dependency': {'$id': '#/definitions/dependency', 'title': 'Dependency', 'type': 'string', 'description': 'Project dependency specification according to PEP 508', 'format': 'pep508'}}}, 'tool': {'type': 'object', 'properties': {'distutils': {'$schema': 'http://json-schema.org/draft-07/schema#', '$id': 'https://setuptools.pypa.io/en/latest/deprecated/distutils/configfile.html', 'title': '``tool.distutils`` table', '$$description': ['**EXPERIMENTAL** (NOT OFFICIALLY SUPPORTED): Use ``tool.distutils``', 'subtables to configure arguments for ``distutils`` commands.', 'Originally, ``distutils`` allowed developers to configure arguments for', '``setup.py`` commands via `distutils configuration files', '<https://setuptools.pypa.io/en/latest/deprecated/distutils/configfile.html>`_.', 'See also `the old Python docs <https://docs.python.org/3.11/install/>_`.'], 'type': 'object', 'properties': {'global': {'type': 'object', 'description': 'Global options applied to all ``distutils`` commands'}}, 'patternProperties': {'.+': {'type': 'object'}}, '$comment': 'TODO: Is there a practical way of making this schema more specific?'}, 'setuptools': {'$schema': 'http://json-schema.org/draft-07/schema#', '$id': 'https://setuptools.pypa.io/en/latest/userguide/pyproject_config.html', 'title': '``tool.setuptools`` table', '$$description': ['``setuptools``-specific configurations that can be set by users that require', 'customization.', 'These configurations are completely optional and probably can be skipped when', 'creating simple packages. They are equivalent to some of the `Keywords', '<https://setuptools.pypa.io/en/latest/references/keywords.html>`_', 'used by the ``setup.py`` file, and can be set via the ``tool.setuptools`` table.', 'It considers only ``setuptools`` `parameters', '<https://setuptools.pypa.io/en/latest/userguide/pyproject_config.html#setuptools-specific-configuration>`_', 'that are not covered by :pep:`621`; and intentionally excludes ``dependency_links``', 'and ``setup_requires`` (incompatible with modern workflows/standards).'], 'type': 'object', 'additionalProperties': False, 'properties': {'platforms': {'type': 'array', 'items': {'type': 'string'}}, 'provides': {'$$description': ['Package and virtual package names contained within this package', '**(not supported by pip)**'], 'type': 'array', 'items': {'type': 'string', 'format': 'pep508-identifier'}}, 'obsoletes': {'$$description': ['Packages which this package renders obsolete', '**(not supported by pip)**'], 'type': 'array', 'items': {'type': 'string', 'format': 'pep508-identifier'}}, 'zip-safe': {'$$description': ['Whether the project can be safely installed and run from a zip file.', '**OBSOLETE**: only relevant for ``pkg_resources``, ``easy_install`` and', '``setup.py install`` in the context of ``eggs`` (**DEPRECATED**).'], 'type': 'boolean'}, 'script-files': {'$$description': ['Legacy way of defining scripts (entry-points are preferred).', 'Equivalent to the ``script`` keyword in ``setup.py``', '(it was renamed to avoid confusion with entry-point based ``project.scripts``', 'defined in :pep:`621`).', '**DISCOURAGED**: generic script wrappers are tricky and may not work properly.', 'Whenever possible, please use ``project.scripts`` instead.'], 'type': 'array', 'items': {'type': 'string'}, '$comment': 'TODO: is this field deprecated/should be removed?'}, 'eager-resources': {'$$description': ['Resources that should be extracted together, if any of them is needed,', 'or if any C extensions included in the project are imported.', '**OBSOLETE**: only relevant for ``pkg_resources``, ``easy_install`` and', '``setup.py install`` in the context of ``eggs`` (**DEPRECATED**).'], 'type': 'array', 'items': {'type': 'string'}}, 'packages': {'$$description': ['Packages that should be included in the distribution.', 'It can be given either as a list of package identifiers', 'or as a ``dict``-like structure with a single key ``find``', 'which corresponds to a dynamic call to', '``setuptools.config.expand.find_packages`` function.', 'The ``find`` key is associated with a nested ``dict``-like structure that can', 'contain ``where``, ``include``, ``exclude`` and ``namespaces`` keys,', 'mimicking the keyword arguments of the associated function.'], 'oneOf': [{'title': 'Array of Python package identifiers', 'type': 'array', 'items': {'$ref': '#/definitions/package-name'}}, {'$ref': '#/definitions/find-directive'}]}, 'package-dir': {'$$description': [':class:`dict`-like structure mapping from package names to directories where their', 'code can be found.', 'The empty string (as key) means that all packages are contained inside', 'the given directory will be included in the distribution.'], 'type': 'object', 'additionalProperties': False, 'propertyNames': {'anyOf': [{'const': ''}, {'$ref': '#/definitions/package-name'}]}, 'patternProperties': {'^.*$': {'type': 'string'}}}, 'package-data': {'$$description': ['Mapping from package names to lists of glob patterns.', 'Usually this option is not needed when using ``include-package-data = true``', 'For more information on how to include data files, check ``setuptools`` `docs', '<https://setuptools.pypa.io/en/latest/userguide/datafiles.html>`_.'], 'type': 'object', 'additionalProperties': False, 'propertyNames': {'anyOf': [{'type': 'string', 'format': 'python-module-name'}, {'const': '*'}]}, 'patternProperties': {'^.*$': {'type': 'array', 'items': {'type': 'string'}}}}, 'include-package-data': {'$$description': ['Automatically include any data files inside the package directories', 'that are specified by ``MANIFEST.in``', 'For more information on how to include data files, check ``setuptools`` `docs', '<https://setuptools.pypa.io/en/latest/userguide/datafiles.html>`_.'], 'type': 'boolean'}, 'exclude-package-data': {'$$description': ['Mapping from package names to lists of glob patterns that should be excluded', 'For more information on how to include data files, check ``setuptools`` `docs', '<https://setuptools.pypa.io/en/latest/userguide/datafiles.html>`_.'], 'type': 'object', 'additionalProperties': False, 'propertyNames': {'anyOf': [{'type': 'string', 'format': 'python-module-name'}, {'const': '*'}]}, 'patternProperties': {'^.*$': {'type': 'array', 'items': {'type': 'string'}}}}, 'namespace-packages': {'type': 'array', 'items': {'type': 'string', 'format': 'python-module-name-relaxed'}, '$comment': 'https://setuptools.pypa.io/en/latest/userguide/package_discovery.html', 'description': '**DEPRECATED**: use implicit namespaces instead (:pep:`420`).'}, 'py-modules': {'description': 'Modules that setuptools will manipulate', 'type': 'array', 'items': {'type': 'string', 'format': 'python-module-name-relaxed'}, '$comment': 'TODO: clarify the relationship with ``packages``'}, 'ext-modules': {'description': 'Extension modules to be compiled by setuptools', 'type': 'array', 'items': {'$ref': '#/definitions/ext-module'}}, 'data-files': {'$$description': ['``dict``-like structure where each key represents a directory and', 'the value is a list of glob patterns that should be installed in them.', '**DISCOURAGED**: please notice this might not work as expected with wheels.', 'Whenever possible, consider using data files inside the package directories', '(or create a new namespace package that only contains data files).', 'See `data files support', '<https://setuptools.pypa.io/en/latest/userguide/datafiles.html>`_.'], 'type': 'object', 'patternProperties': {'^.*$': {'type': 'array', 'items': {'type': 'string'}}}}, 'cmdclass': {'$$description': ['Mapping of distutils-style command names to ``setuptools.Command`` subclasses', 'which in turn should be represented by strings with a qualified class name', '(i.e., \"dotted\" form with module), e.g.::\\n\\n', '    cmdclass = {mycmd = \"pkg.subpkg.module.CommandClass\"}\\n\\n', 'The command class should be a directly defined at the top-level of the', 'containing module (no class nesting).'], 'type': 'object', 'patternProperties': {'^.*$': {'type': 'string', 'format': 'python-qualified-identifier'}}}, 'license-files': {'type': 'array', 'items': {'type': 'string'}, '$$description': ['**PROVISIONAL**: list of glob patterns for all license files being distributed.', '(likely to become standard with :pep:`639`).', \"By default: ``['LICEN[CS]E*', 'COPYING*', 'NOTICE*', 'AUTHORS*']``\"], '$comment': 'TODO: revise if PEP 639 is accepted. Probably ``project.license-files``?'}, 'dynamic': {'type': 'object', 'description': 'Instructions for loading :pep:`621`-related metadata dynamically', 'additionalProperties': False, 'properties': {'version': {'$$description': ['A version dynamically loaded via either the ``attr:`` or ``file:``', 'directives. Please make sure the given file or attribute respects :pep:`440`.', 'Also ensure to set ``project.dynamic`` accordingly.'], 'oneOf': [{'$ref': '#/definitions/attr-directive'}, {'$ref': '#/definitions/file-directive'}]}, 'classifiers': {'$ref': '#/definitions/file-directive'}, 'description': {'$ref': '#/definitions/file-directive'}, 'entry-points': {'$ref': '#/definitions/file-directive'}, 'dependencies': {'$ref': '#/definitions/file-directive-for-dependencies'}, 'optional-dependencies': {'type': 'object', 'propertyNames': {'type': 'string', 'format': 'pep508-identifier'}, 'additionalProperties': False, 'patternProperties': {'.+': {'$ref': '#/definitions/file-directive-for-dependencies'}}}, 'readme': {'type': 'object', 'anyOf': [{'$ref': '#/definitions/file-directive'}, {'type': 'object', 'properties': {'content-type': {'type': 'string'}, 'file': {'$ref': '#/definitions/file-directive/properties/file'}}, 'additionalProperties': False}], 'required': ['file']}}}}, 'definitions': {'package-name': {'$id': '#/definitions/package-name', 'title': 'Valid package name', 'description': 'Valid package name (importable or :pep:`561`).', 'type': 'string', 'anyOf': [{'type': 'string', 'format': 'python-module-name-relaxed'}, {'type': 'string', 'format': 'pep561-stub-name'}]}, 'ext-module': {'$id': '#/definitions/ext-module', 'title': 'Extension module', 'description': 'Parameters to construct a :class:`setuptools.Extension` object', 'type': 'object', 'required': ['name', 'sources'], 'additionalProperties': False, 'properties': {'name': {'type': 'string', 'format': 'python-module-name-relaxed'}, 'sources': {'type': 'array', 'items': {'type': 'string'}}, 'include-dirs': {'type': 'array', 'items': {'type': 'string'}}, 'define-macros': {'type': 'array', 'items': {'type': 'array', 'items': [{'description': 'macro name', 'type': 'string'}, {'description': 'macro value', 'oneOf': [{'type': 'string'}, {'type': 'null'}]}], 'additionalItems': False}}, 'undef-macros': {'type': 'array', 'items': {'type': 'string'}}, 'library-dirs': {'type': 'array', 'items': {'type': 'string'}}, 'libraries': {'type': 'array', 'items': {'type': 'string'}}, 'runtime-library-dirs': {'type': 'array', 'items': {'type': 'string'}}, 'extra-objects': {'type': 'array', 'items': {'type': 'string'}}, 'extra-compile-args': {'type': 'array', 'items': {'type': 'string'}}, 'extra-link-args': {'type': 'array', 'items': {'type': 'string'}}, 'export-symbols': {'type': 'array', 'items': {'type': 'string'}}, 'swig-opts': {'type': 'array', 'items': {'type': 'string'}}, 'depends': {'type': 'array', 'items': {'type': 'string'}}, 'language': {'type': 'string'}, 'optional': {'type': 'boolean'}, 'py-limited-api': {'type': 'boolean'}}}, 'file-directive': {'$id': '#/definitions/file-directive', 'title': \"'file:' directive\", 'description': 'Value is read from a file (or list of files and then concatenated)', 'type': 'object', 'additionalProperties': False, 'properties': {'file': {'oneOf': [{'type': 'string'}, {'type': 'array', 'items': {'type': 'string'}}]}}, 'required': ['file']}, 'file-directive-for-dependencies': {'title': \"'file:' directive for dependencies\", 'allOf': [{'$$description': ['**BETA**: subset of the ``requirements.txt`` format', 'without ``pip`` flags and options', '(one :pep:`508`-compliant string per line,', 'lines that are blank or start with ``#`` are excluded).', 'See `dynamic metadata', '<https://setuptools.pypa.io/en/latest/userguide/pyproject_config.html#dynamic-metadata>`_.']}, {'$ref': '#/definitions/file-directive'}]}, 'attr-directive': {'title': \"'attr:' directive\", '$id': '#/definitions/attr-directive', '$$description': ['Value is read from a module attribute. Supports callables and iterables;', 'unsupported types are cast via ``str()``'], 'type': 'object', 'additionalProperties': False, 'properties': {'attr': {'type': 'string', 'format': 'python-qualified-identifier'}}, 'required': ['attr']}, 'find-directive': {'$id': '#/definitions/find-directive', 'title': \"'find:' directive\", 'type': 'object', 'additionalProperties': False, 'properties': {'find': {'type': 'object', '$$description': ['Dynamic `package discovery', '<https://setuptools.pypa.io/en/latest/userguide/package_discovery.html>`_.'], 'additionalProperties': False, 'properties': {'where': {'description': 'Directories to be searched for packages (Unix-style relative path)', 'type': 'array', 'items': {'type': 'string'}}, 'exclude': {'type': 'array', '$$description': ['Exclude packages that match the values listed in this field.', \"Can container shell-style wildcards (e.g. ``'pkg.*'``)\"], 'items': {'type': 'string'}}, 'include': {'type': 'array', '$$description': ['Restrict the found packages to just the ones listed in this field.', \"Can container shell-style wildcards (e.g. ``'pkg.*'``)\"], 'items': {'type': 'string'}}, 'namespaces': {'type': 'boolean', '$$description': ['When ``True``, directories without a ``__init__.py`` file will also', 'be scanned for :pep:`420`-style implicit namespaces']}}}}}}}}}}, 'project': {'$schema': 'http://json-schema.org/draft-07/schema#', '$id': 'https://packaging.python.org/en/latest/specifications/pyproject-toml/', 'title': 'Package metadata stored in the ``project`` table', '$$description': ['Data structure for the **project** table inside ``pyproject.toml``', '(as initially defined in :pep:`621`)'], 'type': 'object', 'properties': {'name': {'type': 'string', 'description': 'The name (primary identifier) of the project. MUST be statically defined.', 'format': 'pep508-identifier'}, 'version': {'type': 'string', 'description': 'The version of the project as supported by :pep:`440`.', 'format': 'pep440'}, 'description': {'type': 'string', '$$description': ['The `summary description of the project', '<https://packaging.python.org/specifications/core-metadata/#summary>`_']}, 'readme': {'$$description': ['`Full/detailed description of the project in the form of a README', '<https://peps.python.org/pep-0621/#readme>`_', \"with meaning similar to the one defined in `core metadata's Description\", '<https://packaging.python.org/specifications/core-metadata/#description>`_'], 'oneOf': [{'type': 'string', '$$description': ['Relative path to a text file (UTF-8) containing the full description', 'of the project. If the file path ends in case-insensitive ``.md`` or', '``.rst`` suffixes, then the content-type is respectively', '``text/markdown`` or ``text/x-rst``']}, {'type': 'object', 'allOf': [{'anyOf': [{'properties': {'file': {'type': 'string', '$$description': ['Relative path to a text file containing the full description', 'of the project.']}}, 'required': ['file']}, {'properties': {'text': {'type': 'string', 'description': 'Full text describing the project.'}}, 'required': ['text']}]}, {'properties': {'content-type': {'type': 'string', '$$description': ['Content-type (:rfc:`1341`) of the full description', '(e.g. ``text/markdown``). The ``charset`` parameter is assumed', 'UTF-8 when not present.'], '$comment': 'TODO: add regex pattern or format?'}}, 'required': ['content-type']}]}]}, 'requires-python': {'type': 'string', 'format': 'pep508-versionspec', '$$description': ['`The Python version requirements of the project', '<https://packaging.python.org/specifications/core-metadata/#requires-python>`_.']}, 'license': {'description': '`Project license <https://peps.python.org/pep-0621/#license>`_.', 'oneOf': [{'properties': {'file': {'type': 'string', '$$description': ['Relative path to the file (UTF-8) which contains the license for the', 'project.']}}, 'required': ['file']}, {'properties': {'text': {'type': 'string', '$$description': ['The license of the project whose meaning is that of the', '`License field from the core metadata', '<https://packaging.python.org/specifications/core-metadata/#license>`_.']}}, 'required': ['text']}]}, 'authors': {'type': 'array', 'items': {'$ref': '#/definitions/author'}, '$$description': [\"The people or organizations considered to be the 'authors' of the project.\", 'The exact meaning is open to interpretation (e.g. original or primary authors,', 'current maintainers, or owners of the package).']}, 'maintainers': {'type': 'array', 'items': {'$ref': '#/definitions/author'}, '$$description': [\"The people or organizations considered to be the 'maintainers' of the project.\", 'Similarly to ``authors``, the exact meaning is open to interpretation.']}, 'keywords': {'type': 'array', 'items': {'type': 'string'}, 'description': 'List of keywords to assist searching for the distribution in a larger catalog.'}, 'classifiers': {'type': 'array', 'items': {'type': 'string', 'format': 'trove-classifier', 'description': '`PyPI classifier <https://pypi.org/classifiers/>`_.'}, '$$description': ['`Trove classifiers <https://pypi.org/classifiers/>`_', 'which apply to the project.']}, 'urls': {'type': 'object', 'description': 'URLs associated with the project in the form ``label => value``.', 'additionalProperties': False, 'patternProperties': {'^.+$': {'type': 'string', 'format': 'url'}}}, 'scripts': {'$ref': '#/definitions/entry-point-group', '$$description': ['Instruct the installer to create command-line wrappers for the given', '`entry points <https://packaging.python.org/specifications/entry-points/>`_.']}, 'gui-scripts': {'$ref': '#/definitions/entry-point-group', '$$description': ['Instruct the installer to create GUI wrappers for the given', '`entry points <https://packaging.python.org/specifications/entry-points/>`_.', 'The difference between ``scripts`` and ``gui-scripts`` is only relevant in', 'Windows.']}, 'entry-points': {'$$description': ['Instruct the installer to expose the given modules/functions via', '``entry-point`` discovery mechanism (useful for plugins).', 'More information available in the `Python packaging guide', '<https://packaging.python.org/specifications/entry-points/>`_.'], 'propertyNames': {'format': 'python-entrypoint-group'}, 'additionalProperties': False, 'patternProperties': {'^.+$': {'$ref': '#/definitions/entry-point-group'}}}, 'dependencies': {'type': 'array', 'description': 'Project (mandatory) dependencies.', 'items': {'$ref': '#/definitions/dependency'}}, 'optional-dependencies': {'type': 'object', 'description': 'Optional dependency for the project', 'propertyNames': {'format': 'pep508-identifier'}, 'additionalProperties': False, 'patternProperties': {'^.+$': {'type': 'array', 'items': {'$ref': '#/definitions/dependency'}}}}, 'dynamic': {'type': 'array', '$$description': ['Specifies which fields are intentionally unspecified and expected to be', 'dynamically provided by build tools'], 'items': {'enum': ['version', 'description', 'readme', 'requires-python', 'license', 'authors', 'maintainers', 'keywords', 'classifiers', 'urls', 'scripts', 'gui-scripts', 'entry-points', 'dependencies', 'optional-dependencies']}}}, 'required': ['name'], 'additionalProperties': False, 'if': {'not': {'required': ['dynamic'], 'properties': {'dynamic': {'contains': {'const': 'version'}, '$$description': ['version is listed in ``dynamic``']}}}, '$$comment': ['According to :pep:`621`:', '    If the core metadata specification lists a field as \"Required\", then', '    the metadata MUST specify the field statically or list it in dynamic', 'In turn, `core metadata`_ defines:', '    The required fields are: Metadata-Version, Name, Version.', '    All the other fields are optional.', 'Since ``Metadata-Version`` is defined by the build back-end, ``name`` and', '``version`` are the only mandatory information in ``pyproject.toml``.', '.. _core metadata: https://packaging.python.org/specifications/core-metadata/']}, 'then': {'required': ['version'], '$$description': ['version should be statically defined in the ``version`` field']}, 'definitions': {'author': {'$id': '#/definitions/author', 'title': 'Author or Maintainer', '$comment': 'https://peps.python.org/pep-0621/#authors-maintainers', 'type': 'object', 'additionalProperties': False, 'properties': {'name': {'type': 'string', '$$description': ['MUST be a valid email name, i.e. whatever can be put as a name, before an', 'email, in :rfc:`822`.']}, 'email': {'type': 'string', 'format': 'idn-email', 'description': 'MUST be a valid email address'}}}, 'entry-point-group': {'$id': '#/definitions/entry-point-group', 'title': 'Entry-points', 'type': 'object', '$$description': ['Entry-points are grouped together to indicate what sort of capabilities they', 'provide.', 'See the `packaging guides', '<https://packaging.python.org/specifications/entry-points/>`_', 'and `setuptools docs', '<https://setuptools.pypa.io/en/latest/userguide/entry_point.html>`_', 'for more information.'], 'propertyNames': {'format': 'python-entrypoint-name'}, 'additionalProperties': False, 'patternProperties': {'^.+$': {'type': 'string', '$$description': ['Reference to a Python object. It is either in the form', '``importable.module``, or ``importable.module:object.attr``.'], 'format': 'python-entrypoint-reference', '$comment': 'https://packaging.python.org/specifications/entry-points/'}}}, 'dependency': {'$id': '#/definitions/dependency', 'title': 'Dependency', 'type': 'string', 'description': 'Project dependency specification according to PEP 508', 'format': 'pep508'}}}}, rule='additionalProperties')\n    return data\n\ndef validate_https___setuptools_pypa_io_en_latest_userguide_pyproject_config_html(data, custom_formats={}, name_prefix=None):\n    if not isinstance(data, (dict)):\n        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \" must be object\", value=data, name=\"\" + (name_prefix or \"data\") + \"\", definition={'$schema': 'http://json-schema.org/draft-07/schema#', '$id': 'https://setuptools.pypa.io/en/latest/userguide/pyproject_config.html', 'title': '``tool.setuptools`` table', '$$description': ['``setuptools``-specific configurations that can be set by users that require', 'customization.', 'These configurations are completely optional and probably can be skipped when', 'creating simple packages. They are equivalent to some of the `Keywords', '<https://setuptools.pypa.io/en/latest/references/keywords.html>`_', 'used by the ``setup.py`` file, and can be set via the ``tool.setuptools`` table.', 'It considers only ``setuptools`` `parameters', '<https://setuptools.pypa.io/en/latest/userguide/pyproject_config.html#setuptools-specific-configuration>`_', 'that are not covered by :pep:`621`; and intentionally excludes ``dependency_links``', 'and ``setup_requires`` (incompatible with modern workflows/standards).'], 'type': 'object', 'additionalProperties': False, 'properties': {'platforms': {'type': 'array', 'items': {'type': 'string'}}, 'provides': {'$$description': ['Package and virtual package names contained within this package', '**(not supported by pip)**'], 'type': 'array', 'items': {'type': 'string', 'format': 'pep508-identifier'}}, 'obsoletes': {'$$description': ['Packages which this package renders obsolete', '**(not supported by pip)**'], 'type': 'array', 'items': {'type': 'string', 'format': 'pep508-identifier'}}, 'zip-safe': {'$$description': ['Whether the project can be safely installed and run from a zip file.', '**OBSOLETE**: only relevant for ``pkg_resources``, ``easy_install`` and', '``setup.py install`` in the context of ``eggs`` (**DEPRECATED**).'], 'type': 'boolean'}, 'script-files': {'$$description': ['Legacy way of defining scripts (entry-points are preferred).', 'Equivalent to the ``script`` keyword in ``setup.py``', '(it was renamed to avoid confusion with entry-point based ``project.scripts``', 'defined in :pep:`621`).', '**DISCOURAGED**: generic script wrappers are tricky and may not work properly.', 'Whenever possible, please use ``project.scripts`` instead.'], 'type': 'array', 'items': {'type': 'string'}, '$comment': 'TODO: is this field deprecated/should be removed?'}, 'eager-resources': {'$$description': ['Resources that should be extracted together, if any of them is needed,', 'or if any C extensions included in the project are imported.', '**OBSOLETE**: only relevant for ``pkg_resources``, ``easy_install`` and', '``setup.py install`` in the context of ``eggs`` (**DEPRECATED**).'], 'type': 'array', 'items': {'type': 'string'}}, 'packages': {'$$description': ['Packages that should be included in the distribution.', 'It can be given either as a list of package identifiers', 'or as a ``dict``-like structure with a single key ``find``', 'which corresponds to a dynamic call to', '``setuptools.config.expand.find_packages`` function.', 'The ``find`` key is associated with a nested ``dict``-like structure that can', 'contain ``where``, ``include``, ``exclude`` and ``namespaces`` keys,', 'mimicking the keyword arguments of the associated function.'], 'oneOf': [{'title': 'Array of Python package identifiers', 'type': 'array', 'items': {'$id': '#/definitions/package-name', 'title': 'Valid package name', 'description': 'Valid package name (importable or :pep:`561`).', 'type': 'string', 'anyOf': [{'type': 'string', 'format': 'python-module-name-relaxed'}, {'type': 'string', 'format': 'pep561-stub-name'}]}}, {'$id': '#/definitions/find-directive', 'title': \"'find:' directive\", 'type': 'object', 'additionalProperties': False, 'properties': {'find': {'type': 'object', '$$description': ['Dynamic `package discovery', '<https://setuptools.pypa.io/en/latest/userguide/package_discovery.html>`_.'], 'additionalProperties': False, 'properties': {'where': {'description': 'Directories to be searched for packages (Unix-style relative path)', 'type': 'array', 'items': {'type': 'string'}}, 'exclude': {'type': 'array', '$$description': ['Exclude packages that match the values listed in this field.', \"Can container shell-style wildcards (e.g. ``'pkg.*'``)\"], 'items': {'type': 'string'}}, 'include': {'type': 'array', '$$description': ['Restrict the found packages to just the ones listed in this field.', \"Can container shell-style wildcards (e.g. ``'pkg.*'``)\"], 'items': {'type': 'string'}}, 'namespaces': {'type': 'boolean', '$$description': ['When ``True``, directories without a ``__init__.py`` file will also', 'be scanned for :pep:`420`-style implicit namespaces']}}}}}]}, 'package-dir': {'$$description': [':class:`dict`-like structure mapping from package names to directories where their', 'code can be found.', 'The empty string (as key) means that all packages are contained inside', 'the given directory will be included in the distribution.'], 'type': 'object', 'additionalProperties': False, 'propertyNames': {'anyOf': [{'const': ''}, {'$id': '#/definitions/package-name', 'title': 'Valid package name', 'description': 'Valid package name (importable or :pep:`561`).', 'type': 'string', 'anyOf': [{'type': 'string', 'format': 'python-module-name-relaxed'}, {'type': 'string', 'format': 'pep561-stub-name'}]}]}, 'patternProperties': {'^.*$': {'type': 'string'}}}, 'package-data': {'$$description': ['Mapping from package names to lists of glob patterns.', 'Usually this option is not needed when using ``include-package-data = true``', 'For more information on how to include data files, check ``setuptools`` `docs', '<https://setuptools.pypa.io/en/latest/userguide/datafiles.html>`_.'], 'type': 'object', 'additionalProperties': False, 'propertyNames': {'anyOf': [{'type': 'string', 'format': 'python-module-name'}, {'const': '*'}]}, 'patternProperties': {'^.*$': {'type': 'array', 'items': {'type': 'string'}}}}, 'include-package-data': {'$$description': ['Automatically include any data files inside the package directories', 'that are specified by ``MANIFEST.in``', 'For more information on how to include data files, check ``setuptools`` `docs', '<https://setuptools.pypa.io/en/latest/userguide/datafiles.html>`_.'], 'type': 'boolean'}, 'exclude-package-data': {'$$description': ['Mapping from package names to lists of glob patterns that should be excluded', 'For more information on how to include data files, check ``setuptools`` `docs', '<https://setuptools.pypa.io/en/latest/userguide/datafiles.html>`_.'], 'type': 'object', 'additionalProperties': False, 'propertyNames': {'anyOf': [{'type': 'string', 'format': 'python-module-name'}, {'const': '*'}]}, 'patternProperties': {'^.*$': {'type': 'array', 'items': {'type': 'string'}}}}, 'namespace-packages': {'type': 'array', 'items': {'type': 'string', 'format': 'python-module-name-relaxed'}, '$comment': 'https://setuptools.pypa.io/en/latest/userguide/package_discovery.html', 'description': '**DEPRECATED**: use implicit namespaces instead (:pep:`420`).'}, 'py-modules': {'description': 'Modules that setuptools will manipulate', 'type': 'array', 'items': {'type': 'string', 'format': 'python-module-name-relaxed'}, '$comment': 'TODO: clarify the relationship with ``packages``'}, 'ext-modules': {'description': 'Extension modules to be compiled by setuptools', 'type': 'array', 'items': {'$id': '#/definitions/ext-module', 'title': 'Extension module', 'description': 'Parameters to construct a :class:`setuptools.Extension` object', 'type': 'object', 'required': ['name', 'sources'], 'additionalProperties': False, 'properties': {'name': {'type': 'string', 'format': 'python-module-name-relaxed'}, 'sources': {'type': 'array', 'items': {'type': 'string'}}, 'include-dirs': {'type': 'array', 'items': {'type': 'string'}}, 'define-macros': {'type': 'array', 'items': {'type': 'array', 'items': [{'description': 'macro name', 'type': 'string'}, {'description': 'macro value', 'oneOf': [{'type': 'string'}, {'type': 'null'}]}], 'additionalItems': False}}, 'undef-macros': {'type': 'array', 'items': {'type': 'string'}}, 'library-dirs': {'type': 'array', 'items': {'type': 'string'}}, 'libraries': {'type': 'array', 'items': {'type': 'string'}}, 'runtime-library-dirs': {'type': 'array', 'items': {'type': 'string'}}, 'extra-objects': {'type': 'array', 'items': {'type': 'string'}}, 'extra-compile-args': {'type': 'array', 'items': {'type': 'string'}}, 'extra-link-args': {'type': 'array', 'items': {'type': 'string'}}, 'export-symbols': {'type': 'array', 'items': {'type': 'string'}}, 'swig-opts': {'type': 'array', 'items': {'type': 'string'}}, 'depends': {'type': 'array', 'items': {'type': 'string'}}, 'language': {'type': 'string'}, 'optional': {'type': 'boolean'}, 'py-limited-api': {'type': 'boolean'}}}}, 'data-files': {'$$description': ['``dict``-like structure where each key represents a directory and', 'the value is a list of glob patterns that should be installed in them.', '**DISCOURAGED**: please notice this might not work as expected with wheels.', 'Whenever possible, consider using data files inside the package directories', '(or create a new namespace package that only contains data files).', 'See `data files support', '<https://setuptools.pypa.io/en/latest/userguide/datafiles.html>`_.'], 'type': 'object', 'patternProperties': {'^.*$': {'type': 'array', 'items': {'type': 'string'}}}}, 'cmdclass': {'$$description': ['Mapping of distutils-style command names to ``setuptools.Command`` subclasses', 'which in turn should be represented by strings with a qualified class name', '(i.e., \"dotted\" form with module), e.g.::\\n\\n', '    cmdclass = {mycmd = \"pkg.subpkg.module.CommandClass\"}\\n\\n', 'The command class should be a directly defined at the top-level of the', 'containing module (no class nesting).'], 'type': 'object', 'patternProperties': {'^.*$': {'type': 'string', 'format': 'python-qualified-identifier'}}}, 'license-files': {'type': 'array', 'items': {'type': 'string'}, '$$description': ['**PROVISIONAL**: list of glob patterns for all license files being distributed.', '(likely to become standard with :pep:`639`).', \"By default: ``['LICEN[CS]E*', 'COPYING*', 'NOTICE*', 'AUTHORS*']``\"], '$comment': 'TODO: revise if PEP 639 is accepted. Probably ``project.license-files``?'}, 'dynamic': {'type': 'object', 'description': 'Instructions for loading :pep:`621`-related metadata dynamically', 'additionalProperties': False, 'properties': {'version': {'$$description': ['A version dynamically loaded via either the ``attr:`` or ``file:``', 'directives. Please make sure the given file or attribute respects :pep:`440`.', 'Also ensure to set ``project.dynamic`` accordingly.'], 'oneOf': [{'title': \"'attr:' directive\", '$id': '#/definitions/attr-directive', '$$description': ['Value is read from a module attribute. Supports callables and iterables;', 'unsupported types are cast via ``str()``'], 'type': 'object', 'additionalProperties': False, 'properties': {'attr': {'type': 'string', 'format': 'python-qualified-identifier'}}, 'required': ['attr']}, {'$id': '#/definitions/file-directive', 'title': \"'file:' directive\", 'description': 'Value is read from a file (or list of files and then concatenated)', 'type': 'object', 'additionalProperties': False, 'properties': {'file': {'oneOf': [{'type': 'string'}, {'type': 'array', 'items': {'type': 'string'}}]}}, 'required': ['file']}]}, 'classifiers': {'$id': '#/definitions/file-directive', 'title': \"'file:' directive\", 'description': 'Value is read from a file (or list of files and then concatenated)', 'type': 'object', 'additionalProperties': False, 'properties': {'file': {'oneOf': [{'type': 'string'}, {'type': 'array', 'items': {'type': 'string'}}]}}, 'required': ['file']}, 'description': {'$id': '#/definitions/file-directive', 'title': \"'file:' directive\", 'description': 'Value is read from a file (or list of files and then concatenated)', 'type': 'object', 'additionalProperties': False, 'properties': {'file': {'oneOf': [{'type': 'string'}, {'type': 'array', 'items': {'type': 'string'}}]}}, 'required': ['file']}, 'entry-points': {'$id': '#/definitions/file-directive', 'title': \"'file:' directive\", 'description': 'Value is read from a file (or list of files and then concatenated)', 'type': 'object', 'additionalProperties': False, 'properties': {'file': {'oneOf': [{'type': 'string'}, {'type': 'array', 'items': {'type': 'string'}}]}}, 'required': ['file']}, 'dependencies': {'title': \"'file:' directive for dependencies\", 'allOf': [{'$$description': ['**BETA**: subset of the ``requirements.txt`` format', 'without ``pip`` flags and options', '(one :pep:`508`-compliant string per line,', 'lines that are blank or start with ``#`` are excluded).', 'See `dynamic metadata', '<https://setuptools.pypa.io/en/latest/userguide/pyproject_config.html#dynamic-metadata>`_.']}, {'$ref': '#/definitions/file-directive'}]}, 'optional-dependencies': {'type': 'object', 'propertyNames': {'type': 'string', 'format': 'pep508-identifier'}, 'additionalProperties': False, 'patternProperties': {'.+': {'title': \"'file:' directive for dependencies\", 'allOf': [{'$$description': ['**BETA**: subset of the ``requirements.txt`` format', 'without ``pip`` flags and options', '(one :pep:`508`-compliant string per line,', 'lines that are blank or start with ``#`` are excluded).', 'See `dynamic metadata', '<https://setuptools.pypa.io/en/latest/userguide/pyproject_config.html#dynamic-metadata>`_.']}, {'$ref': '#/definitions/file-directive'}]}}}, 'readme': {'type': 'object', 'anyOf': [{'$id': '#/definitions/file-directive', 'title': \"'file:' directive\", 'description': 'Value is read from a file (or list of files and then concatenated)', 'type': 'object', 'additionalProperties': False, 'properties': {'file': {'oneOf': [{'type': 'string'}, {'type': 'array', 'items': {'type': 'string'}}]}}, 'required': ['file']}, {'type': 'object', 'properties': {'content-type': {'type': 'string'}, 'file': {'oneOf': [{'type': 'string'}, {'type': 'array', 'items': {'type': 'string'}}]}}, 'additionalProperties': False}], 'required': ['file']}}}}, 'definitions': {'package-name': {'$id': '#/definitions/package-name', 'title': 'Valid package name', 'description': 'Valid package name (importable or :pep:`561`).', 'type': 'string', 'anyOf': [{'type': 'string', 'format': 'python-module-name-relaxed'}, {'type': 'string', 'format': 'pep561-stub-name'}]}, 'ext-module': {'$id': '#/definitions/ext-module', 'title': 'Extension module', 'description': 'Parameters to construct a :class:`setuptools.Extension` object', 'type': 'object', 'required': ['name', 'sources'], 'additionalProperties': False, 'properties': {'name': {'type': 'string', 'format': 'python-module-name-relaxed'}, 'sources': {'type': 'array', 'items': {'type': 'string'}}, 'include-dirs': {'type': 'array', 'items': {'type': 'string'}}, 'define-macros': {'type': 'array', 'items': {'type': 'array', 'items': [{'description': 'macro name', 'type': 'string'}, {'description': 'macro value', 'oneOf': [{'type': 'string'}, {'type': 'null'}]}], 'additionalItems': False}}, 'undef-macros': {'type': 'array', 'items': {'type': 'string'}}, 'library-dirs': {'type': 'array', 'items': {'type': 'string'}}, 'libraries': {'type': 'array', 'items': {'type': 'string'}}, 'runtime-library-dirs': {'type': 'array', 'items': {'type': 'string'}}, 'extra-objects': {'type': 'array', 'items': {'type': 'string'}}, 'extra-compile-args': {'type': 'array', 'items': {'type': 'string'}}, 'extra-link-args': {'type': 'array', 'items': {'type': 'string'}}, 'export-symbols': {'type': 'array', 'items': {'type': 'string'}}, 'swig-opts': {'type': 'array', 'items': {'type': 'string'}}, 'depends': {'type': 'array', 'items': {'type': 'string'}}, 'language': {'type': 'string'}, 'optional': {'type': 'boolean'}, 'py-limited-api': {'type': 'boolean'}}}, 'file-directive': {'$id': '#/definitions/file-directive', 'title': \"'file:' directive\", 'description': 'Value is read from a file (or list of files and then concatenated)', 'type': 'object', 'additionalProperties': False, 'properties': {'file': {'oneOf': [{'type': 'string'}, {'type': 'array', 'items': {'type': 'string'}}]}}, 'required': ['file']}, 'file-directive-for-dependencies': {'title': \"'file:' directive for dependencies\", 'allOf': [{'$$description': ['**BETA**: subset of the ``requirements.txt`` format', 'without ``pip`` flags and options', '(one :pep:`508`-compliant string per line,', 'lines that are blank or start with ``#`` are excluded).', 'See `dynamic metadata', '<https://setuptools.pypa.io/en/latest/userguide/pyproject_config.html#dynamic-metadata>`_.']}, {'$id': '#/definitions/file-directive', 'title': \"'file:' directive\", 'description': 'Value is read from a file (or list of files and then concatenated)', 'type': 'object', 'additionalProperties': False, 'properties': {'file': {'oneOf': [{'type': 'string'}, {'type': 'array', 'items': {'type': 'string'}}]}}, 'required': ['file']}]}, 'attr-directive': {'title': \"'attr:' directive\", '$id': '#/definitions/attr-directive', '$$description': ['Value is read from a module attribute. Supports callables and iterables;', 'unsupported types are cast via ``str()``'], 'type': 'object', 'additionalProperties': False, 'properties': {'attr': {'type': 'string', 'format': 'python-qualified-identifier'}}, 'required': ['attr']}, 'find-directive': {'$id': '#/definitions/find-directive', 'title': \"'find:' directive\", 'type': 'object', 'additionalProperties': False, 'properties': {'find': {'type': 'object', '$$description': ['Dynamic `package discovery', '<https://setuptools.pypa.io/en/latest/userguide/package_discovery.html>`_.'], 'additionalProperties': False, 'properties': {'where': {'description': 'Directories to be searched for packages (Unix-style relative path)', 'type': 'array', 'items': {'type': 'string'}}, 'exclude': {'type': 'array', '$$description': ['Exclude packages that match the values listed in this field.', \"Can container shell-style wildcards (e.g. ``'pkg.*'``)\"], 'items': {'type': 'string'}}, 'include': {'type': 'array', '$$description': ['Restrict the found packages to just the ones listed in this field.', \"Can container shell-style wildcards (e.g. ``'pkg.*'``)\"], 'items': {'type': 'string'}}, 'namespaces': {'type': 'boolean', '$$description': ['When ``True``, directories without a ``__init__.py`` file will also', 'be scanned for :pep:`420`-style implicit namespaces']}}}}}}}, rule='type')\n    data_is_dict = isinstance(data, dict)\n    if data_is_dict:\n        data_keys = set(data.keys())\n        if \"platforms\" in data_keys:\n            data_keys.remove(\"platforms\")\n            data__platforms = data[\"platforms\"]\n            if not isinstance(data__platforms, (list, tuple)):\n                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".platforms must be array\", value=data__platforms, name=\"\" + (name_prefix or \"data\") + \".platforms\", definition={'type': 'array', 'items': {'type': 'string'}}, rule='type')\n            data__platforms_is_list = isinstance(data__platforms, (list, tuple))\n            if data__platforms_is_list:\n                data__platforms_len = len(data__platforms)\n                for data__platforms_x, data__platforms_item in enumerate(data__platforms):\n                    if not isinstance(data__platforms_item, (str)):\n                        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".platforms[{data__platforms_x}]\".format(**locals()) + \" must be string\", value=data__platforms_item, name=\"\" + (name_prefix or \"data\") + \".platforms[{data__platforms_x}]\".format(**locals()) + \"\", definition={'type': 'string'}, rule='type')\n        if \"provides\" in data_keys:\n            data_keys.remove(\"provides\")\n            data__provides = data[\"provides\"]\n            if not isinstance(data__provides, (list, tuple)):\n                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".provides must be array\", value=data__provides, name=\"\" + (name_prefix or \"data\") + \".provides\", definition={'$$description': ['Package and virtual package names contained within this package', '**(not supported by pip)**'], 'type': 'array', 'items': {'type': 'string', 'format': 'pep508-identifier'}}, rule='type')\n            data__provides_is_list = isinstance(data__provides, (list, tuple))\n            if data__provides_is_list:\n                data__provides_len = len(data__provides)\n                for data__provides_x, data__provides_item in enumerate(data__provides):\n                    if not isinstance(data__provides_item, (str)):\n                        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".provides[{data__provides_x}]\".format(**locals()) + \" must be string\", value=data__provides_item, name=\"\" + (name_prefix or \"data\") + \".provides[{data__provides_x}]\".format(**locals()) + \"\", definition={'type': 'string', 'format': 'pep508-identifier'}, rule='type')\n                    if isinstance(data__provides_item, str):\n                        if not custom_formats[\"pep508-identifier\"](data__provides_item):\n                            raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".provides[{data__provides_x}]\".format(**locals()) + \" must be pep508-identifier\", value=data__provides_item, name=\"\" + (name_prefix or \"data\") + \".provides[{data__provides_x}]\".format(**locals()) + \"\", definition={'type': 'string', 'format': 'pep508-identifier'}, rule='format')\n        if \"obsoletes\" in data_keys:\n            data_keys.remove(\"obsoletes\")\n            data__obsoletes = data[\"obsoletes\"]\n            if not isinstance(data__obsoletes, (list, tuple)):\n                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".obsoletes must be array\", value=data__obsoletes, name=\"\" + (name_prefix or \"data\") + \".obsoletes\", definition={'$$description': ['Packages which this package renders obsolete', '**(not supported by pip)**'], 'type': 'array', 'items': {'type': 'string', 'format': 'pep508-identifier'}}, rule='type')\n            data__obsoletes_is_list = isinstance(data__obsoletes, (list, tuple))\n            if data__obsoletes_is_list:\n                data__obsoletes_len = len(data__obsoletes)\n                for data__obsoletes_x, data__obsoletes_item in enumerate(data__obsoletes):\n                    if not isinstance(data__obsoletes_item, (str)):\n                        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".obsoletes[{data__obsoletes_x}]\".format(**locals()) + \" must be string\", value=data__obsoletes_item, name=\"\" + (name_prefix or \"data\") + \".obsoletes[{data__obsoletes_x}]\".format(**locals()) + \"\", definition={'type': 'string', 'format': 'pep508-identifier'}, rule='type')\n                    if isinstance(data__obsoletes_item, str):\n                        if not custom_formats[\"pep508-identifier\"](data__obsoletes_item):\n                            raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".obsoletes[{data__obsoletes_x}]\".format(**locals()) + \" must be pep508-identifier\", value=data__obsoletes_item, name=\"\" + (name_prefix or \"data\") + \".obsoletes[{data__obsoletes_x}]\".format(**locals()) + \"\", definition={'type': 'string', 'format': 'pep508-identifier'}, rule='format')\n        if \"zip-safe\" in data_keys:\n            data_keys.remove(\"zip-safe\")\n            data__zipsafe = data[\"zip-safe\"]\n            if not isinstance(data__zipsafe, (bool)):\n                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".zip-safe must be boolean\", value=data__zipsafe, name=\"\" + (name_prefix or \"data\") + \".zip-safe\", definition={'$$description': ['Whether the project can be safely installed and run from a zip file.', '**OBSOLETE**: only relevant for ``pkg_resources``, ``easy_install`` and', '``setup.py install`` in the context of ``eggs`` (**DEPRECATED**).'], 'type': 'boolean'}, rule='type')\n        if \"script-files\" in data_keys:\n            data_keys.remove(\"script-files\")\n            data__scriptfiles = data[\"script-files\"]\n            if not isinstance(data__scriptfiles, (list, tuple)):\n                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".script-files must be array\", value=data__scriptfiles, name=\"\" + (name_prefix or \"data\") + \".script-files\", definition={'$$description': ['Legacy way of defining scripts (entry-points are preferred).', 'Equivalent to the ``script`` keyword in ``setup.py``', '(it was renamed to avoid confusion with entry-point based ``project.scripts``', 'defined in :pep:`621`).', '**DISCOURAGED**: generic script wrappers are tricky and may not work properly.', 'Whenever possible, please use ``project.scripts`` instead.'], 'type': 'array', 'items': {'type': 'string'}, '$comment': 'TODO: is this field deprecated/should be removed?'}, rule='type')\n            data__scriptfiles_is_list = isinstance(data__scriptfiles, (list, tuple))\n            if data__scriptfiles_is_list:\n                data__scriptfiles_len = len(data__scriptfiles)\n                for data__scriptfiles_x, data__scriptfiles_item in enumerate(data__scriptfiles):\n                    if not isinstance(data__scriptfiles_item, (str)):\n                        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".script-files[{data__scriptfiles_x}]\".format(**locals()) + \" must be string\", value=data__scriptfiles_item, name=\"\" + (name_prefix or \"data\") + \".script-files[{data__scriptfiles_x}]\".format(**locals()) + \"\", definition={'type': 'string'}, rule='type')\n        if \"eager-resources\" in data_keys:\n            data_keys.remove(\"eager-resources\")\n            data__eagerresources = data[\"eager-resources\"]\n            if not isinstance(data__eagerresources, (list, tuple)):\n                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".eager-resources must be array\", value=data__eagerresources, name=\"\" + (name_prefix or \"data\") + \".eager-resources\", definition={'$$description': ['Resources that should be extracted together, if any of them is needed,', 'or if any C extensions included in the project are imported.', '**OBSOLETE**: only relevant for ``pkg_resources``, ``easy_install`` and', '``setup.py install`` in the context of ``eggs`` (**DEPRECATED**).'], 'type': 'array', 'items': {'type': 'string'}}, rule='type')\n            data__eagerresources_is_list = isinstance(data__eagerresources, (list, tuple))\n            if data__eagerresources_is_list:\n                data__eagerresources_len = len(data__eagerresources)\n                for data__eagerresources_x, data__eagerresources_item in enumerate(data__eagerresources):\n                    if not isinstance(data__eagerresources_item, (str)):\n                        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".eager-resources[{data__eagerresources_x}]\".format(**locals()) + \" must be string\", value=data__eagerresources_item, name=\"\" + (name_prefix or \"data\") + \".eager-resources[{data__eagerresources_x}]\".format(**locals()) + \"\", definition={'type': 'string'}, rule='type')\n        if \"packages\" in data_keys:\n            data_keys.remove(\"packages\")\n            data__packages = data[\"packages\"]\n            data__packages_one_of_count1 = 0\n            if data__packages_one_of_count1 < 2:\n                try:\n                    if not isinstance(data__packages, (list, tuple)):\n                        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".packages must be array\", value=data__packages, name=\"\" + (name_prefix or \"data\") + \".packages\", definition={'title': 'Array of Python package identifiers', 'type': 'array', 'items': {'$id': '#/definitions/package-name', 'title': 'Valid package name', 'description': 'Valid package name (importable or :pep:`561`).', 'type': 'string', 'anyOf': [{'type': 'string', 'format': 'python-module-name-relaxed'}, {'type': 'string', 'format': 'pep561-stub-name'}]}}, rule='type')\n                    data__packages_is_list = isinstance(data__packages, (list, tuple))\n                    if data__packages_is_list:\n                        data__packages_len = len(data__packages)\n                        for data__packages_x, data__packages_item in enumerate(data__packages):\n                            validate_https___setuptools_pypa_io_en_latest_userguide_pyproject_config_html__definitions_package_name(data__packages_item, custom_formats, (name_prefix or \"data\") + \".packages[{data__packages_x}]\".format(**locals()))\n                    data__packages_one_of_count1 += 1\n                except JsonSchemaValueException: pass\n            if data__packages_one_of_count1 < 2:\n                try:\n                    validate_https___setuptools_pypa_io_en_latest_userguide_pyproject_config_html__definitions_find_directive(data__packages, custom_formats, (name_prefix or \"data\") + \".packages\")\n                    data__packages_one_of_count1 += 1\n                except JsonSchemaValueException: pass\n            if data__packages_one_of_count1 != 1:\n                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".packages must be valid exactly by one definition\" + (\" (\" + str(data__packages_one_of_count1) + \" matches found)\"), value=data__packages, name=\"\" + (name_prefix or \"data\") + \".packages\", definition={'$$description': ['Packages that should be included in the distribution.', 'It can be given either as a list of package identifiers', 'or as a ``dict``-like structure with a single key ``find``', 'which corresponds to a dynamic call to', '``setuptools.config.expand.find_packages`` function.', 'The ``find`` key is associated with a nested ``dict``-like structure that can', 'contain ``where``, ``include``, ``exclude`` and ``namespaces`` keys,', 'mimicking the keyword arguments of the associated function.'], 'oneOf': [{'title': 'Array of Python package identifiers', 'type': 'array', 'items': {'$id': '#/definitions/package-name', 'title': 'Valid package name', 'description': 'Valid package name (importable or :pep:`561`).', 'type': 'string', 'anyOf': [{'type': 'string', 'format': 'python-module-name-relaxed'}, {'type': 'string', 'format': 'pep561-stub-name'}]}}, {'$id': '#/definitions/find-directive', 'title': \"'find:' directive\", 'type': 'object', 'additionalProperties': False, 'properties': {'find': {'type': 'object', '$$description': ['Dynamic `package discovery', '<https://setuptools.pypa.io/en/latest/userguide/package_discovery.html>`_.'], 'additionalProperties': False, 'properties': {'where': {'description': 'Directories to be searched for packages (Unix-style relative path)', 'type': 'array', 'items': {'type': 'string'}}, 'exclude': {'type': 'array', '$$description': ['Exclude packages that match the values listed in this field.', \"Can container shell-style wildcards (e.g. ``'pkg.*'``)\"], 'items': {'type': 'string'}}, 'include': {'type': 'array', '$$description': ['Restrict the found packages to just the ones listed in this field.', \"Can container shell-style wildcards (e.g. ``'pkg.*'``)\"], 'items': {'type': 'string'}}, 'namespaces': {'type': 'boolean', '$$description': ['When ``True``, directories without a ``__init__.py`` file will also', 'be scanned for :pep:`420`-style implicit namespaces']}}}}}]}, rule='oneOf')\n        if \"package-dir\" in data_keys:\n            data_keys.remove(\"package-dir\")\n            data__packagedir = data[\"package-dir\"]\n            if not isinstance(data__packagedir, (dict)):\n                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".package-dir must be object\", value=data__packagedir, name=\"\" + (name_prefix or \"data\") + \".package-dir\", definition={'$$description': [':class:`dict`-like structure mapping from package names to directories where their', 'code can be found.', 'The empty string (as key) means that all packages are contained inside', 'the given directory will be included in the distribution.'], 'type': 'object', 'additionalProperties': False, 'propertyNames': {'anyOf': [{'const': ''}, {'$id': '#/definitions/package-name', 'title': 'Valid package name', 'description': 'Valid package name (importable or :pep:`561`).', 'type': 'string', 'anyOf': [{'type': 'string', 'format': 'python-module-name-relaxed'}, {'type': 'string', 'format': 'pep561-stub-name'}]}]}, 'patternProperties': {'^.*$': {'type': 'string'}}}, rule='type')\n            data__packagedir_is_dict = isinstance(data__packagedir, dict)\n            if data__packagedir_is_dict:\n                data__packagedir_keys = set(data__packagedir.keys())\n                for data__packagedir_key, data__packagedir_val in data__packagedir.items():\n                    if REGEX_PATTERNS['^.*$'].search(data__packagedir_key):\n                        if data__packagedir_key in data__packagedir_keys:\n                            data__packagedir_keys.remove(data__packagedir_key)\n                        if not isinstance(data__packagedir_val, (str)):\n                            raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".package-dir.{data__packagedir_key}\".format(**locals()) + \" must be string\", value=data__packagedir_val, name=\"\" + (name_prefix or \"data\") + \".package-dir.{data__packagedir_key}\".format(**locals()) + \"\", definition={'type': 'string'}, rule='type')\n                if data__packagedir_keys:\n                    raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".package-dir must not contain \"+str(data__packagedir_keys)+\" properties\", value=data__packagedir, name=\"\" + (name_prefix or \"data\") + \".package-dir\", definition={'$$description': [':class:`dict`-like structure mapping from package names to directories where their', 'code can be found.', 'The empty string (as key) means that all packages are contained inside', 'the given directory will be included in the distribution.'], 'type': 'object', 'additionalProperties': False, 'propertyNames': {'anyOf': [{'const': ''}, {'$id': '#/definitions/package-name', 'title': 'Valid package name', 'description': 'Valid package name (importable or :pep:`561`).', 'type': 'string', 'anyOf': [{'type': 'string', 'format': 'python-module-name-relaxed'}, {'type': 'string', 'format': 'pep561-stub-name'}]}]}, 'patternProperties': {'^.*$': {'type': 'string'}}}, rule='additionalProperties')\n                data__packagedir_len = len(data__packagedir)\n                if data__packagedir_len != 0:\n                    data__packagedir_property_names = True\n                    for data__packagedir_key in data__packagedir:\n                        try:\n                            data__packagedir_key_any_of_count2 = 0\n                            if not data__packagedir_key_any_of_count2:\n                                try:\n                                    if data__packagedir_key != \"\":\n                                        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".package-dir must be same as const definition: \", value=data__packagedir_key, name=\"\" + (name_prefix or \"data\") + \".package-dir\", definition={'const': ''}, rule='const')\n                                    data__packagedir_key_any_of_count2 += 1\n                                except JsonSchemaValueException: pass\n                            if not data__packagedir_key_any_of_count2:\n                                try:\n                                    validate_https___setuptools_pypa_io_en_latest_userguide_pyproject_config_html__definitions_package_name(data__packagedir_key, custom_formats, (name_prefix or \"data\") + \".package-dir\")\n                                    data__packagedir_key_any_of_count2 += 1\n                                except JsonSchemaValueException: pass\n                            if not data__packagedir_key_any_of_count2:\n                                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".package-dir cannot be validated by any definition\", value=data__packagedir_key, name=\"\" + (name_prefix or \"data\") + \".package-dir\", definition={'anyOf': [{'const': ''}, {'$id': '#/definitions/package-name', 'title': 'Valid package name', 'description': 'Valid package name (importable or :pep:`561`).', 'type': 'string', 'anyOf': [{'type': 'string', 'format': 'python-module-name-relaxed'}, {'type': 'string', 'format': 'pep561-stub-name'}]}]}, rule='anyOf')\n                        except JsonSchemaValueException:\n                            data__packagedir_property_names = False\n                    if not data__packagedir_property_names:\n                        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".package-dir must be named by propertyName definition\", value=data__packagedir, name=\"\" + (name_prefix or \"data\") + \".package-dir\", definition={'$$description': [':class:`dict`-like structure mapping from package names to directories where their', 'code can be found.', 'The empty string (as key) means that all packages are contained inside', 'the given directory will be included in the distribution.'], 'type': 'object', 'additionalProperties': False, 'propertyNames': {'anyOf': [{'const': ''}, {'$id': '#/definitions/package-name', 'title': 'Valid package name', 'description': 'Valid package name (importable or :pep:`561`).', 'type': 'string', 'anyOf': [{'type': 'string', 'format': 'python-module-name-relaxed'}, {'type': 'string', 'format': 'pep561-stub-name'}]}]}, 'patternProperties': {'^.*$': {'type': 'string'}}}, rule='propertyNames')\n        if \"package-data\" in data_keys:\n            data_keys.remove(\"package-data\")\n            data__packagedata = data[\"package-data\"]\n            if not isinstance(data__packagedata, (dict)):\n                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".package-data must be object\", value=data__packagedata, name=\"\" + (name_prefix or \"data\") + \".package-data\", definition={'$$description': ['Mapping from package names to lists of glob patterns.', 'Usually this option is not needed when using ``include-package-data = true``', 'For more information on how to include data files, check ``setuptools`` `docs', '<https://setuptools.pypa.io/en/latest/userguide/datafiles.html>`_.'], 'type': 'object', 'additionalProperties': False, 'propertyNames': {'anyOf': [{'type': 'string', 'format': 'python-module-name'}, {'const': '*'}]}, 'patternProperties': {'^.*$': {'type': 'array', 'items': {'type': 'string'}}}}, rule='type')\n            data__packagedata_is_dict = isinstance(data__packagedata, dict)\n            if data__packagedata_is_dict:\n                data__packagedata_keys = set(data__packagedata.keys())\n                for data__packagedata_key, data__packagedata_val in data__packagedata.items():\n                    if REGEX_PATTERNS['^.*$'].search(data__packagedata_key):\n                        if data__packagedata_key in data__packagedata_keys:\n                            data__packagedata_keys.remove(data__packagedata_key)\n                        if not isinstance(data__packagedata_val, (list, tuple)):\n                            raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".package-data.{data__packagedata_key}\".format(**locals()) + \" must be array\", value=data__packagedata_val, name=\"\" + (name_prefix or \"data\") + \".package-data.{data__packagedata_key}\".format(**locals()) + \"\", definition={'type': 'array', 'items': {'type': 'string'}}, rule='type')\n                        data__packagedata_val_is_list = isinstance(data__packagedata_val, (list, tuple))\n                        if data__packagedata_val_is_list:\n                            data__packagedata_val_len = len(data__packagedata_val)\n                            for data__packagedata_val_x, data__packagedata_val_item in enumerate(data__packagedata_val):\n                                if not isinstance(data__packagedata_val_item, (str)):\n                                    raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".package-data.{data__packagedata_key}[{data__packagedata_val_x}]\".format(**locals()) + \" must be string\", value=data__packagedata_val_item, name=\"\" + (name_prefix or \"data\") + \".package-data.{data__packagedata_key}[{data__packagedata_val_x}]\".format(**locals()) + \"\", definition={'type': 'string'}, rule='type')\n                if data__packagedata_keys:\n                    raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".package-data must not contain \"+str(data__packagedata_keys)+\" properties\", value=data__packagedata, name=\"\" + (name_prefix or \"data\") + \".package-data\", definition={'$$description': ['Mapping from package names to lists of glob patterns.', 'Usually this option is not needed when using ``include-package-data = true``', 'For more information on how to include data files, check ``setuptools`` `docs', '<https://setuptools.pypa.io/en/latest/userguide/datafiles.html>`_.'], 'type': 'object', 'additionalProperties': False, 'propertyNames': {'anyOf': [{'type': 'string', 'format': 'python-module-name'}, {'const': '*'}]}, 'patternProperties': {'^.*$': {'type': 'array', 'items': {'type': 'string'}}}}, rule='additionalProperties')\n                data__packagedata_len = len(data__packagedata)\n                if data__packagedata_len != 0:\n                    data__packagedata_property_names = True\n                    for data__packagedata_key in data__packagedata:\n                        try:\n                            data__packagedata_key_any_of_count3 = 0\n                            if not data__packagedata_key_any_of_count3:\n                                try:\n                                    if not isinstance(data__packagedata_key, (str)):\n                                        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".package-data must be string\", value=data__packagedata_key, name=\"\" + (name_prefix or \"data\") + \".package-data\", definition={'type': 'string', 'format': 'python-module-name'}, rule='type')\n                                    if isinstance(data__packagedata_key, str):\n                                        if not custom_formats[\"python-module-name\"](data__packagedata_key):\n                                            raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".package-data must be python-module-name\", value=data__packagedata_key, name=\"\" + (name_prefix or \"data\") + \".package-data\", definition={'type': 'string', 'format': 'python-module-name'}, rule='format')\n                                    data__packagedata_key_any_of_count3 += 1\n                                except JsonSchemaValueException: pass\n                            if not data__packagedata_key_any_of_count3:\n                                try:\n                                    if data__packagedata_key != \"*\":\n                                        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".package-data must be same as const definition: *\", value=data__packagedata_key, name=\"\" + (name_prefix or \"data\") + \".package-data\", definition={'const': '*'}, rule='const')\n                                    data__packagedata_key_any_of_count3 += 1\n                                except JsonSchemaValueException: pass\n                            if not data__packagedata_key_any_of_count3:\n                                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".package-data cannot be validated by any definition\", value=data__packagedata_key, name=\"\" + (name_prefix or \"data\") + \".package-data\", definition={'anyOf': [{'type': 'string', 'format': 'python-module-name'}, {'const': '*'}]}, rule='anyOf')\n                        except JsonSchemaValueException:\n                            data__packagedata_property_names = False\n                    if not data__packagedata_property_names:\n                        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".package-data must be named by propertyName definition\", value=data__packagedata, name=\"\" + (name_prefix or \"data\") + \".package-data\", definition={'$$description': ['Mapping from package names to lists of glob patterns.', 'Usually this option is not needed when using ``include-package-data = true``', 'For more information on how to include data files, check ``setuptools`` `docs', '<https://setuptools.pypa.io/en/latest/userguide/datafiles.html>`_.'], 'type': 'object', 'additionalProperties': False, 'propertyNames': {'anyOf': [{'type': 'string', 'format': 'python-module-name'}, {'const': '*'}]}, 'patternProperties': {'^.*$': {'type': 'array', 'items': {'type': 'string'}}}}, rule='propertyNames')\n        if \"include-package-data\" in data_keys:\n            data_keys.remove(\"include-package-data\")\n            data__includepackagedata = data[\"include-package-data\"]\n            if not isinstance(data__includepackagedata, (bool)):\n                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".include-package-data must be boolean\", value=data__includepackagedata, name=\"\" + (name_prefix or \"data\") + \".include-package-data\", definition={'$$description': ['Automatically include any data files inside the package directories', 'that are specified by ``MANIFEST.in``', 'For more information on how to include data files, check ``setuptools`` `docs', '<https://setuptools.pypa.io/en/latest/userguide/datafiles.html>`_.'], 'type': 'boolean'}, rule='type')\n        if \"exclude-package-data\" in data_keys:\n            data_keys.remove(\"exclude-package-data\")\n            data__excludepackagedata = data[\"exclude-package-data\"]\n            if not isinstance(data__excludepackagedata, (dict)):\n                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".exclude-package-data must be object\", value=data__excludepackagedata, name=\"\" + (name_prefix or \"data\") + \".exclude-package-data\", definition={'$$description': ['Mapping from package names to lists of glob patterns that should be excluded', 'For more information on how to include data files, check ``setuptools`` `docs', '<https://setuptools.pypa.io/en/latest/userguide/datafiles.html>`_.'], 'type': 'object', 'additionalProperties': False, 'propertyNames': {'anyOf': [{'type': 'string', 'format': 'python-module-name'}, {'const': '*'}]}, 'patternProperties': {'^.*$': {'type': 'array', 'items': {'type': 'string'}}}}, rule='type')\n            data__excludepackagedata_is_dict = isinstance(data__excludepackagedata, dict)\n            if data__excludepackagedata_is_dict:\n                data__excludepackagedata_keys = set(data__excludepackagedata.keys())\n                for data__excludepackagedata_key, data__excludepackagedata_val in data__excludepackagedata.items():\n                    if REGEX_PATTERNS['^.*$'].search(data__excludepackagedata_key):\n                        if data__excludepackagedata_key in data__excludepackagedata_keys:\n                            data__excludepackagedata_keys.remove(data__excludepackagedata_key)\n                        if not isinstance(data__excludepackagedata_val, (list, tuple)):\n                            raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".exclude-package-data.{data__excludepackagedata_key}\".format(**locals()) + \" must be array\", value=data__excludepackagedata_val, name=\"\" + (name_prefix or \"data\") + \".exclude-package-data.{data__excludepackagedata_key}\".format(**locals()) + \"\", definition={'type': 'array', 'items': {'type': 'string'}}, rule='type')\n                        data__excludepackagedata_val_is_list = isinstance(data__excludepackagedata_val, (list, tuple))\n                        if data__excludepackagedata_val_is_list:\n                            data__excludepackagedata_val_len = len(data__excludepackagedata_val)\n                            for data__excludepackagedata_val_x, data__excludepackagedata_val_item in enumerate(data__excludepackagedata_val):\n                                if not isinstance(data__excludepackagedata_val_item, (str)):\n                                    raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".exclude-package-data.{data__excludepackagedata_key}[{data__excludepackagedata_val_x}]\".format(**locals()) + \" must be string\", value=data__excludepackagedata_val_item, name=\"\" + (name_prefix or \"data\") + \".exclude-package-data.{data__excludepackagedata_key}[{data__excludepackagedata_val_x}]\".format(**locals()) + \"\", definition={'type': 'string'}, rule='type')\n                if data__excludepackagedata_keys:\n                    raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".exclude-package-data must not contain \"+str(data__excludepackagedata_keys)+\" properties\", value=data__excludepackagedata, name=\"\" + (name_prefix or \"data\") + \".exclude-package-data\", definition={'$$description': ['Mapping from package names to lists of glob patterns that should be excluded', 'For more information on how to include data files, check ``setuptools`` `docs', '<https://setuptools.pypa.io/en/latest/userguide/datafiles.html>`_.'], 'type': 'object', 'additionalProperties': False, 'propertyNames': {'anyOf': [{'type': 'string', 'format': 'python-module-name'}, {'const': '*'}]}, 'patternProperties': {'^.*$': {'type': 'array', 'items': {'type': 'string'}}}}, rule='additionalProperties')\n                data__excludepackagedata_len = len(data__excludepackagedata)\n                if data__excludepackagedata_len != 0:\n                    data__excludepackagedata_property_names = True\n                    for data__excludepackagedata_key in data__excludepackagedata:\n                        try:\n                            data__excludepackagedata_key_any_of_count4 = 0\n                            if not data__excludepackagedata_key_any_of_count4:\n                                try:\n                                    if not isinstance(data__excludepackagedata_key, (str)):\n                                        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".exclude-package-data must be string\", value=data__excludepackagedata_key, name=\"\" + (name_prefix or \"data\") + \".exclude-package-data\", definition={'type': 'string', 'format': 'python-module-name'}, rule='type')\n                                    if isinstance(data__excludepackagedata_key, str):\n                                        if not custom_formats[\"python-module-name\"](data__excludepackagedata_key):\n                                            raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".exclude-package-data must be python-module-name\", value=data__excludepackagedata_key, name=\"\" + (name_prefix or \"data\") + \".exclude-package-data\", definition={'type': 'string', 'format': 'python-module-name'}, rule='format')\n                                    data__excludepackagedata_key_any_of_count4 += 1\n                                except JsonSchemaValueException: pass\n                            if not data__excludepackagedata_key_any_of_count4:\n                                try:\n                                    if data__excludepackagedata_key != \"*\":\n                                        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".exclude-package-data must be same as const definition: *\", value=data__excludepackagedata_key, name=\"\" + (name_prefix or \"data\") + \".exclude-package-data\", definition={'const': '*'}, rule='const')\n                                    data__excludepackagedata_key_any_of_count4 += 1\n                                except JsonSchemaValueException: pass\n                            if not data__excludepackagedata_key_any_of_count4:\n                                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".exclude-package-data cannot be validated by any definition\", value=data__excludepackagedata_key, name=\"\" + (name_prefix or \"data\") + \".exclude-package-data\", definition={'anyOf': [{'type': 'string', 'format': 'python-module-name'}, {'const': '*'}]}, rule='anyOf')\n                        except JsonSchemaValueException:\n                            data__excludepackagedata_property_names = False\n                    if not data__excludepackagedata_property_names:\n                        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".exclude-package-data must be named by propertyName definition\", value=data__excludepackagedata, name=\"\" + (name_prefix or \"data\") + \".exclude-package-data\", definition={'$$description': ['Mapping from package names to lists of glob patterns that should be excluded', 'For more information on how to include data files, check ``setuptools`` `docs', '<https://setuptools.pypa.io/en/latest/userguide/datafiles.html>`_.'], 'type': 'object', 'additionalProperties': False, 'propertyNames': {'anyOf': [{'type': 'string', 'format': 'python-module-name'}, {'const': '*'}]}, 'patternProperties': {'^.*$': {'type': 'array', 'items': {'type': 'string'}}}}, rule='propertyNames')\n        if \"namespace-packages\" in data_keys:\n            data_keys.remove(\"namespace-packages\")\n            data__namespacepackages = data[\"namespace-packages\"]\n            if not isinstance(data__namespacepackages, (list, tuple)):\n                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".namespace-packages must be array\", value=data__namespacepackages, name=\"\" + (name_prefix or \"data\") + \".namespace-packages\", definition={'type': 'array', 'items': {'type': 'string', 'format': 'python-module-name-relaxed'}, '$comment': 'https://setuptools.pypa.io/en/latest/userguide/package_discovery.html', 'description': '**DEPRECATED**: use implicit namespaces instead (:pep:`420`).'}, rule='type')\n            data__namespacepackages_is_list = isinstance(data__namespacepackages, (list, tuple))\n            if data__namespacepackages_is_list:\n                data__namespacepackages_len = len(data__namespacepackages)\n                for data__namespacepackages_x, data__namespacepackages_item in enumerate(data__namespacepackages):\n                    if not isinstance(data__namespacepackages_item, (str)):\n                        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".namespace-packages[{data__namespacepackages_x}]\".format(**locals()) + \" must be string\", value=data__namespacepackages_item, name=\"\" + (name_prefix or \"data\") + \".namespace-packages[{data__namespacepackages_x}]\".format(**locals()) + \"\", definition={'type': 'string', 'format': 'python-module-name-relaxed'}, rule='type')\n                    if isinstance(data__namespacepackages_item, str):\n                        if not custom_formats[\"python-module-name-relaxed\"](data__namespacepackages_item):\n                            raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".namespace-packages[{data__namespacepackages_x}]\".format(**locals()) + \" must be python-module-name-relaxed\", value=data__namespacepackages_item, name=\"\" + (name_prefix or \"data\") + \".namespace-packages[{data__namespacepackages_x}]\".format(**locals()) + \"\", definition={'type': 'string', 'format': 'python-module-name-relaxed'}, rule='format')\n        if \"py-modules\" in data_keys:\n            data_keys.remove(\"py-modules\")\n            data__pymodules = data[\"py-modules\"]\n            if not isinstance(data__pymodules, (list, tuple)):\n                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".py-modules must be array\", value=data__pymodules, name=\"\" + (name_prefix or \"data\") + \".py-modules\", definition={'description': 'Modules that setuptools will manipulate', 'type': 'array', 'items': {'type': 'string', 'format': 'python-module-name-relaxed'}, '$comment': 'TODO: clarify the relationship with ``packages``'}, rule='type')\n            data__pymodules_is_list = isinstance(data__pymodules, (list, tuple))\n            if data__pymodules_is_list:\n                data__pymodules_len = len(data__pymodules)\n                for data__pymodules_x, data__pymodules_item in enumerate(data__pymodules):\n                    if not isinstance(data__pymodules_item, (str)):\n                        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".py-modules[{data__pymodules_x}]\".format(**locals()) + \" must be string\", value=data__pymodules_item, name=\"\" + (name_prefix or \"data\") + \".py-modules[{data__pymodules_x}]\".format(**locals()) + \"\", definition={'type': 'string', 'format': 'python-module-name-relaxed'}, rule='type')\n                    if isinstance(data__pymodules_item, str):\n                        if not custom_formats[\"python-module-name-relaxed\"](data__pymodules_item):\n                            raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".py-modules[{data__pymodules_x}]\".format(**locals()) + \" must be python-module-name-relaxed\", value=data__pymodules_item, name=\"\" + (name_prefix or \"data\") + \".py-modules[{data__pymodules_x}]\".format(**locals()) + \"\", definition={'type': 'string', 'format': 'python-module-name-relaxed'}, rule='format')\n        if \"ext-modules\" in data_keys:\n            data_keys.remove(\"ext-modules\")\n            data__extmodules = data[\"ext-modules\"]\n            if not isinstance(data__extmodules, (list, tuple)):\n                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".ext-modules must be array\", value=data__extmodules, name=\"\" + (name_prefix or \"data\") + \".ext-modules\", definition={'description': 'Extension modules to be compiled by setuptools', 'type': 'array', 'items': {'$id': '#/definitions/ext-module', 'title': 'Extension module', 'description': 'Parameters to construct a :class:`setuptools.Extension` object', 'type': 'object', 'required': ['name', 'sources'], 'additionalProperties': False, 'properties': {'name': {'type': 'string', 'format': 'python-module-name-relaxed'}, 'sources': {'type': 'array', 'items': {'type': 'string'}}, 'include-dirs': {'type': 'array', 'items': {'type': 'string'}}, 'define-macros': {'type': 'array', 'items': {'type': 'array', 'items': [{'description': 'macro name', 'type': 'string'}, {'description': 'macro value', 'oneOf': [{'type': 'string'}, {'type': 'null'}]}], 'additionalItems': False}}, 'undef-macros': {'type': 'array', 'items': {'type': 'string'}}, 'library-dirs': {'type': 'array', 'items': {'type': 'string'}}, 'libraries': {'type': 'array', 'items': {'type': 'string'}}, 'runtime-library-dirs': {'type': 'array', 'items': {'type': 'string'}}, 'extra-objects': {'type': 'array', 'items': {'type': 'string'}}, 'extra-compile-args': {'type': 'array', 'items': {'type': 'string'}}, 'extra-link-args': {'type': 'array', 'items': {'type': 'string'}}, 'export-symbols': {'type': 'array', 'items': {'type': 'string'}}, 'swig-opts': {'type': 'array', 'items': {'type': 'string'}}, 'depends': {'type': 'array', 'items': {'type': 'string'}}, 'language': {'type': 'string'}, 'optional': {'type': 'boolean'}, 'py-limited-api': {'type': 'boolean'}}}}, rule='type')\n            data__extmodules_is_list = isinstance(data__extmodules, (list, tuple))\n            if data__extmodules_is_list:\n                data__extmodules_len = len(data__extmodules)\n                for data__extmodules_x, data__extmodules_item in enumerate(data__extmodules):\n                    validate_https___setuptools_pypa_io_en_latest_userguide_pyproject_config_html__definitions_ext_module(data__extmodules_item, custom_formats, (name_prefix or \"data\") + \".ext-modules[{data__extmodules_x}]\".format(**locals()))\n        if \"data-files\" in data_keys:\n            data_keys.remove(\"data-files\")\n            data__datafiles = data[\"data-files\"]\n            if not isinstance(data__datafiles, (dict)):\n                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".data-files must be object\", value=data__datafiles, name=\"\" + (name_prefix or \"data\") + \".data-files\", definition={'$$description': ['``dict``-like structure where each key represents a directory and', 'the value is a list of glob patterns that should be installed in them.', '**DISCOURAGED**: please notice this might not work as expected with wheels.', 'Whenever possible, consider using data files inside the package directories', '(or create a new namespace package that only contains data files).', 'See `data files support', '<https://setuptools.pypa.io/en/latest/userguide/datafiles.html>`_.'], 'type': 'object', 'patternProperties': {'^.*$': {'type': 'array', 'items': {'type': 'string'}}}}, rule='type')\n            data__datafiles_is_dict = isinstance(data__datafiles, dict)\n            if data__datafiles_is_dict:\n                data__datafiles_keys = set(data__datafiles.keys())\n                for data__datafiles_key, data__datafiles_val in data__datafiles.items():\n                    if REGEX_PATTERNS['^.*$'].search(data__datafiles_key):\n                        if data__datafiles_key in data__datafiles_keys:\n                            data__datafiles_keys.remove(data__datafiles_key)\n                        if not isinstance(data__datafiles_val, (list, tuple)):\n                            raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".data-files.{data__datafiles_key}\".format(**locals()) + \" must be array\", value=data__datafiles_val, name=\"\" + (name_prefix or \"data\") + \".data-files.{data__datafiles_key}\".format(**locals()) + \"\", definition={'type': 'array', 'items': {'type': 'string'}}, rule='type')\n                        data__datafiles_val_is_list = isinstance(data__datafiles_val, (list, tuple))\n                        if data__datafiles_val_is_list:\n                            data__datafiles_val_len = len(data__datafiles_val)\n                            for data__datafiles_val_x, data__datafiles_val_item in enumerate(data__datafiles_val):\n                                if not isinstance(data__datafiles_val_item, (str)):\n                                    raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".data-files.{data__datafiles_key}[{data__datafiles_val_x}]\".format(**locals()) + \" must be string\", value=data__datafiles_val_item, name=\"\" + (name_prefix or \"data\") + \".data-files.{data__datafiles_key}[{data__datafiles_val_x}]\".format(**locals()) + \"\", definition={'type': 'string'}, rule='type')\n        if \"cmdclass\" in data_keys:\n            data_keys.remove(\"cmdclass\")\n            data__cmdclass = data[\"cmdclass\"]\n            if not isinstance(data__cmdclass, (dict)):\n                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".cmdclass must be object\", value=data__cmdclass, name=\"\" + (name_prefix or \"data\") + \".cmdclass\", definition={'$$description': ['Mapping of distutils-style command names to ``setuptools.Command`` subclasses', 'which in turn should be represented by strings with a qualified class name', '(i.e., \"dotted\" form with module), e.g.::\\n\\n', '    cmdclass = {mycmd = \"pkg.subpkg.module.CommandClass\"}\\n\\n', 'The command class should be a directly defined at the top-level of the', 'containing module (no class nesting).'], 'type': 'object', 'patternProperties': {'^.*$': {'type': 'string', 'format': 'python-qualified-identifier'}}}, rule='type')\n            data__cmdclass_is_dict = isinstance(data__cmdclass, dict)\n            if data__cmdclass_is_dict:\n                data__cmdclass_keys = set(data__cmdclass.keys())\n                for data__cmdclass_key, data__cmdclass_val in data__cmdclass.items():\n                    if REGEX_PATTERNS['^.*$'].search(data__cmdclass_key):\n                        if data__cmdclass_key in data__cmdclass_keys:\n                            data__cmdclass_keys.remove(data__cmdclass_key)\n                        if not isinstance(data__cmdclass_val, (str)):\n                            raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".cmdclass.{data__cmdclass_key}\".format(**locals()) + \" must be string\", value=data__cmdclass_val, name=\"\" + (name_prefix or \"data\") + \".cmdclass.{data__cmdclass_key}\".format(**locals()) + \"\", definition={'type': 'string', 'format': 'python-qualified-identifier'}, rule='type')\n                        if isinstance(data__cmdclass_val, str):\n                            if not custom_formats[\"python-qualified-identifier\"](data__cmdclass_val):\n                                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".cmdclass.{data__cmdclass_key}\".format(**locals()) + \" must be python-qualified-identifier\", value=data__cmdclass_val, name=\"\" + (name_prefix or \"data\") + \".cmdclass.{data__cmdclass_key}\".format(**locals()) + \"\", definition={'type': 'string', 'format': 'python-qualified-identifier'}, rule='format')\n        if \"license-files\" in data_keys:\n            data_keys.remove(\"license-files\")\n            data__licensefiles = data[\"license-files\"]\n            if not isinstance(data__licensefiles, (list, tuple)):\n                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".license-files must be array\", value=data__licensefiles, name=\"\" + (name_prefix or \"data\") + \".license-files\", definition={'type': 'array', 'items': {'type': 'string'}, '$$description': ['**PROVISIONAL**: list of glob patterns for all license files being distributed.', '(likely to become standard with :pep:`639`).', \"By default: ``['LICEN[CS]E*', 'COPYING*', 'NOTICE*', 'AUTHORS*']``\"], '$comment': 'TODO: revise if PEP 639 is accepted. Probably ``project.license-files``?'}, rule='type')\n            data__licensefiles_is_list = isinstance(data__licensefiles, (list, tuple))\n            if data__licensefiles_is_list:\n                data__licensefiles_len = len(data__licensefiles)\n                for data__licensefiles_x, data__licensefiles_item in enumerate(data__licensefiles):\n                    if not isinstance(data__licensefiles_item, (str)):\n                        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".license-files[{data__licensefiles_x}]\".format(**locals()) + \" must be string\", value=data__licensefiles_item, name=\"\" + (name_prefix or \"data\") + \".license-files[{data__licensefiles_x}]\".format(**locals()) + \"\", definition={'type': 'string'}, rule='type')\n        if \"dynamic\" in data_keys:\n            data_keys.remove(\"dynamic\")\n            data__dynamic = data[\"dynamic\"]\n            if not isinstance(data__dynamic, (dict)):\n                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".dynamic must be object\", value=data__dynamic, name=\"\" + (name_prefix or \"data\") + \".dynamic\", definition={'type': 'object', 'description': 'Instructions for loading :pep:`621`-related metadata dynamically', 'additionalProperties': False, 'properties': {'version': {'$$description': ['A version dynamically loaded via either the ``attr:`` or ``file:``', 'directives. Please make sure the given file or attribute respects :pep:`440`.', 'Also ensure to set ``project.dynamic`` accordingly.'], 'oneOf': [{'title': \"'attr:' directive\", '$id': '#/definitions/attr-directive', '$$description': ['Value is read from a module attribute. Supports callables and iterables;', 'unsupported types are cast via ``str()``'], 'type': 'object', 'additionalProperties': False, 'properties': {'attr': {'type': 'string', 'format': 'python-qualified-identifier'}}, 'required': ['attr']}, {'$id': '#/definitions/file-directive', 'title': \"'file:' directive\", 'description': 'Value is read from a file (or list of files and then concatenated)', 'type': 'object', 'additionalProperties': False, 'properties': {'file': {'oneOf': [{'type': 'string'}, {'type': 'array', 'items': {'type': 'string'}}]}}, 'required': ['file']}]}, 'classifiers': {'$id': '#/definitions/file-directive', 'title': \"'file:' directive\", 'description': 'Value is read from a file (or list of files and then concatenated)', 'type': 'object', 'additionalProperties': False, 'properties': {'file': {'oneOf': [{'type': 'string'}, {'type': 'array', 'items': {'type': 'string'}}]}}, 'required': ['file']}, 'description': {'$id': '#/definitions/file-directive', 'title': \"'file:' directive\", 'description': 'Value is read from a file (or list of files and then concatenated)', 'type': 'object', 'additionalProperties': False, 'properties': {'file': {'oneOf': [{'type': 'string'}, {'type': 'array', 'items': {'type': 'string'}}]}}, 'required': ['file']}, 'entry-points': {'$id': '#/definitions/file-directive', 'title': \"'file:' directive\", 'description': 'Value is read from a file (or list of files and then concatenated)', 'type': 'object', 'additionalProperties': False, 'properties': {'file': {'oneOf': [{'type': 'string'}, {'type': 'array', 'items': {'type': 'string'}}]}}, 'required': ['file']}, 'dependencies': {'title': \"'file:' directive for dependencies\", 'allOf': [{'$$description': ['**BETA**: subset of the ``requirements.txt`` format', 'without ``pip`` flags and options', '(one :pep:`508`-compliant string per line,', 'lines that are blank or start with ``#`` are excluded).', 'See `dynamic metadata', '<https://setuptools.pypa.io/en/latest/userguide/pyproject_config.html#dynamic-metadata>`_.']}, {'$ref': '#/definitions/file-directive'}]}, 'optional-dependencies': {'type': 'object', 'propertyNames': {'type': 'string', 'format': 'pep508-identifier'}, 'additionalProperties': False, 'patternProperties': {'.+': {'title': \"'file:' directive for dependencies\", 'allOf': [{'$$description': ['**BETA**: subset of the ``requirements.txt`` format', 'without ``pip`` flags and options', '(one :pep:`508`-compliant string per line,', 'lines that are blank or start with ``#`` are excluded).', 'See `dynamic metadata', '<https://setuptools.pypa.io/en/latest/userguide/pyproject_config.html#dynamic-metadata>`_.']}, {'$ref': '#/definitions/file-directive'}]}}}, 'readme': {'type': 'object', 'anyOf': [{'$id': '#/definitions/file-directive', 'title': \"'file:' directive\", 'description': 'Value is read from a file (or list of files and then concatenated)', 'type': 'object', 'additionalProperties': False, 'properties': {'file': {'oneOf': [{'type': 'string'}, {'type': 'array', 'items': {'type': 'string'}}]}}, 'required': ['file']}, {'type': 'object', 'properties': {'content-type': {'type': 'string'}, 'file': {'oneOf': [{'type': 'string'}, {'type': 'array', 'items': {'type': 'string'}}]}}, 'additionalProperties': False}], 'required': ['file']}}}, rule='type')\n            data__dynamic_is_dict = isinstance(data__dynamic, dict)\n            if data__dynamic_is_dict:\n                data__dynamic_keys = set(data__dynamic.keys())\n                if \"version\" in data__dynamic_keys:\n                    data__dynamic_keys.remove(\"version\")\n                    data__dynamic__version = data__dynamic[\"version\"]\n                    data__dynamic__version_one_of_count5 = 0\n                    if data__dynamic__version_one_of_count5 < 2:\n                        try:\n                            validate_https___setuptools_pypa_io_en_latest_userguide_pyproject_config_html__definitions_attr_directive(data__dynamic__version, custom_formats, (name_prefix or \"data\") + \".dynamic.version\")\n                            data__dynamic__version_one_of_count5 += 1\n                        except JsonSchemaValueException: pass\n                    if data__dynamic__version_one_of_count5 < 2:\n                        try:\n                            validate_https___setuptools_pypa_io_en_latest_userguide_pyproject_config_html__definitions_file_directive(data__dynamic__version, custom_formats, (name_prefix or \"data\") + \".dynamic.version\")\n                            data__dynamic__version_one_of_count5 += 1\n                        except JsonSchemaValueException: pass\n                    if data__dynamic__version_one_of_count5 != 1:\n                        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".dynamic.version must be valid exactly by one definition\" + (\" (\" + str(data__dynamic__version_one_of_count5) + \" matches found)\"), value=data__dynamic__version, name=\"\" + (name_prefix or \"data\") + \".dynamic.version\", definition={'$$description': ['A version dynamically loaded via either the ``attr:`` or ``file:``', 'directives. Please make sure the given file or attribute respects :pep:`440`.', 'Also ensure to set ``project.dynamic`` accordingly.'], 'oneOf': [{'title': \"'attr:' directive\", '$id': '#/definitions/attr-directive', '$$description': ['Value is read from a module attribute. Supports callables and iterables;', 'unsupported types are cast via ``str()``'], 'type': 'object', 'additionalProperties': False, 'properties': {'attr': {'type': 'string', 'format': 'python-qualified-identifier'}}, 'required': ['attr']}, {'$id': '#/definitions/file-directive', 'title': \"'file:' directive\", 'description': 'Value is read from a file (or list of files and then concatenated)', 'type': 'object', 'additionalProperties': False, 'properties': {'file': {'oneOf': [{'type': 'string'}, {'type': 'array', 'items': {'type': 'string'}}]}}, 'required': ['file']}]}, rule='oneOf')\n                if \"classifiers\" in data__dynamic_keys:\n                    data__dynamic_keys.remove(\"classifiers\")\n                    data__dynamic__classifiers = data__dynamic[\"classifiers\"]\n                    validate_https___setuptools_pypa_io_en_latest_userguide_pyproject_config_html__definitions_file_directive(data__dynamic__classifiers, custom_formats, (name_prefix or \"data\") + \".dynamic.classifiers\")\n                if \"description\" in data__dynamic_keys:\n                    data__dynamic_keys.remove(\"description\")\n                    data__dynamic__description = data__dynamic[\"description\"]\n                    validate_https___setuptools_pypa_io_en_latest_userguide_pyproject_config_html__definitions_file_directive(data__dynamic__description, custom_formats, (name_prefix or \"data\") + \".dynamic.description\")\n                if \"entry-points\" in data__dynamic_keys:\n                    data__dynamic_keys.remove(\"entry-points\")\n                    data__dynamic__entrypoints = data__dynamic[\"entry-points\"]\n                    validate_https___setuptools_pypa_io_en_latest_userguide_pyproject_config_html__definitions_file_directive(data__dynamic__entrypoints, custom_formats, (name_prefix or \"data\") + \".dynamic.entry-points\")\n                if \"dependencies\" in data__dynamic_keys:\n                    data__dynamic_keys.remove(\"dependencies\")\n                    data__dynamic__dependencies = data__dynamic[\"dependencies\"]\n                    validate_https___setuptools_pypa_io_en_latest_userguide_pyproject_config_html__definitions_file_directive_for_dependencies(data__dynamic__dependencies, custom_formats, (name_prefix or \"data\") + \".dynamic.dependencies\")\n                if \"optional-dependencies\" in data__dynamic_keys:\n                    data__dynamic_keys.remove(\"optional-dependencies\")\n                    data__dynamic__optionaldependencies = data__dynamic[\"optional-dependencies\"]\n                    if not isinstance(data__dynamic__optionaldependencies, (dict)):\n                        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".dynamic.optional-dependencies must be object\", value=data__dynamic__optionaldependencies, name=\"\" + (name_prefix or \"data\") + \".dynamic.optional-dependencies\", definition={'type': 'object', 'propertyNames': {'type': 'string', 'format': 'pep508-identifier'}, 'additionalProperties': False, 'patternProperties': {'.+': {'title': \"'file:' directive for dependencies\", 'allOf': [{'$$description': ['**BETA**: subset of the ``requirements.txt`` format', 'without ``pip`` flags and options', '(one :pep:`508`-compliant string per line,', 'lines that are blank or start with ``#`` are excluded).', 'See `dynamic metadata', '<https://setuptools.pypa.io/en/latest/userguide/pyproject_config.html#dynamic-metadata>`_.']}, {'$ref': '#/definitions/file-directive'}]}}}, rule='type')\n                    data__dynamic__optionaldependencies_is_dict = isinstance(data__dynamic__optionaldependencies, dict)\n                    if data__dynamic__optionaldependencies_is_dict:\n                        data__dynamic__optionaldependencies_keys = set(data__dynamic__optionaldependencies.keys())\n                        for data__dynamic__optionaldependencies_key, data__dynamic__optionaldependencies_val in data__dynamic__optionaldependencies.items():\n                            if REGEX_PATTERNS['.+'].search(data__dynamic__optionaldependencies_key):\n                                if data__dynamic__optionaldependencies_key in data__dynamic__optionaldependencies_keys:\n                                    data__dynamic__optionaldependencies_keys.remove(data__dynamic__optionaldependencies_key)\n                                validate_https___setuptools_pypa_io_en_latest_userguide_pyproject_config_html__definitions_file_directive_for_dependencies(data__dynamic__optionaldependencies_val, custom_formats, (name_prefix or \"data\") + \".dynamic.optional-dependencies.{data__dynamic__optionaldependencies_key}\".format(**locals()))\n                        if data__dynamic__optionaldependencies_keys:\n                            raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".dynamic.optional-dependencies must not contain \"+str(data__dynamic__optionaldependencies_keys)+\" properties\", value=data__dynamic__optionaldependencies, name=\"\" + (name_prefix or \"data\") + \".dynamic.optional-dependencies\", definition={'type': 'object', 'propertyNames': {'type': 'string', 'format': 'pep508-identifier'}, 'additionalProperties': False, 'patternProperties': {'.+': {'title': \"'file:' directive for dependencies\", 'allOf': [{'$$description': ['**BETA**: subset of the ``requirements.txt`` format', 'without ``pip`` flags and options', '(one :pep:`508`-compliant string per line,', 'lines that are blank or start with ``#`` are excluded).', 'See `dynamic metadata', '<https://setuptools.pypa.io/en/latest/userguide/pyproject_config.html#dynamic-metadata>`_.']}, {'$ref': '#/definitions/file-directive'}]}}}, rule='additionalProperties')\n                        data__dynamic__optionaldependencies_len = len(data__dynamic__optionaldependencies)\n                        if data__dynamic__optionaldependencies_len != 0:\n                            data__dynamic__optionaldependencies_property_names = True\n                            for data__dynamic__optionaldependencies_key in data__dynamic__optionaldependencies:\n                                try:\n                                    if not isinstance(data__dynamic__optionaldependencies_key, (str)):\n                                        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".dynamic.optional-dependencies must be string\", value=data__dynamic__optionaldependencies_key, name=\"\" + (name_prefix or \"data\") + \".dynamic.optional-dependencies\", definition={'type': 'string', 'format': 'pep508-identifier'}, rule='type')\n                                    if isinstance(data__dynamic__optionaldependencies_key, str):\n                                        if not custom_formats[\"pep508-identifier\"](data__dynamic__optionaldependencies_key):\n                                            raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".dynamic.optional-dependencies must be pep508-identifier\", value=data__dynamic__optionaldependencies_key, name=\"\" + (name_prefix or \"data\") + \".dynamic.optional-dependencies\", definition={'type': 'string', 'format': 'pep508-identifier'}, rule='format')\n                                except JsonSchemaValueException:\n                                    data__dynamic__optionaldependencies_property_names = False\n                            if not data__dynamic__optionaldependencies_property_names:\n                                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".dynamic.optional-dependencies must be named by propertyName definition\", value=data__dynamic__optionaldependencies, name=\"\" + (name_prefix or \"data\") + \".dynamic.optional-dependencies\", definition={'type': 'object', 'propertyNames': {'type': 'string', 'format': 'pep508-identifier'}, 'additionalProperties': False, 'patternProperties': {'.+': {'title': \"'file:' directive for dependencies\", 'allOf': [{'$$description': ['**BETA**: subset of the ``requirements.txt`` format', 'without ``pip`` flags and options', '(one :pep:`508`-compliant string per line,', 'lines that are blank or start with ``#`` are excluded).', 'See `dynamic metadata', '<https://setuptools.pypa.io/en/latest/userguide/pyproject_config.html#dynamic-metadata>`_.']}, {'$ref': '#/definitions/file-directive'}]}}}, rule='propertyNames')\n                if \"readme\" in data__dynamic_keys:\n                    data__dynamic_keys.remove(\"readme\")\n                    data__dynamic__readme = data__dynamic[\"readme\"]\n                    if not isinstance(data__dynamic__readme, (dict)):\n                        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".dynamic.readme must be object\", value=data__dynamic__readme, name=\"\" + (name_prefix or \"data\") + \".dynamic.readme\", definition={'type': 'object', 'anyOf': [{'$id': '#/definitions/file-directive', 'title': \"'file:' directive\", 'description': 'Value is read from a file (or list of files and then concatenated)', 'type': 'object', 'additionalProperties': False, 'properties': {'file': {'oneOf': [{'type': 'string'}, {'type': 'array', 'items': {'type': 'string'}}]}}, 'required': ['file']}, {'type': 'object', 'properties': {'content-type': {'type': 'string'}, 'file': {'oneOf': [{'type': 'string'}, {'type': 'array', 'items': {'type': 'string'}}]}}, 'additionalProperties': False}], 'required': ['file']}, rule='type')\n                    data__dynamic__readme_any_of_count6 = 0\n                    if not data__dynamic__readme_any_of_count6:\n                        try:\n                            validate_https___setuptools_pypa_io_en_latest_userguide_pyproject_config_html__definitions_file_directive(data__dynamic__readme, custom_formats, (name_prefix or \"data\") + \".dynamic.readme\")\n                            data__dynamic__readme_any_of_count6 += 1\n                        except JsonSchemaValueException: pass\n                    if not data__dynamic__readme_any_of_count6:\n                        try:\n                            if not isinstance(data__dynamic__readme, (dict)):\n                                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".dynamic.readme must be object\", value=data__dynamic__readme, name=\"\" + (name_prefix or \"data\") + \".dynamic.readme\", definition={'type': 'object', 'properties': {'content-type': {'type': 'string'}, 'file': {'oneOf': [{'type': 'string'}, {'type': 'array', 'items': {'type': 'string'}}]}}, 'additionalProperties': False}, rule='type')\n                            data__dynamic__readme_is_dict = isinstance(data__dynamic__readme, dict)\n                            if data__dynamic__readme_is_dict:\n                                data__dynamic__readme_keys = set(data__dynamic__readme.keys())\n                                if \"content-type\" in data__dynamic__readme_keys:\n                                    data__dynamic__readme_keys.remove(\"content-type\")\n                                    data__dynamic__readme__contenttype = data__dynamic__readme[\"content-type\"]\n                                    if not isinstance(data__dynamic__readme__contenttype, (str)):\n                                        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".dynamic.readme.content-type must be string\", value=data__dynamic__readme__contenttype, name=\"\" + (name_prefix or \"data\") + \".dynamic.readme.content-type\", definition={'type': 'string'}, rule='type')\n                                if \"file\" in data__dynamic__readme_keys:\n                                    data__dynamic__readme_keys.remove(\"file\")\n                                    data__dynamic__readme__file = data__dynamic__readme[\"file\"]\n                                    validate_https___setuptools_pypa_io_en_latest_userguide_pyproject_config_html__definitions_file_directive_properties_file(data__dynamic__readme__file, custom_formats, (name_prefix or \"data\") + \".dynamic.readme.file\")\n                                if data__dynamic__readme_keys:\n                                    raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".dynamic.readme must not contain \"+str(data__dynamic__readme_keys)+\" properties\", value=data__dynamic__readme, name=\"\" + (name_prefix or \"data\") + \".dynamic.readme\", definition={'type': 'object', 'properties': {'content-type': {'type': 'string'}, 'file': {'oneOf': [{'type': 'string'}, {'type': 'array', 'items': {'type': 'string'}}]}}, 'additionalProperties': False}, rule='additionalProperties')\n                            data__dynamic__readme_any_of_count6 += 1\n                        except JsonSchemaValueException: pass\n                    if not data__dynamic__readme_any_of_count6:\n                        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".dynamic.readme cannot be validated by any definition\", value=data__dynamic__readme, name=\"\" + (name_prefix or \"data\") + \".dynamic.readme\", definition={'type': 'object', 'anyOf': [{'$id': '#/definitions/file-directive', 'title': \"'file:' directive\", 'description': 'Value is read from a file (or list of files and then concatenated)', 'type': 'object', 'additionalProperties': False, 'properties': {'file': {'oneOf': [{'type': 'string'}, {'type': 'array', 'items': {'type': 'string'}}]}}, 'required': ['file']}, {'type': 'object', 'properties': {'content-type': {'type': 'string'}, 'file': {'oneOf': [{'type': 'string'}, {'type': 'array', 'items': {'type': 'string'}}]}}, 'additionalProperties': False}], 'required': ['file']}, rule='anyOf')\n                    data__dynamic__readme_is_dict = isinstance(data__dynamic__readme, dict)\n                    if data__dynamic__readme_is_dict:\n                        data__dynamic__readme__missing_keys = set(['file']) - data__dynamic__readme.keys()\n                        if data__dynamic__readme__missing_keys:\n                            raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".dynamic.readme must contain \" + (str(sorted(data__dynamic__readme__missing_keys)) + \" properties\"), value=data__dynamic__readme, name=\"\" + (name_prefix or \"data\") + \".dynamic.readme\", definition={'type': 'object', 'anyOf': [{'$id': '#/definitions/file-directive', 'title': \"'file:' directive\", 'description': 'Value is read from a file (or list of files and then concatenated)', 'type': 'object', 'additionalProperties': False, 'properties': {'file': {'oneOf': [{'type': 'string'}, {'type': 'array', 'items': {'type': 'string'}}]}}, 'required': ['file']}, {'type': 'object', 'properties': {'content-type': {'type': 'string'}, 'file': {'oneOf': [{'type': 'string'}, {'type': 'array', 'items': {'type': 'string'}}]}}, 'additionalProperties': False}], 'required': ['file']}, rule='required')\n                if data__dynamic_keys:\n                    raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".dynamic must not contain \"+str(data__dynamic_keys)+\" properties\", value=data__dynamic, name=\"\" + (name_prefix or \"data\") + \".dynamic\", definition={'type': 'object', 'description': 'Instructions for loading :pep:`621`-related metadata dynamically', 'additionalProperties': False, 'properties': {'version': {'$$description': ['A version dynamically loaded via either the ``attr:`` or ``file:``', 'directives. Please make sure the given file or attribute respects :pep:`440`.', 'Also ensure to set ``project.dynamic`` accordingly.'], 'oneOf': [{'title': \"'attr:' directive\", '$id': '#/definitions/attr-directive', '$$description': ['Value is read from a module attribute. Supports callables and iterables;', 'unsupported types are cast via ``str()``'], 'type': 'object', 'additionalProperties': False, 'properties': {'attr': {'type': 'string', 'format': 'python-qualified-identifier'}}, 'required': ['attr']}, {'$id': '#/definitions/file-directive', 'title': \"'file:' directive\", 'description': 'Value is read from a file (or list of files and then concatenated)', 'type': 'object', 'additionalProperties': False, 'properties': {'file': {'oneOf': [{'type': 'string'}, {'type': 'array', 'items': {'type': 'string'}}]}}, 'required': ['file']}]}, 'classifiers': {'$id': '#/definitions/file-directive', 'title': \"'file:' directive\", 'description': 'Value is read from a file (or list of files and then concatenated)', 'type': 'object', 'additionalProperties': False, 'properties': {'file': {'oneOf': [{'type': 'string'}, {'type': 'array', 'items': {'type': 'string'}}]}}, 'required': ['file']}, 'description': {'$id': '#/definitions/file-directive', 'title': \"'file:' directive\", 'description': 'Value is read from a file (or list of files and then concatenated)', 'type': 'object', 'additionalProperties': False, 'properties': {'file': {'oneOf': [{'type': 'string'}, {'type': 'array', 'items': {'type': 'string'}}]}}, 'required': ['file']}, 'entry-points': {'$id': '#/definitions/file-directive', 'title': \"'file:' directive\", 'description': 'Value is read from a file (or list of files and then concatenated)', 'type': 'object', 'additionalProperties': False, 'properties': {'file': {'oneOf': [{'type': 'string'}, {'type': 'array', 'items': {'type': 'string'}}]}}, 'required': ['file']}, 'dependencies': {'title': \"'file:' directive for dependencies\", 'allOf': [{'$$description': ['**BETA**: subset of the ``requirements.txt`` format', 'without ``pip`` flags and options', '(one :pep:`508`-compliant string per line,', 'lines that are blank or start with ``#`` are excluded).', 'See `dynamic metadata', '<https://setuptools.pypa.io/en/latest/userguide/pyproject_config.html#dynamic-metadata>`_.']}, {'$ref': '#/definitions/file-directive'}]}, 'optional-dependencies': {'type': 'object', 'propertyNames': {'type': 'string', 'format': 'pep508-identifier'}, 'additionalProperties': False, 'patternProperties': {'.+': {'title': \"'file:' directive for dependencies\", 'allOf': [{'$$description': ['**BETA**: subset of the ``requirements.txt`` format', 'without ``pip`` flags and options', '(one :pep:`508`-compliant string per line,', 'lines that are blank or start with ``#`` are excluded).', 'See `dynamic metadata', '<https://setuptools.pypa.io/en/latest/userguide/pyproject_config.html#dynamic-metadata>`_.']}, {'$ref': '#/definitions/file-directive'}]}}}, 'readme': {'type': 'object', 'anyOf': [{'$id': '#/definitions/file-directive', 'title': \"'file:' directive\", 'description': 'Value is read from a file (or list of files and then concatenated)', 'type': 'object', 'additionalProperties': False, 'properties': {'file': {'oneOf': [{'type': 'string'}, {'type': 'array', 'items': {'type': 'string'}}]}}, 'required': ['file']}, {'type': 'object', 'properties': {'content-type': {'type': 'string'}, 'file': {'oneOf': [{'type': 'string'}, {'type': 'array', 'items': {'type': 'string'}}]}}, 'additionalProperties': False}], 'required': ['file']}}}, rule='additionalProperties')\n        if data_keys:\n            raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \" must not contain \"+str(data_keys)+\" properties\", value=data, name=\"\" + (name_prefix or \"data\") + \"\", definition={'$schema': 'http://json-schema.org/draft-07/schema#', '$id': 'https://setuptools.pypa.io/en/latest/userguide/pyproject_config.html', 'title': '``tool.setuptools`` table', '$$description': ['``setuptools``-specific configurations that can be set by users that require', 'customization.', 'These configurations are completely optional and probably can be skipped when', 'creating simple packages. They are equivalent to some of the `Keywords', '<https://setuptools.pypa.io/en/latest/references/keywords.html>`_', 'used by the ``setup.py`` file, and can be set via the ``tool.setuptools`` table.', 'It considers only ``setuptools`` `parameters', '<https://setuptools.pypa.io/en/latest/userguide/pyproject_config.html#setuptools-specific-configuration>`_', 'that are not covered by :pep:`621`; and intentionally excludes ``dependency_links``', 'and ``setup_requires`` (incompatible with modern workflows/standards).'], 'type': 'object', 'additionalProperties': False, 'properties': {'platforms': {'type': 'array', 'items': {'type': 'string'}}, 'provides': {'$$description': ['Package and virtual package names contained within this package', '**(not supported by pip)**'], 'type': 'array', 'items': {'type': 'string', 'format': 'pep508-identifier'}}, 'obsoletes': {'$$description': ['Packages which this package renders obsolete', '**(not supported by pip)**'], 'type': 'array', 'items': {'type': 'string', 'format': 'pep508-identifier'}}, 'zip-safe': {'$$description': ['Whether the project can be safely installed and run from a zip file.', '**OBSOLETE**: only relevant for ``pkg_resources``, ``easy_install`` and', '``setup.py install`` in the context of ``eggs`` (**DEPRECATED**).'], 'type': 'boolean'}, 'script-files': {'$$description': ['Legacy way of defining scripts (entry-points are preferred).', 'Equivalent to the ``script`` keyword in ``setup.py``', '(it was renamed to avoid confusion with entry-point based ``project.scripts``', 'defined in :pep:`621`).', '**DISCOURAGED**: generic script wrappers are tricky and may not work properly.', 'Whenever possible, please use ``project.scripts`` instead.'], 'type': 'array', 'items': {'type': 'string'}, '$comment': 'TODO: is this field deprecated/should be removed?'}, 'eager-resources': {'$$description': ['Resources that should be extracted together, if any of them is needed,', 'or if any C extensions included in the project are imported.', '**OBSOLETE**: only relevant for ``pkg_resources``, ``easy_install`` and', '``setup.py install`` in the context of ``eggs`` (**DEPRECATED**).'], 'type': 'array', 'items': {'type': 'string'}}, 'packages': {'$$description': ['Packages that should be included in the distribution.', 'It can be given either as a list of package identifiers', 'or as a ``dict``-like structure with a single key ``find``', 'which corresponds to a dynamic call to', '``setuptools.config.expand.find_packages`` function.', 'The ``find`` key is associated with a nested ``dict``-like structure that can', 'contain ``where``, ``include``, ``exclude`` and ``namespaces`` keys,', 'mimicking the keyword arguments of the associated function.'], 'oneOf': [{'title': 'Array of Python package identifiers', 'type': 'array', 'items': {'$id': '#/definitions/package-name', 'title': 'Valid package name', 'description': 'Valid package name (importable or :pep:`561`).', 'type': 'string', 'anyOf': [{'type': 'string', 'format': 'python-module-name-relaxed'}, {'type': 'string', 'format': 'pep561-stub-name'}]}}, {'$id': '#/definitions/find-directive', 'title': \"'find:' directive\", 'type': 'object', 'additionalProperties': False, 'properties': {'find': {'type': 'object', '$$description': ['Dynamic `package discovery', '<https://setuptools.pypa.io/en/latest/userguide/package_discovery.html>`_.'], 'additionalProperties': False, 'properties': {'where': {'description': 'Directories to be searched for packages (Unix-style relative path)', 'type': 'array', 'items': {'type': 'string'}}, 'exclude': {'type': 'array', '$$description': ['Exclude packages that match the values listed in this field.', \"Can container shell-style wildcards (e.g. ``'pkg.*'``)\"], 'items': {'type': 'string'}}, 'include': {'type': 'array', '$$description': ['Restrict the found packages to just the ones listed in this field.', \"Can container shell-style wildcards (e.g. ``'pkg.*'``)\"], 'items': {'type': 'string'}}, 'namespaces': {'type': 'boolean', '$$description': ['When ``True``, directories without a ``__init__.py`` file will also', 'be scanned for :pep:`420`-style implicit namespaces']}}}}}]}, 'package-dir': {'$$description': [':class:`dict`-like structure mapping from package names to directories where their', 'code can be found.', 'The empty string (as key) means that all packages are contained inside', 'the given directory will be included in the distribution.'], 'type': 'object', 'additionalProperties': False, 'propertyNames': {'anyOf': [{'const': ''}, {'$id': '#/definitions/package-name', 'title': 'Valid package name', 'description': 'Valid package name (importable or :pep:`561`).', 'type': 'string', 'anyOf': [{'type': 'string', 'format': 'python-module-name-relaxed'}, {'type': 'string', 'format': 'pep561-stub-name'}]}]}, 'patternProperties': {'^.*$': {'type': 'string'}}}, 'package-data': {'$$description': ['Mapping from package names to lists of glob patterns.', 'Usually this option is not needed when using ``include-package-data = true``', 'For more information on how to include data files, check ``setuptools`` `docs', '<https://setuptools.pypa.io/en/latest/userguide/datafiles.html>`_.'], 'type': 'object', 'additionalProperties': False, 'propertyNames': {'anyOf': [{'type': 'string', 'format': 'python-module-name'}, {'const': '*'}]}, 'patternProperties': {'^.*$': {'type': 'array', 'items': {'type': 'string'}}}}, 'include-package-data': {'$$description': ['Automatically include any data files inside the package directories', 'that are specified by ``MANIFEST.in``', 'For more information on how to include data files, check ``setuptools`` `docs', '<https://setuptools.pypa.io/en/latest/userguide/datafiles.html>`_.'], 'type': 'boolean'}, 'exclude-package-data': {'$$description': ['Mapping from package names to lists of glob patterns that should be excluded', 'For more information on how to include data files, check ``setuptools`` `docs', '<https://setuptools.pypa.io/en/latest/userguide/datafiles.html>`_.'], 'type': 'object', 'additionalProperties': False, 'propertyNames': {'anyOf': [{'type': 'string', 'format': 'python-module-name'}, {'const': '*'}]}, 'patternProperties': {'^.*$': {'type': 'array', 'items': {'type': 'string'}}}}, 'namespace-packages': {'type': 'array', 'items': {'type': 'string', 'format': 'python-module-name-relaxed'}, '$comment': 'https://setuptools.pypa.io/en/latest/userguide/package_discovery.html', 'description': '**DEPRECATED**: use implicit namespaces instead (:pep:`420`).'}, 'py-modules': {'description': 'Modules that setuptools will manipulate', 'type': 'array', 'items': {'type': 'string', 'format': 'python-module-name-relaxed'}, '$comment': 'TODO: clarify the relationship with ``packages``'}, 'ext-modules': {'description': 'Extension modules to be compiled by setuptools', 'type': 'array', 'items': {'$id': '#/definitions/ext-module', 'title': 'Extension module', 'description': 'Parameters to construct a :class:`setuptools.Extension` object', 'type': 'object', 'required': ['name', 'sources'], 'additionalProperties': False, 'properties': {'name': {'type': 'string', 'format': 'python-module-name-relaxed'}, 'sources': {'type': 'array', 'items': {'type': 'string'}}, 'include-dirs': {'type': 'array', 'items': {'type': 'string'}}, 'define-macros': {'type': 'array', 'items': {'type': 'array', 'items': [{'description': 'macro name', 'type': 'string'}, {'description': 'macro value', 'oneOf': [{'type': 'string'}, {'type': 'null'}]}], 'additionalItems': False}}, 'undef-macros': {'type': 'array', 'items': {'type': 'string'}}, 'library-dirs': {'type': 'array', 'items': {'type': 'string'}}, 'libraries': {'type': 'array', 'items': {'type': 'string'}}, 'runtime-library-dirs': {'type': 'array', 'items': {'type': 'string'}}, 'extra-objects': {'type': 'array', 'items': {'type': 'string'}}, 'extra-compile-args': {'type': 'array', 'items': {'type': 'string'}}, 'extra-link-args': {'type': 'array', 'items': {'type': 'string'}}, 'export-symbols': {'type': 'array', 'items': {'type': 'string'}}, 'swig-opts': {'type': 'array', 'items': {'type': 'string'}}, 'depends': {'type': 'array', 'items': {'type': 'string'}}, 'language': {'type': 'string'}, 'optional': {'type': 'boolean'}, 'py-limited-api': {'type': 'boolean'}}}}, 'data-files': {'$$description': ['``dict``-like structure where each key represents a directory and', 'the value is a list of glob patterns that should be installed in them.', '**DISCOURAGED**: please notice this might not work as expected with wheels.', 'Whenever possible, consider using data files inside the package directories', '(or create a new namespace package that only contains data files).', 'See `data files support', '<https://setuptools.pypa.io/en/latest/userguide/datafiles.html>`_.'], 'type': 'object', 'patternProperties': {'^.*$': {'type': 'array', 'items': {'type': 'string'}}}}, 'cmdclass': {'$$description': ['Mapping of distutils-style command names to ``setuptools.Command`` subclasses', 'which in turn should be represented by strings with a qualified class name', '(i.e., \"dotted\" form with module), e.g.::\\n\\n', '    cmdclass = {mycmd = \"pkg.subpkg.module.CommandClass\"}\\n\\n', 'The command class should be a directly defined at the top-level of the', 'containing module (no class nesting).'], 'type': 'object', 'patternProperties': {'^.*$': {'type': 'string', 'format': 'python-qualified-identifier'}}}, 'license-files': {'type': 'array', 'items': {'type': 'string'}, '$$description': ['**PROVISIONAL**: list of glob patterns for all license files being distributed.', '(likely to become standard with :pep:`639`).', \"By default: ``['LICEN[CS]E*', 'COPYING*', 'NOTICE*', 'AUTHORS*']``\"], '$comment': 'TODO: revise if PEP 639 is accepted. Probably ``project.license-files``?'}, 'dynamic': {'type': 'object', 'description': 'Instructions for loading :pep:`621`-related metadata dynamically', 'additionalProperties': False, 'properties': {'version': {'$$description': ['A version dynamically loaded via either the ``attr:`` or ``file:``', 'directives. Please make sure the given file or attribute respects :pep:`440`.', 'Also ensure to set ``project.dynamic`` accordingly.'], 'oneOf': [{'title': \"'attr:' directive\", '$id': '#/definitions/attr-directive', '$$description': ['Value is read from a module attribute. Supports callables and iterables;', 'unsupported types are cast via ``str()``'], 'type': 'object', 'additionalProperties': False, 'properties': {'attr': {'type': 'string', 'format': 'python-qualified-identifier'}}, 'required': ['attr']}, {'$id': '#/definitions/file-directive', 'title': \"'file:' directive\", 'description': 'Value is read from a file (or list of files and then concatenated)', 'type': 'object', 'additionalProperties': False, 'properties': {'file': {'oneOf': [{'type': 'string'}, {'type': 'array', 'items': {'type': 'string'}}]}}, 'required': ['file']}]}, 'classifiers': {'$id': '#/definitions/file-directive', 'title': \"'file:' directive\", 'description': 'Value is read from a file (or list of files and then concatenated)', 'type': 'object', 'additionalProperties': False, 'properties': {'file': {'oneOf': [{'type': 'string'}, {'type': 'array', 'items': {'type': 'string'}}]}}, 'required': ['file']}, 'description': {'$id': '#/definitions/file-directive', 'title': \"'file:' directive\", 'description': 'Value is read from a file (or list of files and then concatenated)', 'type': 'object', 'additionalProperties': False, 'properties': {'file': {'oneOf': [{'type': 'string'}, {'type': 'array', 'items': {'type': 'string'}}]}}, 'required': ['file']}, 'entry-points': {'$id': '#/definitions/file-directive', 'title': \"'file:' directive\", 'description': 'Value is read from a file (or list of files and then concatenated)', 'type': 'object', 'additionalProperties': False, 'properties': {'file': {'oneOf': [{'type': 'string'}, {'type': 'array', 'items': {'type': 'string'}}]}}, 'required': ['file']}, 'dependencies': {'title': \"'file:' directive for dependencies\", 'allOf': [{'$$description': ['**BETA**: subset of the ``requirements.txt`` format', 'without ``pip`` flags and options', '(one :pep:`508`-compliant string per line,', 'lines that are blank or start with ``#`` are excluded).', 'See `dynamic metadata', '<https://setuptools.pypa.io/en/latest/userguide/pyproject_config.html#dynamic-metadata>`_.']}, {'$ref': '#/definitions/file-directive'}]}, 'optional-dependencies': {'type': 'object', 'propertyNames': {'type': 'string', 'format': 'pep508-identifier'}, 'additionalProperties': False, 'patternProperties': {'.+': {'title': \"'file:' directive for dependencies\", 'allOf': [{'$$description': ['**BETA**: subset of the ``requirements.txt`` format', 'without ``pip`` flags and options', '(one :pep:`508`-compliant string per line,', 'lines that are blank or start with ``#`` are excluded).', 'See `dynamic metadata', '<https://setuptools.pypa.io/en/latest/userguide/pyproject_config.html#dynamic-metadata>`_.']}, {'$ref': '#/definitions/file-directive'}]}}}, 'readme': {'type': 'object', 'anyOf': [{'$id': '#/definitions/file-directive', 'title': \"'file:' directive\", 'description': 'Value is read from a file (or list of files and then concatenated)', 'type': 'object', 'additionalProperties': False, 'properties': {'file': {'oneOf': [{'type': 'string'}, {'type': 'array', 'items': {'type': 'string'}}]}}, 'required': ['file']}, {'type': 'object', 'properties': {'content-type': {'type': 'string'}, 'file': {'oneOf': [{'type': 'string'}, {'type': 'array', 'items': {'type': 'string'}}]}}, 'additionalProperties': False}], 'required': ['file']}}}}, 'definitions': {'package-name': {'$id': '#/definitions/package-name', 'title': 'Valid package name', 'description': 'Valid package name (importable or :pep:`561`).', 'type': 'string', 'anyOf': [{'type': 'string', 'format': 'python-module-name-relaxed'}, {'type': 'string', 'format': 'pep561-stub-name'}]}, 'ext-module': {'$id': '#/definitions/ext-module', 'title': 'Extension module', 'description': 'Parameters to construct a :class:`setuptools.Extension` object', 'type': 'object', 'required': ['name', 'sources'], 'additionalProperties': False, 'properties': {'name': {'type': 'string', 'format': 'python-module-name-relaxed'}, 'sources': {'type': 'array', 'items': {'type': 'string'}}, 'include-dirs': {'type': 'array', 'items': {'type': 'string'}}, 'define-macros': {'type': 'array', 'items': {'type': 'array', 'items': [{'description': 'macro name', 'type': 'string'}, {'description': 'macro value', 'oneOf': [{'type': 'string'}, {'type': 'null'}]}], 'additionalItems': False}}, 'undef-macros': {'type': 'array', 'items': {'type': 'string'}}, 'library-dirs': {'type': 'array', 'items': {'type': 'string'}}, 'libraries': {'type': 'array', 'items': {'type': 'string'}}, 'runtime-library-dirs': {'type': 'array', 'items': {'type': 'string'}}, 'extra-objects': {'type': 'array', 'items': {'type': 'string'}}, 'extra-compile-args': {'type': 'array', 'items': {'type': 'string'}}, 'extra-link-args': {'type': 'array', 'items': {'type': 'string'}}, 'export-symbols': {'type': 'array', 'items': {'type': 'string'}}, 'swig-opts': {'type': 'array', 'items': {'type': 'string'}}, 'depends': {'type': 'array', 'items': {'type': 'string'}}, 'language': {'type': 'string'}, 'optional': {'type': 'boolean'}, 'py-limited-api': {'type': 'boolean'}}}, 'file-directive': {'$id': '#/definitions/file-directive', 'title': \"'file:' directive\", 'description': 'Value is read from a file (or list of files and then concatenated)', 'type': 'object', 'additionalProperties': False, 'properties': {'file': {'oneOf': [{'type': 'string'}, {'type': 'array', 'items': {'type': 'string'}}]}}, 'required': ['file']}, 'file-directive-for-dependencies': {'title': \"'file:' directive for dependencies\", 'allOf': [{'$$description': ['**BETA**: subset of the ``requirements.txt`` format', 'without ``pip`` flags and options', '(one :pep:`508`-compliant string per line,', 'lines that are blank or start with ``#`` are excluded).', 'See `dynamic metadata', '<https://setuptools.pypa.io/en/latest/userguide/pyproject_config.html#dynamic-metadata>`_.']}, {'$id': '#/definitions/file-directive', 'title': \"'file:' directive\", 'description': 'Value is read from a file (or list of files and then concatenated)', 'type': 'object', 'additionalProperties': False, 'properties': {'file': {'oneOf': [{'type': 'string'}, {'type': 'array', 'items': {'type': 'string'}}]}}, 'required': ['file']}]}, 'attr-directive': {'title': \"'attr:' directive\", '$id': '#/definitions/attr-directive', '$$description': ['Value is read from a module attribute. Supports callables and iterables;', 'unsupported types are cast via ``str()``'], 'type': 'object', 'additionalProperties': False, 'properties': {'attr': {'type': 'string', 'format': 'python-qualified-identifier'}}, 'required': ['attr']}, 'find-directive': {'$id': '#/definitions/find-directive', 'title': \"'find:' directive\", 'type': 'object', 'additionalProperties': False, 'properties': {'find': {'type': 'object', '$$description': ['Dynamic `package discovery', '<https://setuptools.pypa.io/en/latest/userguide/package_discovery.html>`_.'], 'additionalProperties': False, 'properties': {'where': {'description': 'Directories to be searched for packages (Unix-style relative path)', 'type': 'array', 'items': {'type': 'string'}}, 'exclude': {'type': 'array', '$$description': ['Exclude packages that match the values listed in this field.', \"Can container shell-style wildcards (e.g. ``'pkg.*'``)\"], 'items': {'type': 'string'}}, 'include': {'type': 'array', '$$description': ['Restrict the found packages to just the ones listed in this field.', \"Can container shell-style wildcards (e.g. ``'pkg.*'``)\"], 'items': {'type': 'string'}}, 'namespaces': {'type': 'boolean', '$$description': ['When ``True``, directories without a ``__init__.py`` file will also', 'be scanned for :pep:`420`-style implicit namespaces']}}}}}}}, rule='additionalProperties')\n    return data\n\ndef validate_https___setuptools_pypa_io_en_latest_userguide_pyproject_config_html__definitions_file_directive_properties_file(data, custom_formats={}, name_prefix=None):\n    data_one_of_count7 = 0\n    if data_one_of_count7 < 2:\n        try:\n            if not isinstance(data, (str)):\n                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \" must be string\", value=data, name=\"\" + (name_prefix or \"data\") + \"\", definition={'type': 'string'}, rule='type')\n            data_one_of_count7 += 1\n        except JsonSchemaValueException: pass\n    if data_one_of_count7 < 2:\n        try:\n            if not isinstance(data, (list, tuple)):\n                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \" must be array\", value=data, name=\"\" + (name_prefix or \"data\") + \"\", definition={'type': 'array', 'items': {'type': 'string'}}, rule='type')\n            data_is_list = isinstance(data, (list, tuple))\n            if data_is_list:\n                data_len = len(data)\n                for data_x, data_item in enumerate(data):\n                    if not isinstance(data_item, (str)):\n                        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \"[{data_x}]\".format(**locals()) + \" must be string\", value=data_item, name=\"\" + (name_prefix or \"data\") + \"[{data_x}]\".format(**locals()) + \"\", definition={'type': 'string'}, rule='type')\n            data_one_of_count7 += 1\n        except JsonSchemaValueException: pass\n    if data_one_of_count7 != 1:\n        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \" must be valid exactly by one definition\" + (\" (\" + str(data_one_of_count7) + \" matches found)\"), value=data, name=\"\" + (name_prefix or \"data\") + \"\", definition={'oneOf': [{'type': 'string'}, {'type': 'array', 'items': {'type': 'string'}}]}, rule='oneOf')\n    return data\n\ndef validate_https___setuptools_pypa_io_en_latest_userguide_pyproject_config_html__definitions_file_directive_for_dependencies(data, custom_formats={}, name_prefix=None):\n    validate_https___setuptools_pypa_io_en_latest_userguide_pyproject_config_html__definitions_file_directive(data, custom_formats, (name_prefix or \"data\") + \"\")\n    return data\n\ndef validate_https___setuptools_pypa_io_en_latest_userguide_pyproject_config_html__definitions_file_directive(data, custom_formats={}, name_prefix=None):\n    if not isinstance(data, (dict)):\n        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \" must be object\", value=data, name=\"\" + (name_prefix or \"data\") + \"\", definition={'$id': '#/definitions/file-directive', 'title': \"'file:' directive\", 'description': 'Value is read from a file (or list of files and then concatenated)', 'type': 'object', 'additionalProperties': False, 'properties': {'file': {'oneOf': [{'type': 'string'}, {'type': 'array', 'items': {'type': 'string'}}]}}, 'required': ['file']}, rule='type')\n    data_is_dict = isinstance(data, dict)\n    if data_is_dict:\n        data__missing_keys = set(['file']) - data.keys()\n        if data__missing_keys:\n            raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \" must contain \" + (str(sorted(data__missing_keys)) + \" properties\"), value=data, name=\"\" + (name_prefix or \"data\") + \"\", definition={'$id': '#/definitions/file-directive', 'title': \"'file:' directive\", 'description': 'Value is read from a file (or list of files and then concatenated)', 'type': 'object', 'additionalProperties': False, 'properties': {'file': {'oneOf': [{'type': 'string'}, {'type': 'array', 'items': {'type': 'string'}}]}}, 'required': ['file']}, rule='required')\n        data_keys = set(data.keys())\n        if \"file\" in data_keys:\n            data_keys.remove(\"file\")\n            data__file = data[\"file\"]\n            data__file_one_of_count8 = 0\n            if data__file_one_of_count8 < 2:\n                try:\n                    if not isinstance(data__file, (str)):\n                        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".file must be string\", value=data__file, name=\"\" + (name_prefix or \"data\") + \".file\", definition={'type': 'string'}, rule='type')\n                    data__file_one_of_count8 += 1\n                except JsonSchemaValueException: pass\n            if data__file_one_of_count8 < 2:\n                try:\n                    if not isinstance(data__file, (list, tuple)):\n                        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".file must be array\", value=data__file, name=\"\" + (name_prefix or \"data\") + \".file\", definition={'type': 'array', 'items': {'type': 'string'}}, rule='type')\n                    data__file_is_list = isinstance(data__file, (list, tuple))\n                    if data__file_is_list:\n                        data__file_len = len(data__file)\n                        for data__file_x, data__file_item in enumerate(data__file):\n                            if not isinstance(data__file_item, (str)):\n                                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".file[{data__file_x}]\".format(**locals()) + \" must be string\", value=data__file_item, name=\"\" + (name_prefix or \"data\") + \".file[{data__file_x}]\".format(**locals()) + \"\", definition={'type': 'string'}, rule='type')\n                    data__file_one_of_count8 += 1\n                except JsonSchemaValueException: pass\n            if data__file_one_of_count8 != 1:\n                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".file must be valid exactly by one definition\" + (\" (\" + str(data__file_one_of_count8) + \" matches found)\"), value=data__file, name=\"\" + (name_prefix or \"data\") + \".file\", definition={'oneOf': [{'type': 'string'}, {'type': 'array', 'items': {'type': 'string'}}]}, rule='oneOf')\n        if data_keys:\n            raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \" must not contain \"+str(data_keys)+\" properties\", value=data, name=\"\" + (name_prefix or \"data\") + \"\", definition={'$id': '#/definitions/file-directive', 'title': \"'file:' directive\", 'description': 'Value is read from a file (or list of files and then concatenated)', 'type': 'object', 'additionalProperties': False, 'properties': {'file': {'oneOf': [{'type': 'string'}, {'type': 'array', 'items': {'type': 'string'}}]}}, 'required': ['file']}, rule='additionalProperties')\n    return data\n\ndef validate_https___setuptools_pypa_io_en_latest_userguide_pyproject_config_html__definitions_attr_directive(data, custom_formats={}, name_prefix=None):\n    if not isinstance(data, (dict)):\n        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \" must be object\", value=data, name=\"\" + (name_prefix or \"data\") + \"\", definition={'title': \"'attr:' directive\", '$id': '#/definitions/attr-directive', '$$description': ['Value is read from a module attribute. Supports callables and iterables;', 'unsupported types are cast via ``str()``'], 'type': 'object', 'additionalProperties': False, 'properties': {'attr': {'type': 'string', 'format': 'python-qualified-identifier'}}, 'required': ['attr']}, rule='type')\n    data_is_dict = isinstance(data, dict)\n    if data_is_dict:\n        data__missing_keys = set(['attr']) - data.keys()\n        if data__missing_keys:\n            raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \" must contain \" + (str(sorted(data__missing_keys)) + \" properties\"), value=data, name=\"\" + (name_prefix or \"data\") + \"\", definition={'title': \"'attr:' directive\", '$id': '#/definitions/attr-directive', '$$description': ['Value is read from a module attribute. Supports callables and iterables;', 'unsupported types are cast via ``str()``'], 'type': 'object', 'additionalProperties': False, 'properties': {'attr': {'type': 'string', 'format': 'python-qualified-identifier'}}, 'required': ['attr']}, rule='required')\n        data_keys = set(data.keys())\n        if \"attr\" in data_keys:\n            data_keys.remove(\"attr\")\n            data__attr = data[\"attr\"]\n            if not isinstance(data__attr, (str)):\n                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".attr must be string\", value=data__attr, name=\"\" + (name_prefix or \"data\") + \".attr\", definition={'type': 'string', 'format': 'python-qualified-identifier'}, rule='type')\n            if isinstance(data__attr, str):\n                if not custom_formats[\"python-qualified-identifier\"](data__attr):\n                    raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".attr must be python-qualified-identifier\", value=data__attr, name=\"\" + (name_prefix or \"data\") + \".attr\", definition={'type': 'string', 'format': 'python-qualified-identifier'}, rule='format')\n        if data_keys:\n            raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \" must not contain \"+str(data_keys)+\" properties\", value=data, name=\"\" + (name_prefix or \"data\") + \"\", definition={'title': \"'attr:' directive\", '$id': '#/definitions/attr-directive', '$$description': ['Value is read from a module attribute. Supports callables and iterables;', 'unsupported types are cast via ``str()``'], 'type': 'object', 'additionalProperties': False, 'properties': {'attr': {'type': 'string', 'format': 'python-qualified-identifier'}}, 'required': ['attr']}, rule='additionalProperties')\n    return data\n\ndef validate_https___setuptools_pypa_io_en_latest_userguide_pyproject_config_html__definitions_ext_module(data, custom_formats={}, name_prefix=None):\n    if not isinstance(data, (dict)):\n        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \" must be object\", value=data, name=\"\" + (name_prefix or \"data\") + \"\", definition={'$id': '#/definitions/ext-module', 'title': 'Extension module', 'description': 'Parameters to construct a :class:`setuptools.Extension` object', 'type': 'object', 'required': ['name', 'sources'], 'additionalProperties': False, 'properties': {'name': {'type': 'string', 'format': 'python-module-name-relaxed'}, 'sources': {'type': 'array', 'items': {'type': 'string'}}, 'include-dirs': {'type': 'array', 'items': {'type': 'string'}}, 'define-macros': {'type': 'array', 'items': {'type': 'array', 'items': [{'description': 'macro name', 'type': 'string'}, {'description': 'macro value', 'oneOf': [{'type': 'string'}, {'type': 'null'}]}], 'additionalItems': False}}, 'undef-macros': {'type': 'array', 'items': {'type': 'string'}}, 'library-dirs': {'type': 'array', 'items': {'type': 'string'}}, 'libraries': {'type': 'array', 'items': {'type': 'string'}}, 'runtime-library-dirs': {'type': 'array', 'items': {'type': 'string'}}, 'extra-objects': {'type': 'array', 'items': {'type': 'string'}}, 'extra-compile-args': {'type': 'array', 'items': {'type': 'string'}}, 'extra-link-args': {'type': 'array', 'items': {'type': 'string'}}, 'export-symbols': {'type': 'array', 'items': {'type': 'string'}}, 'swig-opts': {'type': 'array', 'items': {'type': 'string'}}, 'depends': {'type': 'array', 'items': {'type': 'string'}}, 'language': {'type': 'string'}, 'optional': {'type': 'boolean'}, 'py-limited-api': {'type': 'boolean'}}}, rule='type')\n    data_is_dict = isinstance(data, dict)\n    if data_is_dict:\n        data__missing_keys = set(['name', 'sources']) - data.keys()\n        if data__missing_keys:\n            raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \" must contain \" + (str(sorted(data__missing_keys)) + \" properties\"), value=data, name=\"\" + (name_prefix or \"data\") + \"\", definition={'$id': '#/definitions/ext-module', 'title': 'Extension module', 'description': 'Parameters to construct a :class:`setuptools.Extension` object', 'type': 'object', 'required': ['name', 'sources'], 'additionalProperties': False, 'properties': {'name': {'type': 'string', 'format': 'python-module-name-relaxed'}, 'sources': {'type': 'array', 'items': {'type': 'string'}}, 'include-dirs': {'type': 'array', 'items': {'type': 'string'}}, 'define-macros': {'type': 'array', 'items': {'type': 'array', 'items': [{'description': 'macro name', 'type': 'string'}, {'description': 'macro value', 'oneOf': [{'type': 'string'}, {'type': 'null'}]}], 'additionalItems': False}}, 'undef-macros': {'type': 'array', 'items': {'type': 'string'}}, 'library-dirs': {'type': 'array', 'items': {'type': 'string'}}, 'libraries': {'type': 'array', 'items': {'type': 'string'}}, 'runtime-library-dirs': {'type': 'array', 'items': {'type': 'string'}}, 'extra-objects': {'type': 'array', 'items': {'type': 'string'}}, 'extra-compile-args': {'type': 'array', 'items': {'type': 'string'}}, 'extra-link-args': {'type': 'array', 'items': {'type': 'string'}}, 'export-symbols': {'type': 'array', 'items': {'type': 'string'}}, 'swig-opts': {'type': 'array', 'items': {'type': 'string'}}, 'depends': {'type': 'array', 'items': {'type': 'string'}}, 'language': {'type': 'string'}, 'optional': {'type': 'boolean'}, 'py-limited-api': {'type': 'boolean'}}}, rule='required')\n        data_keys = set(data.keys())\n        if \"name\" in data_keys:\n            data_keys.remove(\"name\")\n            data__name = data[\"name\"]\n            if not isinstance(data__name, (str)):\n                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".name must be string\", value=data__name, name=\"\" + (name_prefix or \"data\") + \".name\", definition={'type': 'string', 'format': 'python-module-name-relaxed'}, rule='type')\n            if isinstance(data__name, str):\n                if not custom_formats[\"python-module-name-relaxed\"](data__name):\n                    raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".name must be python-module-name-relaxed\", value=data__name, name=\"\" + (name_prefix or \"data\") + \".name\", definition={'type': 'string', 'format': 'python-module-name-relaxed'}, rule='format')\n        if \"sources\" in data_keys:\n            data_keys.remove(\"sources\")\n            data__sources = data[\"sources\"]\n            if not isinstance(data__sources, (list, tuple)):\n                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".sources must be array\", value=data__sources, name=\"\" + (name_prefix or \"data\") + \".sources\", definition={'type': 'array', 'items': {'type': 'string'}}, rule='type')\n            data__sources_is_list = isinstance(data__sources, (list, tuple))\n            if data__sources_is_list:\n                data__sources_len = len(data__sources)\n                for data__sources_x, data__sources_item in enumerate(data__sources):\n                    if not isinstance(data__sources_item, (str)):\n                        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".sources[{data__sources_x}]\".format(**locals()) + \" must be string\", value=data__sources_item, name=\"\" + (name_prefix or \"data\") + \".sources[{data__sources_x}]\".format(**locals()) + \"\", definition={'type': 'string'}, rule='type')\n        if \"include-dirs\" in data_keys:\n            data_keys.remove(\"include-dirs\")\n            data__includedirs = data[\"include-dirs\"]\n            if not isinstance(data__includedirs, (list, tuple)):\n                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".include-dirs must be array\", value=data__includedirs, name=\"\" + (name_prefix or \"data\") + \".include-dirs\", definition={'type': 'array', 'items': {'type': 'string'}}, rule='type')\n            data__includedirs_is_list = isinstance(data__includedirs, (list, tuple))\n            if data__includedirs_is_list:\n                data__includedirs_len = len(data__includedirs)\n                for data__includedirs_x, data__includedirs_item in enumerate(data__includedirs):\n                    if not isinstance(data__includedirs_item, (str)):\n                        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".include-dirs[{data__includedirs_x}]\".format(**locals()) + \" must be string\", value=data__includedirs_item, name=\"\" + (name_prefix or \"data\") + \".include-dirs[{data__includedirs_x}]\".format(**locals()) + \"\", definition={'type': 'string'}, rule='type')\n        if \"define-macros\" in data_keys:\n            data_keys.remove(\"define-macros\")\n            data__definemacros = data[\"define-macros\"]\n            if not isinstance(data__definemacros, (list, tuple)):\n                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".define-macros must be array\", value=data__definemacros, name=\"\" + (name_prefix or \"data\") + \".define-macros\", definition={'type': 'array', 'items': {'type': 'array', 'items': [{'description': 'macro name', 'type': 'string'}, {'description': 'macro value', 'oneOf': [{'type': 'string'}, {'type': 'null'}]}], 'additionalItems': False}}, rule='type')\n            data__definemacros_is_list = isinstance(data__definemacros, (list, tuple))\n            if data__definemacros_is_list:\n                data__definemacros_len = len(data__definemacros)\n                for data__definemacros_x, data__definemacros_item in enumerate(data__definemacros):\n                    if not isinstance(data__definemacros_item, (list, tuple)):\n                        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".define-macros[{data__definemacros_x}]\".format(**locals()) + \" must be array\", value=data__definemacros_item, name=\"\" + (name_prefix or \"data\") + \".define-macros[{data__definemacros_x}]\".format(**locals()) + \"\", definition={'type': 'array', 'items': [{'description': 'macro name', 'type': 'string'}, {'description': 'macro value', 'oneOf': [{'type': 'string'}, {'type': 'null'}]}], 'additionalItems': False}, rule='type')\n                    data__definemacros_item_is_list = isinstance(data__definemacros_item, (list, tuple))\n                    if data__definemacros_item_is_list:\n                        data__definemacros_item_len = len(data__definemacros_item)\n                        if data__definemacros_item_len > 0:\n                            data__definemacros_item__0 = data__definemacros_item[0]\n                            if not isinstance(data__definemacros_item__0, (str)):\n                                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".define-macros[{data__definemacros_x}][0]\".format(**locals()) + \" must be string\", value=data__definemacros_item__0, name=\"\" + (name_prefix or \"data\") + \".define-macros[{data__definemacros_x}][0]\".format(**locals()) + \"\", definition={'description': 'macro name', 'type': 'string'}, rule='type')\n                        if data__definemacros_item_len > 1:\n                            data__definemacros_item__1 = data__definemacros_item[1]\n                            data__definemacros_item__1_one_of_count9 = 0\n                            if data__definemacros_item__1_one_of_count9 < 2:\n                                try:\n                                    if not isinstance(data__definemacros_item__1, (str)):\n                                        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".define-macros[{data__definemacros_x}][1]\".format(**locals()) + \" must be string\", value=data__definemacros_item__1, name=\"\" + (name_prefix or \"data\") + \".define-macros[{data__definemacros_x}][1]\".format(**locals()) + \"\", definition={'type': 'string'}, rule='type')\n                                    data__definemacros_item__1_one_of_count9 += 1\n                                except JsonSchemaValueException: pass\n                            if data__definemacros_item__1_one_of_count9 < 2:\n                                try:\n                                    if not isinstance(data__definemacros_item__1, (NoneType)):\n                                        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".define-macros[{data__definemacros_x}][1]\".format(**locals()) + \" must be null\", value=data__definemacros_item__1, name=\"\" + (name_prefix or \"data\") + \".define-macros[{data__definemacros_x}][1]\".format(**locals()) + \"\", definition={'type': 'null'}, rule='type')\n                                    data__definemacros_item__1_one_of_count9 += 1\n                                except JsonSchemaValueException: pass\n                            if data__definemacros_item__1_one_of_count9 != 1:\n                                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".define-macros[{data__definemacros_x}][1]\".format(**locals()) + \" must be valid exactly by one definition\" + (\" (\" + str(data__definemacros_item__1_one_of_count9) + \" matches found)\"), value=data__definemacros_item__1, name=\"\" + (name_prefix or \"data\") + \".define-macros[{data__definemacros_x}][1]\".format(**locals()) + \"\", definition={'description': 'macro value', 'oneOf': [{'type': 'string'}, {'type': 'null'}]}, rule='oneOf')\n                        if data__definemacros_item_len > 2:\n                            raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".define-macros[{data__definemacros_x}]\".format(**locals()) + \" must contain only specified items\", value=data__definemacros_item, name=\"\" + (name_prefix or \"data\") + \".define-macros[{data__definemacros_x}]\".format(**locals()) + \"\", definition={'type': 'array', 'items': [{'description': 'macro name', 'type': 'string'}, {'description': 'macro value', 'oneOf': [{'type': 'string'}, {'type': 'null'}]}], 'additionalItems': False}, rule='items')\n        if \"undef-macros\" in data_keys:\n            data_keys.remove(\"undef-macros\")\n            data__undefmacros = data[\"undef-macros\"]\n            if not isinstance(data__undefmacros, (list, tuple)):\n                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".undef-macros must be array\", value=data__undefmacros, name=\"\" + (name_prefix or \"data\") + \".undef-macros\", definition={'type': 'array', 'items': {'type': 'string'}}, rule='type')\n            data__undefmacros_is_list = isinstance(data__undefmacros, (list, tuple))\n            if data__undefmacros_is_list:\n                data__undefmacros_len = len(data__undefmacros)\n                for data__undefmacros_x, data__undefmacros_item in enumerate(data__undefmacros):\n                    if not isinstance(data__undefmacros_item, (str)):\n                        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".undef-macros[{data__undefmacros_x}]\".format(**locals()) + \" must be string\", value=data__undefmacros_item, name=\"\" + (name_prefix or \"data\") + \".undef-macros[{data__undefmacros_x}]\".format(**locals()) + \"\", definition={'type': 'string'}, rule='type')\n        if \"library-dirs\" in data_keys:\n            data_keys.remove(\"library-dirs\")\n            data__librarydirs = data[\"library-dirs\"]\n            if not isinstance(data__librarydirs, (list, tuple)):\n                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".library-dirs must be array\", value=data__librarydirs, name=\"\" + (name_prefix or \"data\") + \".library-dirs\", definition={'type': 'array', 'items': {'type': 'string'}}, rule='type')\n            data__librarydirs_is_list = isinstance(data__librarydirs, (list, tuple))\n            if data__librarydirs_is_list:\n                data__librarydirs_len = len(data__librarydirs)\n                for data__librarydirs_x, data__librarydirs_item in enumerate(data__librarydirs):\n                    if not isinstance(data__librarydirs_item, (str)):\n                        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".library-dirs[{data__librarydirs_x}]\".format(**locals()) + \" must be string\", value=data__librarydirs_item, name=\"\" + (name_prefix or \"data\") + \".library-dirs[{data__librarydirs_x}]\".format(**locals()) + \"\", definition={'type': 'string'}, rule='type')\n        if \"libraries\" in data_keys:\n            data_keys.remove(\"libraries\")\n            data__libraries = data[\"libraries\"]\n            if not isinstance(data__libraries, (list, tuple)):\n                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".libraries must be array\", value=data__libraries, name=\"\" + (name_prefix or \"data\") + \".libraries\", definition={'type': 'array', 'items': {'type': 'string'}}, rule='type')\n            data__libraries_is_list = isinstance(data__libraries, (list, tuple))\n            if data__libraries_is_list:\n                data__libraries_len = len(data__libraries)\n                for data__libraries_x, data__libraries_item in enumerate(data__libraries):\n                    if not isinstance(data__libraries_item, (str)):\n                        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".libraries[{data__libraries_x}]\".format(**locals()) + \" must be string\", value=data__libraries_item, name=\"\" + (name_prefix or \"data\") + \".libraries[{data__libraries_x}]\".format(**locals()) + \"\", definition={'type': 'string'}, rule='type')\n        if \"runtime-library-dirs\" in data_keys:\n            data_keys.remove(\"runtime-library-dirs\")\n            data__runtimelibrarydirs = data[\"runtime-library-dirs\"]\n            if not isinstance(data__runtimelibrarydirs, (list, tuple)):\n                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".runtime-library-dirs must be array\", value=data__runtimelibrarydirs, name=\"\" + (name_prefix or \"data\") + \".runtime-library-dirs\", definition={'type': 'array', 'items': {'type': 'string'}}, rule='type')\n            data__runtimelibrarydirs_is_list = isinstance(data__runtimelibrarydirs, (list, tuple))\n            if data__runtimelibrarydirs_is_list:\n                data__runtimelibrarydirs_len = len(data__runtimelibrarydirs)\n                for data__runtimelibrarydirs_x, data__runtimelibrarydirs_item in enumerate(data__runtimelibrarydirs):\n                    if not isinstance(data__runtimelibrarydirs_item, (str)):\n                        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".runtime-library-dirs[{data__runtimelibrarydirs_x}]\".format(**locals()) + \" must be string\", value=data__runtimelibrarydirs_item, name=\"\" + (name_prefix or \"data\") + \".runtime-library-dirs[{data__runtimelibrarydirs_x}]\".format(**locals()) + \"\", definition={'type': 'string'}, rule='type')\n        if \"extra-objects\" in data_keys:\n            data_keys.remove(\"extra-objects\")\n            data__extraobjects = data[\"extra-objects\"]\n            if not isinstance(data__extraobjects, (list, tuple)):\n                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".extra-objects must be array\", value=data__extraobjects, name=\"\" + (name_prefix or \"data\") + \".extra-objects\", definition={'type': 'array', 'items': {'type': 'string'}}, rule='type')\n            data__extraobjects_is_list = isinstance(data__extraobjects, (list, tuple))\n            if data__extraobjects_is_list:\n                data__extraobjects_len = len(data__extraobjects)\n                for data__extraobjects_x, data__extraobjects_item in enumerate(data__extraobjects):\n                    if not isinstance(data__extraobjects_item, (str)):\n                        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".extra-objects[{data__extraobjects_x}]\".format(**locals()) + \" must be string\", value=data__extraobjects_item, name=\"\" + (name_prefix or \"data\") + \".extra-objects[{data__extraobjects_x}]\".format(**locals()) + \"\", definition={'type': 'string'}, rule='type')\n        if \"extra-compile-args\" in data_keys:\n            data_keys.remove(\"extra-compile-args\")\n            data__extracompileargs = data[\"extra-compile-args\"]\n            if not isinstance(data__extracompileargs, (list, tuple)):\n                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".extra-compile-args must be array\", value=data__extracompileargs, name=\"\" + (name_prefix or \"data\") + \".extra-compile-args\", definition={'type': 'array', 'items': {'type': 'string'}}, rule='type')\n            data__extracompileargs_is_list = isinstance(data__extracompileargs, (list, tuple))\n            if data__extracompileargs_is_list:\n                data__extracompileargs_len = len(data__extracompileargs)\n                for data__extracompileargs_x, data__extracompileargs_item in enumerate(data__extracompileargs):\n                    if not isinstance(data__extracompileargs_item, (str)):\n                        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".extra-compile-args[{data__extracompileargs_x}]\".format(**locals()) + \" must be string\", value=data__extracompileargs_item, name=\"\" + (name_prefix or \"data\") + \".extra-compile-args[{data__extracompileargs_x}]\".format(**locals()) + \"\", definition={'type': 'string'}, rule='type')\n        if \"extra-link-args\" in data_keys:\n            data_keys.remove(\"extra-link-args\")\n            data__extralinkargs = data[\"extra-link-args\"]\n            if not isinstance(data__extralinkargs, (list, tuple)):\n                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".extra-link-args must be array\", value=data__extralinkargs, name=\"\" + (name_prefix or \"data\") + \".extra-link-args\", definition={'type': 'array', 'items': {'type': 'string'}}, rule='type')\n            data__extralinkargs_is_list = isinstance(data__extralinkargs, (list, tuple))\n            if data__extralinkargs_is_list:\n                data__extralinkargs_len = len(data__extralinkargs)\n                for data__extralinkargs_x, data__extralinkargs_item in enumerate(data__extralinkargs):\n                    if not isinstance(data__extralinkargs_item, (str)):\n                        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".extra-link-args[{data__extralinkargs_x}]\".format(**locals()) + \" must be string\", value=data__extralinkargs_item, name=\"\" + (name_prefix or \"data\") + \".extra-link-args[{data__extralinkargs_x}]\".format(**locals()) + \"\", definition={'type': 'string'}, rule='type')\n        if \"export-symbols\" in data_keys:\n            data_keys.remove(\"export-symbols\")\n            data__exportsymbols = data[\"export-symbols\"]\n            if not isinstance(data__exportsymbols, (list, tuple)):\n                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".export-symbols must be array\", value=data__exportsymbols, name=\"\" + (name_prefix or \"data\") + \".export-symbols\", definition={'type': 'array', 'items': {'type': 'string'}}, rule='type')\n            data__exportsymbols_is_list = isinstance(data__exportsymbols, (list, tuple))\n            if data__exportsymbols_is_list:\n                data__exportsymbols_len = len(data__exportsymbols)\n                for data__exportsymbols_x, data__exportsymbols_item in enumerate(data__exportsymbols):\n                    if not isinstance(data__exportsymbols_item, (str)):\n                        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".export-symbols[{data__exportsymbols_x}]\".format(**locals()) + \" must be string\", value=data__exportsymbols_item, name=\"\" + (name_prefix or \"data\") + \".export-symbols[{data__exportsymbols_x}]\".format(**locals()) + \"\", definition={'type': 'string'}, rule='type')\n        if \"swig-opts\" in data_keys:\n            data_keys.remove(\"swig-opts\")\n            data__swigopts = data[\"swig-opts\"]\n            if not isinstance(data__swigopts, (list, tuple)):\n                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".swig-opts must be array\", value=data__swigopts, name=\"\" + (name_prefix or \"data\") + \".swig-opts\", definition={'type': 'array', 'items': {'type': 'string'}}, rule='type')\n            data__swigopts_is_list = isinstance(data__swigopts, (list, tuple))\n            if data__swigopts_is_list:\n                data__swigopts_len = len(data__swigopts)\n                for data__swigopts_x, data__swigopts_item in enumerate(data__swigopts):\n                    if not isinstance(data__swigopts_item, (str)):\n                        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".swig-opts[{data__swigopts_x}]\".format(**locals()) + \" must be string\", value=data__swigopts_item, name=\"\" + (name_prefix or \"data\") + \".swig-opts[{data__swigopts_x}]\".format(**locals()) + \"\", definition={'type': 'string'}, rule='type')\n        if \"depends\" in data_keys:\n            data_keys.remove(\"depends\")\n            data__depends = data[\"depends\"]\n            if not isinstance(data__depends, (list, tuple)):\n                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".depends must be array\", value=data__depends, name=\"\" + (name_prefix or \"data\") + \".depends\", definition={'type': 'array', 'items': {'type': 'string'}}, rule='type')\n            data__depends_is_list = isinstance(data__depends, (list, tuple))\n            if data__depends_is_list:\n                data__depends_len = len(data__depends)\n                for data__depends_x, data__depends_item in enumerate(data__depends):\n                    if not isinstance(data__depends_item, (str)):\n                        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".depends[{data__depends_x}]\".format(**locals()) + \" must be string\", value=data__depends_item, name=\"\" + (name_prefix or \"data\") + \".depends[{data__depends_x}]\".format(**locals()) + \"\", definition={'type': 'string'}, rule='type')\n        if \"language\" in data_keys:\n            data_keys.remove(\"language\")\n            data__language = data[\"language\"]\n            if not isinstance(data__language, (str)):\n                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".language must be string\", value=data__language, name=\"\" + (name_prefix or \"data\") + \".language\", definition={'type': 'string'}, rule='type')\n        if \"optional\" in data_keys:\n            data_keys.remove(\"optional\")\n            data__optional = data[\"optional\"]\n            if not isinstance(data__optional, (bool)):\n                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".optional must be boolean\", value=data__optional, name=\"\" + (name_prefix or \"data\") + \".optional\", definition={'type': 'boolean'}, rule='type')\n        if \"py-limited-api\" in data_keys:\n            data_keys.remove(\"py-limited-api\")\n            data__pylimitedapi = data[\"py-limited-api\"]\n            if not isinstance(data__pylimitedapi, (bool)):\n                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".py-limited-api must be boolean\", value=data__pylimitedapi, name=\"\" + (name_prefix or \"data\") + \".py-limited-api\", definition={'type': 'boolean'}, rule='type')\n        if data_keys:\n            raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \" must not contain \"+str(data_keys)+\" properties\", value=data, name=\"\" + (name_prefix or \"data\") + \"\", definition={'$id': '#/definitions/ext-module', 'title': 'Extension module', 'description': 'Parameters to construct a :class:`setuptools.Extension` object', 'type': 'object', 'required': ['name', 'sources'], 'additionalProperties': False, 'properties': {'name': {'type': 'string', 'format': 'python-module-name-relaxed'}, 'sources': {'type': 'array', 'items': {'type': 'string'}}, 'include-dirs': {'type': 'array', 'items': {'type': 'string'}}, 'define-macros': {'type': 'array', 'items': {'type': 'array', 'items': [{'description': 'macro name', 'type': 'string'}, {'description': 'macro value', 'oneOf': [{'type': 'string'}, {'type': 'null'}]}], 'additionalItems': False}}, 'undef-macros': {'type': 'array', 'items': {'type': 'string'}}, 'library-dirs': {'type': 'array', 'items': {'type': 'string'}}, 'libraries': {'type': 'array', 'items': {'type': 'string'}}, 'runtime-library-dirs': {'type': 'array', 'items': {'type': 'string'}}, 'extra-objects': {'type': 'array', 'items': {'type': 'string'}}, 'extra-compile-args': {'type': 'array', 'items': {'type': 'string'}}, 'extra-link-args': {'type': 'array', 'items': {'type': 'string'}}, 'export-symbols': {'type': 'array', 'items': {'type': 'string'}}, 'swig-opts': {'type': 'array', 'items': {'type': 'string'}}, 'depends': {'type': 'array', 'items': {'type': 'string'}}, 'language': {'type': 'string'}, 'optional': {'type': 'boolean'}, 'py-limited-api': {'type': 'boolean'}}}, rule='additionalProperties')\n    return data\n\ndef validate_https___setuptools_pypa_io_en_latest_userguide_pyproject_config_html__definitions_find_directive(data, custom_formats={}, name_prefix=None):\n    if not isinstance(data, (dict)):\n        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \" must be object\", value=data, name=\"\" + (name_prefix or \"data\") + \"\", definition={'$id': '#/definitions/find-directive', 'title': \"'find:' directive\", 'type': 'object', 'additionalProperties': False, 'properties': {'find': {'type': 'object', '$$description': ['Dynamic `package discovery', '<https://setuptools.pypa.io/en/latest/userguide/package_discovery.html>`_.'], 'additionalProperties': False, 'properties': {'where': {'description': 'Directories to be searched for packages (Unix-style relative path)', 'type': 'array', 'items': {'type': 'string'}}, 'exclude': {'type': 'array', '$$description': ['Exclude packages that match the values listed in this field.', \"Can container shell-style wildcards (e.g. ``'pkg.*'``)\"], 'items': {'type': 'string'}}, 'include': {'type': 'array', '$$description': ['Restrict the found packages to just the ones listed in this field.', \"Can container shell-style wildcards (e.g. ``'pkg.*'``)\"], 'items': {'type': 'string'}}, 'namespaces': {'type': 'boolean', '$$description': ['When ``True``, directories without a ``__init__.py`` file will also', 'be scanned for :pep:`420`-style implicit namespaces']}}}}}, rule='type')\n    data_is_dict = isinstance(data, dict)\n    if data_is_dict:\n        data_keys = set(data.keys())\n        if \"find\" in data_keys:\n            data_keys.remove(\"find\")\n            data__find = data[\"find\"]\n            if not isinstance(data__find, (dict)):\n                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".find must be object\", value=data__find, name=\"\" + (name_prefix or \"data\") + \".find\", definition={'type': 'object', '$$description': ['Dynamic `package discovery', '<https://setuptools.pypa.io/en/latest/userguide/package_discovery.html>`_.'], 'additionalProperties': False, 'properties': {'where': {'description': 'Directories to be searched for packages (Unix-style relative path)', 'type': 'array', 'items': {'type': 'string'}}, 'exclude': {'type': 'array', '$$description': ['Exclude packages that match the values listed in this field.', \"Can container shell-style wildcards (e.g. ``'pkg.*'``)\"], 'items': {'type': 'string'}}, 'include': {'type': 'array', '$$description': ['Restrict the found packages to just the ones listed in this field.', \"Can container shell-style wildcards (e.g. ``'pkg.*'``)\"], 'items': {'type': 'string'}}, 'namespaces': {'type': 'boolean', '$$description': ['When ``True``, directories without a ``__init__.py`` file will also', 'be scanned for :pep:`420`-style implicit namespaces']}}}, rule='type')\n            data__find_is_dict = isinstance(data__find, dict)\n            if data__find_is_dict:\n                data__find_keys = set(data__find.keys())\n                if \"where\" in data__find_keys:\n                    data__find_keys.remove(\"where\")\n                    data__find__where = data__find[\"where\"]\n                    if not isinstance(data__find__where, (list, tuple)):\n                        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".find.where must be array\", value=data__find__where, name=\"\" + (name_prefix or \"data\") + \".find.where\", definition={'description': 'Directories to be searched for packages (Unix-style relative path)', 'type': 'array', 'items': {'type': 'string'}}, rule='type')\n                    data__find__where_is_list = isinstance(data__find__where, (list, tuple))\n                    if data__find__where_is_list:\n                        data__find__where_len = len(data__find__where)\n                        for data__find__where_x, data__find__where_item in enumerate(data__find__where):\n                            if not isinstance(data__find__where_item, (str)):\n                                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".find.where[{data__find__where_x}]\".format(**locals()) + \" must be string\", value=data__find__where_item, name=\"\" + (name_prefix or \"data\") + \".find.where[{data__find__where_x}]\".format(**locals()) + \"\", definition={'type': 'string'}, rule='type')\n                if \"exclude\" in data__find_keys:\n                    data__find_keys.remove(\"exclude\")\n                    data__find__exclude = data__find[\"exclude\"]\n                    if not isinstance(data__find__exclude, (list, tuple)):\n                        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".find.exclude must be array\", value=data__find__exclude, name=\"\" + (name_prefix or \"data\") + \".find.exclude\", definition={'type': 'array', '$$description': ['Exclude packages that match the values listed in this field.', \"Can container shell-style wildcards (e.g. ``'pkg.*'``)\"], 'items': {'type': 'string'}}, rule='type')\n                    data__find__exclude_is_list = isinstance(data__find__exclude, (list, tuple))\n                    if data__find__exclude_is_list:\n                        data__find__exclude_len = len(data__find__exclude)\n                        for data__find__exclude_x, data__find__exclude_item in enumerate(data__find__exclude):\n                            if not isinstance(data__find__exclude_item, (str)):\n                                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".find.exclude[{data__find__exclude_x}]\".format(**locals()) + \" must be string\", value=data__find__exclude_item, name=\"\" + (name_prefix or \"data\") + \".find.exclude[{data__find__exclude_x}]\".format(**locals()) + \"\", definition={'type': 'string'}, rule='type')\n                if \"include\" in data__find_keys:\n                    data__find_keys.remove(\"include\")\n                    data__find__include = data__find[\"include\"]\n                    if not isinstance(data__find__include, (list, tuple)):\n                        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".find.include must be array\", value=data__find__include, name=\"\" + (name_prefix or \"data\") + \".find.include\", definition={'type': 'array', '$$description': ['Restrict the found packages to just the ones listed in this field.', \"Can container shell-style wildcards (e.g. ``'pkg.*'``)\"], 'items': {'type': 'string'}}, rule='type')\n                    data__find__include_is_list = isinstance(data__find__include, (list, tuple))\n                    if data__find__include_is_list:\n                        data__find__include_len = len(data__find__include)\n                        for data__find__include_x, data__find__include_item in enumerate(data__find__include):\n                            if not isinstance(data__find__include_item, (str)):\n                                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".find.include[{data__find__include_x}]\".format(**locals()) + \" must be string\", value=data__find__include_item, name=\"\" + (name_prefix or \"data\") + \".find.include[{data__find__include_x}]\".format(**locals()) + \"\", definition={'type': 'string'}, rule='type')\n                if \"namespaces\" in data__find_keys:\n                    data__find_keys.remove(\"namespaces\")\n                    data__find__namespaces = data__find[\"namespaces\"]\n                    if not isinstance(data__find__namespaces, (bool)):\n                        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".find.namespaces must be boolean\", value=data__find__namespaces, name=\"\" + (name_prefix or \"data\") + \".find.namespaces\", definition={'type': 'boolean', '$$description': ['When ``True``, directories without a ``__init__.py`` file will also', 'be scanned for :pep:`420`-style implicit namespaces']}, rule='type')\n                if data__find_keys:\n                    raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".find must not contain \"+str(data__find_keys)+\" properties\", value=data__find, name=\"\" + (name_prefix or \"data\") + \".find\", definition={'type': 'object', '$$description': ['Dynamic `package discovery', '<https://setuptools.pypa.io/en/latest/userguide/package_discovery.html>`_.'], 'additionalProperties': False, 'properties': {'where': {'description': 'Directories to be searched for packages (Unix-style relative path)', 'type': 'array', 'items': {'type': 'string'}}, 'exclude': {'type': 'array', '$$description': ['Exclude packages that match the values listed in this field.', \"Can container shell-style wildcards (e.g. ``'pkg.*'``)\"], 'items': {'type': 'string'}}, 'include': {'type': 'array', '$$description': ['Restrict the found packages to just the ones listed in this field.', \"Can container shell-style wildcards (e.g. ``'pkg.*'``)\"], 'items': {'type': 'string'}}, 'namespaces': {'type': 'boolean', '$$description': ['When ``True``, directories without a ``__init__.py`` file will also', 'be scanned for :pep:`420`-style implicit namespaces']}}}, rule='additionalProperties')\n        if data_keys:\n            raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \" must not contain \"+str(data_keys)+\" properties\", value=data, name=\"\" + (name_prefix or \"data\") + \"\", definition={'$id': '#/definitions/find-directive', 'title': \"'find:' directive\", 'type': 'object', 'additionalProperties': False, 'properties': {'find': {'type': 'object', '$$description': ['Dynamic `package discovery', '<https://setuptools.pypa.io/en/latest/userguide/package_discovery.html>`_.'], 'additionalProperties': False, 'properties': {'where': {'description': 'Directories to be searched for packages (Unix-style relative path)', 'type': 'array', 'items': {'type': 'string'}}, 'exclude': {'type': 'array', '$$description': ['Exclude packages that match the values listed in this field.', \"Can container shell-style wildcards (e.g. ``'pkg.*'``)\"], 'items': {'type': 'string'}}, 'include': {'type': 'array', '$$description': ['Restrict the found packages to just the ones listed in this field.', \"Can container shell-style wildcards (e.g. ``'pkg.*'``)\"], 'items': {'type': 'string'}}, 'namespaces': {'type': 'boolean', '$$description': ['When ``True``, directories without a ``__init__.py`` file will also', 'be scanned for :pep:`420`-style implicit namespaces']}}}}}, rule='additionalProperties')\n    return data\n\ndef validate_https___setuptools_pypa_io_en_latest_userguide_pyproject_config_html__definitions_package_name(data, custom_formats={}, name_prefix=None):\n    if not isinstance(data, (str)):\n        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \" must be string\", value=data, name=\"\" + (name_prefix or \"data\") + \"\", definition={'$id': '#/definitions/package-name', 'title': 'Valid package name', 'description': 'Valid package name (importable or :pep:`561`).', 'type': 'string', 'anyOf': [{'type': 'string', 'format': 'python-module-name-relaxed'}, {'type': 'string', 'format': 'pep561-stub-name'}]}, rule='type')\n    data_any_of_count10 = 0\n    if not data_any_of_count10:\n        try:\n            if not isinstance(data, (str)):\n                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \" must be string\", value=data, name=\"\" + (name_prefix or \"data\") + \"\", definition={'type': 'string', 'format': 'python-module-name-relaxed'}, rule='type')\n            if isinstance(data, str):\n                if not custom_formats[\"python-module-name-relaxed\"](data):\n                    raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \" must be python-module-name-relaxed\", value=data, name=\"\" + (name_prefix or \"data\") + \"\", definition={'type': 'string', 'format': 'python-module-name-relaxed'}, rule='format')\n            data_any_of_count10 += 1\n        except JsonSchemaValueException: pass\n    if not data_any_of_count10:\n        try:\n            if not isinstance(data, (str)):\n                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \" must be string\", value=data, name=\"\" + (name_prefix or \"data\") + \"\", definition={'type': 'string', 'format': 'pep561-stub-name'}, rule='type')\n            if isinstance(data, str):\n                if not custom_formats[\"pep561-stub-name\"](data):\n                    raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \" must be pep561-stub-name\", value=data, name=\"\" + (name_prefix or \"data\") + \"\", definition={'type': 'string', 'format': 'pep561-stub-name'}, rule='format')\n            data_any_of_count10 += 1\n        except JsonSchemaValueException: pass\n    if not data_any_of_count10:\n        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \" cannot be validated by any definition\", value=data, name=\"\" + (name_prefix or \"data\") + \"\", definition={'$id': '#/definitions/package-name', 'title': 'Valid package name', 'description': 'Valid package name (importable or :pep:`561`).', 'type': 'string', 'anyOf': [{'type': 'string', 'format': 'python-module-name-relaxed'}, {'type': 'string', 'format': 'pep561-stub-name'}]}, rule='anyOf')\n    return data\n\ndef validate_https___setuptools_pypa_io_en_latest_deprecated_distutils_configfile_html(data, custom_formats={}, name_prefix=None):\n    if not isinstance(data, (dict)):\n        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \" must be object\", value=data, name=\"\" + (name_prefix or \"data\") + \"\", definition={'$schema': 'http://json-schema.org/draft-07/schema#', '$id': 'https://setuptools.pypa.io/en/latest/deprecated/distutils/configfile.html', 'title': '``tool.distutils`` table', '$$description': ['**EXPERIMENTAL** (NOT OFFICIALLY SUPPORTED): Use ``tool.distutils``', 'subtables to configure arguments for ``distutils`` commands.', 'Originally, ``distutils`` allowed developers to configure arguments for', '``setup.py`` commands via `distutils configuration files', '<https://setuptools.pypa.io/en/latest/deprecated/distutils/configfile.html>`_.', 'See also `the old Python docs <https://docs.python.org/3.11/install/>_`.'], 'type': 'object', 'properties': {'global': {'type': 'object', 'description': 'Global options applied to all ``distutils`` commands'}}, 'patternProperties': {'.+': {'type': 'object'}}, '$comment': 'TODO: Is there a practical way of making this schema more specific?'}, rule='type')\n    data_is_dict = isinstance(data, dict)\n    if data_is_dict:\n        data_keys = set(data.keys())\n        if \"global\" in data_keys:\n            data_keys.remove(\"global\")\n            data__global = data[\"global\"]\n            if not isinstance(data__global, (dict)):\n                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".global must be object\", value=data__global, name=\"\" + (name_prefix or \"data\") + \".global\", definition={'type': 'object', 'description': 'Global options applied to all ``distutils`` commands'}, rule='type')\n        for data_key, data_val in data.items():\n            if REGEX_PATTERNS['.+'].search(data_key):\n                if data_key in data_keys:\n                    data_keys.remove(data_key)\n                if not isinstance(data_val, (dict)):\n                    raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".{data_key}\".format(**locals()) + \" must be object\", value=data_val, name=\"\" + (name_prefix or \"data\") + \".{data_key}\".format(**locals()) + \"\", definition={'type': 'object'}, rule='type')\n    return data\n\ndef validate_https___packaging_python_org_en_latest_specifications_pyproject_toml(data, custom_formats={}, name_prefix=None):\n    if not isinstance(data, (dict)):\n        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \" must be object\", value=data, name=\"\" + (name_prefix or \"data\") + \"\", definition={'$schema': 'http://json-schema.org/draft-07/schema#', '$id': 'https://packaging.python.org/en/latest/specifications/pyproject-toml/', 'title': 'Package metadata stored in the ``project`` table', '$$description': ['Data structure for the **project** table inside ``pyproject.toml``', '(as initially defined in :pep:`621`)'], 'type': 'object', 'properties': {'name': {'type': 'string', 'description': 'The name (primary identifier) of the project. MUST be statically defined.', 'format': 'pep508-identifier'}, 'version': {'type': 'string', 'description': 'The version of the project as supported by :pep:`440`.', 'format': 'pep440'}, 'description': {'type': 'string', '$$description': ['The `summary description of the project', '<https://packaging.python.org/specifications/core-metadata/#summary>`_']}, 'readme': {'$$description': ['`Full/detailed description of the project in the form of a README', '<https://peps.python.org/pep-0621/#readme>`_', \"with meaning similar to the one defined in `core metadata's Description\", '<https://packaging.python.org/specifications/core-metadata/#description>`_'], 'oneOf': [{'type': 'string', '$$description': ['Relative path to a text file (UTF-8) containing the full description', 'of the project. If the file path ends in case-insensitive ``.md`` or', '``.rst`` suffixes, then the content-type is respectively', '``text/markdown`` or ``text/x-rst``']}, {'type': 'object', 'allOf': [{'anyOf': [{'properties': {'file': {'type': 'string', '$$description': ['Relative path to a text file containing the full description', 'of the project.']}}, 'required': ['file']}, {'properties': {'text': {'type': 'string', 'description': 'Full text describing the project.'}}, 'required': ['text']}]}, {'properties': {'content-type': {'type': 'string', '$$description': ['Content-type (:rfc:`1341`) of the full description', '(e.g. ``text/markdown``). The ``charset`` parameter is assumed', 'UTF-8 when not present.'], '$comment': 'TODO: add regex pattern or format?'}}, 'required': ['content-type']}]}]}, 'requires-python': {'type': 'string', 'format': 'pep508-versionspec', '$$description': ['`The Python version requirements of the project', '<https://packaging.python.org/specifications/core-metadata/#requires-python>`_.']}, 'license': {'description': '`Project license <https://peps.python.org/pep-0621/#license>`_.', 'oneOf': [{'properties': {'file': {'type': 'string', '$$description': ['Relative path to the file (UTF-8) which contains the license for the', 'project.']}}, 'required': ['file']}, {'properties': {'text': {'type': 'string', '$$description': ['The license of the project whose meaning is that of the', '`License field from the core metadata', '<https://packaging.python.org/specifications/core-metadata/#license>`_.']}}, 'required': ['text']}]}, 'authors': {'type': 'array', 'items': {'$id': '#/definitions/author', 'title': 'Author or Maintainer', '$comment': 'https://peps.python.org/pep-0621/#authors-maintainers', 'type': 'object', 'additionalProperties': False, 'properties': {'name': {'type': 'string', '$$description': ['MUST be a valid email name, i.e. whatever can be put as a name, before an', 'email, in :rfc:`822`.']}, 'email': {'type': 'string', 'format': 'idn-email', 'description': 'MUST be a valid email address'}}}, '$$description': [\"The people or organizations considered to be the 'authors' of the project.\", 'The exact meaning is open to interpretation (e.g. original or primary authors,', 'current maintainers, or owners of the package).']}, 'maintainers': {'type': 'array', 'items': {'$id': '#/definitions/author', 'title': 'Author or Maintainer', '$comment': 'https://peps.python.org/pep-0621/#authors-maintainers', 'type': 'object', 'additionalProperties': False, 'properties': {'name': {'type': 'string', '$$description': ['MUST be a valid email name, i.e. whatever can be put as a name, before an', 'email, in :rfc:`822`.']}, 'email': {'type': 'string', 'format': 'idn-email', 'description': 'MUST be a valid email address'}}}, '$$description': [\"The people or organizations considered to be the 'maintainers' of the project.\", 'Similarly to ``authors``, the exact meaning is open to interpretation.']}, 'keywords': {'type': 'array', 'items': {'type': 'string'}, 'description': 'List of keywords to assist searching for the distribution in a larger catalog.'}, 'classifiers': {'type': 'array', 'items': {'type': 'string', 'format': 'trove-classifier', 'description': '`PyPI classifier <https://pypi.org/classifiers/>`_.'}, '$$description': ['`Trove classifiers <https://pypi.org/classifiers/>`_', 'which apply to the project.']}, 'urls': {'type': 'object', 'description': 'URLs associated with the project in the form ``label => value``.', 'additionalProperties': False, 'patternProperties': {'^.+$': {'type': 'string', 'format': 'url'}}}, 'scripts': {'$id': '#/definitions/entry-point-group', 'title': 'Entry-points', 'type': 'object', '$$description': ['Entry-points are grouped together to indicate what sort of capabilities they', 'provide.', 'See the `packaging guides', '<https://packaging.python.org/specifications/entry-points/>`_', 'and `setuptools docs', '<https://setuptools.pypa.io/en/latest/userguide/entry_point.html>`_', 'for more information.'], 'propertyNames': {'format': 'python-entrypoint-name'}, 'additionalProperties': False, 'patternProperties': {'^.+$': {'type': 'string', '$$description': ['Reference to a Python object. It is either in the form', '``importable.module``, or ``importable.module:object.attr``.'], 'format': 'python-entrypoint-reference', '$comment': 'https://packaging.python.org/specifications/entry-points/'}}}, 'gui-scripts': {'$id': '#/definitions/entry-point-group', 'title': 'Entry-points', 'type': 'object', '$$description': ['Entry-points are grouped together to indicate what sort of capabilities they', 'provide.', 'See the `packaging guides', '<https://packaging.python.org/specifications/entry-points/>`_', 'and `setuptools docs', '<https://setuptools.pypa.io/en/latest/userguide/entry_point.html>`_', 'for more information.'], 'propertyNames': {'format': 'python-entrypoint-name'}, 'additionalProperties': False, 'patternProperties': {'^.+$': {'type': 'string', '$$description': ['Reference to a Python object. It is either in the form', '``importable.module``, or ``importable.module:object.attr``.'], 'format': 'python-entrypoint-reference', '$comment': 'https://packaging.python.org/specifications/entry-points/'}}}, 'entry-points': {'$$description': ['Instruct the installer to expose the given modules/functions via', '``entry-point`` discovery mechanism (useful for plugins).', 'More information available in the `Python packaging guide', '<https://packaging.python.org/specifications/entry-points/>`_.'], 'propertyNames': {'format': 'python-entrypoint-group'}, 'additionalProperties': False, 'patternProperties': {'^.+$': {'$id': '#/definitions/entry-point-group', 'title': 'Entry-points', 'type': 'object', '$$description': ['Entry-points are grouped together to indicate what sort of capabilities they', 'provide.', 'See the `packaging guides', '<https://packaging.python.org/specifications/entry-points/>`_', 'and `setuptools docs', '<https://setuptools.pypa.io/en/latest/userguide/entry_point.html>`_', 'for more information.'], 'propertyNames': {'format': 'python-entrypoint-name'}, 'additionalProperties': False, 'patternProperties': {'^.+$': {'type': 'string', '$$description': ['Reference to a Python object. It is either in the form', '``importable.module``, or ``importable.module:object.attr``.'], 'format': 'python-entrypoint-reference', '$comment': 'https://packaging.python.org/specifications/entry-points/'}}}}}, 'dependencies': {'type': 'array', 'description': 'Project (mandatory) dependencies.', 'items': {'$id': '#/definitions/dependency', 'title': 'Dependency', 'type': 'string', 'description': 'Project dependency specification according to PEP 508', 'format': 'pep508'}}, 'optional-dependencies': {'type': 'object', 'description': 'Optional dependency for the project', 'propertyNames': {'format': 'pep508-identifier'}, 'additionalProperties': False, 'patternProperties': {'^.+$': {'type': 'array', 'items': {'$id': '#/definitions/dependency', 'title': 'Dependency', 'type': 'string', 'description': 'Project dependency specification according to PEP 508', 'format': 'pep508'}}}}, 'dynamic': {'type': 'array', '$$description': ['Specifies which fields are intentionally unspecified and expected to be', 'dynamically provided by build tools'], 'items': {'enum': ['version', 'description', 'readme', 'requires-python', 'license', 'authors', 'maintainers', 'keywords', 'classifiers', 'urls', 'scripts', 'gui-scripts', 'entry-points', 'dependencies', 'optional-dependencies']}}}, 'required': ['name'], 'additionalProperties': False, 'if': {'not': {'required': ['dynamic'], 'properties': {'dynamic': {'contains': {'const': 'version'}, '$$description': ['version is listed in ``dynamic``']}}}, '$$comment': ['According to :pep:`621`:', '    If the core metadata specification lists a field as \"Required\", then', '    the metadata MUST specify the field statically or list it in dynamic', 'In turn, `core metadata`_ defines:', '    The required fields are: Metadata-Version, Name, Version.', '    All the other fields are optional.', 'Since ``Metadata-Version`` is defined by the build back-end, ``name`` and', '``version`` are the only mandatory information in ``pyproject.toml``.', '.. _core metadata: https://packaging.python.org/specifications/core-metadata/']}, 'then': {'required': ['version'], '$$description': ['version should be statically defined in the ``version`` field']}, 'definitions': {'author': {'$id': '#/definitions/author', 'title': 'Author or Maintainer', '$comment': 'https://peps.python.org/pep-0621/#authors-maintainers', 'type': 'object', 'additionalProperties': False, 'properties': {'name': {'type': 'string', '$$description': ['MUST be a valid email name, i.e. whatever can be put as a name, before an', 'email, in :rfc:`822`.']}, 'email': {'type': 'string', 'format': 'idn-email', 'description': 'MUST be a valid email address'}}}, 'entry-point-group': {'$id': '#/definitions/entry-point-group', 'title': 'Entry-points', 'type': 'object', '$$description': ['Entry-points are grouped together to indicate what sort of capabilities they', 'provide.', 'See the `packaging guides', '<https://packaging.python.org/specifications/entry-points/>`_', 'and `setuptools docs', '<https://setuptools.pypa.io/en/latest/userguide/entry_point.html>`_', 'for more information.'], 'propertyNames': {'format': 'python-entrypoint-name'}, 'additionalProperties': False, 'patternProperties': {'^.+$': {'type': 'string', '$$description': ['Reference to a Python object. It is either in the form', '``importable.module``, or ``importable.module:object.attr``.'], 'format': 'python-entrypoint-reference', '$comment': 'https://packaging.python.org/specifications/entry-points/'}}}, 'dependency': {'$id': '#/definitions/dependency', 'title': 'Dependency', 'type': 'string', 'description': 'Project dependency specification according to PEP 508', 'format': 'pep508'}}}, rule='type')\n    data_is_dict = isinstance(data, dict)\n    if data_is_dict:\n        data__missing_keys = set(['name']) - data.keys()\n        if data__missing_keys:\n            raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \" must contain \" + (str(sorted(data__missing_keys)) + \" properties\"), value=data, name=\"\" + (name_prefix or \"data\") + \"\", definition={'$schema': 'http://json-schema.org/draft-07/schema#', '$id': 'https://packaging.python.org/en/latest/specifications/pyproject-toml/', 'title': 'Package metadata stored in the ``project`` table', '$$description': ['Data structure for the **project** table inside ``pyproject.toml``', '(as initially defined in :pep:`621`)'], 'type': 'object', 'properties': {'name': {'type': 'string', 'description': 'The name (primary identifier) of the project. MUST be statically defined.', 'format': 'pep508-identifier'}, 'version': {'type': 'string', 'description': 'The version of the project as supported by :pep:`440`.', 'format': 'pep440'}, 'description': {'type': 'string', '$$description': ['The `summary description of the project', '<https://packaging.python.org/specifications/core-metadata/#summary>`_']}, 'readme': {'$$description': ['`Full/detailed description of the project in the form of a README', '<https://peps.python.org/pep-0621/#readme>`_', \"with meaning similar to the one defined in `core metadata's Description\", '<https://packaging.python.org/specifications/core-metadata/#description>`_'], 'oneOf': [{'type': 'string', '$$description': ['Relative path to a text file (UTF-8) containing the full description', 'of the project. If the file path ends in case-insensitive ``.md`` or', '``.rst`` suffixes, then the content-type is respectively', '``text/markdown`` or ``text/x-rst``']}, {'type': 'object', 'allOf': [{'anyOf': [{'properties': {'file': {'type': 'string', '$$description': ['Relative path to a text file containing the full description', 'of the project.']}}, 'required': ['file']}, {'properties': {'text': {'type': 'string', 'description': 'Full text describing the project.'}}, 'required': ['text']}]}, {'properties': {'content-type': {'type': 'string', '$$description': ['Content-type (:rfc:`1341`) of the full description', '(e.g. ``text/markdown``). The ``charset`` parameter is assumed', 'UTF-8 when not present.'], '$comment': 'TODO: add regex pattern or format?'}}, 'required': ['content-type']}]}]}, 'requires-python': {'type': 'string', 'format': 'pep508-versionspec', '$$description': ['`The Python version requirements of the project', '<https://packaging.python.org/specifications/core-metadata/#requires-python>`_.']}, 'license': {'description': '`Project license <https://peps.python.org/pep-0621/#license>`_.', 'oneOf': [{'properties': {'file': {'type': 'string', '$$description': ['Relative path to the file (UTF-8) which contains the license for the', 'project.']}}, 'required': ['file']}, {'properties': {'text': {'type': 'string', '$$description': ['The license of the project whose meaning is that of the', '`License field from the core metadata', '<https://packaging.python.org/specifications/core-metadata/#license>`_.']}}, 'required': ['text']}]}, 'authors': {'type': 'array', 'items': {'$id': '#/definitions/author', 'title': 'Author or Maintainer', '$comment': 'https://peps.python.org/pep-0621/#authors-maintainers', 'type': 'object', 'additionalProperties': False, 'properties': {'name': {'type': 'string', '$$description': ['MUST be a valid email name, i.e. whatever can be put as a name, before an', 'email, in :rfc:`822`.']}, 'email': {'type': 'string', 'format': 'idn-email', 'description': 'MUST be a valid email address'}}}, '$$description': [\"The people or organizations considered to be the 'authors' of the project.\", 'The exact meaning is open to interpretation (e.g. original or primary authors,', 'current maintainers, or owners of the package).']}, 'maintainers': {'type': 'array', 'items': {'$id': '#/definitions/author', 'title': 'Author or Maintainer', '$comment': 'https://peps.python.org/pep-0621/#authors-maintainers', 'type': 'object', 'additionalProperties': False, 'properties': {'name': {'type': 'string', '$$description': ['MUST be a valid email name, i.e. whatever can be put as a name, before an', 'email, in :rfc:`822`.']}, 'email': {'type': 'string', 'format': 'idn-email', 'description': 'MUST be a valid email address'}}}, '$$description': [\"The people or organizations considered to be the 'maintainers' of the project.\", 'Similarly to ``authors``, the exact meaning is open to interpretation.']}, 'keywords': {'type': 'array', 'items': {'type': 'string'}, 'description': 'List of keywords to assist searching for the distribution in a larger catalog.'}, 'classifiers': {'type': 'array', 'items': {'type': 'string', 'format': 'trove-classifier', 'description': '`PyPI classifier <https://pypi.org/classifiers/>`_.'}, '$$description': ['`Trove classifiers <https://pypi.org/classifiers/>`_', 'which apply to the project.']}, 'urls': {'type': 'object', 'description': 'URLs associated with the project in the form ``label => value``.', 'additionalProperties': False, 'patternProperties': {'^.+$': {'type': 'string', 'format': 'url'}}}, 'scripts': {'$id': '#/definitions/entry-point-group', 'title': 'Entry-points', 'type': 'object', '$$description': ['Entry-points are grouped together to indicate what sort of capabilities they', 'provide.', 'See the `packaging guides', '<https://packaging.python.org/specifications/entry-points/>`_', 'and `setuptools docs', '<https://setuptools.pypa.io/en/latest/userguide/entry_point.html>`_', 'for more information.'], 'propertyNames': {'format': 'python-entrypoint-name'}, 'additionalProperties': False, 'patternProperties': {'^.+$': {'type': 'string', '$$description': ['Reference to a Python object. It is either in the form', '``importable.module``, or ``importable.module:object.attr``.'], 'format': 'python-entrypoint-reference', '$comment': 'https://packaging.python.org/specifications/entry-points/'}}}, 'gui-scripts': {'$id': '#/definitions/entry-point-group', 'title': 'Entry-points', 'type': 'object', '$$description': ['Entry-points are grouped together to indicate what sort of capabilities they', 'provide.', 'See the `packaging guides', '<https://packaging.python.org/specifications/entry-points/>`_', 'and `setuptools docs', '<https://setuptools.pypa.io/en/latest/userguide/entry_point.html>`_', 'for more information.'], 'propertyNames': {'format': 'python-entrypoint-name'}, 'additionalProperties': False, 'patternProperties': {'^.+$': {'type': 'string', '$$description': ['Reference to a Python object. It is either in the form', '``importable.module``, or ``importable.module:object.attr``.'], 'format': 'python-entrypoint-reference', '$comment': 'https://packaging.python.org/specifications/entry-points/'}}}, 'entry-points': {'$$description': ['Instruct the installer to expose the given modules/functions via', '``entry-point`` discovery mechanism (useful for plugins).', 'More information available in the `Python packaging guide', '<https://packaging.python.org/specifications/entry-points/>`_.'], 'propertyNames': {'format': 'python-entrypoint-group'}, 'additionalProperties': False, 'patternProperties': {'^.+$': {'$id': '#/definitions/entry-point-group', 'title': 'Entry-points', 'type': 'object', '$$description': ['Entry-points are grouped together to indicate what sort of capabilities they', 'provide.', 'See the `packaging guides', '<https://packaging.python.org/specifications/entry-points/>`_', 'and `setuptools docs', '<https://setuptools.pypa.io/en/latest/userguide/entry_point.html>`_', 'for more information.'], 'propertyNames': {'format': 'python-entrypoint-name'}, 'additionalProperties': False, 'patternProperties': {'^.+$': {'type': 'string', '$$description': ['Reference to a Python object. It is either in the form', '``importable.module``, or ``importable.module:object.attr``.'], 'format': 'python-entrypoint-reference', '$comment': 'https://packaging.python.org/specifications/entry-points/'}}}}}, 'dependencies': {'type': 'array', 'description': 'Project (mandatory) dependencies.', 'items': {'$id': '#/definitions/dependency', 'title': 'Dependency', 'type': 'string', 'description': 'Project dependency specification according to PEP 508', 'format': 'pep508'}}, 'optional-dependencies': {'type': 'object', 'description': 'Optional dependency for the project', 'propertyNames': {'format': 'pep508-identifier'}, 'additionalProperties': False, 'patternProperties': {'^.+$': {'type': 'array', 'items': {'$id': '#/definitions/dependency', 'title': 'Dependency', 'type': 'string', 'description': 'Project dependency specification according to PEP 508', 'format': 'pep508'}}}}, 'dynamic': {'type': 'array', '$$description': ['Specifies which fields are intentionally unspecified and expected to be', 'dynamically provided by build tools'], 'items': {'enum': ['version', 'description', 'readme', 'requires-python', 'license', 'authors', 'maintainers', 'keywords', 'classifiers', 'urls', 'scripts', 'gui-scripts', 'entry-points', 'dependencies', 'optional-dependencies']}}}, 'required': ['name'], 'additionalProperties': False, 'if': {'not': {'required': ['dynamic'], 'properties': {'dynamic': {'contains': {'const': 'version'}, '$$description': ['version is listed in ``dynamic``']}}}, '$$comment': ['According to :pep:`621`:', '    If the core metadata specification lists a field as \"Required\", then', '    the metadata MUST specify the field statically or list it in dynamic', 'In turn, `core metadata`_ defines:', '    The required fields are: Metadata-Version, Name, Version.', '    All the other fields are optional.', 'Since ``Metadata-Version`` is defined by the build back-end, ``name`` and', '``version`` are the only mandatory information in ``pyproject.toml``.', '.. _core metadata: https://packaging.python.org/specifications/core-metadata/']}, 'then': {'required': ['version'], '$$description': ['version should be statically defined in the ``version`` field']}, 'definitions': {'author': {'$id': '#/definitions/author', 'title': 'Author or Maintainer', '$comment': 'https://peps.python.org/pep-0621/#authors-maintainers', 'type': 'object', 'additionalProperties': False, 'properties': {'name': {'type': 'string', '$$description': ['MUST be a valid email name, i.e. whatever can be put as a name, before an', 'email, in :rfc:`822`.']}, 'email': {'type': 'string', 'format': 'idn-email', 'description': 'MUST be a valid email address'}}}, 'entry-point-group': {'$id': '#/definitions/entry-point-group', 'title': 'Entry-points', 'type': 'object', '$$description': ['Entry-points are grouped together to indicate what sort of capabilities they', 'provide.', 'See the `packaging guides', '<https://packaging.python.org/specifications/entry-points/>`_', 'and `setuptools docs', '<https://setuptools.pypa.io/en/latest/userguide/entry_point.html>`_', 'for more information.'], 'propertyNames': {'format': 'python-entrypoint-name'}, 'additionalProperties': False, 'patternProperties': {'^.+$': {'type': 'string', '$$description': ['Reference to a Python object. It is either in the form', '``importable.module``, or ``importable.module:object.attr``.'], 'format': 'python-entrypoint-reference', '$comment': 'https://packaging.python.org/specifications/entry-points/'}}}, 'dependency': {'$id': '#/definitions/dependency', 'title': 'Dependency', 'type': 'string', 'description': 'Project dependency specification according to PEP 508', 'format': 'pep508'}}}, rule='required')\n        data_keys = set(data.keys())\n        if \"name\" in data_keys:\n            data_keys.remove(\"name\")\n            data__name = data[\"name\"]\n            if not isinstance(data__name, (str)):\n                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".name must be string\", value=data__name, name=\"\" + (name_prefix or \"data\") + \".name\", definition={'type': 'string', 'description': 'The name (primary identifier) of the project. MUST be statically defined.', 'format': 'pep508-identifier'}, rule='type')\n            if isinstance(data__name, str):\n                if not custom_formats[\"pep508-identifier\"](data__name):\n                    raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".name must be pep508-identifier\", value=data__name, name=\"\" + (name_prefix or \"data\") + \".name\", definition={'type': 'string', 'description': 'The name (primary identifier) of the project. MUST be statically defined.', 'format': 'pep508-identifier'}, rule='format')\n        if \"version\" in data_keys:\n            data_keys.remove(\"version\")\n            data__version = data[\"version\"]\n            if not isinstance(data__version, (str)):\n                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".version must be string\", value=data__version, name=\"\" + (name_prefix or \"data\") + \".version\", definition={'type': 'string', 'description': 'The version of the project as supported by :pep:`440`.', 'format': 'pep440'}, rule='type')\n            if isinstance(data__version, str):\n                if not custom_formats[\"pep440\"](data__version):\n                    raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".version must be pep440\", value=data__version, name=\"\" + (name_prefix or \"data\") + \".version\", definition={'type': 'string', 'description': 'The version of the project as supported by :pep:`440`.', 'format': 'pep440'}, rule='format')\n        if \"description\" in data_keys:\n            data_keys.remove(\"description\")\n            data__description = data[\"description\"]\n            if not isinstance(data__description, (str)):\n                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".description must be string\", value=data__description, name=\"\" + (name_prefix or \"data\") + \".description\", definition={'type': 'string', '$$description': ['The `summary description of the project', '<https://packaging.python.org/specifications/core-metadata/#summary>`_']}, rule='type')\n        if \"readme\" in data_keys:\n            data_keys.remove(\"readme\")\n            data__readme = data[\"readme\"]\n            data__readme_one_of_count11 = 0\n            if data__readme_one_of_count11 < 2:\n                try:\n                    if not isinstance(data__readme, (str)):\n                        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".readme must be string\", value=data__readme, name=\"\" + (name_prefix or \"data\") + \".readme\", definition={'type': 'string', '$$description': ['Relative path to a text file (UTF-8) containing the full description', 'of the project. If the file path ends in case-insensitive ``.md`` or', '``.rst`` suffixes, then the content-type is respectively', '``text/markdown`` or ``text/x-rst``']}, rule='type')\n                    data__readme_one_of_count11 += 1\n                except JsonSchemaValueException: pass\n            if data__readme_one_of_count11 < 2:\n                try:\n                    if not isinstance(data__readme, (dict)):\n                        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".readme must be object\", value=data__readme, name=\"\" + (name_prefix or \"data\") + \".readme\", definition={'type': 'object', 'allOf': [{'anyOf': [{'properties': {'file': {'type': 'string', '$$description': ['Relative path to a text file containing the full description', 'of the project.']}}, 'required': ['file']}, {'properties': {'text': {'type': 'string', 'description': 'Full text describing the project.'}}, 'required': ['text']}]}, {'properties': {'content-type': {'type': 'string', '$$description': ['Content-type (:rfc:`1341`) of the full description', '(e.g. ``text/markdown``). The ``charset`` parameter is assumed', 'UTF-8 when not present.'], '$comment': 'TODO: add regex pattern or format?'}}, 'required': ['content-type']}]}, rule='type')\n                    data__readme_any_of_count12 = 0\n                    if not data__readme_any_of_count12:\n                        try:\n                            data__readme_is_dict = isinstance(data__readme, dict)\n                            if data__readme_is_dict:\n                                data__readme__missing_keys = set(['file']) - data__readme.keys()\n                                if data__readme__missing_keys:\n                                    raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".readme must contain \" + (str(sorted(data__readme__missing_keys)) + \" properties\"), value=data__readme, name=\"\" + (name_prefix or \"data\") + \".readme\", definition={'properties': {'file': {'type': 'string', '$$description': ['Relative path to a text file containing the full description', 'of the project.']}}, 'required': ['file']}, rule='required')\n                                data__readme_keys = set(data__readme.keys())\n                                if \"file\" in data__readme_keys:\n                                    data__readme_keys.remove(\"file\")\n                                    data__readme__file = data__readme[\"file\"]\n                                    if not isinstance(data__readme__file, (str)):\n                                        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".readme.file must be string\", value=data__readme__file, name=\"\" + (name_prefix or \"data\") + \".readme.file\", definition={'type': 'string', '$$description': ['Relative path to a text file containing the full description', 'of the project.']}, rule='type')\n                            data__readme_any_of_count12 += 1\n                        except JsonSchemaValueException: pass\n                    if not data__readme_any_of_count12:\n                        try:\n                            data__readme_is_dict = isinstance(data__readme, dict)\n                            if data__readme_is_dict:\n                                data__readme__missing_keys = set(['text']) - data__readme.keys()\n                                if data__readme__missing_keys:\n                                    raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".readme must contain \" + (str(sorted(data__readme__missing_keys)) + \" properties\"), value=data__readme, name=\"\" + (name_prefix or \"data\") + \".readme\", definition={'properties': {'text': {'type': 'string', 'description': 'Full text describing the project.'}}, 'required': ['text']}, rule='required')\n                                data__readme_keys = set(data__readme.keys())\n                                if \"text\" in data__readme_keys:\n                                    data__readme_keys.remove(\"text\")\n                                    data__readme__text = data__readme[\"text\"]\n                                    if not isinstance(data__readme__text, (str)):\n                                        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".readme.text must be string\", value=data__readme__text, name=\"\" + (name_prefix or \"data\") + \".readme.text\", definition={'type': 'string', 'description': 'Full text describing the project.'}, rule='type')\n                            data__readme_any_of_count12 += 1\n                        except JsonSchemaValueException: pass\n                    if not data__readme_any_of_count12:\n                        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".readme cannot be validated by any definition\", value=data__readme, name=\"\" + (name_prefix or \"data\") + \".readme\", definition={'anyOf': [{'properties': {'file': {'type': 'string', '$$description': ['Relative path to a text file containing the full description', 'of the project.']}}, 'required': ['file']}, {'properties': {'text': {'type': 'string', 'description': 'Full text describing the project.'}}, 'required': ['text']}]}, rule='anyOf')\n                    data__readme_is_dict = isinstance(data__readme, dict)\n                    if data__readme_is_dict:\n                        data__readme__missing_keys = set(['content-type']) - data__readme.keys()\n                        if data__readme__missing_keys:\n                            raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".readme must contain \" + (str(sorted(data__readme__missing_keys)) + \" properties\"), value=data__readme, name=\"\" + (name_prefix or \"data\") + \".readme\", definition={'properties': {'content-type': {'type': 'string', '$$description': ['Content-type (:rfc:`1341`) of the full description', '(e.g. ``text/markdown``). The ``charset`` parameter is assumed', 'UTF-8 when not present.'], '$comment': 'TODO: add regex pattern or format?'}}, 'required': ['content-type']}, rule='required')\n                        data__readme_keys = set(data__readme.keys())\n                        if \"content-type\" in data__readme_keys:\n                            data__readme_keys.remove(\"content-type\")\n                            data__readme__contenttype = data__readme[\"content-type\"]\n                            if not isinstance(data__readme__contenttype, (str)):\n                                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".readme.content-type must be string\", value=data__readme__contenttype, name=\"\" + (name_prefix or \"data\") + \".readme.content-type\", definition={'type': 'string', '$$description': ['Content-type (:rfc:`1341`) of the full description', '(e.g. ``text/markdown``). The ``charset`` parameter is assumed', 'UTF-8 when not present.'], '$comment': 'TODO: add regex pattern or format?'}, rule='type')\n                    data__readme_one_of_count11 += 1\n                except JsonSchemaValueException: pass\n            if data__readme_one_of_count11 != 1:\n                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".readme must be valid exactly by one definition\" + (\" (\" + str(data__readme_one_of_count11) + \" matches found)\"), value=data__readme, name=\"\" + (name_prefix or \"data\") + \".readme\", definition={'$$description': ['`Full/detailed description of the project in the form of a README', '<https://peps.python.org/pep-0621/#readme>`_', \"with meaning similar to the one defined in `core metadata's Description\", '<https://packaging.python.org/specifications/core-metadata/#description>`_'], 'oneOf': [{'type': 'string', '$$description': ['Relative path to a text file (UTF-8) containing the full description', 'of the project. If the file path ends in case-insensitive ``.md`` or', '``.rst`` suffixes, then the content-type is respectively', '``text/markdown`` or ``text/x-rst``']}, {'type': 'object', 'allOf': [{'anyOf': [{'properties': {'file': {'type': 'string', '$$description': ['Relative path to a text file containing the full description', 'of the project.']}}, 'required': ['file']}, {'properties': {'text': {'type': 'string', 'description': 'Full text describing the project.'}}, 'required': ['text']}]}, {'properties': {'content-type': {'type': 'string', '$$description': ['Content-type (:rfc:`1341`) of the full description', '(e.g. ``text/markdown``). The ``charset`` parameter is assumed', 'UTF-8 when not present.'], '$comment': 'TODO: add regex pattern or format?'}}, 'required': ['content-type']}]}]}, rule='oneOf')\n        if \"requires-python\" in data_keys:\n            data_keys.remove(\"requires-python\")\n            data__requirespython = data[\"requires-python\"]\n            if not isinstance(data__requirespython, (str)):\n                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".requires-python must be string\", value=data__requirespython, name=\"\" + (name_prefix or \"data\") + \".requires-python\", definition={'type': 'string', 'format': 'pep508-versionspec', '$$description': ['`The Python version requirements of the project', '<https://packaging.python.org/specifications/core-metadata/#requires-python>`_.']}, rule='type')\n            if isinstance(data__requirespython, str):\n                if not custom_formats[\"pep508-versionspec\"](data__requirespython):\n                    raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".requires-python must be pep508-versionspec\", value=data__requirespython, name=\"\" + (name_prefix or \"data\") + \".requires-python\", definition={'type': 'string', 'format': 'pep508-versionspec', '$$description': ['`The Python version requirements of the project', '<https://packaging.python.org/specifications/core-metadata/#requires-python>`_.']}, rule='format')\n        if \"license\" in data_keys:\n            data_keys.remove(\"license\")\n            data__license = data[\"license\"]\n            data__license_one_of_count13 = 0\n            if data__license_one_of_count13 < 2:\n                try:\n                    data__license_is_dict = isinstance(data__license, dict)\n                    if data__license_is_dict:\n                        data__license__missing_keys = set(['file']) - data__license.keys()\n                        if data__license__missing_keys:\n                            raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".license must contain \" + (str(sorted(data__license__missing_keys)) + \" properties\"), value=data__license, name=\"\" + (name_prefix or \"data\") + \".license\", definition={'properties': {'file': {'type': 'string', '$$description': ['Relative path to the file (UTF-8) which contains the license for the', 'project.']}}, 'required': ['file']}, rule='required')\n                        data__license_keys = set(data__license.keys())\n                        if \"file\" in data__license_keys:\n                            data__license_keys.remove(\"file\")\n                            data__license__file = data__license[\"file\"]\n                            if not isinstance(data__license__file, (str)):\n                                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".license.file must be string\", value=data__license__file, name=\"\" + (name_prefix or \"data\") + \".license.file\", definition={'type': 'string', '$$description': ['Relative path to the file (UTF-8) which contains the license for the', 'project.']}, rule='type')\n                    data__license_one_of_count13 += 1\n                except JsonSchemaValueException: pass\n            if data__license_one_of_count13 < 2:\n                try:\n                    data__license_is_dict = isinstance(data__license, dict)\n                    if data__license_is_dict:\n                        data__license__missing_keys = set(['text']) - data__license.keys()\n                        if data__license__missing_keys:\n                            raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".license must contain \" + (str(sorted(data__license__missing_keys)) + \" properties\"), value=data__license, name=\"\" + (name_prefix or \"data\") + \".license\", definition={'properties': {'text': {'type': 'string', '$$description': ['The license of the project whose meaning is that of the', '`License field from the core metadata', '<https://packaging.python.org/specifications/core-metadata/#license>`_.']}}, 'required': ['text']}, rule='required')\n                        data__license_keys = set(data__license.keys())\n                        if \"text\" in data__license_keys:\n                            data__license_keys.remove(\"text\")\n                            data__license__text = data__license[\"text\"]\n                            if not isinstance(data__license__text, (str)):\n                                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".license.text must be string\", value=data__license__text, name=\"\" + (name_prefix or \"data\") + \".license.text\", definition={'type': 'string', '$$description': ['The license of the project whose meaning is that of the', '`License field from the core metadata', '<https://packaging.python.org/specifications/core-metadata/#license>`_.']}, rule='type')\n                    data__license_one_of_count13 += 1\n                except JsonSchemaValueException: pass\n            if data__license_one_of_count13 != 1:\n                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".license must be valid exactly by one definition\" + (\" (\" + str(data__license_one_of_count13) + \" matches found)\"), value=data__license, name=\"\" + (name_prefix or \"data\") + \".license\", definition={'description': '`Project license <https://peps.python.org/pep-0621/#license>`_.', 'oneOf': [{'properties': {'file': {'type': 'string', '$$description': ['Relative path to the file (UTF-8) which contains the license for the', 'project.']}}, 'required': ['file']}, {'properties': {'text': {'type': 'string', '$$description': ['The license of the project whose meaning is that of the', '`License field from the core metadata', '<https://packaging.python.org/specifications/core-metadata/#license>`_.']}}, 'required': ['text']}]}, rule='oneOf')\n        if \"authors\" in data_keys:\n            data_keys.remove(\"authors\")\n            data__authors = data[\"authors\"]\n            if not isinstance(data__authors, (list, tuple)):\n                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".authors must be array\", value=data__authors, name=\"\" + (name_prefix or \"data\") + \".authors\", definition={'type': 'array', 'items': {'$id': '#/definitions/author', 'title': 'Author or Maintainer', '$comment': 'https://peps.python.org/pep-0621/#authors-maintainers', 'type': 'object', 'additionalProperties': False, 'properties': {'name': {'type': 'string', '$$description': ['MUST be a valid email name, i.e. whatever can be put as a name, before an', 'email, in :rfc:`822`.']}, 'email': {'type': 'string', 'format': 'idn-email', 'description': 'MUST be a valid email address'}}}, '$$description': [\"The people or organizations considered to be the 'authors' of the project.\", 'The exact meaning is open to interpretation (e.g. original or primary authors,', 'current maintainers, or owners of the package).']}, rule='type')\n            data__authors_is_list = isinstance(data__authors, (list, tuple))\n            if data__authors_is_list:\n                data__authors_len = len(data__authors)\n                for data__authors_x, data__authors_item in enumerate(data__authors):\n                    validate_https___packaging_python_org_en_latest_specifications_pyproject_toml___definitions_author(data__authors_item, custom_formats, (name_prefix or \"data\") + \".authors[{data__authors_x}]\".format(**locals()))\n        if \"maintainers\" in data_keys:\n            data_keys.remove(\"maintainers\")\n            data__maintainers = data[\"maintainers\"]\n            if not isinstance(data__maintainers, (list, tuple)):\n                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".maintainers must be array\", value=data__maintainers, name=\"\" + (name_prefix or \"data\") + \".maintainers\", definition={'type': 'array', 'items': {'$id': '#/definitions/author', 'title': 'Author or Maintainer', '$comment': 'https://peps.python.org/pep-0621/#authors-maintainers', 'type': 'object', 'additionalProperties': False, 'properties': {'name': {'type': 'string', '$$description': ['MUST be a valid email name, i.e. whatever can be put as a name, before an', 'email, in :rfc:`822`.']}, 'email': {'type': 'string', 'format': 'idn-email', 'description': 'MUST be a valid email address'}}}, '$$description': [\"The people or organizations considered to be the 'maintainers' of the project.\", 'Similarly to ``authors``, the exact meaning is open to interpretation.']}, rule='type')\n            data__maintainers_is_list = isinstance(data__maintainers, (list, tuple))\n            if data__maintainers_is_list:\n                data__maintainers_len = len(data__maintainers)\n                for data__maintainers_x, data__maintainers_item in enumerate(data__maintainers):\n                    validate_https___packaging_python_org_en_latest_specifications_pyproject_toml___definitions_author(data__maintainers_item, custom_formats, (name_prefix or \"data\") + \".maintainers[{data__maintainers_x}]\".format(**locals()))\n        if \"keywords\" in data_keys:\n            data_keys.remove(\"keywords\")\n            data__keywords = data[\"keywords\"]\n            if not isinstance(data__keywords, (list, tuple)):\n                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".keywords must be array\", value=data__keywords, name=\"\" + (name_prefix or \"data\") + \".keywords\", definition={'type': 'array', 'items': {'type': 'string'}, 'description': 'List of keywords to assist searching for the distribution in a larger catalog.'}, rule='type')\n            data__keywords_is_list = isinstance(data__keywords, (list, tuple))\n            if data__keywords_is_list:\n                data__keywords_len = len(data__keywords)\n                for data__keywords_x, data__keywords_item in enumerate(data__keywords):\n                    if not isinstance(data__keywords_item, (str)):\n                        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".keywords[{data__keywords_x}]\".format(**locals()) + \" must be string\", value=data__keywords_item, name=\"\" + (name_prefix or \"data\") + \".keywords[{data__keywords_x}]\".format(**locals()) + \"\", definition={'type': 'string'}, rule='type')\n        if \"classifiers\" in data_keys:\n            data_keys.remove(\"classifiers\")\n            data__classifiers = data[\"classifiers\"]\n            if not isinstance(data__classifiers, (list, tuple)):\n                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".classifiers must be array\", value=data__classifiers, name=\"\" + (name_prefix or \"data\") + \".classifiers\", definition={'type': 'array', 'items': {'type': 'string', 'format': 'trove-classifier', 'description': '`PyPI classifier <https://pypi.org/classifiers/>`_.'}, '$$description': ['`Trove classifiers <https://pypi.org/classifiers/>`_', 'which apply to the project.']}, rule='type')\n            data__classifiers_is_list = isinstance(data__classifiers, (list, tuple))\n            if data__classifiers_is_list:\n                data__classifiers_len = len(data__classifiers)\n                for data__classifiers_x, data__classifiers_item in enumerate(data__classifiers):\n                    if not isinstance(data__classifiers_item, (str)):\n                        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".classifiers[{data__classifiers_x}]\".format(**locals()) + \" must be string\", value=data__classifiers_item, name=\"\" + (name_prefix or \"data\") + \".classifiers[{data__classifiers_x}]\".format(**locals()) + \"\", definition={'type': 'string', 'format': 'trove-classifier', 'description': '`PyPI classifier <https://pypi.org/classifiers/>`_.'}, rule='type')\n                    if isinstance(data__classifiers_item, str):\n                        if not custom_formats[\"trove-classifier\"](data__classifiers_item):\n                            raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".classifiers[{data__classifiers_x}]\".format(**locals()) + \" must be trove-classifier\", value=data__classifiers_item, name=\"\" + (name_prefix or \"data\") + \".classifiers[{data__classifiers_x}]\".format(**locals()) + \"\", definition={'type': 'string', 'format': 'trove-classifier', 'description': '`PyPI classifier <https://pypi.org/classifiers/>`_.'}, rule='format')\n        if \"urls\" in data_keys:\n            data_keys.remove(\"urls\")\n            data__urls = data[\"urls\"]\n            if not isinstance(data__urls, (dict)):\n                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".urls must be object\", value=data__urls, name=\"\" + (name_prefix or \"data\") + \".urls\", definition={'type': 'object', 'description': 'URLs associated with the project in the form ``label => value``.', 'additionalProperties': False, 'patternProperties': {'^.+$': {'type': 'string', 'format': 'url'}}}, rule='type')\n            data__urls_is_dict = isinstance(data__urls, dict)\n            if data__urls_is_dict:\n                data__urls_keys = set(data__urls.keys())\n                for data__urls_key, data__urls_val in data__urls.items():\n                    if REGEX_PATTERNS['^.+$'].search(data__urls_key):\n                        if data__urls_key in data__urls_keys:\n                            data__urls_keys.remove(data__urls_key)\n                        if not isinstance(data__urls_val, (str)):\n                            raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".urls.{data__urls_key}\".format(**locals()) + \" must be string\", value=data__urls_val, name=\"\" + (name_prefix or \"data\") + \".urls.{data__urls_key}\".format(**locals()) + \"\", definition={'type': 'string', 'format': 'url'}, rule='type')\n                        if isinstance(data__urls_val, str):\n                            if not custom_formats[\"url\"](data__urls_val):\n                                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".urls.{data__urls_key}\".format(**locals()) + \" must be url\", value=data__urls_val, name=\"\" + (name_prefix or \"data\") + \".urls.{data__urls_key}\".format(**locals()) + \"\", definition={'type': 'string', 'format': 'url'}, rule='format')\n                if data__urls_keys:\n                    raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".urls must not contain \"+str(data__urls_keys)+\" properties\", value=data__urls, name=\"\" + (name_prefix or \"data\") + \".urls\", definition={'type': 'object', 'description': 'URLs associated with the project in the form ``label => value``.', 'additionalProperties': False, 'patternProperties': {'^.+$': {'type': 'string', 'format': 'url'}}}, rule='additionalProperties')\n        if \"scripts\" in data_keys:\n            data_keys.remove(\"scripts\")\n            data__scripts = data[\"scripts\"]\n            validate_https___packaging_python_org_en_latest_specifications_pyproject_toml___definitions_entry_point_group(data__scripts, custom_formats, (name_prefix or \"data\") + \".scripts\")\n        if \"gui-scripts\" in data_keys:\n            data_keys.remove(\"gui-scripts\")\n            data__guiscripts = data[\"gui-scripts\"]\n            validate_https___packaging_python_org_en_latest_specifications_pyproject_toml___definitions_entry_point_group(data__guiscripts, custom_formats, (name_prefix or \"data\") + \".gui-scripts\")\n        if \"entry-points\" in data_keys:\n            data_keys.remove(\"entry-points\")\n            data__entrypoints = data[\"entry-points\"]\n            data__entrypoints_is_dict = isinstance(data__entrypoints, dict)\n            if data__entrypoints_is_dict:\n                data__entrypoints_keys = set(data__entrypoints.keys())\n                for data__entrypoints_key, data__entrypoints_val in data__entrypoints.items():\n                    if REGEX_PATTERNS['^.+$'].search(data__entrypoints_key):\n                        if data__entrypoints_key in data__entrypoints_keys:\n                            data__entrypoints_keys.remove(data__entrypoints_key)\n                        validate_https___packaging_python_org_en_latest_specifications_pyproject_toml___definitions_entry_point_group(data__entrypoints_val, custom_formats, (name_prefix or \"data\") + \".entry-points.{data__entrypoints_key}\".format(**locals()))\n                if data__entrypoints_keys:\n                    raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".entry-points must not contain \"+str(data__entrypoints_keys)+\" properties\", value=data__entrypoints, name=\"\" + (name_prefix or \"data\") + \".entry-points\", definition={'$$description': ['Instruct the installer to expose the given modules/functions via', '``entry-point`` discovery mechanism (useful for plugins).', 'More information available in the `Python packaging guide', '<https://packaging.python.org/specifications/entry-points/>`_.'], 'propertyNames': {'format': 'python-entrypoint-group'}, 'additionalProperties': False, 'patternProperties': {'^.+$': {'$id': '#/definitions/entry-point-group', 'title': 'Entry-points', 'type': 'object', '$$description': ['Entry-points are grouped together to indicate what sort of capabilities they', 'provide.', 'See the `packaging guides', '<https://packaging.python.org/specifications/entry-points/>`_', 'and `setuptools docs', '<https://setuptools.pypa.io/en/latest/userguide/entry_point.html>`_', 'for more information.'], 'propertyNames': {'format': 'python-entrypoint-name'}, 'additionalProperties': False, 'patternProperties': {'^.+$': {'type': 'string', '$$description': ['Reference to a Python object. It is either in the form', '``importable.module``, or ``importable.module:object.attr``.'], 'format': 'python-entrypoint-reference', '$comment': 'https://packaging.python.org/specifications/entry-points/'}}}}}, rule='additionalProperties')\n                data__entrypoints_len = len(data__entrypoints)\n                if data__entrypoints_len != 0:\n                    data__entrypoints_property_names = True\n                    for data__entrypoints_key in data__entrypoints:\n                        try:\n                            if isinstance(data__entrypoints_key, str):\n                                if not custom_formats[\"python-entrypoint-group\"](data__entrypoints_key):\n                                    raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".entry-points must be python-entrypoint-group\", value=data__entrypoints_key, name=\"\" + (name_prefix or \"data\") + \".entry-points\", definition={'format': 'python-entrypoint-group'}, rule='format')\n                        except JsonSchemaValueException:\n                            data__entrypoints_property_names = False\n                    if not data__entrypoints_property_names:\n                        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".entry-points must be named by propertyName definition\", value=data__entrypoints, name=\"\" + (name_prefix or \"data\") + \".entry-points\", definition={'$$description': ['Instruct the installer to expose the given modules/functions via', '``entry-point`` discovery mechanism (useful for plugins).', 'More information available in the `Python packaging guide', '<https://packaging.python.org/specifications/entry-points/>`_.'], 'propertyNames': {'format': 'python-entrypoint-group'}, 'additionalProperties': False, 'patternProperties': {'^.+$': {'$id': '#/definitions/entry-point-group', 'title': 'Entry-points', 'type': 'object', '$$description': ['Entry-points are grouped together to indicate what sort of capabilities they', 'provide.', 'See the `packaging guides', '<https://packaging.python.org/specifications/entry-points/>`_', 'and `setuptools docs', '<https://setuptools.pypa.io/en/latest/userguide/entry_point.html>`_', 'for more information.'], 'propertyNames': {'format': 'python-entrypoint-name'}, 'additionalProperties': False, 'patternProperties': {'^.+$': {'type': 'string', '$$description': ['Reference to a Python object. It is either in the form', '``importable.module``, or ``importable.module:object.attr``.'], 'format': 'python-entrypoint-reference', '$comment': 'https://packaging.python.org/specifications/entry-points/'}}}}}, rule='propertyNames')\n        if \"dependencies\" in data_keys:\n            data_keys.remove(\"dependencies\")\n            data__dependencies = data[\"dependencies\"]\n            if not isinstance(data__dependencies, (list, tuple)):\n                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".dependencies must be array\", value=data__dependencies, name=\"\" + (name_prefix or \"data\") + \".dependencies\", definition={'type': 'array', 'description': 'Project (mandatory) dependencies.', 'items': {'$id': '#/definitions/dependency', 'title': 'Dependency', 'type': 'string', 'description': 'Project dependency specification according to PEP 508', 'format': 'pep508'}}, rule='type')\n            data__dependencies_is_list = isinstance(data__dependencies, (list, tuple))\n            if data__dependencies_is_list:\n                data__dependencies_len = len(data__dependencies)\n                for data__dependencies_x, data__dependencies_item in enumerate(data__dependencies):\n                    validate_https___packaging_python_org_en_latest_specifications_pyproject_toml___definitions_dependency(data__dependencies_item, custom_formats, (name_prefix or \"data\") + \".dependencies[{data__dependencies_x}]\".format(**locals()))\n        if \"optional-dependencies\" in data_keys:\n            data_keys.remove(\"optional-dependencies\")\n            data__optionaldependencies = data[\"optional-dependencies\"]\n            if not isinstance(data__optionaldependencies, (dict)):\n                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".optional-dependencies must be object\", value=data__optionaldependencies, name=\"\" + (name_prefix or \"data\") + \".optional-dependencies\", definition={'type': 'object', 'description': 'Optional dependency for the project', 'propertyNames': {'format': 'pep508-identifier'}, 'additionalProperties': False, 'patternProperties': {'^.+$': {'type': 'array', 'items': {'$id': '#/definitions/dependency', 'title': 'Dependency', 'type': 'string', 'description': 'Project dependency specification according to PEP 508', 'format': 'pep508'}}}}, rule='type')\n            data__optionaldependencies_is_dict = isinstance(data__optionaldependencies, dict)\n            if data__optionaldependencies_is_dict:\n                data__optionaldependencies_keys = set(data__optionaldependencies.keys())\n                for data__optionaldependencies_key, data__optionaldependencies_val in data__optionaldependencies.items():\n                    if REGEX_PATTERNS['^.+$'].search(data__optionaldependencies_key):\n                        if data__optionaldependencies_key in data__optionaldependencies_keys:\n                            data__optionaldependencies_keys.remove(data__optionaldependencies_key)\n                        if not isinstance(data__optionaldependencies_val, (list, tuple)):\n                            raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".optional-dependencies.{data__optionaldependencies_key}\".format(**locals()) + \" must be array\", value=data__optionaldependencies_val, name=\"\" + (name_prefix or \"data\") + \".optional-dependencies.{data__optionaldependencies_key}\".format(**locals()) + \"\", definition={'type': 'array', 'items': {'$id': '#/definitions/dependency', 'title': 'Dependency', 'type': 'string', 'description': 'Project dependency specification according to PEP 508', 'format': 'pep508'}}, rule='type')\n                        data__optionaldependencies_val_is_list = isinstance(data__optionaldependencies_val, (list, tuple))\n                        if data__optionaldependencies_val_is_list:\n                            data__optionaldependencies_val_len = len(data__optionaldependencies_val)\n                            for data__optionaldependencies_val_x, data__optionaldependencies_val_item in enumerate(data__optionaldependencies_val):\n                                validate_https___packaging_python_org_en_latest_specifications_pyproject_toml___definitions_dependency(data__optionaldependencies_val_item, custom_formats, (name_prefix or \"data\") + \".optional-dependencies.{data__optionaldependencies_key}[{data__optionaldependencies_val_x}]\".format(**locals()))\n                if data__optionaldependencies_keys:\n                    raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".optional-dependencies must not contain \"+str(data__optionaldependencies_keys)+\" properties\", value=data__optionaldependencies, name=\"\" + (name_prefix or \"data\") + \".optional-dependencies\", definition={'type': 'object', 'description': 'Optional dependency for the project', 'propertyNames': {'format': 'pep508-identifier'}, 'additionalProperties': False, 'patternProperties': {'^.+$': {'type': 'array', 'items': {'$id': '#/definitions/dependency', 'title': 'Dependency', 'type': 'string', 'description': 'Project dependency specification according to PEP 508', 'format': 'pep508'}}}}, rule='additionalProperties')\n                data__optionaldependencies_len = len(data__optionaldependencies)\n                if data__optionaldependencies_len != 0:\n                    data__optionaldependencies_property_names = True\n                    for data__optionaldependencies_key in data__optionaldependencies:\n                        try:\n                            if isinstance(data__optionaldependencies_key, str):\n                                if not custom_formats[\"pep508-identifier\"](data__optionaldependencies_key):\n                                    raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".optional-dependencies must be pep508-identifier\", value=data__optionaldependencies_key, name=\"\" + (name_prefix or \"data\") + \".optional-dependencies\", definition={'format': 'pep508-identifier'}, rule='format')\n                        except JsonSchemaValueException:\n                            data__optionaldependencies_property_names = False\n                    if not data__optionaldependencies_property_names:\n                        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".optional-dependencies must be named by propertyName definition\", value=data__optionaldependencies, name=\"\" + (name_prefix or \"data\") + \".optional-dependencies\", definition={'type': 'object', 'description': 'Optional dependency for the project', 'propertyNames': {'format': 'pep508-identifier'}, 'additionalProperties': False, 'patternProperties': {'^.+$': {'type': 'array', 'items': {'$id': '#/definitions/dependency', 'title': 'Dependency', 'type': 'string', 'description': 'Project dependency specification according to PEP 508', 'format': 'pep508'}}}}, rule='propertyNames')\n        if \"dynamic\" in data_keys:\n            data_keys.remove(\"dynamic\")\n            data__dynamic = data[\"dynamic\"]\n            if not isinstance(data__dynamic, (list, tuple)):\n                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".dynamic must be array\", value=data__dynamic, name=\"\" + (name_prefix or \"data\") + \".dynamic\", definition={'type': 'array', '$$description': ['Specifies which fields are intentionally unspecified and expected to be', 'dynamically provided by build tools'], 'items': {'enum': ['version', 'description', 'readme', 'requires-python', 'license', 'authors', 'maintainers', 'keywords', 'classifiers', 'urls', 'scripts', 'gui-scripts', 'entry-points', 'dependencies', 'optional-dependencies']}}, rule='type')\n            data__dynamic_is_list = isinstance(data__dynamic, (list, tuple))\n            if data__dynamic_is_list:\n                data__dynamic_len = len(data__dynamic)\n                for data__dynamic_x, data__dynamic_item in enumerate(data__dynamic):\n                    if data__dynamic_item not in ['version', 'description', 'readme', 'requires-python', 'license', 'authors', 'maintainers', 'keywords', 'classifiers', 'urls', 'scripts', 'gui-scripts', 'entry-points', 'dependencies', 'optional-dependencies']:\n                        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".dynamic[{data__dynamic_x}]\".format(**locals()) + \" must be one of ['version', 'description', 'readme', 'requires-python', 'license', 'authors', 'maintainers', 'keywords', 'classifiers', 'urls', 'scripts', 'gui-scripts', 'entry-points', 'dependencies', 'optional-dependencies']\", value=data__dynamic_item, name=\"\" + (name_prefix or \"data\") + \".dynamic[{data__dynamic_x}]\".format(**locals()) + \"\", definition={'enum': ['version', 'description', 'readme', 'requires-python', 'license', 'authors', 'maintainers', 'keywords', 'classifiers', 'urls', 'scripts', 'gui-scripts', 'entry-points', 'dependencies', 'optional-dependencies']}, rule='enum')\n        if data_keys:\n            raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \" must not contain \"+str(data_keys)+\" properties\", value=data, name=\"\" + (name_prefix or \"data\") + \"\", definition={'$schema': 'http://json-schema.org/draft-07/schema#', '$id': 'https://packaging.python.org/en/latest/specifications/pyproject-toml/', 'title': 'Package metadata stored in the ``project`` table', '$$description': ['Data structure for the **project** table inside ``pyproject.toml``', '(as initially defined in :pep:`621`)'], 'type': 'object', 'properties': {'name': {'type': 'string', 'description': 'The name (primary identifier) of the project. MUST be statically defined.', 'format': 'pep508-identifier'}, 'version': {'type': 'string', 'description': 'The version of the project as supported by :pep:`440`.', 'format': 'pep440'}, 'description': {'type': 'string', '$$description': ['The `summary description of the project', '<https://packaging.python.org/specifications/core-metadata/#summary>`_']}, 'readme': {'$$description': ['`Full/detailed description of the project in the form of a README', '<https://peps.python.org/pep-0621/#readme>`_', \"with meaning similar to the one defined in `core metadata's Description\", '<https://packaging.python.org/specifications/core-metadata/#description>`_'], 'oneOf': [{'type': 'string', '$$description': ['Relative path to a text file (UTF-8) containing the full description', 'of the project. If the file path ends in case-insensitive ``.md`` or', '``.rst`` suffixes, then the content-type is respectively', '``text/markdown`` or ``text/x-rst``']}, {'type': 'object', 'allOf': [{'anyOf': [{'properties': {'file': {'type': 'string', '$$description': ['Relative path to a text file containing the full description', 'of the project.']}}, 'required': ['file']}, {'properties': {'text': {'type': 'string', 'description': 'Full text describing the project.'}}, 'required': ['text']}]}, {'properties': {'content-type': {'type': 'string', '$$description': ['Content-type (:rfc:`1341`) of the full description', '(e.g. ``text/markdown``). The ``charset`` parameter is assumed', 'UTF-8 when not present.'], '$comment': 'TODO: add regex pattern or format?'}}, 'required': ['content-type']}]}]}, 'requires-python': {'type': 'string', 'format': 'pep508-versionspec', '$$description': ['`The Python version requirements of the project', '<https://packaging.python.org/specifications/core-metadata/#requires-python>`_.']}, 'license': {'description': '`Project license <https://peps.python.org/pep-0621/#license>`_.', 'oneOf': [{'properties': {'file': {'type': 'string', '$$description': ['Relative path to the file (UTF-8) which contains the license for the', 'project.']}}, 'required': ['file']}, {'properties': {'text': {'type': 'string', '$$description': ['The license of the project whose meaning is that of the', '`License field from the core metadata', '<https://packaging.python.org/specifications/core-metadata/#license>`_.']}}, 'required': ['text']}]}, 'authors': {'type': 'array', 'items': {'$id': '#/definitions/author', 'title': 'Author or Maintainer', '$comment': 'https://peps.python.org/pep-0621/#authors-maintainers', 'type': 'object', 'additionalProperties': False, 'properties': {'name': {'type': 'string', '$$description': ['MUST be a valid email name, i.e. whatever can be put as a name, before an', 'email, in :rfc:`822`.']}, 'email': {'type': 'string', 'format': 'idn-email', 'description': 'MUST be a valid email address'}}}, '$$description': [\"The people or organizations considered to be the 'authors' of the project.\", 'The exact meaning is open to interpretation (e.g. original or primary authors,', 'current maintainers, or owners of the package).']}, 'maintainers': {'type': 'array', 'items': {'$id': '#/definitions/author', 'title': 'Author or Maintainer', '$comment': 'https://peps.python.org/pep-0621/#authors-maintainers', 'type': 'object', 'additionalProperties': False, 'properties': {'name': {'type': 'string', '$$description': ['MUST be a valid email name, i.e. whatever can be put as a name, before an', 'email, in :rfc:`822`.']}, 'email': {'type': 'string', 'format': 'idn-email', 'description': 'MUST be a valid email address'}}}, '$$description': [\"The people or organizations considered to be the 'maintainers' of the project.\", 'Similarly to ``authors``, the exact meaning is open to interpretation.']}, 'keywords': {'type': 'array', 'items': {'type': 'string'}, 'description': 'List of keywords to assist searching for the distribution in a larger catalog.'}, 'classifiers': {'type': 'array', 'items': {'type': 'string', 'format': 'trove-classifier', 'description': '`PyPI classifier <https://pypi.org/classifiers/>`_.'}, '$$description': ['`Trove classifiers <https://pypi.org/classifiers/>`_', 'which apply to the project.']}, 'urls': {'type': 'object', 'description': 'URLs associated with the project in the form ``label => value``.', 'additionalProperties': False, 'patternProperties': {'^.+$': {'type': 'string', 'format': 'url'}}}, 'scripts': {'$id': '#/definitions/entry-point-group', 'title': 'Entry-points', 'type': 'object', '$$description': ['Entry-points are grouped together to indicate what sort of capabilities they', 'provide.', 'See the `packaging guides', '<https://packaging.python.org/specifications/entry-points/>`_', 'and `setuptools docs', '<https://setuptools.pypa.io/en/latest/userguide/entry_point.html>`_', 'for more information.'], 'propertyNames': {'format': 'python-entrypoint-name'}, 'additionalProperties': False, 'patternProperties': {'^.+$': {'type': 'string', '$$description': ['Reference to a Python object. It is either in the form', '``importable.module``, or ``importable.module:object.attr``.'], 'format': 'python-entrypoint-reference', '$comment': 'https://packaging.python.org/specifications/entry-points/'}}}, 'gui-scripts': {'$id': '#/definitions/entry-point-group', 'title': 'Entry-points', 'type': 'object', '$$description': ['Entry-points are grouped together to indicate what sort of capabilities they', 'provide.', 'See the `packaging guides', '<https://packaging.python.org/specifications/entry-points/>`_', 'and `setuptools docs', '<https://setuptools.pypa.io/en/latest/userguide/entry_point.html>`_', 'for more information.'], 'propertyNames': {'format': 'python-entrypoint-name'}, 'additionalProperties': False, 'patternProperties': {'^.+$': {'type': 'string', '$$description': ['Reference to a Python object. It is either in the form', '``importable.module``, or ``importable.module:object.attr``.'], 'format': 'python-entrypoint-reference', '$comment': 'https://packaging.python.org/specifications/entry-points/'}}}, 'entry-points': {'$$description': ['Instruct the installer to expose the given modules/functions via', '``entry-point`` discovery mechanism (useful for plugins).', 'More information available in the `Python packaging guide', '<https://packaging.python.org/specifications/entry-points/>`_.'], 'propertyNames': {'format': 'python-entrypoint-group'}, 'additionalProperties': False, 'patternProperties': {'^.+$': {'$id': '#/definitions/entry-point-group', 'title': 'Entry-points', 'type': 'object', '$$description': ['Entry-points are grouped together to indicate what sort of capabilities they', 'provide.', 'See the `packaging guides', '<https://packaging.python.org/specifications/entry-points/>`_', 'and `setuptools docs', '<https://setuptools.pypa.io/en/latest/userguide/entry_point.html>`_', 'for more information.'], 'propertyNames': {'format': 'python-entrypoint-name'}, 'additionalProperties': False, 'patternProperties': {'^.+$': {'type': 'string', '$$description': ['Reference to a Python object. It is either in the form', '``importable.module``, or ``importable.module:object.attr``.'], 'format': 'python-entrypoint-reference', '$comment': 'https://packaging.python.org/specifications/entry-points/'}}}}}, 'dependencies': {'type': 'array', 'description': 'Project (mandatory) dependencies.', 'items': {'$id': '#/definitions/dependency', 'title': 'Dependency', 'type': 'string', 'description': 'Project dependency specification according to PEP 508', 'format': 'pep508'}}, 'optional-dependencies': {'type': 'object', 'description': 'Optional dependency for the project', 'propertyNames': {'format': 'pep508-identifier'}, 'additionalProperties': False, 'patternProperties': {'^.+$': {'type': 'array', 'items': {'$id': '#/definitions/dependency', 'title': 'Dependency', 'type': 'string', 'description': 'Project dependency specification according to PEP 508', 'format': 'pep508'}}}}, 'dynamic': {'type': 'array', '$$description': ['Specifies which fields are intentionally unspecified and expected to be', 'dynamically provided by build tools'], 'items': {'enum': ['version', 'description', 'readme', 'requires-python', 'license', 'authors', 'maintainers', 'keywords', 'classifiers', 'urls', 'scripts', 'gui-scripts', 'entry-points', 'dependencies', 'optional-dependencies']}}}, 'required': ['name'], 'additionalProperties': False, 'if': {'not': {'required': ['dynamic'], 'properties': {'dynamic': {'contains': {'const': 'version'}, '$$description': ['version is listed in ``dynamic``']}}}, '$$comment': ['According to :pep:`621`:', '    If the core metadata specification lists a field as \"Required\", then', '    the metadata MUST specify the field statically or list it in dynamic', 'In turn, `core metadata`_ defines:', '    The required fields are: Metadata-Version, Name, Version.', '    All the other fields are optional.', 'Since ``Metadata-Version`` is defined by the build back-end, ``name`` and', '``version`` are the only mandatory information in ``pyproject.toml``.', '.. _core metadata: https://packaging.python.org/specifications/core-metadata/']}, 'then': {'required': ['version'], '$$description': ['version should be statically defined in the ``version`` field']}, 'definitions': {'author': {'$id': '#/definitions/author', 'title': 'Author or Maintainer', '$comment': 'https://peps.python.org/pep-0621/#authors-maintainers', 'type': 'object', 'additionalProperties': False, 'properties': {'name': {'type': 'string', '$$description': ['MUST be a valid email name, i.e. whatever can be put as a name, before an', 'email, in :rfc:`822`.']}, 'email': {'type': 'string', 'format': 'idn-email', 'description': 'MUST be a valid email address'}}}, 'entry-point-group': {'$id': '#/definitions/entry-point-group', 'title': 'Entry-points', 'type': 'object', '$$description': ['Entry-points are grouped together to indicate what sort of capabilities they', 'provide.', 'See the `packaging guides', '<https://packaging.python.org/specifications/entry-points/>`_', 'and `setuptools docs', '<https://setuptools.pypa.io/en/latest/userguide/entry_point.html>`_', 'for more information.'], 'propertyNames': {'format': 'python-entrypoint-name'}, 'additionalProperties': False, 'patternProperties': {'^.+$': {'type': 'string', '$$description': ['Reference to a Python object. It is either in the form', '``importable.module``, or ``importable.module:object.attr``.'], 'format': 'python-entrypoint-reference', '$comment': 'https://packaging.python.org/specifications/entry-points/'}}}, 'dependency': {'$id': '#/definitions/dependency', 'title': 'Dependency', 'type': 'string', 'description': 'Project dependency specification according to PEP 508', 'format': 'pep508'}}}, rule='additionalProperties')\n    try:\n        try:\n            data_is_dict = isinstance(data, dict)\n            if data_is_dict:\n                data__missing_keys = set(['dynamic']) - data.keys()\n                if data__missing_keys:\n                    raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \" must contain \" + (str(sorted(data__missing_keys)) + \" properties\"), value=data, name=\"\" + (name_prefix or \"data\") + \"\", definition={'required': ['dynamic'], 'properties': {'dynamic': {'contains': {'const': 'version'}, '$$description': ['version is listed in ``dynamic``']}}}, rule='required')\n                data_keys = set(data.keys())\n                if \"dynamic\" in data_keys:\n                    data_keys.remove(\"dynamic\")\n                    data__dynamic = data[\"dynamic\"]\n                    data__dynamic_is_list = isinstance(data__dynamic, (list, tuple))\n                    if data__dynamic_is_list:\n                        data__dynamic_contains = False\n                        for data__dynamic_key in data__dynamic:\n                            try:\n                                if data__dynamic_key != \"version\":\n                                    raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".dynamic must be same as const definition: version\", value=data__dynamic_key, name=\"\" + (name_prefix or \"data\") + \".dynamic\", definition={'const': 'version'}, rule='const')\n                                data__dynamic_contains = True\n                                break\n                            except JsonSchemaValueException: pass\n                        if not data__dynamic_contains:\n                            raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".dynamic must contain one of contains definition\", value=data__dynamic, name=\"\" + (name_prefix or \"data\") + \".dynamic\", definition={'contains': {'const': 'version'}, '$$description': ['version is listed in ``dynamic``']}, rule='contains')\n        except JsonSchemaValueException: pass\n        else:\n            raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \" must NOT match a disallowed definition\", value=data, name=\"\" + (name_prefix or \"data\") + \"\", definition={'not': {'required': ['dynamic'], 'properties': {'dynamic': {'contains': {'const': 'version'}, '$$description': ['version is listed in ``dynamic``']}}}, '$$comment': ['According to :pep:`621`:', '    If the core metadata specification lists a field as \"Required\", then', '    the metadata MUST specify the field statically or list it in dynamic', 'In turn, `core metadata`_ defines:', '    The required fields are: Metadata-Version, Name, Version.', '    All the other fields are optional.', 'Since ``Metadata-Version`` is defined by the build back-end, ``name`` and', '``version`` are the only mandatory information in ``pyproject.toml``.', '.. _core metadata: https://packaging.python.org/specifications/core-metadata/']}, rule='not')\n    except JsonSchemaValueException:\n        pass\n    else:\n        data_is_dict = isinstance(data, dict)\n        if data_is_dict:\n            data__missing_keys = set(['version']) - data.keys()\n            if data__missing_keys:\n                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \" must contain \" + (str(sorted(data__missing_keys)) + \" properties\"), value=data, name=\"\" + (name_prefix or \"data\") + \"\", definition={'required': ['version'], '$$description': ['version should be statically defined in the ``version`` field']}, rule='required')\n    return data\n\ndef validate_https___packaging_python_org_en_latest_specifications_pyproject_toml___definitions_dependency(data, custom_formats={}, name_prefix=None):\n    if not isinstance(data, (str)):\n        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \" must be string\", value=data, name=\"\" + (name_prefix or \"data\") + \"\", definition={'$id': '#/definitions/dependency', 'title': 'Dependency', 'type': 'string', 'description': 'Project dependency specification according to PEP 508', 'format': 'pep508'}, rule='type')\n    if isinstance(data, str):\n        if not custom_formats[\"pep508\"](data):\n            raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \" must be pep508\", value=data, name=\"\" + (name_prefix or \"data\") + \"\", definition={'$id': '#/definitions/dependency', 'title': 'Dependency', 'type': 'string', 'description': 'Project dependency specification according to PEP 508', 'format': 'pep508'}, rule='format')\n    return data\n\ndef validate_https___packaging_python_org_en_latest_specifications_pyproject_toml___definitions_entry_point_group(data, custom_formats={}, name_prefix=None):\n    if not isinstance(data, (dict)):\n        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \" must be object\", value=data, name=\"\" + (name_prefix or \"data\") + \"\", definition={'$id': '#/definitions/entry-point-group', 'title': 'Entry-points', 'type': 'object', '$$description': ['Entry-points are grouped together to indicate what sort of capabilities they', 'provide.', 'See the `packaging guides', '<https://packaging.python.org/specifications/entry-points/>`_', 'and `setuptools docs', '<https://setuptools.pypa.io/en/latest/userguide/entry_point.html>`_', 'for more information.'], 'propertyNames': {'format': 'python-entrypoint-name'}, 'additionalProperties': False, 'patternProperties': {'^.+$': {'type': 'string', '$$description': ['Reference to a Python object. It is either in the form', '``importable.module``, or ``importable.module:object.attr``.'], 'format': 'python-entrypoint-reference', '$comment': 'https://packaging.python.org/specifications/entry-points/'}}}, rule='type')\n    data_is_dict = isinstance(data, dict)\n    if data_is_dict:\n        data_keys = set(data.keys())\n        for data_key, data_val in data.items():\n            if REGEX_PATTERNS['^.+$'].search(data_key):\n                if data_key in data_keys:\n                    data_keys.remove(data_key)\n                if not isinstance(data_val, (str)):\n                    raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".{data_key}\".format(**locals()) + \" must be string\", value=data_val, name=\"\" + (name_prefix or \"data\") + \".{data_key}\".format(**locals()) + \"\", definition={'type': 'string', '$$description': ['Reference to a Python object. It is either in the form', '``importable.module``, or ``importable.module:object.attr``.'], 'format': 'python-entrypoint-reference', '$comment': 'https://packaging.python.org/specifications/entry-points/'}, rule='type')\n                if isinstance(data_val, str):\n                    if not custom_formats[\"python-entrypoint-reference\"](data_val):\n                        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".{data_key}\".format(**locals()) + \" must be python-entrypoint-reference\", value=data_val, name=\"\" + (name_prefix or \"data\") + \".{data_key}\".format(**locals()) + \"\", definition={'type': 'string', '$$description': ['Reference to a Python object. It is either in the form', '``importable.module``, or ``importable.module:object.attr``.'], 'format': 'python-entrypoint-reference', '$comment': 'https://packaging.python.org/specifications/entry-points/'}, rule='format')\n        if data_keys:\n            raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \" must not contain \"+str(data_keys)+\" properties\", value=data, name=\"\" + (name_prefix or \"data\") + \"\", definition={'$id': '#/definitions/entry-point-group', 'title': 'Entry-points', 'type': 'object', '$$description': ['Entry-points are grouped together to indicate what sort of capabilities they', 'provide.', 'See the `packaging guides', '<https://packaging.python.org/specifications/entry-points/>`_', 'and `setuptools docs', '<https://setuptools.pypa.io/en/latest/userguide/entry_point.html>`_', 'for more information.'], 'propertyNames': {'format': 'python-entrypoint-name'}, 'additionalProperties': False, 'patternProperties': {'^.+$': {'type': 'string', '$$description': ['Reference to a Python object. It is either in the form', '``importable.module``, or ``importable.module:object.attr``.'], 'format': 'python-entrypoint-reference', '$comment': 'https://packaging.python.org/specifications/entry-points/'}}}, rule='additionalProperties')\n        data_len = len(data)\n        if data_len != 0:\n            data_property_names = True\n            for data_key in data:\n                try:\n                    if isinstance(data_key, str):\n                        if not custom_formats[\"python-entrypoint-name\"](data_key):\n                            raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \" must be python-entrypoint-name\", value=data_key, name=\"\" + (name_prefix or \"data\") + \"\", definition={'format': 'python-entrypoint-name'}, rule='format')\n                except JsonSchemaValueException:\n                    data_property_names = False\n            if not data_property_names:\n                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \" must be named by propertyName definition\", value=data, name=\"\" + (name_prefix or \"data\") + \"\", definition={'$id': '#/definitions/entry-point-group', 'title': 'Entry-points', 'type': 'object', '$$description': ['Entry-points are grouped together to indicate what sort of capabilities they', 'provide.', 'See the `packaging guides', '<https://packaging.python.org/specifications/entry-points/>`_', 'and `setuptools docs', '<https://setuptools.pypa.io/en/latest/userguide/entry_point.html>`_', 'for more information.'], 'propertyNames': {'format': 'python-entrypoint-name'}, 'additionalProperties': False, 'patternProperties': {'^.+$': {'type': 'string', '$$description': ['Reference to a Python object. It is either in the form', '``importable.module``, or ``importable.module:object.attr``.'], 'format': 'python-entrypoint-reference', '$comment': 'https://packaging.python.org/specifications/entry-points/'}}}, rule='propertyNames')\n    return data\n\ndef validate_https___packaging_python_org_en_latest_specifications_pyproject_toml___definitions_author(data, custom_formats={}, name_prefix=None):\n    if not isinstance(data, (dict)):\n        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \" must be object\", value=data, name=\"\" + (name_prefix or \"data\") + \"\", definition={'$id': '#/definitions/author', 'title': 'Author or Maintainer', '$comment': 'https://peps.python.org/pep-0621/#authors-maintainers', 'type': 'object', 'additionalProperties': False, 'properties': {'name': {'type': 'string', '$$description': ['MUST be a valid email name, i.e. whatever can be put as a name, before an', 'email, in :rfc:`822`.']}, 'email': {'type': 'string', 'format': 'idn-email', 'description': 'MUST be a valid email address'}}}, rule='type')\n    data_is_dict = isinstance(data, dict)\n    if data_is_dict:\n        data_keys = set(data.keys())\n        if \"name\" in data_keys:\n            data_keys.remove(\"name\")\n            data__name = data[\"name\"]\n            if not isinstance(data__name, (str)):\n                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".name must be string\", value=data__name, name=\"\" + (name_prefix or \"data\") + \".name\", definition={'type': 'string', '$$description': ['MUST be a valid email name, i.e. whatever can be put as a name, before an', 'email, in :rfc:`822`.']}, rule='type')\n        if \"email\" in data_keys:\n            data_keys.remove(\"email\")\n            data__email = data[\"email\"]\n            if not isinstance(data__email, (str)):\n                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".email must be string\", value=data__email, name=\"\" + (name_prefix or \"data\") + \".email\", definition={'type': 'string', 'format': 'idn-email', 'description': 'MUST be a valid email address'}, rule='type')\n            if isinstance(data__email, str):\n                if not REGEX_PATTERNS[\"idn-email_re_pattern\"].match(data__email):\n                    raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".email must be idn-email\", value=data__email, name=\"\" + (name_prefix or \"data\") + \".email\", definition={'type': 'string', 'format': 'idn-email', 'description': 'MUST be a valid email address'}, rule='format')\n        if data_keys:\n            raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \" must not contain \"+str(data_keys)+\" properties\", value=data, name=\"\" + (name_prefix or \"data\") + \"\", definition={'$id': '#/definitions/author', 'title': 'Author or Maintainer', '$comment': 'https://peps.python.org/pep-0621/#authors-maintainers', 'type': 'object', 'additionalProperties': False, 'properties': {'name': {'type': 'string', '$$description': ['MUST be a valid email name, i.e. whatever can be put as a name, before an', 'email, in :rfc:`822`.']}, 'email': {'type': 'string', 'format': 'idn-email', 'description': 'MUST be a valid email address'}}}, rule='additionalProperties')\n    return data\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/config/_validate_pyproject/formats.py","size":12814,"sha1":"3ee1a871275b553f49d7183545b00ad050e3aa50","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"\nThe functions in this module are used to validate schemas with the\n`format JSON Schema keyword\n<https://json-schema.org/understanding-json-schema/reference/string#format>`_.\n\nThe correspondence is given by replacing the ``_`` character in the name of the\nfunction with a ``-`` to obtain the format name and vice versa.\n\"\"\"\n\nimport builtins\nimport logging\nimport os\nimport re\nimport string\nimport typing\nfrom itertools import chain as _chain\n\nif typing.TYPE_CHECKING:\n    from typing_extensions import Literal\n\n_logger = logging.getLogger(__name__)\n\n# -------------------------------------------------------------------------------------\n# PEP 440\n\nVERSION_PATTERN = r\"\"\"\n    v?\n    (?:\n        (?:(?P<epoch>[0-9]+)!)?                           # epoch\n        (?P<release>[0-9]+(?:\\.[0-9]+)*)                  # release segment\n        (?P<pre>                                          # pre-release\n            [-_\\.]?\n            (?P<pre_l>alpha|a|beta|b|preview|pre|c|rc)\n            [-_\\.]?\n            (?P<pre_n>[0-9]+)?\n        )?\n        (?P<post>                                         # post release\n            (?:-(?P<post_n1>[0-9]+))\n            |\n            (?:\n                [-_\\.]?\n                (?P<post_l>post|rev|r)\n                [-_\\.]?\n                (?P<post_n2>[0-9]+)?\n            )\n        )?\n        (?P<dev>                                          # dev release\n            [-_\\.]?\n            (?P<dev_l>dev)\n            [-_\\.]?\n            (?P<dev_n>[0-9]+)?\n        )?\n    )\n    (?:\\+(?P<local>[a-z0-9]+(?:[-_\\.][a-z0-9]+)*))?       # local version\n\"\"\"\n\nVERSION_REGEX = re.compile(r\"^\\s*\" + VERSION_PATTERN + r\"\\s*$\", re.X | re.I)\n\n\ndef pep440(version: str) -> bool:\n    \"\"\"See :ref:`PyPA's version specification <pypa:version-specifiers>`\n    (initially introduced in :pep:`440`).\n    \"\"\"\n    return VERSION_REGEX.match(version) is not None\n\n\n# -------------------------------------------------------------------------------------\n# PEP 508\n\nPEP508_IDENTIFIER_PATTERN = r\"([A-Z0-9]|[A-Z0-9][A-Z0-9._-]*[A-Z0-9])\"\nPEP508_IDENTIFIER_REGEX = re.compile(f\"^{PEP508_IDENTIFIER_PATTERN}$\", re.I)\n\n\ndef pep508_identifier(name: str) -> bool:\n    \"\"\"See :ref:`PyPA's name specification <pypa:name-format>`\n    (initially introduced in :pep:`508#names`).\n    \"\"\"\n    return PEP508_IDENTIFIER_REGEX.match(name) is not None\n\n\ntry:\n    try:\n        from packaging import requirements as _req\n    except ImportError:  # pragma: no cover\n        # let's try setuptools vendored version\n        from setuptools._vendor.packaging import (  # type: ignore[no-redef]\n            requirements as _req,\n        )\n\n    def pep508(value: str) -> bool:\n        \"\"\"See :ref:`PyPA's dependency specifiers <pypa:dependency-specifiers>`\n        (initially introduced in :pep:`508`).\n        \"\"\"\n        try:\n            _req.Requirement(value)\n            return True\n        except _req.InvalidRequirement:\n            return False\n\nexcept ImportError:  # pragma: no cover\n    _logger.warning(\n        \"Could not find an installation of `packaging`. Requirements, dependencies and \"\n        \"versions might not be validated. \"\n        \"To enforce validation, please install `packaging`.\"\n    )\n\n    def pep508(value: str) -> bool:\n        return True\n\n\ndef pep508_versionspec(value: str) -> bool:\n    \"\"\"Expression that can be used to specify/lock versions (including ranges)\n    See ``versionspec`` in :ref:`PyPA's dependency specifiers\n    <pypa:dependency-specifiers>` (initially introduced in :pep:`508`).\n    \"\"\"\n    if any(c in value for c in (\";\", \"]\", \"@\")):\n        # In PEP 508:\n        # conditional markers, extras and URL specs are not included in the\n        # versionspec\n        return False\n    # Let's pretend we have a dependency called `requirement` with the given\n    # version spec, then we can reuse the pep508 function for validation:\n    return pep508(f\"requirement{value}\")\n\n\n# -------------------------------------------------------------------------------------\n# PEP 517\n\n\ndef pep517_backend_reference(value: str) -> bool:\n    \"\"\"See PyPA's specification for defining build-backend references\n    introduced in :pep:`517#source-trees`.\n\n    This is similar to an entry-point reference (e.g., ``package.module:object``).\n    \"\"\"\n    module, _, obj = value.partition(\":\")\n    identifiers = (i.strip() for i in _chain(module.split(\".\"), obj.split(\".\")))\n    return all(python_identifier(i) for i in identifiers if i)\n\n\n# -------------------------------------------------------------------------------------\n# Classifiers - PEP 301\n\n\ndef _download_classifiers() -> str:\n    import ssl\n    from email.message import Message\n    from urllib.request import urlopen\n\n    url = \"https://pypi.org/pypi?:action=list_classifiers\"\n    context = ssl.create_default_context()\n    with urlopen(url, context=context) as response:  # noqa: S310 (audit URLs)\n        headers = Message()\n        headers[\"content_type\"] = response.getheader(\"content-type\", \"text/plain\")\n        return response.read().decode(headers.get_param(\"charset\", \"utf-8\"))  # type: ignore[no-any-return]\n\n\nclass _TroveClassifier:\n    \"\"\"The ``trove_classifiers`` package is the official way of validating classifiers,\n    however this package might not be always available.\n    As a workaround we can still download a list from PyPI.\n    We also don't want to be over strict about it, so simply skipping silently is an\n    option (classifiers will be validated anyway during the upload to PyPI).\n    \"\"\"\n\n    downloaded: typing.Union[None, \"Literal[False]\", typing.Set[str]]\n\n    def __init__(self) -> None:\n        self.downloaded = None\n        self._skip_download = False\n        # None => not cached yet\n        # False => cache not available\n        self.__name__ = \"trove_classifier\"  # Emulate a public function\n\n    def _disable_download(self) -> None:\n        # This is a private API. Only setuptools has the consent of using it.\n        self._skip_download = True\n\n    def __call__(self, value: str) -> bool:\n        if self.downloaded is False or self._skip_download is True:\n            return True\n\n        if os.getenv(\"NO_NETWORK\") or os.getenv(\"VALIDATE_PYPROJECT_NO_NETWORK\"):\n            self.downloaded = False\n            msg = (\n                \"Install ``trove-classifiers`` to ensure proper validation. \"\n                \"Skipping download of classifiers list from PyPI (NO_NETWORK).\"\n            )\n            _logger.debug(msg)\n            return True\n\n        if self.downloaded is None:\n            msg = (\n                \"Install ``trove-classifiers`` to ensure proper validation. \"\n                \"Meanwhile a list of classifiers will be downloaded from PyPI.\"\n            )\n            _logger.debug(msg)\n            try:\n                self.downloaded = set(_download_classifiers().splitlines())\n            except Exception:\n                self.downloaded = False\n                _logger.debug(\"Problem with download, skipping validation\")\n                return True\n\n        return value in self.downloaded or value.lower().startswith(\"private ::\")\n\n\ntry:\n    from trove_classifiers import classifiers as _trove_classifiers\n\n    def trove_classifier(value: str) -> bool:\n        \"\"\"See https://pypi.org/classifiers/\"\"\"\n        return value in _trove_classifiers or value.lower().startswith(\"private ::\")\n\nexcept ImportError:  # pragma: no cover\n    trove_classifier = _TroveClassifier()\n\n\n# -------------------------------------------------------------------------------------\n# Stub packages - PEP 561\n\n\ndef pep561_stub_name(value: str) -> bool:\n    \"\"\"Name of a directory containing type stubs.\n    It must follow the name scheme ``<package>-stubs`` as defined in\n    :pep:`561#stub-only-packages`.\n    \"\"\"\n    top, *children = value.split(\".\")\n    if not top.endswith(\"-stubs\"):\n        return False\n    return python_module_name(\".\".join([top[: -len(\"-stubs\")], *children]))\n\n\n# -------------------------------------------------------------------------------------\n# Non-PEP related\n\n\ndef url(value: str) -> bool:\n    \"\"\"Valid URL (validation uses :obj:`urllib.parse`).\n    For maximum compatibility please make sure to include a ``scheme`` prefix\n    in your URL (e.g. ``http://``).\n    \"\"\"\n    from urllib.parse import urlparse\n\n    try:\n        parts = urlparse(value)\n        if not parts.scheme:\n            _logger.warning(\n                \"For maximum compatibility please make sure to include a \"\n                \"`scheme` prefix in your URL (e.g. 'http://'). \"\n                f\"Given value: {value}\"\n            )\n            if not (value.startswith(\"/\") or value.startswith(\"\\\\\") or \"@\" in value):\n                parts = urlparse(f\"http://{value}\")\n\n        return bool(parts.scheme and parts.netloc)\n    except Exception:\n        return False\n\n\n# https://packaging.python.org/specifications/entry-points/\nENTRYPOINT_PATTERN = r\"[^\\[\\s=]([^=]*[^\\s=])?\"\nENTRYPOINT_REGEX = re.compile(f\"^{ENTRYPOINT_PATTERN}$\", re.I)\nRECOMMEDED_ENTRYPOINT_PATTERN = r\"[\\w.-]+\"\nRECOMMEDED_ENTRYPOINT_REGEX = re.compile(f\"^{RECOMMEDED_ENTRYPOINT_PATTERN}$\", re.I)\nENTRYPOINT_GROUP_PATTERN = r\"\\w+(\\.\\w+)*\"\nENTRYPOINT_GROUP_REGEX = re.compile(f\"^{ENTRYPOINT_GROUP_PATTERN}$\", re.I)\n\n\ndef python_identifier(value: str) -> bool:\n    \"\"\"Can be used as identifier in Python.\n    (Validation uses :obj:`str.isidentifier`).\n    \"\"\"\n    return value.isidentifier()\n\n\ndef python_qualified_identifier(value: str) -> bool:\n    \"\"\"\n    Python \"dotted identifier\", i.e. a sequence of :obj:`python_identifier`\n    concatenated with ``\".\"`` (e.g.: ``package.module.submodule``).\n    \"\"\"\n    if value.startswith(\".\") or value.endswith(\".\"):\n        return False\n    return all(python_identifier(m) for m in value.split(\".\"))\n\n\ndef python_module_name(value: str) -> bool:\n    \"\"\"Module name that can be used in an ``import``-statement in Python.\n    See :obj:`python_qualified_identifier`.\n    \"\"\"\n    return python_qualified_identifier(value)\n\n\ndef python_module_name_relaxed(value: str) -> bool:\n    \"\"\"Similar to :obj:`python_module_name`, but relaxed to also accept\n    dash characters (``-``) and cover special cases like ``pip-run``.\n\n    It is recommended, however, that beginners avoid dash characters,\n    as they require advanced knowledge about Python internals.\n\n    The following are disallowed:\n\n    * names starting/ending in dashes,\n    * names ending in ``-stubs`` (potentially collide with :obj:`pep561_stub_name`).\n    \"\"\"\n    if value.startswith(\"-\") or value.endswith(\"-\"):\n        return False\n    if value.endswith(\"-stubs\"):\n        return False  # Avoid collision with PEP 561\n    return python_module_name(value.replace(\"-\", \"_\"))\n\n\ndef python_entrypoint_group(value: str) -> bool:\n    \"\"\"See ``Data model > group`` in the :ref:`PyPA's entry-points specification\n    <pypa:entry-points>`.\n    \"\"\"\n    return ENTRYPOINT_GROUP_REGEX.match(value) is not None\n\n\ndef python_entrypoint_name(value: str) -> bool:\n    \"\"\"See ``Data model > name`` in the :ref:`PyPA's entry-points specification\n    <pypa:entry-points>`.\n    \"\"\"\n    if not ENTRYPOINT_REGEX.match(value):\n        return False\n    if not RECOMMEDED_ENTRYPOINT_REGEX.match(value):\n        msg = f\"Entry point `{value}` does not follow recommended pattern: \"\n        msg += RECOMMEDED_ENTRYPOINT_PATTERN\n        _logger.warning(msg)\n    return True\n\n\ndef python_entrypoint_reference(value: str) -> bool:\n    \"\"\"Reference to a Python object using in the format::\n\n        importable.module:object.attr\n\n    See ``Data model >object reference`` in the :ref:`PyPA's entry-points specification\n    <pypa:entry-points>`.\n    \"\"\"\n    module, _, rest = value.partition(\":\")\n    if \"[\" in rest:\n        obj, _, extras_ = rest.partition(\"[\")\n        if extras_.strip()[-1] != \"]\":\n            return False\n        extras = (x.strip() for x in extras_.strip(string.whitespace + \"[]\").split(\",\"))\n        if not all(pep508_identifier(e) for e in extras):\n            return False\n        _logger.warning(f\"`{value}` - using extras for entry points is not recommended\")\n    else:\n        obj = rest\n\n    module_parts = module.split(\".\")\n    identifiers = _chain(module_parts, obj.split(\".\")) if rest else module_parts\n    return all(python_identifier(i.strip()) for i in identifiers)\n\n\ndef uint8(value: builtins.int) -> bool:\n    r\"\"\"Unsigned 8-bit integer (:math:`0 \\leq x < 2^8`)\"\"\"\n    return 0 <= value < 2**8\n\n\ndef uint16(value: builtins.int) -> bool:\n    r\"\"\"Unsigned 16-bit integer (:math:`0 \\leq x < 2^{16}`)\"\"\"\n    return 0 <= value < 2**16\n\n\ndef uint(value: builtins.int) -> bool:\n    r\"\"\"Unsigned 64-bit integer (:math:`0 \\leq x < 2^{64}`)\"\"\"\n    return 0 <= value < 2**64\n\n\ndef int(value: builtins.int) -> bool:\n    r\"\"\"Signed 64-bit integer (:math:`-2^{63} \\leq x < 2^{63}`)\"\"\"\n    return -(2**63) <= value < 2**63\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/config/distutils.schema.json","size":972,"sha1":"154011756ebe3580d0379caaeb8fa4bb12bf1114","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"{\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n\n  \"$id\": \"https://setuptools.pypa.io/en/latest/deprecated/distutils/configfile.html\",\n  \"title\": \"``tool.distutils`` table\",\n  \"$$description\": [\n    \"**EXPERIMENTAL** (NOT OFFICIALLY SUPPORTED): Use ``tool.distutils``\",\n    \"subtables to configure arguments for ``distutils`` commands.\",\n    \"Originally, ``distutils`` allowed developers to configure arguments for\",\n    \"``setup.py`` commands via `distutils configuration files\",\n    \"<https://setuptools.pypa.io/en/latest/deprecated/distutils/configfile.html>`_.\",\n    \"See also `the old Python docs <https://docs.python.org/3.11/install/>_`.\"\n  ],\n\n  \"type\": \"object\",\n  \"properties\": {\n    \"global\": {\n      \"type\": \"object\",\n      \"description\": \"Global options applied to all ``distutils`` commands\"\n    }\n  },\n  \"patternProperties\": {\n    \".+\": {\"type\": \"object\"}\n  },\n  \"$comment\": \"TODO: Is there a practical way of making this schema more specific?\"\n}\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/config/expand.py","size":16041,"sha1":"8551312d7bb9fff25702a05594569fe177d05301","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"Utility functions to expand configuration directives or special values\n(such glob patterns).\n\nWe can split the process of interpreting configuration files into 2 steps:\n\n1. The parsing the file contents from strings to value objects\n   that can be understand by Python (for example a string with a comma\n   separated list of keywords into an actual Python list of strings).\n\n2. The expansion (or post-processing) of these values according to the\n   semantics ``setuptools`` assign to them (for example a configuration field\n   with the ``file:`` directive should be expanded from a list of file paths to\n   a single string with the contents of those files concatenated)\n\nThis module focus on the second step, and therefore allow sharing the expansion\nfunctions among several configuration file formats.\n\n**PRIVATE MODULE**: API reserved for setuptools internal usage only.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport ast\nimport importlib\nimport os\nimport pathlib\nimport sys\nfrom collections.abc import Iterable, Iterator, Mapping\nfrom configparser import ConfigParser\nfrom glob import iglob\nfrom importlib.machinery import ModuleSpec, all_suffixes\nfrom itertools import chain\nfrom pathlib import Path\nfrom types import ModuleType, TracebackType\nfrom typing import TYPE_CHECKING, Any, Callable, TypeVar\n\nfrom .. import _static\nfrom .._path import StrPath, same_path as _same_path\nfrom ..discovery import find_package_path\nfrom ..warnings import SetuptoolsWarning\n\nfrom distutils.errors import DistutilsOptionError\n\nif TYPE_CHECKING:\n    from typing_extensions import Self\n\n    from setuptools.dist import Distribution\n\n_K = TypeVar(\"_K\")\n_V_co = TypeVar(\"_V_co\", covariant=True)\n\n\nclass StaticModule:\n    \"\"\"Proxy to a module object that avoids executing arbitrary code.\"\"\"\n\n    def __init__(self, name: str, spec: ModuleSpec) -> None:\n        module = ast.parse(pathlib.Path(spec.origin).read_bytes())  # type: ignore[arg-type] # Let it raise an error on None\n        vars(self).update(locals())\n        del self.self\n\n    def _find_assignments(self) -> Iterator[tuple[ast.AST, ast.AST]]:\n        for statement in self.module.body:\n            if isinstance(statement, ast.Assign):\n                yield from ((target, statement.value) for target in statement.targets)\n            elif isinstance(statement, ast.AnnAssign) and statement.value:\n                yield (statement.target, statement.value)\n\n    def __getattr__(self, attr: str):\n        \"\"\"Attempt to load an attribute \"statically\", via :func:`ast.literal_eval`.\"\"\"\n        try:\n            return next(\n                ast.literal_eval(value)\n                for target, value in self._find_assignments()\n                if isinstance(target, ast.Name) and target.id == attr\n            )\n        except Exception as e:\n            raise AttributeError(f\"{self.name} has no attribute {attr}\") from e\n\n\ndef glob_relative(\n    patterns: Iterable[str], root_dir: StrPath | None = None\n) -> list[str]:\n    \"\"\"Expand the list of glob patterns, but preserving relative paths.\n\n    :param list[str] patterns: List of glob patterns\n    :param str root_dir: Path to which globs should be relative\n                         (current directory by default)\n    :rtype: list\n    \"\"\"\n    glob_characters = {'*', '?', '[', ']', '{', '}'}\n    expanded_values = []\n    root_dir = root_dir or os.getcwd()\n    for value in patterns:\n        # Has globby characters?\n        if any(char in value for char in glob_characters):\n            # then expand the glob pattern while keeping paths *relative*:\n            glob_path = os.path.abspath(os.path.join(root_dir, value))\n            expanded_values.extend(\n                sorted(\n                    os.path.relpath(path, root_dir).replace(os.sep, \"/\")\n                    for path in iglob(glob_path, recursive=True)\n                )\n            )\n\n        else:\n            # take the value as-is\n            path = os.path.relpath(value, root_dir).replace(os.sep, \"/\")\n            expanded_values.append(path)\n\n    return expanded_values\n\n\ndef read_files(\n    filepaths: StrPath | Iterable[StrPath], root_dir: StrPath | None = None\n) -> str:\n    \"\"\"Return the content of the files concatenated using ``\\n`` as str\n\n    This function is sandboxed and won't reach anything outside ``root_dir``\n\n    (By default ``root_dir`` is the current directory).\n    \"\"\"\n    from more_itertools import always_iterable\n\n    root_dir = os.path.abspath(root_dir or os.getcwd())\n    _filepaths = (os.path.join(root_dir, path) for path in always_iterable(filepaths))\n    return '\\n'.join(\n        _read_file(path)\n        for path in _filter_existing_files(_filepaths)\n        if _assert_local(path, root_dir)\n    )\n\n\ndef _filter_existing_files(filepaths: Iterable[StrPath]) -> Iterator[StrPath]:\n    for path in filepaths:\n        if os.path.isfile(path):\n            yield path\n        else:\n            SetuptoolsWarning.emit(f\"File {path!r} cannot be found\")\n\n\ndef _read_file(filepath: bytes | StrPath) -> str:\n    with open(filepath, encoding='utf-8') as f:\n        return f.read()\n\n\ndef _assert_local(filepath: StrPath, root_dir: str):\n    if Path(os.path.abspath(root_dir)) not in Path(os.path.abspath(filepath)).parents:\n        msg = f\"Cannot access {filepath!r} (or anything outside {root_dir!r})\"\n        raise DistutilsOptionError(msg)\n\n    return True\n\n\ndef read_attr(\n    attr_desc: str,\n    package_dir: Mapping[str, str] | None = None,\n    root_dir: StrPath | None = None,\n) -> Any:\n    \"\"\"Reads the value of an attribute from a module.\n\n    This function will try to read the attributed statically first\n    (via :func:`ast.literal_eval`), and only evaluate the module if it fails.\n\n    Examples:\n        read_attr(\"package.attr\")\n        read_attr(\"package.module.attr\")\n\n    :param str attr_desc: Dot-separated string describing how to reach the\n        attribute (see examples above)\n    :param dict[str, str] package_dir: Mapping of package names to their\n        location in disk (represented by paths relative to ``root_dir``).\n    :param str root_dir: Path to directory containing all the packages in\n        ``package_dir`` (current directory by default).\n    :rtype: str\n    \"\"\"\n    root_dir = root_dir or os.getcwd()\n    attrs_path = attr_desc.strip().split('.')\n    attr_name = attrs_path.pop()\n    module_name = '.'.join(attrs_path)\n    module_name = module_name or '__init__'\n    path = _find_module(module_name, package_dir, root_dir)\n    spec = _find_spec(module_name, path)\n\n    try:\n        value = getattr(StaticModule(module_name, spec), attr_name)\n        # XXX: Is marking as static contents coming from modules too optimistic?\n        return _static.attempt_conversion(value)\n    except Exception:\n        # fallback to evaluate module\n        module = _load_spec(spec, module_name)\n        return getattr(module, attr_name)\n\n\ndef _find_spec(module_name: str, module_path: StrPath | None) -> ModuleSpec:\n    spec = importlib.util.spec_from_file_location(module_name, module_path)\n    spec = spec or importlib.util.find_spec(module_name)\n\n    if spec is None:\n        raise ModuleNotFoundError(module_name)\n\n    return spec\n\n\ndef _load_spec(spec: ModuleSpec, module_name: str) -> ModuleType:\n    name = getattr(spec, \"__name__\", module_name)\n    if name in sys.modules:\n        return sys.modules[name]\n    module = importlib.util.module_from_spec(spec)\n    sys.modules[name] = module  # cache (it also ensures `==` works on loaded items)\n    assert spec.loader is not None\n    spec.loader.exec_module(module)\n    return module\n\n\ndef _find_module(\n    module_name: str, package_dir: Mapping[str, str] | None, root_dir: StrPath\n) -> str | None:\n    \"\"\"Find the path to the module named ``module_name``,\n    considering the ``package_dir`` in the build configuration and ``root_dir``.\n\n    >>> tmp = getfixture('tmpdir')\n    >>> _ = tmp.ensure(\"a/b/c.py\")\n    >>> _ = tmp.ensure(\"a/b/d/__init__.py\")\n    >>> r = lambda x: x.replace(str(tmp), \"tmp\").replace(os.sep, \"/\")\n    >>> r(_find_module(\"a.b.c\", None, tmp))\n    'tmp/a/b/c.py'\n    >>> r(_find_module(\"f.g.h\", {\"\": \"1\", \"f\": \"2\", \"f.g\": \"3\", \"f.g.h\": \"a/b/d\"}, tmp))\n    'tmp/a/b/d/__init__.py'\n    \"\"\"\n    path_start = find_package_path(module_name, package_dir or {}, root_dir)\n    candidates = chain.from_iterable(\n        (f\"{path_start}{ext}\", os.path.join(path_start, f\"__init__{ext}\"))\n        for ext in all_suffixes()\n    )\n    return next((x for x in candidates if os.path.isfile(x)), None)\n\n\ndef resolve_class(\n    qualified_class_name: str,\n    package_dir: Mapping[str, str] | None = None,\n    root_dir: StrPath | None = None,\n) -> Callable:\n    \"\"\"Given a qualified class name, return the associated class object\"\"\"\n    root_dir = root_dir or os.getcwd()\n    idx = qualified_class_name.rfind('.')\n    class_name = qualified_class_name[idx + 1 :]\n    pkg_name = qualified_class_name[:idx]\n\n    path = _find_module(pkg_name, package_dir, root_dir)\n    module = _load_spec(_find_spec(pkg_name, path), pkg_name)\n    return getattr(module, class_name)\n\n\ndef cmdclass(\n    values: dict[str, str],\n    package_dir: Mapping[str, str] | None = None,\n    root_dir: StrPath | None = None,\n) -> dict[str, Callable]:\n    \"\"\"Given a dictionary mapping command names to strings for qualified class\n    names, apply :func:`resolve_class` to the dict values.\n    \"\"\"\n    return {k: resolve_class(v, package_dir, root_dir) for k, v in values.items()}\n\n\ndef find_packages(\n    *,\n    namespaces=True,\n    fill_package_dir: dict[str, str] | None = None,\n    root_dir: StrPath | None = None,\n    **kwargs,\n) -> list[str]:\n    \"\"\"Works similarly to :func:`setuptools.find_packages`, but with all\n    arguments given as keyword arguments. Moreover, ``where`` can be given\n    as a list (the results will be simply concatenated).\n\n    When the additional keyword argument ``namespaces`` is ``True``, it will\n    behave like :func:`setuptools.find_namespace_packages`` (i.e. include\n    implicit namespaces as per :pep:`420`).\n\n    The ``where`` argument will be considered relative to ``root_dir`` (or the current\n    working directory when ``root_dir`` is not given).\n\n    If the ``fill_package_dir`` argument is passed, this function will consider it as a\n    similar data structure to the ``package_dir`` configuration parameter add fill-in\n    any missing package location.\n\n    :rtype: list\n    \"\"\"\n    from more_itertools import always_iterable, unique_everseen\n\n    from setuptools.discovery import construct_package_dir\n\n    # check \"not namespaces\" first due to python/mypy#6232\n    if not namespaces:\n        from setuptools.discovery import PackageFinder\n    else:\n        from setuptools.discovery import PEP420PackageFinder as PackageFinder\n\n    root_dir = root_dir or os.curdir\n    where = kwargs.pop('where', ['.'])\n    packages: list[str] = []\n    fill_package_dir = {} if fill_package_dir is None else fill_package_dir\n    search = list(unique_everseen(always_iterable(where)))\n\n    if len(search) == 1 and all(not _same_path(search[0], x) for x in (\".\", root_dir)):\n        fill_package_dir.setdefault(\"\", search[0])\n\n    for path in search:\n        package_path = _nest_path(root_dir, path)\n        pkgs = PackageFinder.find(package_path, **kwargs)\n        packages.extend(pkgs)\n        if pkgs and not (\n            fill_package_dir.get(\"\") == path or os.path.samefile(package_path, root_dir)\n        ):\n            fill_package_dir.update(construct_package_dir(pkgs, path))\n\n    return packages\n\n\ndef _nest_path(parent: StrPath, path: StrPath) -> str:\n    path = parent if path in {\".\", \"\"} else os.path.join(parent, path)\n    return os.path.normpath(path)\n\n\ndef version(value: Callable | Iterable[str | int] | str) -> str:\n    \"\"\"When getting the version directly from an attribute,\n    it should be normalised to string.\n    \"\"\"\n    _value = value() if callable(value) else value\n\n    if isinstance(_value, str):\n        return _value\n    if hasattr(_value, '__iter__'):\n        return '.'.join(map(str, _value))\n    return f'{_value}'\n\n\ndef canonic_package_data(package_data: dict) -> dict:\n    if \"*\" in package_data:\n        package_data[\"\"] = package_data.pop(\"*\")\n    return package_data\n\n\ndef canonic_data_files(\n    data_files: list | dict, root_dir: StrPath | None = None\n) -> list[tuple[str, list[str]]]:\n    \"\"\"For compatibility with ``setup.py``, ``data_files`` should be a list\n    of pairs instead of a dict.\n\n    This function also expands glob patterns.\n    \"\"\"\n    if isinstance(data_files, list):\n        return data_files\n\n    return [\n        (dest, glob_relative(patterns, root_dir))\n        for dest, patterns in data_files.items()\n    ]\n\n\ndef entry_points(\n    text: str, text_source: str = \"entry-points\"\n) -> dict[str, dict[str, str]]:\n    \"\"\"Given the contents of entry-points file,\n    process it into a 2-level dictionary (``dict[str, dict[str, str]]``).\n    The first level keys are entry-point groups, the second level keys are\n    entry-point names, and the second level values are references to objects\n    (that correspond to the entry-point value).\n    \"\"\"\n    # Using undocumented behaviour, see python/typeshed#12700\n    parser = ConfigParser(default_section=None, delimiters=(\"=\",))  # type: ignore[call-overload]\n    parser.optionxform = str  # case sensitive\n    parser.read_string(text, text_source)\n    groups = {k: dict(v.items()) for k, v in parser.items()}\n    groups.pop(parser.default_section, None)\n    return groups\n\n\nclass EnsurePackagesDiscovered:\n    \"\"\"Some expand functions require all the packages to already be discovered before\n    they run, e.g. :func:`read_attr`, :func:`resolve_class`, :func:`cmdclass`.\n\n    Therefore in some cases we will need to run autodiscovery during the evaluation of\n    the configuration. However, it is better to postpone calling package discovery as\n    much as possible, because some parameters can influence it (e.g. ``package_dir``),\n    and those might not have been processed yet.\n    \"\"\"\n\n    def __init__(self, distribution: Distribution) -> None:\n        self._dist = distribution\n        self._called = False\n\n    def __call__(self):\n        \"\"\"Trigger the automatic package discovery, if it is still necessary.\"\"\"\n        if not self._called:\n            self._called = True\n            self._dist.set_defaults(name=False)  # Skip name, we can still be parsing\n\n    def __enter__(self) -> Self:\n        return self\n\n    def __exit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_value: BaseException | None,\n        traceback: TracebackType | None,\n    ):\n        if self._called:\n            self._dist.set_defaults.analyse_name()  # Now we can set a default name\n\n    def _get_package_dir(self) -> Mapping[str, str]:\n        self()\n        pkg_dir = self._dist.package_dir\n        return {} if pkg_dir is None else pkg_dir\n\n    @property\n    def package_dir(self) -> Mapping[str, str]:\n        \"\"\"Proxy to ``package_dir`` that may trigger auto-discovery when used.\"\"\"\n        return LazyMappingProxy(self._get_package_dir)\n\n\nclass LazyMappingProxy(Mapping[_K, _V_co]):\n    \"\"\"Mapping proxy that delays resolving the target object, until really needed.\n\n    >>> def obtain_mapping():\n    ...     print(\"Running expensive function!\")\n    ...     return {\"key\": \"value\", \"other key\": \"other value\"}\n    >>> mapping = LazyMappingProxy(obtain_mapping)\n    >>> mapping[\"key\"]\n    Running expensive function!\n    'value'\n    >>> mapping[\"other key\"]\n    'other value'\n    \"\"\"\n\n    def __init__(self, obtain_mapping_value: Callable[[], Mapping[_K, _V_co]]) -> None:\n        self._obtain = obtain_mapping_value\n        self._value: Mapping[_K, _V_co] | None = None\n\n    def _target(self) -> Mapping[_K, _V_co]:\n        if self._value is None:\n            self._value = self._obtain()\n        return self._value\n\n    def __getitem__(self, key: _K) -> _V_co:\n        return self._target()[key]\n\n    def __len__(self) -> int:\n        return len(self._target())\n\n    def __iter__(self) -> Iterator[_K]:\n        return iter(self._target())\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/config/pyprojecttoml.py","size":18320,"sha1":"6694e735554819030a35b13abbb278b091729beb","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"\nLoad setuptools configuration from ``pyproject.toml`` files.\n\n**PRIVATE MODULE**: API reserved for setuptools internal usage only.\n\nTo read project metadata, consider using\n``build.util.project_wheel_metadata`` (https://pypi.org/project/build/).\nFor simple scenarios, you can also try parsing the file directly\nwith the help of ``tomllib`` or ``tomli``.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nimport os\nfrom collections.abc import Mapping\nfrom contextlib import contextmanager\nfrom functools import partial\nfrom types import TracebackType\nfrom typing import TYPE_CHECKING, Any, Callable\n\nfrom .._path import StrPath\nfrom ..errors import FileError, InvalidConfigError\nfrom ..warnings import SetuptoolsWarning\nfrom . import expand as _expand\nfrom ._apply_pyprojecttoml import _PREVIOUSLY_DEFINED, _MissingDynamic, apply as _apply\n\nif TYPE_CHECKING:\n    from typing_extensions import Self\n\n    from setuptools.dist import Distribution\n\n_logger = logging.getLogger(__name__)\n\n\ndef load_file(filepath: StrPath) -> dict:\n    from ..compat.py310 import tomllib\n\n    with open(filepath, \"rb\") as file:\n        return tomllib.load(file)\n\n\ndef validate(config: dict, filepath: StrPath) -> bool:\n    from . import _validate_pyproject as validator\n\n    trove_classifier = validator.FORMAT_FUNCTIONS.get(\"trove-classifier\")\n    if hasattr(trove_classifier, \"_disable_download\"):\n        # Improve reproducibility by default. See abravalheri/validate-pyproject#31\n        trove_classifier._disable_download()  # type: ignore[union-attr]\n\n    try:\n        return validator.validate(config)\n    except validator.ValidationError as ex:\n        summary = f\"configuration error: {ex.summary}\"\n        if ex.name.strip(\"`\") != \"project\":\n            # Probably it is just a field missing/misnamed, not worthy the verbosity...\n            _logger.debug(summary)\n            _logger.debug(ex.details)\n\n        error = f\"invalid pyproject.toml config: {ex.name}.\"\n        raise ValueError(f\"{error}\\n{summary}\") from None\n\n\ndef apply_configuration(\n    dist: Distribution,\n    filepath: StrPath,\n    ignore_option_errors: bool = False,\n) -> Distribution:\n    \"\"\"Apply the configuration from a ``pyproject.toml`` file into an existing\n    distribution object.\n    \"\"\"\n    config = read_configuration(filepath, True, ignore_option_errors, dist)\n    return _apply(dist, config, filepath)\n\n\ndef read_configuration(\n    filepath: StrPath,\n    expand: bool = True,\n    ignore_option_errors: bool = False,\n    dist: Distribution | None = None,\n) -> dict[str, Any]:\n    \"\"\"Read given configuration file and returns options from it as a dict.\n\n    :param str|unicode filepath: Path to configuration file in the ``pyproject.toml``\n        format.\n\n    :param bool expand: Whether to expand directives and other computed values\n        (i.e. post-process the given configuration)\n\n    :param bool ignore_option_errors: Whether to silently ignore\n        options, values of which could not be resolved (e.g. due to exceptions\n        in directives such as file:, attr:, etc.).\n        If False exceptions are propagated as expected.\n\n    :param Distribution|None: Distribution object to which the configuration refers.\n        If not given a dummy object will be created and discarded after the\n        configuration is read. This is used for auto-discovery of packages and in the\n        case a dynamic configuration (e.g. ``attr`` or ``cmdclass``) is expanded.\n        When ``expand=False`` this object is simply ignored.\n\n    :rtype: dict\n    \"\"\"\n    filepath = os.path.abspath(filepath)\n\n    if not os.path.isfile(filepath):\n        raise FileError(f\"Configuration file {filepath!r} does not exist.\")\n\n    asdict = load_file(filepath) or {}\n    project_table = asdict.get(\"project\", {})\n    tool_table = asdict.get(\"tool\", {})\n    setuptools_table = tool_table.get(\"setuptools\", {})\n    if not asdict or not (project_table or setuptools_table):\n        return {}  # User is not using pyproject to configure setuptools\n\n    if \"setuptools\" in asdict.get(\"tools\", {}):\n        # let the user know they probably have a typo in their metadata\n        _ToolsTypoInMetadata.emit()\n\n    if \"distutils\" in tool_table:\n        _ExperimentalConfiguration.emit(subject=\"[tool.distutils]\")\n\n    # There is an overall sense in the community that making include_package_data=True\n    # the default would be an improvement.\n    # `ini2toml` backfills include_package_data=False when nothing is explicitly given,\n    # therefore setting a default here is backwards compatible.\n    if dist and dist.include_package_data is not None:\n        setuptools_table.setdefault(\"include-package-data\", dist.include_package_data)\n    else:\n        setuptools_table.setdefault(\"include-package-data\", True)\n    # Persist changes:\n    asdict[\"tool\"] = tool_table\n    tool_table[\"setuptools\"] = setuptools_table\n\n    if \"ext-modules\" in setuptools_table:\n        _ExperimentalConfiguration.emit(subject=\"[tool.setuptools.ext-modules]\")\n\n    with _ignore_errors(ignore_option_errors):\n        # Don't complain about unrelated errors (e.g. tools not using the \"tool\" table)\n        subset = {\"project\": project_table, \"tool\": {\"setuptools\": setuptools_table}}\n        validate(subset, filepath)\n\n    if expand:\n        root_dir = os.path.dirname(filepath)\n        return expand_configuration(asdict, root_dir, ignore_option_errors, dist)\n\n    return asdict\n\n\ndef expand_configuration(\n    config: dict,\n    root_dir: StrPath | None = None,\n    ignore_option_errors: bool = False,\n    dist: Distribution | None = None,\n) -> dict:\n    \"\"\"Given a configuration with unresolved fields (e.g. dynamic, cmdclass, ...)\n    find their final values.\n\n    :param dict config: Dict containing the configuration for the distribution\n    :param str root_dir: Top-level directory for the distribution/project\n        (the same directory where ``pyproject.toml`` is place)\n    :param bool ignore_option_errors: see :func:`read_configuration`\n    :param Distribution|None: Distribution object to which the configuration refers.\n        If not given a dummy object will be created and discarded after the\n        configuration is read. Used in the case a dynamic configuration\n        (e.g. ``attr`` or ``cmdclass``).\n\n    :rtype: dict\n    \"\"\"\n    return _ConfigExpander(config, root_dir, ignore_option_errors, dist).expand()\n\n\nclass _ConfigExpander:\n    def __init__(\n        self,\n        config: dict,\n        root_dir: StrPath | None = None,\n        ignore_option_errors: bool = False,\n        dist: Distribution | None = None,\n    ) -> None:\n        self.config = config\n        self.root_dir = root_dir or os.getcwd()\n        self.project_cfg = config.get(\"project\", {})\n        self.dynamic = self.project_cfg.get(\"dynamic\", [])\n        self.setuptools_cfg = config.get(\"tool\", {}).get(\"setuptools\", {})\n        self.dynamic_cfg = self.setuptools_cfg.get(\"dynamic\", {})\n        self.ignore_option_errors = ignore_option_errors\n        self._dist = dist\n        self._referenced_files = set[str]()\n\n    def _ensure_dist(self) -> Distribution:\n        from setuptools.dist import Distribution\n\n        attrs = {\"src_root\": self.root_dir, \"name\": self.project_cfg.get(\"name\", None)}\n        return self._dist or Distribution(attrs)\n\n    def _process_field(self, container: dict, field: str, fn: Callable):\n        if field in container:\n            with _ignore_errors(self.ignore_option_errors):\n                container[field] = fn(container[field])\n\n    def _canonic_package_data(self, field=\"package-data\"):\n        package_data = self.setuptools_cfg.get(field, {})\n        return _expand.canonic_package_data(package_data)\n\n    def expand(self):\n        self._expand_packages()\n        self._canonic_package_data()\n        self._canonic_package_data(\"exclude-package-data\")\n\n        # A distribution object is required for discovering the correct package_dir\n        dist = self._ensure_dist()\n        ctx = _EnsurePackagesDiscovered(dist, self.project_cfg, self.setuptools_cfg)\n        with ctx as ensure_discovered:\n            package_dir = ensure_discovered.package_dir\n            self._expand_data_files()\n            self._expand_cmdclass(package_dir)\n            self._expand_all_dynamic(dist, package_dir)\n\n        dist._referenced_files.update(self._referenced_files)\n        return self.config\n\n    def _expand_packages(self):\n        packages = self.setuptools_cfg.get(\"packages\")\n        if packages is None or isinstance(packages, (list, tuple)):\n            return\n\n        find = packages.get(\"find\")\n        if isinstance(find, dict):\n            find[\"root_dir\"] = self.root_dir\n            find[\"fill_package_dir\"] = self.setuptools_cfg.setdefault(\"package-dir\", {})\n            with _ignore_errors(self.ignore_option_errors):\n                self.setuptools_cfg[\"packages\"] = _expand.find_packages(**find)\n\n    def _expand_data_files(self):\n        data_files = partial(_expand.canonic_data_files, root_dir=self.root_dir)\n        self._process_field(self.setuptools_cfg, \"data-files\", data_files)\n\n    def _expand_cmdclass(self, package_dir: Mapping[str, str]):\n        root_dir = self.root_dir\n        cmdclass = partial(_expand.cmdclass, package_dir=package_dir, root_dir=root_dir)\n        self._process_field(self.setuptools_cfg, \"cmdclass\", cmdclass)\n\n    def _expand_all_dynamic(self, dist: Distribution, package_dir: Mapping[str, str]):\n        special = (  # need special handling\n            \"version\",\n            \"readme\",\n            \"entry-points\",\n            \"scripts\",\n            \"gui-scripts\",\n            \"classifiers\",\n            \"dependencies\",\n            \"optional-dependencies\",\n        )\n        # `_obtain` functions are assumed to raise appropriate exceptions/warnings.\n        obtained_dynamic = {\n            field: self._obtain(dist, field, package_dir)\n            for field in self.dynamic\n            if field not in special\n        }\n        obtained_dynamic.update(\n            self._obtain_entry_points(dist, package_dir) or {},\n            version=self._obtain_version(dist, package_dir),\n            readme=self._obtain_readme(dist),\n            classifiers=self._obtain_classifiers(dist),\n            dependencies=self._obtain_dependencies(dist),\n            optional_dependencies=self._obtain_optional_dependencies(dist),\n        )\n        # `None` indicates there is nothing in `tool.setuptools.dynamic` but the value\n        # might have already been set by setup.py/extensions, so avoid overwriting.\n        updates = {k: v for k, v in obtained_dynamic.items() if v is not None}\n        self.project_cfg.update(updates)\n\n    def _ensure_previously_set(self, dist: Distribution, field: str):\n        previous = _PREVIOUSLY_DEFINED[field](dist)\n        if previous is None and not self.ignore_option_errors:\n            msg = (\n                f\"No configuration found for dynamic {field!r}.\\n\"\n                \"Some dynamic fields need to be specified via `tool.setuptools.dynamic`\"\n                \"\\nothers must be specified via the equivalent attribute in `setup.py`.\"\n            )\n            raise InvalidConfigError(msg)\n\n    def _expand_directive(\n        self, specifier: str, directive, package_dir: Mapping[str, str]\n    ):\n        from more_itertools import always_iterable\n\n        with _ignore_errors(self.ignore_option_errors):\n            root_dir = self.root_dir\n            if \"file\" in directive:\n                self._referenced_files.update(always_iterable(directive[\"file\"]))\n                return _expand.read_files(directive[\"file\"], root_dir)\n            if \"attr\" in directive:\n                return _expand.read_attr(directive[\"attr\"], package_dir, root_dir)\n            raise ValueError(f\"invalid `{specifier}`: {directive!r}\")\n        return None\n\n    def _obtain(self, dist: Distribution, field: str, package_dir: Mapping[str, str]):\n        if field in self.dynamic_cfg:\n            return self._expand_directive(\n                f\"tool.setuptools.dynamic.{field}\",\n                self.dynamic_cfg[field],\n                package_dir,\n            )\n        self._ensure_previously_set(dist, field)\n        return None\n\n    def _obtain_version(self, dist: Distribution, package_dir: Mapping[str, str]):\n        # Since plugins can set version, let's silently skip if it cannot be obtained\n        if \"version\" in self.dynamic and \"version\" in self.dynamic_cfg:\n            return _expand.version(\n                # We already do an early check for the presence of \"version\"\n                self._obtain(dist, \"version\", package_dir)  # pyright: ignore[reportArgumentType]\n            )\n        return None\n\n    def _obtain_readme(self, dist: Distribution) -> dict[str, str] | None:\n        if \"readme\" not in self.dynamic:\n            return None\n\n        dynamic_cfg = self.dynamic_cfg\n        if \"readme\" in dynamic_cfg:\n            return {\n                # We already do an early check for the presence of \"readme\"\n                \"text\": self._obtain(dist, \"readme\", {}),\n                \"content-type\": dynamic_cfg[\"readme\"].get(\"content-type\", \"text/x-rst\"),\n            }  # pyright: ignore[reportReturnType]\n\n        self._ensure_previously_set(dist, \"readme\")\n        return None\n\n    def _obtain_entry_points(\n        self, dist: Distribution, package_dir: Mapping[str, str]\n    ) -> dict[str, dict[str, Any]] | None:\n        fields = (\"entry-points\", \"scripts\", \"gui-scripts\")\n        if not any(field in self.dynamic for field in fields):\n            return None\n\n        text = self._obtain(dist, \"entry-points\", package_dir)\n        if text is None:\n            return None\n\n        groups = _expand.entry_points(text)\n        # Any is str | dict[str, str], but causes variance issues\n        expanded: dict[str, dict[str, Any]] = {\"entry-points\": groups}\n\n        def _set_scripts(field: str, group: str):\n            if group in groups:\n                value = groups.pop(group)\n                if field not in self.dynamic:\n                    raise InvalidConfigError(_MissingDynamic.details(field, value))\n                expanded[field] = value\n\n        _set_scripts(\"scripts\", \"console_scripts\")\n        _set_scripts(\"gui-scripts\", \"gui_scripts\")\n\n        return expanded\n\n    def _obtain_classifiers(self, dist: Distribution):\n        if \"classifiers\" in self.dynamic:\n            value = self._obtain(dist, \"classifiers\", {})\n            if value:\n                return value.splitlines()\n        return None\n\n    def _obtain_dependencies(self, dist: Distribution):\n        if \"dependencies\" in self.dynamic:\n            value = self._obtain(dist, \"dependencies\", {})\n            if value:\n                return _parse_requirements_list(value)\n        return None\n\n    def _obtain_optional_dependencies(self, dist: Distribution):\n        if \"optional-dependencies\" not in self.dynamic:\n            return None\n        if \"optional-dependencies\" in self.dynamic_cfg:\n            optional_dependencies_map = self.dynamic_cfg[\"optional-dependencies\"]\n            assert isinstance(optional_dependencies_map, dict)\n            return {\n                group: _parse_requirements_list(\n                    self._expand_directive(\n                        f\"tool.setuptools.dynamic.optional-dependencies.{group}\",\n                        directive,\n                        {},\n                    )\n                )\n                for group, directive in optional_dependencies_map.items()\n            }\n        self._ensure_previously_set(dist, \"optional-dependencies\")\n        return None\n\n\ndef _parse_requirements_list(value):\n    return [\n        line\n        for line in value.splitlines()\n        if line.strip() and not line.strip().startswith(\"#\")\n    ]\n\n\n@contextmanager\ndef _ignore_errors(ignore_option_errors: bool):\n    if not ignore_option_errors:\n        yield\n        return\n\n    try:\n        yield\n    except Exception as ex:\n        _logger.debug(f\"ignored error: {ex.__class__.__name__} - {ex}\")\n\n\nclass _EnsurePackagesDiscovered(_expand.EnsurePackagesDiscovered):\n    def __init__(\n        self, distribution: Distribution, project_cfg: dict, setuptools_cfg: dict\n    ) -> None:\n        super().__init__(distribution)\n        self._project_cfg = project_cfg\n        self._setuptools_cfg = setuptools_cfg\n\n    def __enter__(self) -> Self:\n        \"\"\"When entering the context, the values of ``packages``, ``py_modules`` and\n        ``package_dir`` that are missing in ``dist`` are copied from ``setuptools_cfg``.\n        \"\"\"\n        dist, cfg = self._dist, self._setuptools_cfg\n        package_dir: dict[str, str] = cfg.setdefault(\"package-dir\", {})\n        package_dir.update(dist.package_dir or {})\n        dist.package_dir = package_dir  # needs to be the same object\n\n        dist.set_defaults._ignore_ext_modules()  # pyproject.toml-specific behaviour\n\n        # Set `name`, `py_modules` and `packages` in dist to short-circuit\n        # auto-discovery, but avoid overwriting empty lists purposefully set by users.\n        if dist.metadata.name is None:\n            dist.metadata.name = self._project_cfg.get(\"name\")\n        if dist.py_modules is None:\n            dist.py_modules = cfg.get(\"py-modules\")\n        if dist.packages is None:\n            dist.packages = cfg.get(\"packages\")\n\n        return super().__enter__()\n\n    def __exit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_value: BaseException | None,\n        traceback: TracebackType | None,\n    ) -> None:\n        \"\"\"When exiting the context, if values of ``packages``, ``py_modules`` and\n        ``package_dir`` are missing in ``setuptools_cfg``, copy from ``dist``.\n        \"\"\"\n        # If anything was discovered set them back, so they count in the final config.\n        self._setuptools_cfg.setdefault(\"packages\", self._dist.packages)\n        self._setuptools_cfg.setdefault(\"py-modules\", self._dist.py_modules)\n        return super().__exit__(exc_type, exc_value, traceback)\n\n\nclass _ExperimentalConfiguration(SetuptoolsWarning):\n    _SUMMARY = (\n        \"`{subject}` in `pyproject.toml` is still *experimental* \"\n        \"and likely to change in future releases.\"\n    )\n\n\nclass _ToolsTypoInMetadata(SetuptoolsWarning):\n    _SUMMARY = (\n        \"Ignoring [tools.setuptools] in pyproject.toml, did you mean [tool.setuptools]?\"\n    )\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/config/setupcfg.py","size":26575,"sha1":"ae80e02633ffc7485551d8d72de78577420d3dd9","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"\nLoad setuptools configuration from ``setup.cfg`` files.\n\n**API will be made private in the future**\n\nTo read project metadata, consider using\n``build.util.project_wheel_metadata`` (https://pypi.org/project/build/).\nFor simple scenarios, you can also try parsing the file directly\nwith the help of ``configparser``.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport contextlib\nimport functools\nimport os\nfrom collections import defaultdict\nfrom collections.abc import Iterable, Iterator\nfrom functools import partial, wraps\nfrom typing import TYPE_CHECKING, Any, Callable, ClassVar, Generic, TypeVar, cast\n\nfrom packaging.markers import default_environment as marker_env\nfrom packaging.requirements import InvalidRequirement, Requirement\nfrom packaging.version import InvalidVersion, Version\n\nfrom .. import _static\nfrom .._path import StrPath\nfrom ..errors import FileError, OptionError\nfrom ..warnings import SetuptoolsDeprecationWarning\nfrom . import expand\n\nif TYPE_CHECKING:\n    from typing_extensions import TypeAlias\n\n    from setuptools.dist import Distribution\n\n    from distutils.dist import DistributionMetadata\n\nSingleCommandOptions: TypeAlias = dict[str, tuple[str, Any]]\n\"\"\"Dict that associate the name of the options of a particular command to a\ntuple. The first element of the tuple indicates the origin of the option value\n(e.g. the name of the configuration file where it was read from),\nwhile the second element of the tuple is the option value itself\n\"\"\"\nAllCommandOptions: TypeAlias = dict[str, SingleCommandOptions]\n\"\"\"cmd name => its options\"\"\"\nTarget = TypeVar(\"Target\", \"Distribution\", \"DistributionMetadata\")\n\n\ndef read_configuration(\n    filepath: StrPath, find_others: bool = False, ignore_option_errors: bool = False\n) -> dict:\n    \"\"\"Read given configuration file and returns options from it as a dict.\n\n    :param str|unicode filepath: Path to configuration file\n        to get options from.\n\n    :param bool find_others: Whether to search for other configuration files\n        which could be on in various places.\n\n    :param bool ignore_option_errors: Whether to silently ignore\n        options, values of which could not be resolved (e.g. due to exceptions\n        in directives such as file:, attr:, etc.).\n        If False exceptions are propagated as expected.\n\n    :rtype: dict\n    \"\"\"\n    from setuptools.dist import Distribution\n\n    dist = Distribution()\n    filenames = dist.find_config_files() if find_others else []\n    handlers = _apply(dist, filepath, filenames, ignore_option_errors)\n    return configuration_to_dict(handlers)\n\n\ndef apply_configuration(dist: Distribution, filepath: StrPath) -> Distribution:\n    \"\"\"Apply the configuration from a ``setup.cfg`` file into an existing\n    distribution object.\n    \"\"\"\n    _apply(dist, filepath)\n    dist._finalize_requires()\n    return dist\n\n\ndef _apply(\n    dist: Distribution,\n    filepath: StrPath,\n    other_files: Iterable[StrPath] = (),\n    ignore_option_errors: bool = False,\n) -> tuple[ConfigMetadataHandler, ConfigOptionsHandler]:\n    \"\"\"Read configuration from ``filepath`` and applies to the ``dist`` object.\"\"\"\n    from setuptools.dist import _Distribution\n\n    filepath = os.path.abspath(filepath)\n\n    if not os.path.isfile(filepath):\n        raise FileError(f'Configuration file {filepath} does not exist.')\n\n    current_directory = os.getcwd()\n    os.chdir(os.path.dirname(filepath))\n    filenames = [*other_files, filepath]\n\n    try:\n        # TODO: Temporary cast until mypy 1.12 is released with upstream fixes from typeshed\n        _Distribution.parse_config_files(dist, filenames=cast(list[str], filenames))\n        handlers = parse_configuration(\n            dist, dist.command_options, ignore_option_errors=ignore_option_errors\n        )\n        dist._finalize_license_files()\n    finally:\n        os.chdir(current_directory)\n\n    return handlers\n\n\ndef _get_option(target_obj: Distribution | DistributionMetadata, key: str):\n    \"\"\"\n    Given a target object and option key, get that option from\n    the target object, either through a get_{key} method or\n    from an attribute directly.\n    \"\"\"\n    getter_name = f'get_{key}'\n    by_attribute = functools.partial(getattr, target_obj, key)\n    getter = getattr(target_obj, getter_name, by_attribute)\n    return getter()\n\n\ndef configuration_to_dict(\n    handlers: Iterable[\n        ConfigHandler[Distribution] | ConfigHandler[DistributionMetadata]\n    ],\n) -> dict:\n    \"\"\"Returns configuration data gathered by given handlers as a dict.\n\n    :param Iterable[ConfigHandler] handlers: Handlers list,\n        usually from parse_configuration()\n\n    :rtype: dict\n    \"\"\"\n    config_dict: dict = defaultdict(dict)\n\n    for handler in handlers:\n        for option in handler.set_options:\n            value = _get_option(handler.target_obj, option)\n            config_dict[handler.section_prefix][option] = value\n\n    return config_dict\n\n\ndef parse_configuration(\n    distribution: Distribution,\n    command_options: AllCommandOptions,\n    ignore_option_errors: bool = False,\n) -> tuple[ConfigMetadataHandler, ConfigOptionsHandler]:\n    \"\"\"Performs additional parsing of configuration options\n    for a distribution.\n\n    Returns a list of used option handlers.\n\n    :param Distribution distribution:\n    :param dict command_options:\n    :param bool ignore_option_errors: Whether to silently ignore\n        options, values of which could not be resolved (e.g. due to exceptions\n        in directives such as file:, attr:, etc.).\n        If False exceptions are propagated as expected.\n    :rtype: list\n    \"\"\"\n    with expand.EnsurePackagesDiscovered(distribution) as ensure_discovered:\n        options = ConfigOptionsHandler(\n            distribution,\n            command_options,\n            ignore_option_errors,\n            ensure_discovered,\n        )\n\n        options.parse()\n        if not distribution.package_dir:\n            distribution.package_dir = options.package_dir  # Filled by `find_packages`\n\n        meta = ConfigMetadataHandler(\n            distribution.metadata,\n            command_options,\n            ignore_option_errors,\n            ensure_discovered,\n            distribution.package_dir,\n            distribution.src_root,\n        )\n        meta.parse()\n        distribution._referenced_files.update(\n            options._referenced_files, meta._referenced_files\n        )\n\n    return meta, options\n\n\ndef _warn_accidental_env_marker_misconfig(label: str, orig_value: str, parsed: list):\n    \"\"\"Because users sometimes misinterpret this configuration:\n\n    [options.extras_require]\n    foo = bar;python_version<\"4\"\n\n    It looks like one requirement with an environment marker\n    but because there is no newline, it's parsed as two requirements\n    with a semicolon as separator.\n\n    Therefore, if:\n        * input string does not contain a newline AND\n        * parsed result contains two requirements AND\n        * parsing of the two parts from the result (\"<first>;<second>\")\n        leads in a valid Requirement with a valid marker\n    a UserWarning is shown to inform the user about the possible problem.\n    \"\"\"\n    if \"\\n\" in orig_value or len(parsed) != 2:\n        return\n\n    markers = marker_env().keys()\n\n    try:\n        req = Requirement(parsed[1])\n        if req.name in markers:\n            _AmbiguousMarker.emit(field=label, req=parsed[1])\n    except InvalidRequirement as ex:\n        if any(parsed[1].startswith(marker) for marker in markers):\n            msg = _AmbiguousMarker.message(field=label, req=parsed[1])\n            raise InvalidRequirement(msg) from ex\n\n\nclass ConfigHandler(Generic[Target]):\n    \"\"\"Handles metadata supplied in configuration files.\"\"\"\n\n    section_prefix: str\n    \"\"\"Prefix for config sections handled by this handler.\n    Must be provided by class heirs.\n\n    \"\"\"\n\n    aliases: ClassVar[dict[str, str]] = {}\n    \"\"\"Options aliases.\n    For compatibility with various packages. E.g.: d2to1 and pbr.\n    Note: `-` in keys is replaced with `_` by config parser.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        target_obj: Target,\n        options: AllCommandOptions,\n        ignore_option_errors,\n        ensure_discovered: expand.EnsurePackagesDiscovered,\n    ) -> None:\n        self.ignore_option_errors = ignore_option_errors\n        self.target_obj: Target = target_obj\n        self.sections = dict(self._section_options(options))\n        self.set_options: list[str] = []\n        self.ensure_discovered = ensure_discovered\n        self._referenced_files = set[str]()\n        \"\"\"After parsing configurations, this property will enumerate\n        all files referenced by the \"file:\" directive. Private API for setuptools only.\n        \"\"\"\n\n    @classmethod\n    def _section_options(\n        cls, options: AllCommandOptions\n    ) -> Iterator[tuple[str, SingleCommandOptions]]:\n        for full_name, value in options.items():\n            pre, _sep, name = full_name.partition(cls.section_prefix)\n            if pre:\n                continue\n            yield name.lstrip('.'), value\n\n    @property\n    def parsers(self):\n        \"\"\"Metadata item name to parser function mapping.\"\"\"\n        raise NotImplementedError(\n            f'{self.__class__.__name__} must provide .parsers property'\n        )\n\n    def __setitem__(self, option_name, value) -> None:\n        target_obj = self.target_obj\n\n        # Translate alias into real name.\n        option_name = self.aliases.get(option_name, option_name)\n\n        try:\n            current_value = getattr(target_obj, option_name)\n        except AttributeError as e:\n            raise KeyError(option_name) from e\n\n        if current_value:\n            # Already inhabited. Skipping.\n            return\n\n        try:\n            parsed = self.parsers.get(option_name, lambda x: x)(value)\n        except (Exception,) * self.ignore_option_errors:\n            return\n\n        simple_setter = functools.partial(target_obj.__setattr__, option_name)\n        setter = getattr(target_obj, f\"set_{option_name}\", simple_setter)\n        setter(parsed)\n\n        self.set_options.append(option_name)\n\n    @classmethod\n    def _parse_list(cls, value, separator=','):\n        \"\"\"Represents value as a list.\n\n        Value is split either by separator (defaults to comma) or by lines.\n\n        :param value:\n        :param separator: List items separator character.\n        :rtype: list\n        \"\"\"\n        if isinstance(value, list):  # _get_parser_compound case\n            return value\n\n        if '\\n' in value:\n            value = value.splitlines()\n        else:\n            value = value.split(separator)\n\n        return [chunk.strip() for chunk in value if chunk.strip()]\n\n    @classmethod\n    def _parse_dict(cls, value):\n        \"\"\"Represents value as a dict.\n\n        :param value:\n        :rtype: dict\n        \"\"\"\n        separator = '='\n        result = {}\n        for line in cls._parse_list(value):\n            key, sep, val = line.partition(separator)\n            if sep != separator:\n                raise OptionError(f\"Unable to parse option value to dict: {value}\")\n            result[key.strip()] = val.strip()\n\n        return result\n\n    @classmethod\n    def _parse_bool(cls, value):\n        \"\"\"Represents value as boolean.\n\n        :param value:\n        :rtype: bool\n        \"\"\"\n        value = value.lower()\n        return value in ('1', 'true', 'yes')\n\n    @classmethod\n    def _exclude_files_parser(cls, key):\n        \"\"\"Returns a parser function to make sure field inputs\n        are not files.\n\n        Parses a value after getting the key so error messages are\n        more informative.\n\n        :param key:\n        :rtype: callable\n        \"\"\"\n\n        def parser(value):\n            exclude_directive = 'file:'\n            if value.startswith(exclude_directive):\n                raise ValueError(\n                    f'Only strings are accepted for the {key} field, '\n                    'files are not accepted'\n                )\n            return _static.Str(value)\n\n        return parser\n\n    def _parse_file(self, value, root_dir: StrPath | None):\n        \"\"\"Represents value as a string, allowing including text\n        from nearest files using `file:` directive.\n\n        Directive is sandboxed and won't reach anything outside\n        directory with setup.py.\n\n        Examples:\n            file: README.rst, CHANGELOG.md, src/file.txt\n\n        :param str value:\n        :rtype: str\n        \"\"\"\n        include_directive = 'file:'\n\n        if not isinstance(value, str):\n            return value\n\n        if not value.startswith(include_directive):\n            return _static.Str(value)\n\n        spec = value[len(include_directive) :]\n        filepaths = [path.strip() for path in spec.split(',')]\n        self._referenced_files.update(filepaths)\n        # XXX: Is marking as static contents coming from files too optimistic?\n        return _static.Str(expand.read_files(filepaths, root_dir))\n\n    def _parse_attr(self, value, package_dir, root_dir: StrPath):\n        \"\"\"Represents value as a module attribute.\n\n        Examples:\n            attr: package.attr\n            attr: package.module.attr\n\n        :param str value:\n        :rtype: str\n        \"\"\"\n        attr_directive = 'attr:'\n        if not value.startswith(attr_directive):\n            return _static.Str(value)\n\n        attr_desc = value.replace(attr_directive, '')\n\n        # Make sure package_dir is populated correctly, so `attr:` directives can work\n        package_dir.update(self.ensure_discovered.package_dir)\n        return expand.read_attr(attr_desc, package_dir, root_dir)\n\n    @classmethod\n    def _get_parser_compound(cls, *parse_methods):\n        \"\"\"Returns parser function to represents value as a list.\n\n        Parses a value applying given methods one after another.\n\n        :param parse_methods:\n        :rtype: callable\n        \"\"\"\n\n        def parse(value):\n            parsed = value\n\n            for method in parse_methods:\n                parsed = method(parsed)\n\n            return parsed\n\n        return parse\n\n    @classmethod\n    def _parse_section_to_dict_with_key(cls, section_options, values_parser):\n        \"\"\"Parses section options into a dictionary.\n\n        Applies a given parser to each option in a section.\n\n        :param dict section_options:\n        :param callable values_parser: function with 2 args corresponding to key, value\n        :rtype: dict\n        \"\"\"\n        value = {}\n        for key, (_, val) in section_options.items():\n            value[key] = values_parser(key, val)\n        return value\n\n    @classmethod\n    def _parse_section_to_dict(cls, section_options, values_parser=None):\n        \"\"\"Parses section options into a dictionary.\n\n        Optionally applies a given parser to each value.\n\n        :param dict section_options:\n        :param callable values_parser: function with 1 arg corresponding to option value\n        :rtype: dict\n        \"\"\"\n        parser = (lambda _, v: values_parser(v)) if values_parser else (lambda _, v: v)\n        return cls._parse_section_to_dict_with_key(section_options, parser)\n\n    def parse_section(self, section_options) -> None:\n        \"\"\"Parses configuration file section.\n\n        :param dict section_options:\n        \"\"\"\n        for name, (_, value) in section_options.items():\n            with contextlib.suppress(KeyError):\n                # Keep silent for a new option may appear anytime.\n                self[name] = value\n\n    def parse(self) -> None:\n        \"\"\"Parses configuration file items from one\n        or more related sections.\n\n        \"\"\"\n        for section_name, section_options in self.sections.items():\n            method_postfix = ''\n            if section_name:  # [section.option] variant\n                method_postfix = f\"_{section_name}\"\n\n            section_parser_method: Callable | None = getattr(\n                self,\n                # Dots in section names are translated into dunderscores.\n                f'parse_section{method_postfix}'.replace('.', '__'),\n                None,\n            )\n\n            if section_parser_method is None:\n                raise OptionError(\n                    \"Unsupported distribution option section: \"\n                    f\"[{self.section_prefix}.{section_name}]\"\n                )\n\n            section_parser_method(section_options)\n\n    def _deprecated_config_handler(self, func, msg, **kw):\n        \"\"\"this function will wrap around parameters that are deprecated\n\n        :param msg: deprecation message\n        :param func: function to be wrapped around\n        \"\"\"\n\n        @wraps(func)\n        def config_handler(*args, **kwargs):\n            kw.setdefault(\"stacklevel\", 2)\n            _DeprecatedConfig.emit(\"Deprecated config in `setup.cfg`\", msg, **kw)\n            return func(*args, **kwargs)\n\n        return config_handler\n\n\nclass ConfigMetadataHandler(ConfigHandler[\"DistributionMetadata\"]):\n    section_prefix = 'metadata'\n\n    aliases = {\n        'home_page': 'url',\n        'summary': 'description',\n        'classifier': 'classifiers',\n        'platform': 'platforms',\n    }\n\n    strict_mode = False\n    \"\"\"We need to keep it loose, to be partially compatible with\n    `pbr` and `d2to1` packages which also uses `metadata` section.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        target_obj: DistributionMetadata,\n        options: AllCommandOptions,\n        ignore_option_errors: bool,\n        ensure_discovered: expand.EnsurePackagesDiscovered,\n        package_dir: dict | None = None,\n        root_dir: StrPath | None = os.curdir,\n    ) -> None:\n        super().__init__(target_obj, options, ignore_option_errors, ensure_discovered)\n        self.package_dir = package_dir\n        self.root_dir = root_dir\n\n    @property\n    def parsers(self):\n        \"\"\"Metadata item name to parser function mapping.\"\"\"\n        parse_list_static = self._get_parser_compound(self._parse_list, _static.List)\n        parse_dict_static = self._get_parser_compound(self._parse_dict, _static.Dict)\n        parse_file = partial(self._parse_file, root_dir=self.root_dir)\n        exclude_files_parser = self._exclude_files_parser\n\n        return {\n            'author': _static.Str,\n            'author_email': _static.Str,\n            'maintainer': _static.Str,\n            'maintainer_email': _static.Str,\n            'platforms': parse_list_static,\n            'keywords': parse_list_static,\n            'provides': parse_list_static,\n            'obsoletes': parse_list_static,\n            'classifiers': self._get_parser_compound(parse_file, parse_list_static),\n            'license': exclude_files_parser('license'),\n            'license_files': parse_list_static,\n            'description': parse_file,\n            'long_description': parse_file,\n            'long_description_content_type': _static.Str,\n            'version': self._parse_version,  # Cannot be marked as dynamic\n            'url': _static.Str,\n            'project_urls': parse_dict_static,\n        }\n\n    def _parse_version(self, value):\n        \"\"\"Parses `version` option value.\n\n        :param value:\n        :rtype: str\n\n        \"\"\"\n        version = self._parse_file(value, self.root_dir)\n\n        if version != value:\n            version = version.strip()\n            # Be strict about versions loaded from file because it's easy to\n            # accidentally include newlines and other unintended content\n            try:\n                Version(version)\n            except InvalidVersion as e:\n                raise OptionError(\n                    f'Version loaded from {value} does not '\n                    f'comply with PEP 440: {version}'\n                ) from e\n\n            return version\n\n        return expand.version(self._parse_attr(value, self.package_dir, self.root_dir))\n\n\nclass ConfigOptionsHandler(ConfigHandler[\"Distribution\"]):\n    section_prefix = 'options'\n\n    def __init__(\n        self,\n        target_obj: Distribution,\n        options: AllCommandOptions,\n        ignore_option_errors: bool,\n        ensure_discovered: expand.EnsurePackagesDiscovered,\n    ) -> None:\n        super().__init__(target_obj, options, ignore_option_errors, ensure_discovered)\n        self.root_dir = target_obj.src_root\n        self.package_dir: dict[str, str] = {}  # To be filled by `find_packages`\n\n    @classmethod\n    def _parse_list_semicolon(cls, value):\n        return cls._parse_list(value, separator=';')\n\n    def _parse_file_in_root(self, value):\n        return self._parse_file(value, root_dir=self.root_dir)\n\n    def _parse_requirements_list(self, label: str, value: str):\n        # Parse a requirements list, either by reading in a `file:`, or a list.\n        parsed = self._parse_list_semicolon(self._parse_file_in_root(value))\n        _warn_accidental_env_marker_misconfig(label, value, parsed)\n        # Filter it to only include lines that are not comments. `parse_list`\n        # will have stripped each line and filtered out empties.\n        return _static.List(line for line in parsed if not line.startswith(\"#\"))\n        # ^-- Use `_static.List` to mark a non-`Dynamic` Core Metadata\n\n    @property\n    def parsers(self):\n        \"\"\"Metadata item name to parser function mapping.\"\"\"\n        parse_list = self._parse_list\n        parse_bool = self._parse_bool\n        parse_cmdclass = self._parse_cmdclass\n\n        return {\n            'zip_safe': parse_bool,\n            'include_package_data': parse_bool,\n            'package_dir': self._parse_dict,\n            'scripts': parse_list,\n            'eager_resources': parse_list,\n            'dependency_links': parse_list,\n            'namespace_packages': self._deprecated_config_handler(\n                parse_list,\n                \"The namespace_packages parameter is deprecated, \"\n                \"consider using implicit namespaces instead (PEP 420).\",\n                # TODO: define due date, see setuptools.dist:check_nsp.\n            ),\n            'install_requires': partial(  # Core Metadata\n                self._parse_requirements_list, \"install_requires\"\n            ),\n            'setup_requires': self._parse_list_semicolon,\n            'packages': self._parse_packages,\n            'entry_points': self._parse_file_in_root,\n            'py_modules': parse_list,\n            'python_requires': _static.SpecifierSet,  # Core Metadata\n            'cmdclass': parse_cmdclass,\n        }\n\n    def _parse_cmdclass(self, value):\n        package_dir = self.ensure_discovered.package_dir\n        return expand.cmdclass(self._parse_dict(value), package_dir, self.root_dir)\n\n    def _parse_packages(self, value):\n        \"\"\"Parses `packages` option value.\n\n        :param value:\n        :rtype: list\n        \"\"\"\n        find_directives = ['find:', 'find_namespace:']\n        trimmed_value = value.strip()\n\n        if trimmed_value not in find_directives:\n            return self._parse_list(value)\n\n        # Read function arguments from a dedicated section.\n        find_kwargs = self.parse_section_packages__find(\n            self.sections.get('packages.find', {})\n        )\n\n        find_kwargs.update(\n            namespaces=(trimmed_value == find_directives[1]),\n            root_dir=self.root_dir,\n            fill_package_dir=self.package_dir,\n        )\n\n        return expand.find_packages(**find_kwargs)\n\n    def parse_section_packages__find(self, section_options):\n        \"\"\"Parses `packages.find` configuration file section.\n\n        To be used in conjunction with _parse_packages().\n\n        :param dict section_options:\n        \"\"\"\n        section_data = self._parse_section_to_dict(section_options, self._parse_list)\n\n        valid_keys = ['where', 'include', 'exclude']\n        find_kwargs = {k: v for k, v in section_data.items() if k in valid_keys and v}\n\n        where = find_kwargs.get('where')\n        if where is not None:\n            find_kwargs['where'] = where[0]  # cast list to single val\n\n        return find_kwargs\n\n    def parse_section_entry_points(self, section_options) -> None:\n        \"\"\"Parses `entry_points` configuration file section.\n\n        :param dict section_options:\n        \"\"\"\n        parsed = self._parse_section_to_dict(section_options, self._parse_list)\n        self['entry_points'] = parsed\n\n    def _parse_package_data(self, section_options):\n        package_data = self._parse_section_to_dict(section_options, self._parse_list)\n        return expand.canonic_package_data(package_data)\n\n    def parse_section_package_data(self, section_options) -> None:\n        \"\"\"Parses `package_data` configuration file section.\n\n        :param dict section_options:\n        \"\"\"\n        self['package_data'] = self._parse_package_data(section_options)\n\n    def parse_section_exclude_package_data(self, section_options) -> None:\n        \"\"\"Parses `exclude_package_data` configuration file section.\n\n        :param dict section_options:\n        \"\"\"\n        self['exclude_package_data'] = self._parse_package_data(section_options)\n\n    def parse_section_extras_require(self, section_options) -> None:  # Core Metadata\n        \"\"\"Parses `extras_require` configuration file section.\n\n        :param dict section_options:\n        \"\"\"\n        parsed = self._parse_section_to_dict_with_key(\n            section_options,\n            lambda k, v: self._parse_requirements_list(f\"extras_require[{k}]\", v),\n        )\n\n        self['extras_require'] = _static.Dict(parsed)\n        # ^-- Use `_static.Dict` to mark a non-`Dynamic` Core Metadata\n\n    def parse_section_data_files(self, section_options) -> None:\n        \"\"\"Parses `data_files` configuration file section.\n\n        :param dict section_options:\n        \"\"\"\n        parsed = self._parse_section_to_dict(section_options, self._parse_list)\n        self['data_files'] = expand.canonic_data_files(parsed, self.root_dir)\n\n\nclass _AmbiguousMarker(SetuptoolsDeprecationWarning):\n    _SUMMARY = \"Ambiguous requirement marker.\"\n    _DETAILS = \"\"\"\n    One of the parsed requirements in `{field}` looks like a valid environment marker:\n\n        {req!r}\n\n    Please make sure that the configuration file is correct.\n    You can use dangling lines to avoid this problem.\n    \"\"\"\n    _SEE_DOCS = \"userguide/declarative_config.html#opt-2\"\n    # TODO: should we include due_date here? Initially introduced in 6 Aug 2022.\n    # Does this make sense with latest version of packaging?\n\n    @classmethod\n    def message(cls, **kw):\n        docs = f\"https://setuptools.pypa.io/en/latest/{cls._SEE_DOCS}\"\n        return cls._format(cls._SUMMARY, cls._DETAILS, see_url=docs, format_args=kw)\n\n\nclass _DeprecatedConfig(SetuptoolsDeprecationWarning):\n    _SEE_DOCS = \"userguide/declarative_config.html\"\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/config/setuptools.schema.json","size":16071,"sha1":"f844a0f109101db75159a38021e54dc940877c6b","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"{\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n\n  \"$id\": \"https://setuptools.pypa.io/en/latest/userguide/pyproject_config.html\",\n  \"title\": \"``tool.setuptools`` table\",\n  \"$$description\": [\n    \"``setuptools``-specific configurations that can be set by users that require\",\n    \"customization.\",\n    \"These configurations are completely optional and probably can be skipped when\",\n    \"creating simple packages. They are equivalent to some of the `Keywords\",\n    \"<https://setuptools.pypa.io/en/latest/references/keywords.html>`_\",\n    \"used by the ``setup.py`` file, and can be set via the ``tool.setuptools`` table.\",\n    \"It considers only ``setuptools`` `parameters\",\n    \"<https://setuptools.pypa.io/en/latest/userguide/pyproject_config.html#setuptools-specific-configuration>`_\",\n    \"that are not covered by :pep:`621`; and intentionally excludes ``dependency_links``\",\n    \"and ``setup_requires`` (incompatible with modern workflows/standards).\"\n  ],\n\n  \"type\": \"object\",\n  \"additionalProperties\": false,\n  \"properties\": {\n    \"platforms\": {\n      \"type\": \"array\",\n      \"items\": {\"type\": \"string\"}\n    },\n    \"provides\": {\n      \"$$description\": [\n        \"Package and virtual package names contained within this package\",\n        \"**(not supported by pip)**\"\n      ],\n      \"type\": \"array\",\n      \"items\": {\"type\": \"string\", \"format\": \"pep508-identifier\"}\n    },\n    \"obsoletes\": {\n      \"$$description\": [\n        \"Packages which this package renders obsolete\",\n        \"**(not supported by pip)**\"\n      ],\n      \"type\": \"array\",\n      \"items\": {\"type\": \"string\", \"format\": \"pep508-identifier\"}\n    },\n    \"zip-safe\": {\n      \"$$description\": [\n        \"Whether the project can be safely installed and run from a zip file.\",\n        \"**OBSOLETE**: only relevant for ``pkg_resources``, ``easy_install`` and\",\n        \"``setup.py install`` in the context of ``eggs`` (**DEPRECATED**).\"\n      ],\n      \"type\": \"boolean\"\n    },\n    \"script-files\": {\n      \"$$description\": [\n        \"Legacy way of defining scripts (entry-points are preferred).\",\n        \"Equivalent to the ``script`` keyword in ``setup.py``\",\n        \"(it was renamed to avoid confusion with entry-point based ``project.scripts``\",\n        \"defined in :pep:`621`).\",\n        \"**DISCOURAGED**: generic script wrappers are tricky and may not work properly.\",\n        \"Whenever possible, please use ``project.scripts`` instead.\"\n      ],\n      \"type\": \"array\",\n      \"items\": {\"type\": \"string\"},\n      \"$comment\": \"TODO: is this field deprecated/should be removed?\"\n    },\n    \"eager-resources\": {\n      \"$$description\": [\n        \"Resources that should be extracted together, if any of them is needed,\",\n        \"or if any C extensions included in the project are imported.\",\n        \"**OBSOLETE**: only relevant for ``pkg_resources``, ``easy_install`` and\",\n        \"``setup.py install`` in the context of ``eggs`` (**DEPRECATED**).\"\n      ],\n      \"type\": \"array\",\n      \"items\": {\"type\": \"string\"}\n    },\n    \"packages\": {\n      \"$$description\": [\n        \"Packages that should be included in the distribution.\",\n        \"It can be given either as a list of package identifiers\",\n        \"or as a ``dict``-like structure with a single key ``find``\",\n        \"which corresponds to a dynamic call to\",\n        \"``setuptools.config.expand.find_packages`` function.\",\n        \"The ``find`` key is associated with a nested ``dict``-like structure that can\",\n        \"contain ``where``, ``include``, ``exclude`` and ``namespaces`` keys,\",\n        \"mimicking the keyword arguments of the associated function.\"\n      ],\n      \"oneOf\": [\n        {\n          \"title\": \"Array of Python package identifiers\",\n          \"type\": \"array\",\n          \"items\": {\"$ref\": \"#/definitions/package-name\"}\n        },\n        {\"$ref\": \"#/definitions/find-directive\"}\n      ]\n    },\n    \"package-dir\": {\n      \"$$description\": [\n        \":class:`dict`-like structure mapping from package names to directories where their\",\n        \"code can be found.\",\n        \"The empty string (as key) means that all packages are contained inside\",\n        \"the given directory will be included in the distribution.\"\n      ],\n      \"type\": \"object\",\n      \"additionalProperties\": false,\n      \"propertyNames\": {\n        \"anyOf\": [{\"const\": \"\"}, {\"$ref\": \"#/definitions/package-name\"}]\n      },\n      \"patternProperties\": {\n        \"^.*$\": {\"type\": \"string\" }\n      }\n    },\n    \"package-data\": {\n      \"$$description\": [\n        \"Mapping from package names to lists of glob patterns.\",\n        \"Usually this option is not needed when using ``include-package-data = true``\",\n        \"For more information on how to include data files, check ``setuptools`` `docs\",\n        \"<https://setuptools.pypa.io/en/latest/userguide/datafiles.html>`_.\"\n      ],\n      \"type\": \"object\",\n      \"additionalProperties\": false,\n      \"propertyNames\": {\n        \"anyOf\": [{\"type\": \"string\", \"format\": \"python-module-name\"}, {\"const\": \"*\"}]\n      },\n      \"patternProperties\": {\n        \"^.*$\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n      }\n    },\n    \"include-package-data\": {\n      \"$$description\": [\n        \"Automatically include any data files inside the package directories\",\n        \"that are specified by ``MANIFEST.in``\",\n        \"For more information on how to include data files, check ``setuptools`` `docs\",\n        \"<https://setuptools.pypa.io/en/latest/userguide/datafiles.html>`_.\"\n      ],\n      \"type\": \"boolean\"\n    },\n    \"exclude-package-data\": {\n      \"$$description\": [\n        \"Mapping from package names to lists of glob patterns that should be excluded\",\n        \"For more information on how to include data files, check ``setuptools`` `docs\",\n        \"<https://setuptools.pypa.io/en/latest/userguide/datafiles.html>`_.\"\n      ],\n      \"type\": \"object\",\n      \"additionalProperties\": false,\n      \"propertyNames\": {\n        \"anyOf\": [{\"type\": \"string\", \"format\": \"python-module-name\"}, {\"const\": \"*\"}]\n      },\n      \"patternProperties\": {\n          \"^.*$\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n      }\n    },\n    \"namespace-packages\": {\n      \"type\": \"array\",\n      \"items\": {\"type\": \"string\", \"format\": \"python-module-name-relaxed\"},\n      \"$comment\": \"https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\",\n      \"description\": \"**DEPRECATED**: use implicit namespaces instead (:pep:`420`).\"\n    },\n    \"py-modules\": {\n      \"description\": \"Modules that setuptools will manipulate\",\n      \"type\": \"array\",\n      \"items\": {\"type\": \"string\", \"format\": \"python-module-name-relaxed\"},\n      \"$comment\": \"TODO: clarify the relationship with ``packages``\"\n    },\n    \"ext-modules\": {\n      \"description\": \"Extension modules to be compiled by setuptools\",\n      \"type\": \"array\",\n      \"items\": {\"$ref\": \"#/definitions/ext-module\"}\n    },\n    \"data-files\": {\n      \"$$description\": [\n        \"``dict``-like structure where each key represents a directory and\",\n        \"the value is a list of glob patterns that should be installed in them.\",\n        \"**DISCOURAGED**: please notice this might not work as expected with wheels.\",\n        \"Whenever possible, consider using data files inside the package directories\",\n        \"(or create a new namespace package that only contains data files).\",\n        \"See `data files support\",\n        \"<https://setuptools.pypa.io/en/latest/userguide/datafiles.html>`_.\"\n      ],\n      \"type\": \"object\",\n      \"patternProperties\": {\n          \"^.*$\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n      }\n    },\n    \"cmdclass\": {\n      \"$$description\": [\n        \"Mapping of distutils-style command names to ``setuptools.Command`` subclasses\",\n        \"which in turn should be represented by strings with a qualified class name\",\n        \"(i.e., \\\"dotted\\\" form with module), e.g.::\\n\\n\",\n        \"    cmdclass = {mycmd = \\\"pkg.subpkg.module.CommandClass\\\"}\\n\\n\",\n        \"The command class should be a directly defined at the top-level of the\",\n        \"containing module (no class nesting).\"\n      ],\n      \"type\": \"object\",\n      \"patternProperties\": {\n          \"^.*$\": {\"type\": \"string\", \"format\": \"python-qualified-identifier\"}\n      }\n    },\n    \"license-files\": {\n      \"type\": \"array\",\n      \"items\": {\"type\": \"string\"},\n      \"$$description\": [\n        \"**PROVISIONAL**: list of glob patterns for all license files being distributed.\",\n        \"(likely to become standard with :pep:`639`).\",\n        \"By default: ``['LICEN[CS]E*', 'COPYING*', 'NOTICE*', 'AUTHORS*']``\"\n      ],\n      \"$comment\": \"TODO: revise if PEP 639 is accepted. Probably ``project.license-files``?\"\n    },\n    \"dynamic\": {\n      \"type\": \"object\",\n      \"description\": \"Instructions for loading :pep:`621`-related metadata dynamically\",\n      \"additionalProperties\": false,\n      \"properties\": {\n        \"version\": {\n          \"$$description\": [\n            \"A version dynamically loaded via either the ``attr:`` or ``file:``\",\n            \"directives. Please make sure the given file or attribute respects :pep:`440`.\",\n            \"Also ensure to set ``project.dynamic`` accordingly.\"\n          ],\n          \"oneOf\": [\n            {\"$ref\": \"#/definitions/attr-directive\"},\n            {\"$ref\": \"#/definitions/file-directive\"}\n          ]\n        },\n        \"classifiers\": {\"$ref\": \"#/definitions/file-directive\"},\n        \"description\": {\"$ref\": \"#/definitions/file-directive\"},\n        \"entry-points\": {\"$ref\": \"#/definitions/file-directive\"},\n        \"dependencies\": {\"$ref\": \"#/definitions/file-directive-for-dependencies\"},\n        \"optional-dependencies\": {\n          \"type\": \"object\",\n          \"propertyNames\": {\"type\": \"string\", \"format\": \"pep508-identifier\"},\n          \"additionalProperties\": false,\n          \"patternProperties\": {\n            \".+\": {\"$ref\": \"#/definitions/file-directive-for-dependencies\"}\n          }\n        },\n        \"readme\": {\n          \"type\": \"object\",\n          \"anyOf\": [\n            {\"$ref\": \"#/definitions/file-directive\"},\n            {\n              \"type\": \"object\",\n              \"properties\": {\n                \"content-type\": {\"type\": \"string\"},\n                \"file\": { \"$ref\": \"#/definitions/file-directive/properties/file\" }\n              },\n              \"additionalProperties\": false}\n          ],\n          \"required\": [\"file\"]\n        }\n      }\n    }\n  },\n\n  \"definitions\": {\n    \"package-name\": {\n      \"$id\": \"#/definitions/package-name\",\n      \"title\": \"Valid package name\",\n      \"description\": \"Valid package name (importable or :pep:`561`).\",\n      \"type\": \"string\",\n      \"anyOf\": [\n        {\"type\": \"string\", \"format\": \"python-module-name-relaxed\"},\n        {\"type\": \"string\", \"format\": \"pep561-stub-name\"}\n      ]\n    },\n    \"ext-module\": {\n      \"$id\": \"#/definitions/ext-module\",\n      \"title\": \"Extension module\",\n      \"description\": \"Parameters to construct a :class:`setuptools.Extension` object\",\n      \"type\": \"object\",\n      \"required\": [\"name\", \"sources\"],\n      \"additionalProperties\": false,\n      \"properties\": {\n        \"name\": {\n          \"type\": \"string\",\n          \"format\": \"python-module-name-relaxed\"\n        },\n        \"sources\": {\n          \"type\": \"array\",\n          \"items\": {\"type\": \"string\"}\n        },\n        \"include-dirs\":{\n          \"type\": \"array\",\n          \"items\": {\"type\": \"string\"}\n        },\n        \"define-macros\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"type\": \"array\",\n            \"items\": [\n              {\"description\": \"macro name\", \"type\": \"string\"},\n              {\"description\": \"macro value\", \"oneOf\": [{\"type\": \"string\"}, {\"type\": \"null\"}]}\n            ],\n            \"additionalItems\": false\n          }\n        },\n        \"undef-macros\": {\n          \"type\": \"array\",\n          \"items\": {\"type\": \"string\"}\n        },\n        \"library-dirs\": {\n          \"type\": \"array\",\n          \"items\": {\"type\": \"string\"}\n        },\n        \"libraries\": {\n          \"type\": \"array\",\n          \"items\": {\"type\": \"string\"}\n        },\n        \"runtime-library-dirs\": {\n          \"type\": \"array\",\n          \"items\": {\"type\": \"string\"}\n        },\n        \"extra-objects\": {\n          \"type\": \"array\",\n          \"items\": {\"type\": \"string\"}\n        },\n        \"extra-compile-args\": {\n          \"type\": \"array\",\n          \"items\": {\"type\": \"string\"}\n        },\n        \"extra-link-args\": {\n          \"type\": \"array\",\n          \"items\": {\"type\": \"string\"}\n        },\n        \"export-symbols\": {\n          \"type\": \"array\",\n          \"items\": {\"type\": \"string\"}\n        },\n        \"swig-opts\": {\n          \"type\": \"array\",\n          \"items\": {\"type\": \"string\"}\n        },\n        \"depends\": {\n          \"type\": \"array\",\n          \"items\": {\"type\": \"string\"}\n        },\n        \"language\": {\"type\": \"string\"},\n        \"optional\": {\"type\": \"boolean\"},\n        \"py-limited-api\": {\"type\": \"boolean\"}\n      }\n    },\n    \"file-directive\": {\n      \"$id\": \"#/definitions/file-directive\",\n      \"title\": \"'file:' directive\",\n      \"description\":\n        \"Value is read from a file (or list of files and then concatenated)\",\n      \"type\": \"object\",\n      \"additionalProperties\": false,\n      \"properties\": {\n        \"file\": {\n          \"oneOf\": [\n            {\"type\": \"string\"},\n            {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n          ]\n        }\n      },\n      \"required\": [\"file\"]\n    },\n    \"file-directive-for-dependencies\": {\n      \"title\": \"'file:' directive for dependencies\",\n      \"allOf\": [\n        {\n          \"$$description\": [\n            \"**BETA**: subset of the ``requirements.txt`` format\",\n            \"without ``pip`` flags and options\",\n            \"(one :pep:`508`-compliant string per line,\",\n            \"lines that are blank or start with ``#`` are excluded).\",\n            \"See `dynamic metadata\",\n            \"<https://setuptools.pypa.io/en/latest/userguide/pyproject_config.html#dynamic-metadata>`_.\"\n          ]\n        },\n        {\"$ref\": \"#/definitions/file-directive\"}\n      ]\n    },\n    \"attr-directive\": {\n      \"title\": \"'attr:' directive\",\n      \"$id\": \"#/definitions/attr-directive\",\n      \"$$description\": [\n        \"Value is read from a module attribute. Supports callables and iterables;\",\n        \"unsupported types are cast via ``str()``\"\n      ],\n      \"type\": \"object\",\n      \"additionalProperties\": false,\n      \"properties\": {\n        \"attr\": {\"type\": \"string\", \"format\": \"python-qualified-identifier\"}\n      },\n      \"required\": [\"attr\"]\n    },\n    \"find-directive\": {\n      \"$id\": \"#/definitions/find-directive\",\n      \"title\": \"'find:' directive\",\n      \"type\": \"object\",\n      \"additionalProperties\": false,\n      \"properties\": {\n        \"find\": {\n          \"type\": \"object\",\n          \"$$description\": [\n            \"Dynamic `package discovery\",\n            \"<https://setuptools.pypa.io/en/latest/userguide/package_discovery.html>`_.\"\n          ],\n          \"additionalProperties\": false,\n          \"properties\": {\n            \"where\": {\n              \"description\":\n                \"Directories to be searched for packages (Unix-style relative path)\",\n              \"type\": \"array\",\n              \"items\": {\"type\": \"string\"}\n            },\n            \"exclude\": {\n              \"type\": \"array\",\n              \"$$description\": [\n                \"Exclude packages that match the values listed in this field.\",\n                \"Can container shell-style wildcards (e.g. ``'pkg.*'``)\"\n              ],\n              \"items\": {\"type\": \"string\"}\n            },\n            \"include\": {\n              \"type\": \"array\",\n              \"$$description\": [\n                \"Restrict the found packages to just the ones listed in this field.\",\n                \"Can container shell-style wildcards (e.g. ``'pkg.*'``)\"\n              ],\n              \"items\": {\"type\": \"string\"}\n            },\n            \"namespaces\": {\n              \"type\": \"boolean\",\n              \"$$description\": [\n                \"When ``True``, directories without a ``__init__.py`` file will also\",\n                \"be scanned for :pep:`420`-style implicit namespaces\"\n              ]\n            }\n          }\n        }\n      }\n    }\n  }\n}\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/depends.py","size":5965,"sha1":"a667ea8bd53403e4340ecb0c44ce9f0ff0f044cb","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"from __future__ import annotations\n\nimport contextlib\nimport dis\nimport marshal\nimport sys\nfrom types import CodeType\nfrom typing import Any, Literal, TypeVar\n\nfrom packaging.version import Version\n\nfrom . import _imp\nfrom ._imp import PY_COMPILED, PY_FROZEN, PY_SOURCE, find_module\n\n_T = TypeVar(\"_T\")\n\n__all__ = ['Require', 'find_module']\n\n\nclass Require:\n    \"\"\"A prerequisite to building or installing a distribution\"\"\"\n\n    def __init__(\n        self,\n        name,\n        requested_version,\n        module,\n        homepage: str = '',\n        attribute=None,\n        format=None,\n    ) -> None:\n        if format is None and requested_version is not None:\n            format = Version\n\n        if format is not None:\n            requested_version = format(requested_version)\n            if attribute is None:\n                attribute = '__version__'\n\n        self.__dict__.update(locals())\n        del self.self\n\n    def full_name(self):\n        \"\"\"Return full package/distribution name, w/version\"\"\"\n        if self.requested_version is not None:\n            return f'{self.name}-{self.requested_version}'\n        return self.name\n\n    def version_ok(self, version):\n        \"\"\"Is 'version' sufficiently up-to-date?\"\"\"\n        return (\n            self.attribute is None\n            or self.format is None\n            or str(version) != \"unknown\"\n            and self.format(version) >= self.requested_version\n        )\n\n    def get_version(\n        self, paths=None, default: _T | Literal[\"unknown\"] = \"unknown\"\n    ) -> _T | Literal[\"unknown\"] | None | Any:\n        \"\"\"Get version number of installed module, 'None', or 'default'\n\n        Search 'paths' for module.  If not found, return 'None'.  If found,\n        return the extracted version attribute, or 'default' if no version\n        attribute was specified, or the value cannot be determined without\n        importing the module.  The version is formatted according to the\n        requirement's version format (if any), unless it is 'None' or the\n        supplied 'default'.\n        \"\"\"\n\n        if self.attribute is None:\n            try:\n                f, _p, _i = find_module(self.module, paths)\n            except ImportError:\n                return None\n            if f:\n                f.close()\n            return default\n\n        v = get_module_constant(self.module, self.attribute, default, paths)\n\n        if v is not None and v is not default and self.format is not None:\n            return self.format(v)\n\n        return v\n\n    def is_present(self, paths=None):\n        \"\"\"Return true if dependency is present on 'paths'\"\"\"\n        return self.get_version(paths) is not None\n\n    def is_current(self, paths=None):\n        \"\"\"Return true if dependency is present and up-to-date on 'paths'\"\"\"\n        version = self.get_version(paths)\n        if version is None:\n            return False\n        return self.version_ok(str(version))\n\n\ndef maybe_close(f):\n    @contextlib.contextmanager\n    def empty():\n        yield\n        return\n\n    if not f:\n        return empty()\n\n    return contextlib.closing(f)\n\n\n# Some objects are not available on some platforms.\n# XXX it'd be better to test assertions about bytecode instead.\nif not sys.platform.startswith('java') and sys.platform != 'cli':\n\n    def get_module_constant(\n        module, symbol, default: _T | int = -1, paths=None\n    ) -> _T | int | None | Any:\n        \"\"\"Find 'module' by searching 'paths', and extract 'symbol'\n\n        Return 'None' if 'module' does not exist on 'paths', or it does not define\n        'symbol'.  If the module defines 'symbol' as a constant, return the\n        constant.  Otherwise, return 'default'.\"\"\"\n\n        try:\n            f, path, (_suffix, _mode, kind) = info = find_module(module, paths)\n        except ImportError:\n            # Module doesn't exist\n            return None\n\n        with maybe_close(f):\n            if kind == PY_COMPILED:\n                f.read(8)  # skip magic & date\n                code = marshal.load(f)\n            elif kind == PY_FROZEN:\n                code = _imp.get_frozen_object(module, paths)\n            elif kind == PY_SOURCE:\n                code = compile(f.read(), path, 'exec')\n            else:\n                # Not something we can parse; we'll have to import it.  :(\n                imported = _imp.get_module(module, paths, info)\n                return getattr(imported, symbol, None)\n\n        return extract_constant(code, symbol, default)\n\n    def extract_constant(\n        code: CodeType, symbol: str, default: _T | int = -1\n    ) -> _T | int | None | Any:\n        \"\"\"Extract the constant value of 'symbol' from 'code'\n\n        If the name 'symbol' is bound to a constant value by the Python code\n        object 'code', return that value.  If 'symbol' is bound to an expression,\n        return 'default'.  Otherwise, return 'None'.\n\n        Return value is based on the first assignment to 'symbol'.  'symbol' must\n        be a global, or at least a non-\"fast\" local in the code block.  That is,\n        only 'STORE_NAME' and 'STORE_GLOBAL' opcodes are checked, and 'symbol'\n        must be present in 'code.co_names'.\n        \"\"\"\n        if symbol not in code.co_names:\n            # name's not there, can't possibly be an assignment\n            return None\n\n        name_idx = list(code.co_names).index(symbol)\n\n        STORE_NAME = dis.opmap['STORE_NAME']\n        STORE_GLOBAL = dis.opmap['STORE_GLOBAL']\n        LOAD_CONST = dis.opmap['LOAD_CONST']\n\n        const = default\n\n        for byte_code in dis.Bytecode(code):\n            op = byte_code.opcode\n            arg = byte_code.arg\n\n            if op == LOAD_CONST:\n                assert arg is not None\n                const = code.co_consts[arg]\n            elif arg == name_idx and (op == STORE_NAME or op == STORE_GLOBAL):\n                return const\n            else:\n                const = default\n\n        return None\n\n    __all__ += ['get_module_constant', 'extract_constant']\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/discovery.py","size":21258,"sha1":"1fef2480685631c7f5f5616f3e078adffa727a22","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"Automatic discovery of Python modules and packages (for inclusion in the\ndistribution) and other config values.\n\nFor the purposes of this module, the following nomenclature is used:\n\n- \"src-layout\": a directory representing a Python project that contains a \"src\"\n  folder. Everything under the \"src\" folder is meant to be included in the\n  distribution when packaging the project. Example::\n\n    .\n     tox.ini\n     pyproject.toml\n     src/\n         mypkg/\n             __init__.py\n             mymodule.py\n             my_data_file.txt\n\n- \"flat-layout\": a Python project that does not use \"src-layout\" but instead\n  have a directory under the project root for each package::\n\n    .\n     tox.ini\n     pyproject.toml\n     mypkg/\n         __init__.py\n         mymodule.py\n         my_data_file.txt\n\n- \"single-module\": a project that contains a single Python script direct under\n  the project root (no directory used)::\n\n    .\n     tox.ini\n     pyproject.toml\n     mymodule.py\n\n\"\"\"\n\nfrom __future__ import annotations\n\nimport itertools\nimport os\nfrom collections.abc import Iterable, Iterator, Mapping\nfrom fnmatch import fnmatchcase\nfrom glob import glob\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING, ClassVar\n\nimport _distutils_hack.override  # noqa: F401\n\nfrom ._path import StrPath\n\nfrom distutils import log\nfrom distutils.util import convert_path\n\nif TYPE_CHECKING:\n    from setuptools import Distribution\n\nchain_iter = itertools.chain.from_iterable\n\n\ndef _valid_name(path: StrPath) -> bool:\n    # Ignore invalid names that cannot be imported directly\n    return os.path.basename(path).isidentifier()\n\n\nclass _Filter:\n    \"\"\"\n    Given a list of patterns, create a callable that will be true only if\n    the input matches at least one of the patterns.\n    \"\"\"\n\n    def __init__(self, *patterns: str) -> None:\n        self._patterns = dict.fromkeys(patterns)\n\n    def __call__(self, item: str) -> bool:\n        return any(fnmatchcase(item, pat) for pat in self._patterns)\n\n    def __contains__(self, item: str) -> bool:\n        return item in self._patterns\n\n\nclass _Finder:\n    \"\"\"Base class that exposes functionality for module/package finders\"\"\"\n\n    ALWAYS_EXCLUDE: ClassVar[tuple[str, ...]] = ()\n    DEFAULT_EXCLUDE: ClassVar[tuple[str, ...]] = ()\n\n    @classmethod\n    def find(\n        cls,\n        where: StrPath = '.',\n        exclude: Iterable[str] = (),\n        include: Iterable[str] = ('*',),\n    ) -> list[str]:\n        \"\"\"Return a list of all Python items (packages or modules, depending on\n        the finder implementation) found within directory 'where'.\n\n        'where' is the root directory which will be searched.\n        It should be supplied as a \"cross-platform\" (i.e. URL-style) path;\n        it will be converted to the appropriate local path syntax.\n\n        'exclude' is a sequence of names to exclude; '*' can be used\n        as a wildcard in the names.\n        When finding packages, 'foo.*' will exclude all subpackages of 'foo'\n        (but not 'foo' itself).\n\n        'include' is a sequence of names to include.\n        If it's specified, only the named items will be included.\n        If it's not specified, all found items will be included.\n        'include' can contain shell style wildcard patterns just like\n        'exclude'.\n        \"\"\"\n\n        exclude = exclude or cls.DEFAULT_EXCLUDE\n        return list(\n            cls._find_iter(\n                convert_path(str(where)),\n                _Filter(*cls.ALWAYS_EXCLUDE, *exclude),\n                _Filter(*include),\n            )\n        )\n\n    @classmethod\n    def _find_iter(\n        cls, where: StrPath, exclude: _Filter, include: _Filter\n    ) -> Iterator[str]:\n        raise NotImplementedError\n\n\nclass PackageFinder(_Finder):\n    \"\"\"\n    Generate a list of all Python packages found within a directory\n    \"\"\"\n\n    ALWAYS_EXCLUDE = (\"ez_setup\", \"*__pycache__\")\n\n    @classmethod\n    def _find_iter(\n        cls, where: StrPath, exclude: _Filter, include: _Filter\n    ) -> Iterator[str]:\n        \"\"\"\n        All the packages found in 'where' that pass the 'include' filter, but\n        not the 'exclude' filter.\n        \"\"\"\n        for root, dirs, files in os.walk(str(where), followlinks=True):\n            # Copy dirs to iterate over it, then empty dirs.\n            all_dirs = dirs[:]\n            dirs[:] = []\n\n            for dir in all_dirs:\n                full_path = os.path.join(root, dir)\n                rel_path = os.path.relpath(full_path, where)\n                package = rel_path.replace(os.path.sep, '.')\n\n                # Skip directory trees that are not valid packages\n                if '.' in dir or not cls._looks_like_package(full_path, package):\n                    continue\n\n                # Should this package be included?\n                if include(package) and not exclude(package):\n                    yield package\n\n                # Early pruning if there is nothing else to be scanned\n                if f\"{package}*\" in exclude or f\"{package}.*\" in exclude:\n                    continue\n\n                # Keep searching subdirectories, as there may be more packages\n                # down there, even if the parent was excluded.\n                dirs.append(dir)\n\n    @staticmethod\n    def _looks_like_package(path: StrPath, _package_name: str) -> bool:\n        \"\"\"Does a directory look like a package?\"\"\"\n        return os.path.isfile(os.path.join(path, '__init__.py'))\n\n\nclass PEP420PackageFinder(PackageFinder):\n    @staticmethod\n    def _looks_like_package(_path: StrPath, _package_name: str) -> bool:\n        return True\n\n\nclass ModuleFinder(_Finder):\n    \"\"\"Find isolated Python modules.\n    This function will **not** recurse subdirectories.\n    \"\"\"\n\n    @classmethod\n    def _find_iter(\n        cls, where: StrPath, exclude: _Filter, include: _Filter\n    ) -> Iterator[str]:\n        for file in glob(os.path.join(where, \"*.py\")):\n            module, _ext = os.path.splitext(os.path.basename(file))\n\n            if not cls._looks_like_module(module):\n                continue\n\n            if include(module) and not exclude(module):\n                yield module\n\n    _looks_like_module = staticmethod(_valid_name)\n\n\n# We have to be extra careful in the case of flat layout to not include files\n# and directories not meant for distribution (e.g. tool-related)\n\n\nclass FlatLayoutPackageFinder(PEP420PackageFinder):\n    _EXCLUDE = (\n        \"ci\",\n        \"bin\",\n        \"debian\",\n        \"doc\",\n        \"docs\",\n        \"documentation\",\n        \"manpages\",\n        \"news\",\n        \"newsfragments\",\n        \"changelog\",\n        \"test\",\n        \"tests\",\n        \"unit_test\",\n        \"unit_tests\",\n        \"example\",\n        \"examples\",\n        \"scripts\",\n        \"tools\",\n        \"util\",\n        \"utils\",\n        \"python\",\n        \"build\",\n        \"dist\",\n        \"venv\",\n        \"env\",\n        \"requirements\",\n        # ---- Task runners / Build tools ----\n        \"tasks\",  # invoke\n        \"fabfile\",  # fabric\n        \"site_scons\",  # SCons\n        # ---- Other tools ----\n        \"benchmark\",\n        \"benchmarks\",\n        \"exercise\",\n        \"exercises\",\n        \"htmlcov\",  # Coverage.py\n        # ---- Hidden directories/Private packages ----\n        \"[._]*\",\n    )\n\n    DEFAULT_EXCLUDE = tuple(chain_iter((p, f\"{p}.*\") for p in _EXCLUDE))\n    \"\"\"Reserved package names\"\"\"\n\n    @staticmethod\n    def _looks_like_package(_path: StrPath, package_name: str) -> bool:\n        names = package_name.split('.')\n        # Consider PEP 561\n        root_pkg_is_valid = names[0].isidentifier() or names[0].endswith(\"-stubs\")\n        return root_pkg_is_valid and all(name.isidentifier() for name in names[1:])\n\n\nclass FlatLayoutModuleFinder(ModuleFinder):\n    DEFAULT_EXCLUDE = (\n        \"setup\",\n        \"conftest\",\n        \"test\",\n        \"tests\",\n        \"example\",\n        \"examples\",\n        \"build\",\n        # ---- Task runners ----\n        \"toxfile\",\n        \"noxfile\",\n        \"pavement\",\n        \"dodo\",\n        \"tasks\",\n        \"fabfile\",\n        # ---- Other tools ----\n        \"[Ss][Cc]onstruct\",  # SCons\n        \"conanfile\",  # Connan: C/C++ build tool\n        \"manage\",  # Django\n        \"benchmark\",\n        \"benchmarks\",\n        \"exercise\",\n        \"exercises\",\n        # ---- Hidden files/Private modules ----\n        \"[._]*\",\n    )\n    \"\"\"Reserved top-level module names\"\"\"\n\n\ndef _find_packages_within(root_pkg: str, pkg_dir: StrPath) -> list[str]:\n    nested = PEP420PackageFinder.find(pkg_dir)\n    return [root_pkg] + [\".\".join((root_pkg, n)) for n in nested]\n\n\nclass ConfigDiscovery:\n    \"\"\"Fill-in metadata and options that can be automatically derived\n    (from other metadata/options, the file system or conventions)\n    \"\"\"\n\n    def __init__(self, distribution: Distribution) -> None:\n        self.dist = distribution\n        self._called = False\n        self._disabled = False\n        self._skip_ext_modules = False\n\n    def _disable(self):\n        \"\"\"Internal API to disable automatic discovery\"\"\"\n        self._disabled = True\n\n    def _ignore_ext_modules(self):\n        \"\"\"Internal API to disregard ext_modules.\n\n        Normally auto-discovery would not be triggered if ``ext_modules`` are set\n        (this is done for backward compatibility with existing packages relying on\n        ``setup.py`` or ``setup.cfg``). However, ``setuptools`` can call this function\n        to ignore given ``ext_modules`` and proceed with the auto-discovery if\n        ``packages`` and ``py_modules`` are not given (e.g. when using pyproject.toml\n        metadata).\n        \"\"\"\n        self._skip_ext_modules = True\n\n    @property\n    def _root_dir(self) -> StrPath:\n        # The best is to wait until `src_root` is set in dist, before using _root_dir.\n        return self.dist.src_root or os.curdir\n\n    @property\n    def _package_dir(self) -> dict[str, str]:\n        if self.dist.package_dir is None:\n            return {}\n        return self.dist.package_dir\n\n    def __call__(\n        self, force: bool = False, name: bool = True, ignore_ext_modules: bool = False\n    ):\n        \"\"\"Automatically discover missing configuration fields\n        and modifies the given ``distribution`` object in-place.\n\n        Note that by default this will only have an effect the first time the\n        ``ConfigDiscovery`` object is called.\n\n        To repeatedly invoke automatic discovery (e.g. when the project\n        directory changes), please use ``force=True`` (or create a new\n        ``ConfigDiscovery`` instance).\n        \"\"\"\n        if force is False and (self._called or self._disabled):\n            # Avoid overhead of multiple calls\n            return\n\n        self._analyse_package_layout(ignore_ext_modules)\n        if name:\n            self.analyse_name()  # depends on ``packages`` and ``py_modules``\n\n        self._called = True\n\n    def _explicitly_specified(self, ignore_ext_modules: bool) -> bool:\n        \"\"\"``True`` if the user has specified some form of package/module listing\"\"\"\n        ignore_ext_modules = ignore_ext_modules or self._skip_ext_modules\n        ext_modules = not (self.dist.ext_modules is None or ignore_ext_modules)\n        return (\n            self.dist.packages is not None\n            or self.dist.py_modules is not None\n            or ext_modules\n            or hasattr(self.dist, \"configuration\")\n            and self.dist.configuration\n            # ^ Some projects use numpy.distutils.misc_util.Configuration\n        )\n\n    def _analyse_package_layout(self, ignore_ext_modules: bool) -> bool:\n        if self._explicitly_specified(ignore_ext_modules):\n            # For backward compatibility, just try to find modules/packages\n            # when nothing is given\n            return True\n\n        log.debug(\n            \"No `packages` or `py_modules` configuration, performing \"\n            \"automatic discovery.\"\n        )\n\n        return (\n            self._analyse_explicit_layout()\n            or self._analyse_src_layout()\n            # flat-layout is the trickiest for discovery so it should be last\n            or self._analyse_flat_layout()\n        )\n\n    def _analyse_explicit_layout(self) -> bool:\n        \"\"\"The user can explicitly give a package layout via ``package_dir``\"\"\"\n        package_dir = self._package_dir.copy()  # don't modify directly\n        package_dir.pop(\"\", None)  # This falls under the \"src-layout\" umbrella\n        root_dir = self._root_dir\n\n        if not package_dir:\n            return False\n\n        log.debug(f\"`explicit-layout` detected -- analysing {package_dir}\")\n        pkgs = chain_iter(\n            _find_packages_within(pkg, os.path.join(root_dir, parent_dir))\n            for pkg, parent_dir in package_dir.items()\n        )\n        self.dist.packages = list(pkgs)\n        log.debug(f\"discovered packages -- {self.dist.packages}\")\n        return True\n\n    def _analyse_src_layout(self) -> bool:\n        \"\"\"Try to find all packages or modules under the ``src`` directory\n        (or anything pointed by ``package_dir[\"\"]``).\n\n        The \"src-layout\" is relatively safe for automatic discovery.\n        We assume that everything within is meant to be included in the\n        distribution.\n\n        If ``package_dir[\"\"]`` is not given, but the ``src`` directory exists,\n        this function will set ``package_dir[\"\"] = \"src\"``.\n        \"\"\"\n        package_dir = self._package_dir\n        src_dir = os.path.join(self._root_dir, package_dir.get(\"\", \"src\"))\n        if not os.path.isdir(src_dir):\n            return False\n\n        log.debug(f\"`src-layout` detected -- analysing {src_dir}\")\n        package_dir.setdefault(\"\", os.path.basename(src_dir))\n        self.dist.package_dir = package_dir  # persist eventual modifications\n        self.dist.packages = PEP420PackageFinder.find(src_dir)\n        self.dist.py_modules = ModuleFinder.find(src_dir)\n        log.debug(f\"discovered packages -- {self.dist.packages}\")\n        log.debug(f\"discovered py_modules -- {self.dist.py_modules}\")\n        return True\n\n    def _analyse_flat_layout(self) -> bool:\n        \"\"\"Try to find all packages and modules under the project root.\n\n        Since the ``flat-layout`` is more dangerous in terms of accidentally including\n        extra files/directories, this function is more conservative and will raise an\n        error if multiple packages or modules are found.\n\n        This assumes that multi-package dists are uncommon and refuse to support that\n        use case in order to be able to prevent unintended errors.\n        \"\"\"\n        log.debug(f\"`flat-layout` detected -- analysing {self._root_dir}\")\n        return self._analyse_flat_packages() or self._analyse_flat_modules()\n\n    def _analyse_flat_packages(self) -> bool:\n        self.dist.packages = FlatLayoutPackageFinder.find(self._root_dir)\n        top_level = remove_nested_packages(remove_stubs(self.dist.packages))\n        log.debug(f\"discovered packages -- {self.dist.packages}\")\n        self._ensure_no_accidental_inclusion(top_level, \"packages\")\n        return bool(top_level)\n\n    def _analyse_flat_modules(self) -> bool:\n        self.dist.py_modules = FlatLayoutModuleFinder.find(self._root_dir)\n        log.debug(f\"discovered py_modules -- {self.dist.py_modules}\")\n        self._ensure_no_accidental_inclusion(self.dist.py_modules, \"modules\")\n        return bool(self.dist.py_modules)\n\n    def _ensure_no_accidental_inclusion(self, detected: list[str], kind: str):\n        if len(detected) > 1:\n            from inspect import cleandoc\n\n            from setuptools.errors import PackageDiscoveryError\n\n            msg = f\"\"\"Multiple top-level {kind} discovered in a flat-layout: {detected}.\n\n            To avoid accidental inclusion of unwanted files or directories,\n            setuptools will not proceed with this build.\n\n            If you are trying to create a single distribution with multiple {kind}\n            on purpose, you should not rely on automatic discovery.\n            Instead, consider the following options:\n\n            1. set up custom discovery (`find` directive with `include` or `exclude`)\n            2. use a `src-layout`\n            3. explicitly set `py_modules` or `packages` with a list of names\n\n            To find more information, look for \"package discovery\" on setuptools docs.\n            \"\"\"\n            raise PackageDiscoveryError(cleandoc(msg))\n\n    def analyse_name(self) -> None:\n        \"\"\"The packages/modules are the essential contribution of the author.\n        Therefore the name of the distribution can be derived from them.\n        \"\"\"\n        if self.dist.metadata.name or self.dist.name:\n            # get_name() is not reliable (can return \"UNKNOWN\")\n            return\n\n        log.debug(\"No `name` configuration, performing automatic discovery\")\n\n        name = (\n            self._find_name_single_package_or_module()\n            or self._find_name_from_packages()\n        )\n        if name:\n            self.dist.metadata.name = name\n\n    def _find_name_single_package_or_module(self) -> str | None:\n        \"\"\"Exactly one module or package\"\"\"\n        for field in ('packages', 'py_modules'):\n            items = getattr(self.dist, field, None) or []\n            if items and len(items) == 1:\n                log.debug(f\"Single module/package detected, name: {items[0]}\")\n                return items[0]\n\n        return None\n\n    def _find_name_from_packages(self) -> str | None:\n        \"\"\"Try to find the root package that is not a PEP 420 namespace\"\"\"\n        if not self.dist.packages:\n            return None\n\n        packages = remove_stubs(sorted(self.dist.packages, key=len))\n        package_dir = self.dist.package_dir or {}\n\n        parent_pkg = find_parent_package(packages, package_dir, self._root_dir)\n        if parent_pkg:\n            log.debug(f\"Common parent package detected, name: {parent_pkg}\")\n            return parent_pkg\n\n        log.warn(\"No parent package detected, impossible to derive `name`\")\n        return None\n\n\ndef remove_nested_packages(packages: list[str]) -> list[str]:\n    \"\"\"Remove nested packages from a list of packages.\n\n    >>> remove_nested_packages([\"a\", \"a.b1\", \"a.b2\", \"a.b1.c1\"])\n    ['a']\n    >>> remove_nested_packages([\"a\", \"b\", \"c.d\", \"c.d.e.f\", \"g.h\", \"a.a1\"])\n    ['a', 'b', 'c.d', 'g.h']\n    \"\"\"\n    pkgs = sorted(packages, key=len)\n    top_level = pkgs[:]\n    size = len(pkgs)\n    for i, name in enumerate(reversed(pkgs)):\n        if any(name.startswith(f\"{other}.\") for other in top_level):\n            top_level.pop(size - i - 1)\n\n    return top_level\n\n\ndef remove_stubs(packages: list[str]) -> list[str]:\n    \"\"\"Remove type stubs (:pep:`561`) from a list of packages.\n\n    >>> remove_stubs([\"a\", \"a.b\", \"a-stubs\", \"a-stubs.b.c\", \"b\", \"c-stubs\"])\n    ['a', 'a.b', 'b']\n    \"\"\"\n    return [pkg for pkg in packages if not pkg.split(\".\")[0].endswith(\"-stubs\")]\n\n\ndef find_parent_package(\n    packages: list[str], package_dir: Mapping[str, str], root_dir: StrPath\n) -> str | None:\n    \"\"\"Find the parent package that is not a namespace.\"\"\"\n    packages = sorted(packages, key=len)\n    common_ancestors = []\n    for i, name in enumerate(packages):\n        if not all(n.startswith(f\"{name}.\") for n in packages[i + 1 :]):\n            # Since packages are sorted by length, this condition is able\n            # to find a list of all common ancestors.\n            # When there is divergence (e.g. multiple root packages)\n            # the list will be empty\n            break\n        common_ancestors.append(name)\n\n    for name in common_ancestors:\n        pkg_path = find_package_path(name, package_dir, root_dir)\n        init = os.path.join(pkg_path, \"__init__.py\")\n        if os.path.isfile(init):\n            return name\n\n    return None\n\n\ndef find_package_path(\n    name: str, package_dir: Mapping[str, str], root_dir: StrPath\n) -> str:\n    \"\"\"Given a package name, return the path where it should be found on\n    disk, considering the ``package_dir`` option.\n\n    >>> path = find_package_path(\"my.pkg\", {\"\": \"root/is/nested\"}, \".\")\n    >>> path.replace(os.sep, \"/\")\n    './root/is/nested/my/pkg'\n\n    >>> path = find_package_path(\"my.pkg\", {\"my\": \"root/is/nested\"}, \".\")\n    >>> path.replace(os.sep, \"/\")\n    './root/is/nested/pkg'\n\n    >>> path = find_package_path(\"my.pkg\", {\"my.pkg\": \"root/is/nested\"}, \".\")\n    >>> path.replace(os.sep, \"/\")\n    './root/is/nested'\n\n    >>> path = find_package_path(\"other.pkg\", {\"my.pkg\": \"root/is/nested\"}, \".\")\n    >>> path.replace(os.sep, \"/\")\n    './other/pkg'\n    \"\"\"\n    parts = name.split(\".\")\n    for i in range(len(parts), 0, -1):\n        # Look backwards, the most specific package_dir first\n        partial_name = \".\".join(parts[:i])\n        if partial_name in package_dir:\n            parent = package_dir[partial_name]\n            return os.path.join(root_dir, parent, *parts[i:])\n\n    parent = package_dir.get(\"\") or \"\"\n    return os.path.join(root_dir, *parent.split(\"/\"), *parts)\n\n\ndef construct_package_dir(packages: list[str], package_path: StrPath) -> dict[str, str]:\n    parent_pkgs = remove_nested_packages(packages)\n    prefix = Path(package_path).parts\n    return {pkg: \"/\".join([*prefix, *pkg.split(\".\")]) for pkg in parent_pkgs}\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/dist.py","size":38815,"sha1":"9030796a41ec2963bb9e2c224bc6d58bfeb5ad34","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"from __future__ import annotations\n\nimport io\nimport itertools\nimport numbers\nimport os\nimport re\nimport sys\nfrom collections.abc import Iterable, MutableMapping, Sequence\nfrom glob import iglob\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING, Any, Union\n\nfrom more_itertools import partition, unique_everseen\nfrom packaging.markers import InvalidMarker, Marker\nfrom packaging.specifiers import InvalidSpecifier, SpecifierSet\nfrom packaging.version import Version\n\nfrom . import (\n    _entry_points,\n    _reqs,\n    _static,\n    command as _,  # noqa: F401 # imported for side-effects\n)\nfrom ._importlib import metadata\nfrom ._path import StrPath\nfrom ._reqs import _StrOrIter\nfrom .config import pyprojecttoml, setupcfg\nfrom .discovery import ConfigDiscovery\nfrom .monkey import get_unpatched\nfrom .warnings import InformationOnly, SetuptoolsDeprecationWarning\n\nimport distutils.cmd\nimport distutils.command\nimport distutils.core\nimport distutils.dist\nimport distutils.log\nfrom distutils.debug import DEBUG\nfrom distutils.errors import DistutilsOptionError, DistutilsSetupError\nfrom distutils.fancy_getopt import translate_longopt\nfrom distutils.util import strtobool\n\nif TYPE_CHECKING:\n    from typing_extensions import TypeAlias\n\n    from pkg_resources import Distribution as _pkg_resources_Distribution\n\n\n__all__ = ['Distribution']\n\n_sequence = tuple, list\n\"\"\"\n:meta private:\n\nSupported iterable types that are known to be:\n- ordered (which `set` isn't)\n- not match a str (which `Sequence[str]` does)\n- not imply a nested type (like `dict`)\nfor use with `isinstance`.\n\"\"\"\n_Sequence: TypeAlias = Union[tuple[str, ...], list[str]]\n# This is how stringifying _Sequence would look in Python 3.10\n_sequence_type_repr = \"tuple[str, ...] | list[str]\"\n_OrderedStrSequence: TypeAlias = Union[str, dict[str, Any], Sequence[str]]\n\"\"\"\n:meta private:\nAvoid single-use iterable. Disallow sets.\nA poor approximation of an OrderedSequence (dict doesn't match a Sequence).\n\"\"\"\n\n\ndef __getattr__(name: str) -> Any:  # pragma: no cover\n    if name == \"sequence\":\n        SetuptoolsDeprecationWarning.emit(\n            \"`setuptools.dist.sequence` is an internal implementation detail.\",\n            \"Please define your own `sequence = tuple, list` instead.\",\n            due_date=(2025, 8, 28),  # Originally added on 2024-08-27\n        )\n        return _sequence\n    raise AttributeError(f\"module {__name__!r} has no attribute {name!r}\")\n\n\ndef check_importable(dist, attr, value):\n    try:\n        ep = metadata.EntryPoint(value=value, name=None, group=None)\n        assert not ep.extras\n    except (TypeError, ValueError, AttributeError, AssertionError) as e:\n        raise DistutilsSetupError(\n            f\"{attr!r} must be importable 'module:attrs' string (got {value!r})\"\n        ) from e\n\n\ndef assert_string_list(dist, attr: str, value: _Sequence) -> None:\n    \"\"\"Verify that value is a string list\"\"\"\n    try:\n        # verify that value is a list or tuple to exclude unordered\n        # or single-use iterables\n        assert isinstance(value, _sequence)\n        # verify that elements of value are strings\n        assert ''.join(value) != value\n    except (TypeError, ValueError, AttributeError, AssertionError) as e:\n        raise DistutilsSetupError(\n            f\"{attr!r} must be of type <{_sequence_type_repr}> (got {value!r})\"\n        ) from e\n\n\ndef check_nsp(dist, attr, value):\n    \"\"\"Verify that namespace packages are valid\"\"\"\n    ns_packages = value\n    assert_string_list(dist, attr, ns_packages)\n    for nsp in ns_packages:\n        if not dist.has_contents_for(nsp):\n            raise DistutilsSetupError(\n                f\"Distribution contains no modules or packages for namespace package {nsp!r}\"\n            )\n        parent, _sep, _child = nsp.rpartition('.')\n        if parent and parent not in ns_packages:\n            distutils.log.warn(\n                \"WARNING: %r is declared as a package namespace, but %r\"\n                \" is not: please correct this in setup.py\",\n                nsp,\n                parent,\n            )\n        SetuptoolsDeprecationWarning.emit(\n            \"The namespace_packages parameter is deprecated.\",\n            \"Please replace its usage with implicit namespaces (PEP 420).\",\n            see_docs=\"references/keywords.html#keyword-namespace-packages\",\n            # TODO: define due_date, it may break old packages that are no longer\n            # maintained (e.g. sphinxcontrib extensions) when installed from source.\n            # Warning officially introduced in May 2022, however the deprecation\n            # was mentioned much earlier in the docs (May 2020, see #2149).\n        )\n\n\ndef check_extras(dist, attr, value):\n    \"\"\"Verify that extras_require mapping is valid\"\"\"\n    try:\n        list(itertools.starmap(_check_extra, value.items()))\n    except (TypeError, ValueError, AttributeError) as e:\n        raise DistutilsSetupError(\n            \"'extras_require' must be a dictionary whose values are \"\n            \"strings or lists of strings containing valid project/version \"\n            \"requirement specifiers.\"\n        ) from e\n\n\ndef _check_extra(extra, reqs):\n    _name, _sep, marker = extra.partition(':')\n    try:\n        _check_marker(marker)\n    except InvalidMarker:\n        msg = f\"Invalid environment marker: {marker} ({extra!r})\"\n        raise DistutilsSetupError(msg) from None\n    list(_reqs.parse(reqs))\n\n\ndef _check_marker(marker):\n    if not marker:\n        return\n    m = Marker(marker)\n    m.evaluate()\n\n\ndef assert_bool(dist, attr, value):\n    \"\"\"Verify that value is True, False, 0, or 1\"\"\"\n    if bool(value) != value:\n        raise DistutilsSetupError(f\"{attr!r} must be a boolean value (got {value!r})\")\n\n\ndef invalid_unless_false(dist, attr, value):\n    if not value:\n        DistDeprecationWarning.emit(f\"{attr} is ignored.\")\n        # TODO: should there be a `due_date` here?\n        return\n    raise DistutilsSetupError(f\"{attr} is invalid.\")\n\n\ndef check_requirements(dist, attr: str, value: _OrderedStrSequence) -> None:\n    \"\"\"Verify that install_requires is a valid requirements list\"\"\"\n    try:\n        list(_reqs.parse(value))\n        if isinstance(value, set):\n            raise TypeError(\"Unordered types are not allowed\")\n    except (TypeError, ValueError) as error:\n        msg = (\n            f\"{attr!r} must be a string or iterable of strings \"\n            f\"containing valid project/version requirement specifiers; {error}\"\n        )\n        raise DistutilsSetupError(msg) from error\n\n\ndef check_specifier(dist, attr, value):\n    \"\"\"Verify that value is a valid version specifier\"\"\"\n    try:\n        SpecifierSet(value)\n    except (InvalidSpecifier, AttributeError) as error:\n        msg = f\"{attr!r} must be a string containing valid version specifiers; {error}\"\n        raise DistutilsSetupError(msg) from error\n\n\ndef check_entry_points(dist, attr, value):\n    \"\"\"Verify that entry_points map is parseable\"\"\"\n    try:\n        _entry_points.load(value)\n    except Exception as e:\n        raise DistutilsSetupError(e) from e\n\n\ndef check_package_data(dist, attr, value):\n    \"\"\"Verify that value is a dictionary of package names to glob lists\"\"\"\n    if not isinstance(value, dict):\n        raise DistutilsSetupError(\n            f\"{attr!r} must be a dictionary mapping package names to lists of \"\n            \"string wildcard patterns\"\n        )\n    for k, v in value.items():\n        if not isinstance(k, str):\n            raise DistutilsSetupError(\n                f\"keys of {attr!r} dict must be strings (got {k!r})\"\n            )\n        assert_string_list(dist, f'values of {attr!r} dict', v)\n\n\ndef check_packages(dist, attr, value):\n    for pkgname in value:\n        if not re.match(r'\\w+(\\.\\w+)*', pkgname):\n            distutils.log.warn(\n                \"WARNING: %r not a valid package name; please use only \"\n                \".-separated package names in setup.py\",\n                pkgname,\n            )\n\n\nif TYPE_CHECKING:\n    # Work around a mypy issue where type[T] can't be used as a base: https://github.com/python/mypy/issues/10962\n    from distutils.core import Distribution as _Distribution\nelse:\n    _Distribution = get_unpatched(distutils.core.Distribution)\n\n\nclass Distribution(_Distribution):\n    \"\"\"Distribution with support for tests and package data\n\n    This is an enhanced version of 'distutils.dist.Distribution' that\n    effectively adds the following new optional keyword arguments to 'setup()':\n\n     'install_requires' -- a string or sequence of strings specifying project\n        versions that the distribution requires when installed, in the format\n        used by 'pkg_resources.require()'.  They will be installed\n        automatically when the package is installed.  If you wish to use\n        packages that are not available in PyPI, or want to give your users an\n        alternate download location, you can add a 'find_links' option to the\n        '[easy_install]' section of your project's 'setup.cfg' file, and then\n        setuptools will scan the listed web pages for links that satisfy the\n        requirements.\n\n     'extras_require' -- a dictionary mapping names of optional \"extras\" to the\n        additional requirement(s) that using those extras incurs. For example,\n        this::\n\n            extras_require = dict(reST = [\"docutils>=0.3\", \"reSTedit\"])\n\n        indicates that the distribution can optionally provide an extra\n        capability called \"reST\", but it can only be used if docutils and\n        reSTedit are installed.  If the user installs your package using\n        EasyInstall and requests one of your extras, the corresponding\n        additional requirements will be installed if needed.\n\n     'package_data' -- a dictionary mapping package names to lists of filenames\n        or globs to use to find data files contained in the named packages.\n        If the dictionary has filenames or globs listed under '\"\"' (the empty\n        string), those names will be searched for in every package, in addition\n        to any names for the specific package.  Data files found using these\n        names/globs will be installed along with the package, in the same\n        location as the package.  Note that globs are allowed to reference\n        the contents of non-package subdirectories, as long as you use '/' as\n        a path separator.  (Globs are automatically converted to\n        platform-specific paths at runtime.)\n\n    In addition to these new keywords, this class also has several new methods\n    for manipulating the distribution's contents.  For example, the 'include()'\n    and 'exclude()' methods can be thought of as in-place add and subtract\n    commands that add or remove packages, modules, extensions, and so on from\n    the distribution.\n    \"\"\"\n\n    _DISTUTILS_UNSUPPORTED_METADATA = {\n        'long_description_content_type': lambda: None,\n        'project_urls': dict,\n        'provides_extras': dict,  # behaves like an ordered set\n        'license_file': lambda: None,\n        'license_files': lambda: None,\n        'install_requires': list,\n        'extras_require': dict,\n    }\n\n    # Used by build_py, editable_wheel and install_lib commands for legacy namespaces\n    namespace_packages: list[str]  #: :meta private: DEPRECATED\n\n    # Any: Dynamic assignment results in Incompatible types in assignment\n    def __init__(self, attrs: MutableMapping[str, Any] | None = None) -> None:\n        have_package_data = hasattr(self, \"package_data\")\n        if not have_package_data:\n            self.package_data: dict[str, list[str]] = {}\n        attrs = attrs or {}\n        self.dist_files: list[tuple[str, str, str]] = []\n        self.include_package_data: bool | None = None\n        self.exclude_package_data: dict[str, list[str]] | None = None\n        # Filter-out setuptools' specific options.\n        self.src_root: str | None = attrs.pop(\"src_root\", None)\n        self.dependency_links: list[str] = attrs.pop('dependency_links', [])\n        self.setup_requires: list[str] = attrs.pop('setup_requires', [])\n        for ep in metadata.entry_points(group='distutils.setup_keywords'):\n            vars(self).setdefault(ep.name, None)\n\n        metadata_only = set(self._DISTUTILS_UNSUPPORTED_METADATA)\n        metadata_only -= {\"install_requires\", \"extras_require\"}\n        dist_attrs = {k: v for k, v in attrs.items() if k not in metadata_only}\n        _Distribution.__init__(self, dist_attrs)\n\n        # Private API (setuptools-use only, not restricted to Distribution)\n        # Stores files that are referenced by the configuration and need to be in the\n        # sdist (e.g. `version = file: VERSION.txt`)\n        self._referenced_files = set[str]()\n\n        self.set_defaults = ConfigDiscovery(self)\n\n        self._set_metadata_defaults(attrs)\n\n        self.metadata.version = self._normalize_version(self.metadata.version)\n        self._finalize_requires()\n\n    def _validate_metadata(self):\n        required = {\"name\"}\n        provided = {\n            key\n            for key in vars(self.metadata)\n            if getattr(self.metadata, key, None) is not None\n        }\n        missing = required - provided\n\n        if missing:\n            msg = f\"Required package metadata is missing: {missing}\"\n            raise DistutilsSetupError(msg)\n\n    def _set_metadata_defaults(self, attrs):\n        \"\"\"\n        Fill-in missing metadata fields not supported by distutils.\n        Some fields may have been set by other tools (e.g. pbr).\n        Those fields (vars(self.metadata)) take precedence to\n        supplied attrs.\n        \"\"\"\n        for option, default in self._DISTUTILS_UNSUPPORTED_METADATA.items():\n            vars(self.metadata).setdefault(option, attrs.get(option, default()))\n\n    @staticmethod\n    def _normalize_version(version):\n        from . import sic\n\n        if isinstance(version, numbers.Number):\n            # Some people apparently take \"version number\" too literally :)\n            version = str(version)\n        elif isinstance(version, sic) or version is None:\n            return version\n\n        normalized = str(Version(version))\n        if version != normalized:\n            InformationOnly.emit(f\"Normalizing '{version}' to '{normalized}'\")\n            return normalized\n        return version\n\n    def _finalize_requires(self):\n        \"\"\"\n        Set `metadata.python_requires` and fix environment markers\n        in `install_requires` and `extras_require`.\n        \"\"\"\n        if getattr(self, 'python_requires', None):\n            self.metadata.python_requires = self.python_requires\n\n        self._normalize_requires()\n        self.metadata.install_requires = self.install_requires\n        self.metadata.extras_require = self.extras_require\n\n        if self.extras_require:\n            for extra in self.extras_require.keys():\n                # Setuptools allows a weird \"<name>:<env markers> syntax for extras\n                extra = extra.split(':')[0]\n                if extra:\n                    self.metadata.provides_extras.setdefault(extra)\n\n    def _normalize_requires(self):\n        \"\"\"Make sure requirement-related attributes exist and are normalized\"\"\"\n        install_requires = getattr(self, \"install_requires\", None) or []\n        extras_require = getattr(self, \"extras_require\", None) or {}\n\n        # Preserve the \"static\"-ness of values parsed from config files\n        list_ = _static.List if _static.is_static(install_requires) else list\n        self.install_requires = list_(map(str, _reqs.parse(install_requires)))\n\n        dict_ = _static.Dict if _static.is_static(extras_require) else dict\n        self.extras_require = dict_(\n            (k, list(map(str, _reqs.parse(v or [])))) for k, v in extras_require.items()\n        )\n\n    def _finalize_license_files(self) -> None:\n        \"\"\"Compute names of all license files which should be included.\"\"\"\n        license_files: list[str] | None = self.metadata.license_files\n        patterns = license_files or []\n\n        license_file: str | None = self.metadata.license_file\n        if license_file and license_file not in patterns:\n            patterns.append(license_file)\n\n        if license_files is None and license_file is None:\n            # Default patterns match the ones wheel uses\n            # See https://wheel.readthedocs.io/en/stable/user_guide.html\n            # -> 'Including license files in the generated wheel file'\n            patterns = ['LICEN[CS]E*', 'COPYING*', 'NOTICE*', 'AUTHORS*']\n\n        self.metadata.license_files = list(\n            unique_everseen(self._expand_patterns(patterns))\n        )\n\n    @staticmethod\n    def _expand_patterns(patterns):\n        \"\"\"\n        >>> list(Distribution._expand_patterns(['LICENSE']))\n        ['LICENSE']\n        >>> list(Distribution._expand_patterns(['pyproject.toml', 'LIC*']))\n        ['pyproject.toml', 'LICENSE']\n        \"\"\"\n        return (\n            path\n            for pattern in patterns\n            for path in sorted(iglob(pattern))\n            if not path.endswith('~') and os.path.isfile(path)\n        )\n\n    # FIXME: 'Distribution._parse_config_files' is too complex (14)\n    def _parse_config_files(self, filenames=None):  # noqa: C901\n        \"\"\"\n        Adapted from distutils.dist.Distribution.parse_config_files,\n        this method provides the same functionality in subtly-improved\n        ways.\n        \"\"\"\n        from configparser import ConfigParser\n\n        # Ignore install directory options if we have a venv\n        ignore_options = (\n            []\n            if sys.prefix == sys.base_prefix\n            else [\n                'install-base',\n                'install-platbase',\n                'install-lib',\n                'install-platlib',\n                'install-purelib',\n                'install-headers',\n                'install-scripts',\n                'install-data',\n                'prefix',\n                'exec-prefix',\n                'home',\n                'user',\n                'root',\n            ]\n        )\n\n        ignore_options = frozenset(ignore_options)\n\n        if filenames is None:\n            filenames = self.find_config_files()\n\n        if DEBUG:\n            self.announce(\"Distribution.parse_config_files():\")\n\n        parser = ConfigParser()\n        parser.optionxform = str\n        for filename in filenames:\n            with open(filename, encoding='utf-8') as reader:\n                if DEBUG:\n                    self.announce(\"  reading {filename}\".format(**locals()))\n                parser.read_file(reader)\n            for section in parser.sections():\n                options = parser.options(section)\n                opt_dict = self.get_option_dict(section)\n\n                for opt in options:\n                    if opt == '__name__' or opt in ignore_options:\n                        continue\n\n                    val = parser.get(section, opt)\n                    opt = self.warn_dash_deprecation(opt, section)\n                    opt = self.make_option_lowercase(opt, section)\n                    opt_dict[opt] = (filename, val)\n\n            # Make the ConfigParser forget everything (so we retain\n            # the original filenames that options come from)\n            parser.__init__()\n\n        if 'global' not in self.command_options:\n            return\n\n        # If there was a \"global\" section in the config file, use it\n        # to set Distribution options.\n\n        for opt, (src, val) in self.command_options['global'].items():\n            alias = self.negative_opt.get(opt)\n            if alias:\n                val = not strtobool(val)\n            elif opt in ('verbose', 'dry_run'):  # ugh!\n                val = strtobool(val)\n\n            try:\n                setattr(self, alias or opt, val)\n            except ValueError as e:\n                raise DistutilsOptionError(e) from e\n\n    def warn_dash_deprecation(self, opt: str, section: str) -> str:\n        if section in (\n            'options.extras_require',\n            'options.data_files',\n        ):\n            return opt\n\n        underscore_opt = opt.replace('-', '_')\n        commands = list(\n            itertools.chain(\n                distutils.command.__all__,\n                self._setuptools_commands(),\n            )\n        )\n        if (\n            not section.startswith('options')\n            and section != 'metadata'\n            and section not in commands\n        ):\n            return underscore_opt\n\n        if '-' in opt:\n            SetuptoolsDeprecationWarning.emit(\n                \"Invalid dash-separated options\",\n                f\"\"\"\n                Usage of dash-separated {opt!r} will not be supported in future\n                versions. Please use the underscore name {underscore_opt!r} instead.\n                \"\"\",\n                see_docs=\"userguide/declarative_config.html\",\n                due_date=(2025, 3, 3),\n                # Warning initially introduced in 3 Mar 2021\n            )\n        return underscore_opt\n\n    def _setuptools_commands(self):\n        try:\n            entry_points = metadata.distribution('setuptools').entry_points\n            return {ep.name for ep in entry_points}  # Avoid newer API for compatibility\n        except metadata.PackageNotFoundError:\n            # during bootstrapping, distribution doesn't exist\n            return []\n\n    def make_option_lowercase(self, opt: str, section: str) -> str:\n        if section != 'metadata' or opt.islower():\n            return opt\n\n        lowercase_opt = opt.lower()\n        SetuptoolsDeprecationWarning.emit(\n            \"Invalid uppercase configuration\",\n            f\"\"\"\n            Usage of uppercase key {opt!r} in {section!r} will not be supported in\n            future versions. Please use lowercase {lowercase_opt!r} instead.\n            \"\"\",\n            see_docs=\"userguide/declarative_config.html\",\n            due_date=(2025, 3, 3),\n            # Warning initially introduced in 6 Mar 2021\n        )\n        return lowercase_opt\n\n    # FIXME: 'Distribution._set_command_options' is too complex (14)\n    def _set_command_options(self, command_obj, option_dict=None):  # noqa: C901\n        \"\"\"\n        Set the options for 'command_obj' from 'option_dict'.  Basically\n        this means copying elements of a dictionary ('option_dict') to\n        attributes of an instance ('command').\n\n        'command_obj' must be a Command instance.  If 'option_dict' is not\n        supplied, uses the standard option dictionary for this command\n        (from 'self.command_options').\n\n        (Adopted from distutils.dist.Distribution._set_command_options)\n        \"\"\"\n        command_name = command_obj.get_command_name()\n        if option_dict is None:\n            option_dict = self.get_option_dict(command_name)\n\n        if DEBUG:\n            self.announce(f\"  setting options for '{command_name}' command:\")\n        for option, (source, value) in option_dict.items():\n            if DEBUG:\n                self.announce(f\"    {option} = {value} (from {source})\")\n            try:\n                bool_opts = [translate_longopt(o) for o in command_obj.boolean_options]\n            except AttributeError:\n                bool_opts = []\n            try:\n                neg_opt = command_obj.negative_opt\n            except AttributeError:\n                neg_opt = {}\n\n            try:\n                is_string = isinstance(value, str)\n                if option in neg_opt and is_string:\n                    setattr(command_obj, neg_opt[option], not strtobool(value))\n                elif option in bool_opts and is_string:\n                    setattr(command_obj, option, strtobool(value))\n                elif hasattr(command_obj, option):\n                    setattr(command_obj, option, value)\n                else:\n                    raise DistutilsOptionError(\n                        f\"error in {source}: command '{command_name}' has no such option '{option}'\"\n                    )\n            except ValueError as e:\n                raise DistutilsOptionError(e) from e\n\n    def _get_project_config_files(self, filenames: Iterable[StrPath] | None):\n        \"\"\"Add default file and split between INI and TOML\"\"\"\n        tomlfiles = []\n        standard_project_metadata = Path(self.src_root or os.curdir, \"pyproject.toml\")\n        if filenames is not None:\n            parts = partition(lambda f: Path(f).suffix == \".toml\", filenames)\n            filenames = list(parts[0])  # 1st element => predicate is False\n            tomlfiles = list(parts[1])  # 2nd element => predicate is True\n        elif standard_project_metadata.exists():\n            tomlfiles = [standard_project_metadata]\n        return filenames, tomlfiles\n\n    def parse_config_files(\n        self,\n        filenames: Iterable[StrPath] | None = None,\n        ignore_option_errors: bool = False,\n    ) -> None:\n        \"\"\"Parses configuration files from various levels\n        and loads configuration.\n        \"\"\"\n        inifiles, tomlfiles = self._get_project_config_files(filenames)\n\n        self._parse_config_files(filenames=inifiles)\n\n        setupcfg.parse_configuration(\n            self, self.command_options, ignore_option_errors=ignore_option_errors\n        )\n        for filename in tomlfiles:\n            pyprojecttoml.apply_configuration(self, filename, ignore_option_errors)\n\n        self._finalize_requires()\n        self._finalize_license_files()\n\n    def fetch_build_eggs(\n        self, requires: _StrOrIter\n    ) -> list[_pkg_resources_Distribution]:\n        \"\"\"Resolve pre-setup requirements\"\"\"\n        from .installer import _fetch_build_eggs\n\n        return _fetch_build_eggs(self, requires)\n\n    def finalize_options(self) -> None:\n        \"\"\"\n        Allow plugins to apply arbitrary operations to the\n        distribution. Each hook may optionally define a 'order'\n        to influence the order of execution. Smaller numbers\n        go first and the default is 0.\n        \"\"\"\n        group = 'setuptools.finalize_distribution_options'\n\n        def by_order(hook):\n            return getattr(hook, 'order', 0)\n\n        defined = metadata.entry_points(group=group)\n        filtered = itertools.filterfalse(self._removed, defined)\n        loaded = map(lambda e: e.load(), filtered)\n        for ep in sorted(loaded, key=by_order):\n            ep(self)\n\n    @staticmethod\n    def _removed(ep):\n        \"\"\"\n        When removing an entry point, if metadata is loaded\n        from an older version of Setuptools, that removed\n        entry point will attempt to be loaded and will fail.\n        See #2765 for more details.\n        \"\"\"\n        removed = {\n            # removed 2021-09-05\n            '2to3_doctests',\n        }\n        return ep.name in removed\n\n    def _finalize_setup_keywords(self):\n        for ep in metadata.entry_points(group='distutils.setup_keywords'):\n            value = getattr(self, ep.name, None)\n            if value is not None:\n                ep.load()(self, ep.name, value)\n\n    def get_egg_cache_dir(self):\n        from . import windows_support\n\n        egg_cache_dir = os.path.join(os.curdir, '.eggs')\n        if not os.path.exists(egg_cache_dir):\n            os.mkdir(egg_cache_dir)\n            windows_support.hide_file(egg_cache_dir)\n            readme_txt_filename = os.path.join(egg_cache_dir, 'README.txt')\n            with open(readme_txt_filename, 'w', encoding=\"utf-8\") as f:\n                f.write(\n                    'This directory contains eggs that were downloaded '\n                    'by setuptools to build, test, and run plug-ins.\\n\\n'\n                )\n                f.write(\n                    'This directory caches those eggs to prevent '\n                    'repeated downloads.\\n\\n'\n                )\n                f.write('However, it is safe to delete this directory.\\n\\n')\n\n        return egg_cache_dir\n\n    def fetch_build_egg(self, req):\n        \"\"\"Fetch an egg needed for building\"\"\"\n        from .installer import fetch_build_egg\n\n        return fetch_build_egg(self, req)\n\n    def get_command_class(self, command: str) -> type[distutils.cmd.Command]:  # type: ignore[override] # Not doing complex overrides yet\n        \"\"\"Pluggable version of get_command_class()\"\"\"\n        if command in self.cmdclass:\n            return self.cmdclass[command]\n\n        # Special case bdist_wheel so it's never loaded from \"wheel\"\n        if command == 'bdist_wheel':\n            from .command.bdist_wheel import bdist_wheel\n\n            return bdist_wheel\n\n        eps = metadata.entry_points(group='distutils.commands', name=command)\n        for ep in eps:\n            self.cmdclass[command] = cmdclass = ep.load()\n            return cmdclass\n        else:\n            return _Distribution.get_command_class(self, command)\n\n    def print_commands(self):\n        for ep in metadata.entry_points(group='distutils.commands'):\n            if ep.name not in self.cmdclass:\n                cmdclass = ep.load()\n                self.cmdclass[ep.name] = cmdclass\n        return _Distribution.print_commands(self)\n\n    def get_command_list(self):\n        for ep in metadata.entry_points(group='distutils.commands'):\n            if ep.name not in self.cmdclass:\n                cmdclass = ep.load()\n                self.cmdclass[ep.name] = cmdclass\n        return _Distribution.get_command_list(self)\n\n    def include(self, **attrs) -> None:\n        \"\"\"Add items to distribution that are named in keyword arguments\n\n        For example, 'dist.include(py_modules=[\"x\"])' would add 'x' to\n        the distribution's 'py_modules' attribute, if it was not already\n        there.\n\n        Currently, this method only supports inclusion for attributes that are\n        lists or tuples.  If you need to add support for adding to other\n        attributes in this or a subclass, you can add an '_include_X' method,\n        where 'X' is the name of the attribute.  The method will be called with\n        the value passed to 'include()'.  So, 'dist.include(foo={\"bar\":\"baz\"})'\n        will try to call 'dist._include_foo({\"bar\":\"baz\"})', which can then\n        handle whatever special inclusion logic is needed.\n        \"\"\"\n        for k, v in attrs.items():\n            include = getattr(self, '_include_' + k, None)\n            if include:\n                include(v)\n            else:\n                self._include_misc(k, v)\n\n    def exclude_package(self, package: str) -> None:\n        \"\"\"Remove packages, modules, and extensions in named package\"\"\"\n\n        pfx = package + '.'\n        if self.packages:\n            self.packages = [\n                p for p in self.packages if p != package and not p.startswith(pfx)\n            ]\n\n        if self.py_modules:\n            self.py_modules = [\n                p for p in self.py_modules if p != package and not p.startswith(pfx)\n            ]\n\n        if self.ext_modules:\n            self.ext_modules = [\n                p\n                for p in self.ext_modules\n                if p.name != package and not p.name.startswith(pfx)\n            ]\n\n    def has_contents_for(self, package: str) -> bool:\n        \"\"\"Return true if 'exclude_package(package)' would do something\"\"\"\n\n        pfx = package + '.'\n\n        for p in self.iter_distribution_names():\n            if p == package or p.startswith(pfx):\n                return True\n\n        return False\n\n    def _exclude_misc(self, name: str, value: _Sequence) -> None:\n        \"\"\"Handle 'exclude()' for list/tuple attrs without a special handler\"\"\"\n        if not isinstance(value, _sequence):\n            raise DistutilsSetupError(\n                f\"{name}: setting must be of type <{_sequence_type_repr}> (got {value!r})\"\n            )\n        try:\n            old = getattr(self, name)\n        except AttributeError as e:\n            raise DistutilsSetupError(f\"{name}: No such distribution setting\") from e\n        if old is not None and not isinstance(old, _sequence):\n            raise DistutilsSetupError(\n                name + \": this setting cannot be changed via include/exclude\"\n            )\n        elif old:\n            setattr(self, name, [item for item in old if item not in value])\n\n    def _include_misc(self, name: str, value: _Sequence) -> None:\n        \"\"\"Handle 'include()' for list/tuple attrs without a special handler\"\"\"\n\n        if not isinstance(value, _sequence):\n            raise DistutilsSetupError(\n                f\"{name}: setting must be of type <{_sequence_type_repr}> (got {value!r})\"\n            )\n        try:\n            old = getattr(self, name)\n        except AttributeError as e:\n            raise DistutilsSetupError(f\"{name}: No such distribution setting\") from e\n        if old is None:\n            setattr(self, name, value)\n        elif not isinstance(old, _sequence):\n            raise DistutilsSetupError(\n                name + \": this setting cannot be changed via include/exclude\"\n            )\n        else:\n            new = [item for item in value if item not in old]\n            setattr(self, name, list(old) + new)\n\n    def exclude(self, **attrs) -> None:\n        \"\"\"Remove items from distribution that are named in keyword arguments\n\n        For example, 'dist.exclude(py_modules=[\"x\"])' would remove 'x' from\n        the distribution's 'py_modules' attribute.  Excluding packages uses\n        the 'exclude_package()' method, so all of the package's contained\n        packages, modules, and extensions are also excluded.\n\n        Currently, this method only supports exclusion from attributes that are\n        lists or tuples.  If you need to add support for excluding from other\n        attributes in this or a subclass, you can add an '_exclude_X' method,\n        where 'X' is the name of the attribute.  The method will be called with\n        the value passed to 'exclude()'.  So, 'dist.exclude(foo={\"bar\":\"baz\"})'\n        will try to call 'dist._exclude_foo({\"bar\":\"baz\"})', which can then\n        handle whatever special exclusion logic is needed.\n        \"\"\"\n        for k, v in attrs.items():\n            exclude = getattr(self, '_exclude_' + k, None)\n            if exclude:\n                exclude(v)\n            else:\n                self._exclude_misc(k, v)\n\n    def _exclude_packages(self, packages: _Sequence) -> None:\n        if not isinstance(packages, _sequence):\n            raise DistutilsSetupError(\n                f\"packages: setting must be of type <{_sequence_type_repr}> (got {packages!r})\"\n            )\n        list(map(self.exclude_package, packages))\n\n    def _parse_command_opts(self, parser, args):\n        # Remove --with-X/--without-X options when processing command args\n        self.global_options = self.__class__.global_options\n        self.negative_opt = self.__class__.negative_opt\n\n        # First, expand any aliases\n        command = args[0]\n        aliases = self.get_option_dict('aliases')\n        while command in aliases:\n            _src, alias = aliases[command]\n            del aliases[command]  # ensure each alias can expand only once!\n            import shlex\n\n            args[:1] = shlex.split(alias, True)\n            command = args[0]\n\n        nargs = _Distribution._parse_command_opts(self, parser, args)\n\n        # Handle commands that want to consume all remaining arguments\n        cmd_class = self.get_command_class(command)\n        if getattr(cmd_class, 'command_consumes_arguments', None):\n            self.get_option_dict(command)['args'] = (\"command line\", nargs)\n            if nargs is not None:\n                return []\n\n        return nargs\n\n    def get_cmdline_options(self) -> dict[str, dict[str, str | None]]:\n        \"\"\"Return a '{cmd: {opt:val}}' map of all command-line options\n\n        Option names are all long, but do not include the leading '--', and\n        contain dashes rather than underscores.  If the option doesn't take\n        an argument (e.g. '--quiet'), the 'val' is 'None'.\n\n        Note that options provided by config files are intentionally excluded.\n        \"\"\"\n\n        d: dict[str, dict[str, str | None]] = {}\n\n        for cmd, opts in self.command_options.items():\n            val: str | None\n            for opt, (src, val) in opts.items():\n                if src != \"command line\":\n                    continue\n\n                opt = opt.replace('_', '-')\n\n                if val == 0:\n                    cmdobj = self.get_command_obj(cmd)\n                    neg_opt = self.negative_opt.copy()\n                    neg_opt.update(getattr(cmdobj, 'negative_opt', {}))\n                    for neg, pos in neg_opt.items():\n                        if pos == opt:\n                            opt = neg\n                            val = None\n                            break\n                    else:\n                        raise AssertionError(\"Shouldn't be able to get here\")\n\n                elif val == 1:\n                    val = None\n\n                d.setdefault(cmd, {})[opt] = val\n\n        return d\n\n    def iter_distribution_names(self):\n        \"\"\"Yield all packages, modules, and extension names in distribution\"\"\"\n\n        yield from self.packages or ()\n\n        yield from self.py_modules or ()\n\n        for ext in self.ext_modules or ():\n            if isinstance(ext, tuple):\n                name, _buildinfo = ext\n            else:\n                name = ext.name\n            if name.endswith('module'):\n                name = name[:-6]\n            yield name\n\n    def handle_display_options(self, option_order):\n        \"\"\"If there were any non-global \"display-only\" options\n        (--help-commands or the metadata display options) on the command\n        line, display the requested info and return true; else return\n        false.\n        \"\"\"\n        import sys\n\n        if self.help_commands:\n            return _Distribution.handle_display_options(self, option_order)\n\n        # Stdout may be StringIO (e.g. in tests)\n        if not isinstance(sys.stdout, io.TextIOWrapper):\n            return _Distribution.handle_display_options(self, option_order)\n\n        # Don't wrap stdout if utf-8 is already the encoding. Provides\n        #  workaround for #334.\n        if sys.stdout.encoding.lower() in ('utf-8', 'utf8'):\n            return _Distribution.handle_display_options(self, option_order)\n\n        # Print metadata in UTF-8 no matter the platform\n        encoding = sys.stdout.encoding\n        sys.stdout.reconfigure(encoding='utf-8')\n        try:\n            return _Distribution.handle_display_options(self, option_order)\n        finally:\n            sys.stdout.reconfigure(encoding=encoding)\n\n    def run_command(self, command) -> None:\n        self.set_defaults()\n        # Postpone defaults until all explicit configuration is considered\n        # (setup() args, config files, command line and plugins)\n\n        super().run_command(command)\n\n\nclass DistDeprecationWarning(SetuptoolsDeprecationWarning):\n    \"\"\"Class for warning about deprecations in dist in\n    setuptools. Not ignored by default, unlike DeprecationWarning.\"\"\"\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/errors.py","size":3024,"sha1":"3503df82b1c2bc63e7dde8e5e489f99ffa7b98ce","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"setuptools.errors\n\nProvides exceptions used by setuptools modules.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom distutils import errors as _distutils_errors\n\n# Re-export errors from distutils to facilitate the migration to PEP632\n\nByteCompileError = _distutils_errors.DistutilsByteCompileError\nCCompilerError = _distutils_errors.CCompilerError\nClassError = _distutils_errors.DistutilsClassError\nCompileError = _distutils_errors.CompileError\nExecError = _distutils_errors.DistutilsExecError\nFileError = _distutils_errors.DistutilsFileError\nInternalError = _distutils_errors.DistutilsInternalError\nLibError = _distutils_errors.LibError\nLinkError = _distutils_errors.LinkError\nModuleError = _distutils_errors.DistutilsModuleError\nOptionError = _distutils_errors.DistutilsOptionError\nPlatformError = _distutils_errors.DistutilsPlatformError\nPreprocessError = _distutils_errors.PreprocessError\nSetupError = _distutils_errors.DistutilsSetupError\nTemplateError = _distutils_errors.DistutilsTemplateError\nUnknownFileError = _distutils_errors.UnknownFileError\n\n# The root error class in the hierarchy\nBaseError = _distutils_errors.DistutilsError\n\n\nclass InvalidConfigError(OptionError):  # type: ignore[valid-type, misc] # distutils imports are `Any` on python 3.12+\n    \"\"\"Error used for invalid configurations.\"\"\"\n\n\nclass RemovedConfigError(OptionError):  # type: ignore[valid-type, misc] # distutils imports are `Any` on python 3.12+\n    \"\"\"Error used for configurations that were deprecated and removed.\"\"\"\n\n\nclass RemovedCommandError(BaseError, RuntimeError):  # type: ignore[valid-type, misc] # distutils imports are `Any` on python 3.12+\n    \"\"\"Error used for commands that have been removed in setuptools.\n\n    Since ``setuptools`` is built on ``distutils``, simply removing a command\n    from ``setuptools`` will make the behavior fall back to ``distutils``; this\n    error is raised if a command exists in ``distutils`` but has been actively\n    removed in ``setuptools``.\n    \"\"\"\n\n\nclass PackageDiscoveryError(BaseError, RuntimeError):  # type: ignore[valid-type, misc] # distutils imports are `Any` on python 3.12+\n    \"\"\"Impossible to perform automatic discovery of packages and/or modules.\n\n    The current project layout or given discovery options can lead to problems when\n    scanning the project directory.\n\n    Setuptools might also refuse to complete auto-discovery if an error prone condition\n    is detected (e.g. when a project is organised as a flat-layout but contains\n    multiple directories that can be taken as top-level packages inside a single\n    distribution [*]_). In these situations the users are encouraged to be explicit\n    about which packages to include or to make the discovery parameters more specific.\n\n    .. [*] Since multi-package distributions are uncommon it is very likely that the\n       developers did not intend for all the directories to be packaged, and are just\n       leaving auxiliary code in the repository top-level, such as maintenance-related\n       scripts.\n    \"\"\"\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/extension.py","size":6683,"sha1":"e5ffe1d1fcaac36c1e1c9b23eeb3bb0976bbc6ed","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"from __future__ import annotations\n\nimport functools\nimport re\nfrom typing import TYPE_CHECKING\n\nfrom setuptools._path import StrPath\n\nfrom .monkey import get_unpatched\n\nimport distutils.core\nimport distutils.errors\nimport distutils.extension\n\n\ndef _have_cython():\n    \"\"\"\n    Return True if Cython can be imported.\n    \"\"\"\n    cython_impl = 'Cython.Distutils.build_ext'\n    try:\n        # from (cython_impl) import build_ext\n        __import__(cython_impl, fromlist=['build_ext']).build_ext\n    except Exception:\n        return False\n    return True\n\n\n# for compatibility\nhave_pyrex = _have_cython\nif TYPE_CHECKING:\n    # Work around a mypy issue where type[T] can't be used as a base: https://github.com/python/mypy/issues/10962\n    from distutils.core import Extension as _Extension\nelse:\n    _Extension = get_unpatched(distutils.core.Extension)\n\n\nclass Extension(_Extension):\n    \"\"\"\n    Describes a single extension module.\n\n    This means that all source files will be compiled into a single binary file\n    ``<module path>.<suffix>`` (with ``<module path>`` derived from ``name`` and\n    ``<suffix>`` defined by one of the values in\n    ``importlib.machinery.EXTENSION_SUFFIXES``).\n\n    In the case ``.pyx`` files are passed as ``sources and`` ``Cython`` is **not**\n    installed in the build environment, ``setuptools`` may also try to look for the\n    equivalent ``.cpp`` or ``.c`` files.\n\n    :arg str name:\n      the full name of the extension, including any packages -- ie.\n      *not* a filename or pathname, but Python dotted name\n\n    :arg list[str|os.PathLike[str]] sources:\n      list of source filenames, relative to the distribution root\n      (where the setup script lives), in Unix form (slash-separated)\n      for portability.  Source files may be C, C++, SWIG (.i),\n      platform-specific resource files, or whatever else is recognized\n      by the \"build_ext\" command as source for a Python extension.\n\n    :keyword list[str] include_dirs:\n      list of directories to search for C/C++ header files (in Unix\n      form for portability)\n\n    :keyword list[tuple[str, str|None]] define_macros:\n      list of macros to define; each macro is defined using a 2-tuple:\n      the first item corresponding to the name of the macro and the second\n      item either a string with its value or None to\n      define it without a particular value (equivalent of \"#define\n      FOO\" in source or -DFOO on Unix C compiler command line)\n\n    :keyword list[str] undef_macros:\n      list of macros to undefine explicitly\n\n    :keyword list[str] library_dirs:\n      list of directories to search for C/C++ libraries at link time\n\n    :keyword list[str] libraries:\n      list of library names (not filenames or paths) to link against\n\n    :keyword list[str] runtime_library_dirs:\n      list of directories to search for C/C++ libraries at run time\n      (for shared extensions, this is when the extension is loaded).\n      Setting this will cause an exception during build on Windows\n      platforms.\n\n    :keyword list[str] extra_objects:\n      list of extra files to link with (eg. object files not implied\n      by 'sources', static library that must be explicitly specified,\n      binary resource files, etc.)\n\n    :keyword list[str] extra_compile_args:\n      any extra platform- and compiler-specific information to use\n      when compiling the source files in 'sources'.  For platforms and\n      compilers where \"command line\" makes sense, this is typically a\n      list of command-line arguments, but for other platforms it could\n      be anything.\n\n    :keyword list[str] extra_link_args:\n      any extra platform- and compiler-specific information to use\n      when linking object files together to create the extension (or\n      to create a new static Python interpreter).  Similar\n      interpretation as for 'extra_compile_args'.\n\n    :keyword list[str] export_symbols:\n      list of symbols to be exported from a shared extension.  Not\n      used on all platforms, and not generally necessary for Python\n      extensions, which typically export exactly one symbol: \"init\" +\n      extension_name.\n\n    :keyword list[str] swig_opts:\n      any extra options to pass to SWIG if a source file has the .i\n      extension.\n\n    :keyword list[str] depends:\n      list of files that the extension depends on\n\n    :keyword str language:\n      extension language (i.e. \"c\", \"c++\", \"objc\"). Will be detected\n      from the source extensions if not provided.\n\n    :keyword bool optional:\n      specifies that a build failure in the extension should not abort the\n      build process, but simply not install the failing extension.\n\n    :keyword bool py_limited_api:\n      opt-in flag for the usage of :doc:`Python's limited API <python:c-api/stable>`.\n\n    :raises setuptools.errors.PlatformError: if ``runtime_library_dirs`` is\n      specified on Windows. (since v63)\n    \"\"\"\n\n    # These 4 are set and used in setuptools/command/build_ext.py\n    # The lack of a default value and risk of `AttributeError` is purposeful\n    # to avoid people forgetting to call finalize_options if they modify the extension list.\n    # See example/rationale in https://github.com/pypa/setuptools/issues/4529.\n    _full_name: str  #: Private API, internal use only.\n    _links_to_dynamic: bool  #: Private API, internal use only.\n    _needs_stub: bool  #: Private API, internal use only.\n    _file_name: str  #: Private API, internal use only.\n\n    def __init__(\n        self,\n        name: str,\n        sources: list[StrPath],\n        *args,\n        py_limited_api: bool = False,\n        **kw,\n    ) -> None:\n        # The *args is needed for compatibility as calls may use positional\n        # arguments. py_limited_api may be set only via keyword.\n        self.py_limited_api = py_limited_api\n        super().__init__(\n            name,\n            sources,  # type: ignore[arg-type] # Vendored version of setuptools supports PathLike\n            *args,\n            **kw,\n        )\n\n    def _convert_pyx_sources_to_lang(self):\n        \"\"\"\n        Replace sources with .pyx extensions to sources with the target\n        language extension. This mechanism allows language authors to supply\n        pre-converted sources but to prefer the .pyx sources.\n        \"\"\"\n        if _have_cython():\n            # the build has Cython, so allow it to compile the .pyx files\n            return\n        lang = self.language or ''\n        target_ext = '.cpp' if lang.lower() == 'c++' else '.c'\n        sub = functools.partial(re.sub, '.pyx$', target_ext)\n        self.sources = list(map(sub, self.sources))\n\n\nclass Library(Extension):\n    \"\"\"Just like a regular Extension, but built as a library instead\"\"\"\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/glob.py","size":6062,"sha1":"f1af717c7c25eef8903b854586ca0fe0e760ae36","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"\nFilename globbing utility. Mostly a copy of `glob` from Python 3.5.\n\nChanges include:\n * `yield from` and PEP3102 `*` removed.\n * Hidden files are not ignored.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport fnmatch\nimport os\nimport re\nfrom collections.abc import Iterable, Iterator\nfrom typing import TYPE_CHECKING, AnyStr, overload\n\nif TYPE_CHECKING:\n    from _typeshed import BytesPath, StrOrBytesPath, StrPath\n\n__all__ = [\"glob\", \"iglob\", \"escape\"]\n\n\ndef glob(pathname: AnyStr, recursive: bool = False) -> list[AnyStr]:\n    \"\"\"Return a list of paths matching a pathname pattern.\n\n    The pattern may contain simple shell-style wildcards a la\n    fnmatch. However, unlike fnmatch, filenames starting with a\n    dot are special cases that are not matched by '*' and '?'\n    patterns.\n\n    If recursive is true, the pattern '**' will match any files and\n    zero or more directories and subdirectories.\n    \"\"\"\n    return list(iglob(pathname, recursive=recursive))\n\n\ndef iglob(pathname: AnyStr, recursive: bool = False) -> Iterator[AnyStr]:\n    \"\"\"Return an iterator which yields the paths matching a pathname pattern.\n\n    The pattern may contain simple shell-style wildcards a la\n    fnmatch. However, unlike fnmatch, filenames starting with a\n    dot are special cases that are not matched by '*' and '?'\n    patterns.\n\n    If recursive is true, the pattern '**' will match any files and\n    zero or more directories and subdirectories.\n    \"\"\"\n    it = _iglob(pathname, recursive)\n    if recursive and _isrecursive(pathname):\n        s = next(it)  # skip empty string\n        assert not s\n    return it\n\n\ndef _iglob(pathname: AnyStr, recursive: bool) -> Iterator[AnyStr]:\n    dirname, basename = os.path.split(pathname)\n    glob_in_dir = glob2 if recursive and _isrecursive(basename) else glob1\n\n    if not has_magic(pathname):\n        if basename:\n            if os.path.lexists(pathname):\n                yield pathname\n        else:\n            # Patterns ending with a slash should match only directories\n            if os.path.isdir(dirname):\n                yield pathname\n        return\n\n    if not dirname:\n        yield from glob_in_dir(dirname, basename)\n        return\n    # `os.path.split()` returns the argument itself as a dirname if it is a\n    # drive or UNC path.  Prevent an infinite recursion if a drive or UNC path\n    # contains magic characters (i.e. r'\\\\?\\C:').\n    if dirname != pathname and has_magic(dirname):\n        dirs: Iterable[AnyStr] = _iglob(dirname, recursive)\n    else:\n        dirs = [dirname]\n    if not has_magic(basename):\n        glob_in_dir = glob0\n    for dirname in dirs:\n        for name in glob_in_dir(dirname, basename):\n            yield os.path.join(dirname, name)\n\n\n# These 2 helper functions non-recursively glob inside a literal directory.\n# They return a list of basenames. `glob1` accepts a pattern while `glob0`\n# takes a literal basename (so it only has to check for its existence).\n\n\n@overload\ndef glob1(dirname: StrPath, pattern: str) -> list[str]: ...\n@overload\ndef glob1(dirname: BytesPath, pattern: bytes) -> list[bytes]: ...\ndef glob1(dirname: StrOrBytesPath, pattern: str | bytes) -> list[str] | list[bytes]:\n    if not dirname:\n        if isinstance(pattern, bytes):\n            dirname = os.curdir.encode('ASCII')\n        else:\n            dirname = os.curdir\n    try:\n        names = os.listdir(dirname)\n    except OSError:\n        return []\n    # mypy false-positives: str or bytes type possibility is always kept in sync\n    return fnmatch.filter(names, pattern)  # type: ignore[type-var, return-value]\n\n\ndef glob0(dirname, basename):\n    if not basename:\n        # `os.path.split()` returns an empty basename for paths ending with a\n        # directory separator.  'q*x/' should match only directories.\n        if os.path.isdir(dirname):\n            return [basename]\n    else:\n        if os.path.lexists(os.path.join(dirname, basename)):\n            return [basename]\n    return []\n\n\n# This helper function recursively yields relative pathnames inside a literal\n# directory.\n\n\n@overload\ndef glob2(dirname: StrPath, pattern: str) -> Iterator[str]: ...\n@overload\ndef glob2(dirname: BytesPath, pattern: bytes) -> Iterator[bytes]: ...\ndef glob2(dirname: StrOrBytesPath, pattern: str | bytes) -> Iterator[str | bytes]:\n    assert _isrecursive(pattern)\n    yield pattern[:0]\n    yield from _rlistdir(dirname)\n\n\n# Recursively yields relative pathnames inside a literal directory.\n@overload\ndef _rlistdir(dirname: StrPath) -> Iterator[str]: ...\n@overload\ndef _rlistdir(dirname: BytesPath) -> Iterator[bytes]: ...\ndef _rlistdir(dirname: StrOrBytesPath) -> Iterator[str | bytes]:\n    if not dirname:\n        if isinstance(dirname, bytes):\n            dirname = os.curdir.encode('ASCII')\n        else:\n            dirname = os.curdir\n    try:\n        names = os.listdir(dirname)\n    except OSError:\n        return\n    for x in names:\n        yield x\n        # mypy false-positives: str or bytes type possibility is always kept in sync\n        path = os.path.join(dirname, x) if dirname else x  # type: ignore[arg-type]\n        for y in _rlistdir(path):\n            yield os.path.join(x, y)  # type: ignore[arg-type]\n\n\nmagic_check = re.compile('([*?[])')\nmagic_check_bytes = re.compile(b'([*?[])')\n\n\ndef has_magic(s: str | bytes) -> bool:\n    if isinstance(s, bytes):\n        return magic_check_bytes.search(s) is not None\n    else:\n        return magic_check.search(s) is not None\n\n\ndef _isrecursive(pattern: str | bytes) -> bool:\n    if isinstance(pattern, bytes):\n        return pattern == b'**'\n    else:\n        return pattern == '**'\n\n\ndef escape(pathname):\n    \"\"\"Escape all special characters.\"\"\"\n    # Escaping is done by wrapping any of \"*?[\" between square brackets.\n    # Metacharacters do not work in the drive part and shouldn't be escaped.\n    drive, pathname = os.path.splitdrive(pathname)\n    if isinstance(pathname, bytes):\n        pathname = magic_check_bytes.sub(rb'[\\1]', pathname)\n    else:\n        pathname = magic_check.sub(r'[\\1]', pathname)\n    return drive + pathname\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/installer.py","size":5110,"sha1":"615db398a50fcdb1a3a39a1e53709d8594edef34","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"from __future__ import annotations\n\nimport glob\nimport os\nimport subprocess\nimport sys\nimport tempfile\nfrom functools import partial\n\nfrom pkg_resources import Distribution\n\nfrom . import _reqs\nfrom ._reqs import _StrOrIter\nfrom .warnings import SetuptoolsDeprecationWarning\nfrom .wheel import Wheel\n\nfrom distutils import log\nfrom distutils.errors import DistutilsError\n\n\ndef _fixup_find_links(find_links):\n    \"\"\"Ensure find-links option end-up being a list of strings.\"\"\"\n    if isinstance(find_links, str):\n        return find_links.split()\n    assert isinstance(find_links, (tuple, list))\n    return find_links\n\n\ndef fetch_build_egg(dist, req):\n    \"\"\"Fetch an egg needed for building.\n\n    Use pip/wheel to fetch/build a wheel.\"\"\"\n    _DeprecatedInstaller.emit()\n    _warn_wheel_not_available(dist)\n    return _fetch_build_egg_no_warn(dist, req)\n\n\ndef _fetch_build_eggs(dist, requires: _StrOrIter) -> list[Distribution]:\n    import pkg_resources  # Delay import to avoid unnecessary side-effects\n\n    _DeprecatedInstaller.emit(stacklevel=3)\n    _warn_wheel_not_available(dist)\n\n    resolved_dists = pkg_resources.working_set.resolve(\n        _reqs.parse(requires, pkg_resources.Requirement),  # required for compatibility\n        installer=partial(_fetch_build_egg_no_warn, dist),  # avoid warning twice\n        replace_conflicting=True,\n    )\n    for dist in resolved_dists:\n        pkg_resources.working_set.add(dist, replace=True)\n    return resolved_dists\n\n\ndef _fetch_build_egg_no_warn(dist, req):  # noqa: C901  # is too complex (16)  # FIXME\n    import pkg_resources  # Delay import to avoid unnecessary side-effects\n\n    # Ignore environment markers; if supplied, it is required.\n    req = strip_marker(req)\n    # Take easy_install options into account, but do not override relevant\n    # pip environment variables (like PIP_INDEX_URL or PIP_QUIET); they'll\n    # take precedence.\n    opts = dist.get_option_dict('easy_install')\n    if 'allow_hosts' in opts:\n        raise DistutilsError(\n            'the `allow-hosts` option is not supported '\n            'when using pip to install requirements.'\n        )\n    quiet = 'PIP_QUIET' not in os.environ and 'PIP_VERBOSE' not in os.environ\n    if 'PIP_INDEX_URL' in os.environ:\n        index_url = None\n    elif 'index_url' in opts:\n        index_url = opts['index_url'][1]\n    else:\n        index_url = None\n    find_links = (\n        _fixup_find_links(opts['find_links'][1])[:] if 'find_links' in opts else []\n    )\n    if dist.dependency_links:\n        find_links.extend(dist.dependency_links)\n    eggs_dir = os.path.realpath(dist.get_egg_cache_dir())\n    environment = pkg_resources.Environment()\n    for egg_dist in pkg_resources.find_distributions(eggs_dir):\n        if egg_dist in req and environment.can_add(egg_dist):\n            return egg_dist\n    with tempfile.TemporaryDirectory() as tmpdir:\n        cmd = [\n            sys.executable,\n            '-m',\n            'pip',\n            '--disable-pip-version-check',\n            'wheel',\n            '--no-deps',\n            '-w',\n            tmpdir,\n        ]\n        if quiet:\n            cmd.append('--quiet')\n        if index_url is not None:\n            cmd.extend(('--index-url', index_url))\n        for link in find_links or []:\n            cmd.extend(('--find-links', link))\n        # If requirement is a PEP 508 direct URL, directly pass\n        # the URL to pip, as `req @ url` does not work on the\n        # command line.\n        cmd.append(req.url or str(req))\n        try:\n            subprocess.check_call(cmd)\n        except subprocess.CalledProcessError as e:\n            raise DistutilsError(str(e)) from e\n        wheel = Wheel(glob.glob(os.path.join(tmpdir, '*.whl'))[0])\n        dist_location = os.path.join(eggs_dir, wheel.egg_name())\n        wheel.install_as_egg(dist_location)\n        dist_metadata = pkg_resources.PathMetadata(\n            dist_location, os.path.join(dist_location, 'EGG-INFO')\n        )\n        return pkg_resources.Distribution.from_filename(\n            dist_location, metadata=dist_metadata\n        )\n\n\ndef strip_marker(req):\n    \"\"\"\n    Return a new requirement without the environment marker to avoid\n    calling pip with something like `babel; extra == \"i18n\"`, which\n    would always be ignored.\n    \"\"\"\n    import pkg_resources  # Delay import to avoid unnecessary side-effects\n\n    # create a copy to avoid mutating the input\n    req = pkg_resources.Requirement.parse(str(req))\n    req.marker = None\n    return req\n\n\ndef _warn_wheel_not_available(dist):\n    import pkg_resources  # Delay import to avoid unnecessary side-effects\n\n    try:\n        pkg_resources.get_distribution('wheel')\n    except pkg_resources.DistributionNotFound:\n        dist.announce('WARNING: The wheel package is not available.', log.WARN)\n\n\nclass _DeprecatedInstaller(SetuptoolsDeprecationWarning):\n    _SUMMARY = \"setuptools.installer and fetch_build_eggs are deprecated.\"\n    _DETAILS = \"\"\"\n    Requirements should be satisfied by a PEP 517 installer.\n    If you are using pip, you can try `pip install --use-pep517`.\n    \"\"\"\n    # _DUE_DATE not decided yet\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/launch.py","size":820,"sha1":"fcf5c2073fb9cf65649e399dea6ee965e4ad295c","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"\nLaunch the Python script on the command line after\nsetuptools is bootstrapped via import.\n\"\"\"\n\n# Note that setuptools gets imported implicitly by the\n# invocation of this script using python -m setuptools.launch\n\nimport sys\nimport tokenize\n\n\ndef run() -> None:\n    \"\"\"\n    Run the script in sys.argv[1] as if it had\n    been invoked naturally.\n    \"\"\"\n    __builtins__\n    script_name = sys.argv[1]\n    namespace = dict(\n        __file__=script_name,\n        __name__='__main__',\n        __doc__=None,\n    )\n    sys.argv[:] = sys.argv[1:]\n\n    open_ = getattr(tokenize, 'open', open)\n    with open_(script_name) as fid:\n        script = fid.read()\n    norm_script = script.replace('\\\\r\\\\n', '\\\\n')\n    code = compile(norm_script, script_name, 'exec')\n    exec(code, namespace)\n\n\nif __name__ == '__main__':\n    run()\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/logging.py","size":1261,"sha1":"1ae5c8f8243b68e3ae4f09c038f9877e42db594f","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import inspect\nimport logging\nimport sys\n\nfrom . import monkey\n\nimport distutils.log\n\n\ndef _not_warning(record):\n    return record.levelno < logging.WARNING\n\n\ndef configure() -> None:\n    \"\"\"\n    Configure logging to emit warning and above to stderr\n    and everything else to stdout. This behavior is provided\n    for compatibility with distutils.log but may change in\n    the future.\n    \"\"\"\n    err_handler = logging.StreamHandler()\n    err_handler.setLevel(logging.WARNING)\n    out_handler = logging.StreamHandler(sys.stdout)\n    out_handler.addFilter(_not_warning)\n    handlers = err_handler, out_handler\n    logging.basicConfig(\n        format=\"{message}\", style='{', handlers=handlers, level=logging.DEBUG\n    )\n    if inspect.ismodule(distutils.dist.log):\n        monkey.patch_func(set_threshold, distutils.log, 'set_threshold')\n        # For some reason `distutils.log` module is getting cached in `distutils.dist`\n        # and then loaded again when patched,\n        # implying: id(distutils.log) != id(distutils.dist.log).\n        # Make sure the same module object is used everywhere:\n        distutils.dist.log = distutils.log\n\n\ndef set_threshold(level: int) -> int:\n    logging.root.setLevel(level * 10)\n    return set_threshold.unpatched(level)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/modified.py","size":568,"sha1":"44046e21b4254f74db5b1b06e906859fdd0b21f7","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"try:\n    # Ensure a DistutilsError raised by these methods is the same as distutils.errors.DistutilsError\n    from distutils._modified import (\n        newer,\n        newer_group,\n        newer_pairwise,\n        newer_pairwise_group,\n    )\nexcept ImportError:\n    # fallback for SETUPTOOLS_USE_DISTUTILS=stdlib, because _modified never existed in stdlib\n    from ._distutils._modified import (\n        newer,\n        newer_group,\n        newer_pairwise,\n        newer_pairwise_group,\n    )\n\n__all__ = ['newer', 'newer_pairwise', 'newer_group', 'newer_pairwise_group']\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/monkey.py","size":3717,"sha1":"bef46f89b480c5f62a80fcb7b5fefe9741b0be9e","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"\nMonkey patching of distutils.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport inspect\nimport platform\nimport sys\nimport types\nfrom typing import TypeVar, cast, overload\n\nimport distutils.filelist\n\n_T = TypeVar(\"_T\")\n_UnpatchT = TypeVar(\"_UnpatchT\", type, types.FunctionType)\n\n\n__all__: list[str] = []\n\"\"\"\nEverything is private. Contact the project team\nif you think you need this functionality.\n\"\"\"\n\n\ndef _get_mro(cls):\n    \"\"\"\n    Returns the bases classes for cls sorted by the MRO.\n\n    Works around an issue on Jython where inspect.getmro will not return all\n    base classes if multiple classes share the same name. Instead, this\n    function will return a tuple containing the class itself, and the contents\n    of cls.__bases__. See https://github.com/pypa/setuptools/issues/1024.\n    \"\"\"\n    if platform.python_implementation() == \"Jython\":\n        return (cls,) + cls.__bases__\n    return inspect.getmro(cls)\n\n\n@overload\ndef get_unpatched(item: _UnpatchT) -> _UnpatchT: ...\n@overload\ndef get_unpatched(item: object) -> None: ...\ndef get_unpatched(\n    item: type | types.FunctionType | object,\n) -> type | types.FunctionType | None:\n    if isinstance(item, type):\n        return get_unpatched_class(item)\n    if isinstance(item, types.FunctionType):\n        return get_unpatched_function(item)\n    return None\n\n\ndef get_unpatched_class(cls: type[_T]) -> type[_T]:\n    \"\"\"Protect against re-patching the distutils if reloaded\n\n    Also ensures that no other distutils extension monkeypatched the distutils\n    first.\n    \"\"\"\n    external_bases = (\n        cast(type[_T], cls)\n        for cls in _get_mro(cls)\n        if not cls.__module__.startswith('setuptools')\n    )\n    base = next(external_bases)\n    if not base.__module__.startswith('distutils'):\n        msg = f\"distutils has already been patched by {cls!r}\"\n        raise AssertionError(msg)\n    return base\n\n\ndef patch_all():\n    import setuptools\n\n    # we can't patch distutils.cmd, alas\n    distutils.core.Command = setuptools.Command  # type: ignore[misc,assignment] # monkeypatching\n\n    _patch_distribution_metadata()\n\n    # Install Distribution throughout the distutils\n    for module in distutils.dist, distutils.core, distutils.cmd:\n        module.Distribution = setuptools.dist.Distribution\n\n    # Install the patched Extension\n    distutils.core.Extension = setuptools.extension.Extension  # type: ignore[misc,assignment] # monkeypatching\n    distutils.extension.Extension = setuptools.extension.Extension  # type: ignore[misc,assignment] # monkeypatching\n    if 'distutils.command.build_ext' in sys.modules:\n        sys.modules[\n            'distutils.command.build_ext'\n        ].Extension = setuptools.extension.Extension\n\n\ndef _patch_distribution_metadata():\n    from . import _core_metadata\n\n    \"\"\"Patch write_pkg_file and read_pkg_file for higher metadata standards\"\"\"\n    for attr in (\n        'write_pkg_info',\n        'write_pkg_file',\n        'read_pkg_file',\n        'get_metadata_version',\n        'get_fullname',\n    ):\n        new_val = getattr(_core_metadata, attr)\n        setattr(distutils.dist.DistributionMetadata, attr, new_val)\n\n\ndef patch_func(replacement, target_mod, func_name):\n    \"\"\"\n    Patch func_name in target_mod with replacement\n\n    Important - original must be resolved by name to avoid\n    patching an already patched function.\n    \"\"\"\n    original = getattr(target_mod, func_name)\n\n    # set the 'unpatched' attribute on the replacement to\n    # point to the original.\n    vars(replacement).setdefault('unpatched', original)\n\n    # replace the function in the original module\n    setattr(target_mod, func_name, replacement)\n\n\ndef get_unpatched_function(candidate):\n    return candidate.unpatched\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/msvc.py","size":41255,"sha1":"8a4cdd322e3192da55a2909bc6318c3bd89a90d2","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"\nEnvironment info about Microsoft Compilers.\n\n>>> getfixture('windows_only')\n>>> ei = EnvironmentInfo('amd64')\n\"\"\"\n\nfrom __future__ import annotations\n\nimport contextlib\nimport itertools\nimport json\nimport os\nimport os.path\nimport platform\nfrom typing import TYPE_CHECKING, TypedDict\n\nfrom more_itertools import unique_everseen\n\nimport distutils.errors\n\nif TYPE_CHECKING:\n    from typing_extensions import LiteralString, NotRequired\n\n# https://github.com/python/mypy/issues/8166\nif not TYPE_CHECKING and platform.system() == 'Windows':\n    import winreg\n    from os import environ\nelse:\n    # Mock winreg and environ so the module can be imported on this platform.\n\n    class winreg:\n        HKEY_USERS = None\n        HKEY_CURRENT_USER = None\n        HKEY_LOCAL_MACHINE = None\n        HKEY_CLASSES_ROOT = None\n\n    environ: dict[str, str] = dict()\n\n\nclass PlatformInfo:\n    \"\"\"\n    Current and Target Architectures information.\n\n    Parameters\n    ----------\n    arch: str\n        Target architecture.\n    \"\"\"\n\n    current_cpu = environ.get('processor_architecture', '').lower()\n\n    def __init__(self, arch) -> None:\n        self.arch = arch.lower().replace('x64', 'amd64')\n\n    @property\n    def target_cpu(self):\n        \"\"\"\n        Return Target CPU architecture.\n\n        Return\n        ------\n        str\n            Target CPU\n        \"\"\"\n        return self.arch[self.arch.find('_') + 1 :]\n\n    def target_is_x86(self):\n        \"\"\"\n        Return True if target CPU is x86 32 bits..\n\n        Return\n        ------\n        bool\n            CPU is x86 32 bits\n        \"\"\"\n        return self.target_cpu == 'x86'\n\n    def current_is_x86(self):\n        \"\"\"\n        Return True if current CPU is x86 32 bits..\n\n        Return\n        ------\n        bool\n            CPU is x86 32 bits\n        \"\"\"\n        return self.current_cpu == 'x86'\n\n    def current_dir(self, hidex86=False, x64=False) -> str:\n        \"\"\"\n        Current platform specific subfolder.\n\n        Parameters\n        ----------\n        hidex86: bool\n            return '' and not '\\x86' if architecture is x86.\n        x64: bool\n            return '\\x64' and not '\\amd64' if architecture is amd64.\n\n        Return\n        ------\n        str\n            subfolder: '\\target', or '' (see hidex86 parameter)\n        \"\"\"\n        return (\n            ''\n            if (self.current_cpu == 'x86' and hidex86)\n            else r'\\x64'\n            if (self.current_cpu == 'amd64' and x64)\n            else rf'\\{self.current_cpu}'\n        )\n\n    def target_dir(self, hidex86=False, x64=False) -> str:\n        r\"\"\"\n        Target platform specific subfolder.\n\n        Parameters\n        ----------\n        hidex86: bool\n            return '' and not '\\x86' if architecture is x86.\n        x64: bool\n            return '\\x64' and not '\\amd64' if architecture is amd64.\n\n        Return\n        ------\n        str\n            subfolder: '\\current', or '' (see hidex86 parameter)\n        \"\"\"\n        return (\n            ''\n            if (self.target_cpu == 'x86' and hidex86)\n            else r'\\x64'\n            if (self.target_cpu == 'amd64' and x64)\n            else rf'\\{self.target_cpu}'\n        )\n\n    def cross_dir(self, forcex86=False):\n        r\"\"\"\n        Cross platform specific subfolder.\n\n        Parameters\n        ----------\n        forcex86: bool\n            Use 'x86' as current architecture even if current architecture is\n            not x86.\n\n        Return\n        ------\n        str\n            subfolder: '' if target architecture is current architecture,\n            '\\current_target' if not.\n        \"\"\"\n        current = 'x86' if forcex86 else self.current_cpu\n        return (\n            ''\n            if self.target_cpu == current\n            else self.target_dir().replace('\\\\', f'\\\\{current}_')\n        )\n\n\nclass RegistryInfo:\n    \"\"\"\n    Microsoft Visual Studio related registry information.\n\n    Parameters\n    ----------\n    platform_info: PlatformInfo\n        \"PlatformInfo\" instance.\n    \"\"\"\n\n    HKEYS = (\n        winreg.HKEY_USERS,\n        winreg.HKEY_CURRENT_USER,\n        winreg.HKEY_LOCAL_MACHINE,\n        winreg.HKEY_CLASSES_ROOT,\n    )\n\n    def __init__(self, platform_info) -> None:\n        self.pi = platform_info\n\n    @property\n    def visualstudio(self) -> str:\n        \"\"\"\n        Microsoft Visual Studio root registry key.\n\n        Return\n        ------\n        str\n            Registry key\n        \"\"\"\n        return 'VisualStudio'\n\n    @property\n    def sxs(self):\n        \"\"\"\n        Microsoft Visual Studio SxS registry key.\n\n        Return\n        ------\n        str\n            Registry key\n        \"\"\"\n        return os.path.join(self.visualstudio, 'SxS')\n\n    @property\n    def vc(self):\n        \"\"\"\n        Microsoft Visual C++ VC7 registry key.\n\n        Return\n        ------\n        str\n            Registry key\n        \"\"\"\n        return os.path.join(self.sxs, 'VC7')\n\n    @property\n    def vs(self):\n        \"\"\"\n        Microsoft Visual Studio VS7 registry key.\n\n        Return\n        ------\n        str\n            Registry key\n        \"\"\"\n        return os.path.join(self.sxs, 'VS7')\n\n    @property\n    def vc_for_python(self) -> str:\n        \"\"\"\n        Microsoft Visual C++ for Python registry key.\n\n        Return\n        ------\n        str\n            Registry key\n        \"\"\"\n        return r'DevDiv\\VCForPython'\n\n    @property\n    def microsoft_sdk(self) -> str:\n        \"\"\"\n        Microsoft SDK registry key.\n\n        Return\n        ------\n        str\n            Registry key\n        \"\"\"\n        return 'Microsoft SDKs'\n\n    @property\n    def windows_sdk(self):\n        \"\"\"\n        Microsoft Windows/Platform SDK registry key.\n\n        Return\n        ------\n        str\n            Registry key\n        \"\"\"\n        return os.path.join(self.microsoft_sdk, 'Windows')\n\n    @property\n    def netfx_sdk(self):\n        \"\"\"\n        Microsoft .NET Framework SDK registry key.\n\n        Return\n        ------\n        str\n            Registry key\n        \"\"\"\n        return os.path.join(self.microsoft_sdk, 'NETFXSDK')\n\n    @property\n    def windows_kits_roots(self) -> str:\n        \"\"\"\n        Microsoft Windows Kits Roots registry key.\n\n        Return\n        ------\n        str\n            Registry key\n        \"\"\"\n        return r'Windows Kits\\Installed Roots'\n\n    def microsoft(self, key, x86=False):\n        \"\"\"\n        Return key in Microsoft software registry.\n\n        Parameters\n        ----------\n        key: str\n            Registry key path where look.\n        x86: str\n            Force x86 software registry.\n\n        Return\n        ------\n        str\n            Registry key\n        \"\"\"\n        node64 = '' if self.pi.current_is_x86() or x86 else 'Wow6432Node'\n        return os.path.join('Software', node64, 'Microsoft', key)\n\n    def lookup(self, key, name):\n        \"\"\"\n        Look for values in registry in Microsoft software registry.\n\n        Parameters\n        ----------\n        key: str\n            Registry key path where look.\n        name: str\n            Value name to find.\n\n        Return\n        ------\n        str\n            value\n        \"\"\"\n        key_read = winreg.KEY_READ\n        openkey = winreg.OpenKey\n        closekey = winreg.CloseKey\n        ms = self.microsoft\n        for hkey in self.HKEYS:\n            bkey = None\n            try:\n                bkey = openkey(hkey, ms(key), 0, key_read)\n            except OSError:\n                if not self.pi.current_is_x86():\n                    try:\n                        bkey = openkey(hkey, ms(key, True), 0, key_read)\n                    except OSError:\n                        continue\n                else:\n                    continue\n            try:\n                return winreg.QueryValueEx(bkey, name)[0]\n            except OSError:\n                pass\n            finally:\n                if bkey:\n                    closekey(bkey)\n        return None\n\n\nclass SystemInfo:\n    \"\"\"\n    Microsoft Windows and Visual Studio related system information.\n\n    Parameters\n    ----------\n    registry_info: RegistryInfo\n        \"RegistryInfo\" instance.\n    vc_ver: float\n        Required Microsoft Visual C++ version.\n    \"\"\"\n\n    # Variables and properties in this class use originals CamelCase variables\n    # names from Microsoft source files for more easy comparison.\n    WinDir = environ.get('WinDir', '')\n    ProgramFiles = environ.get('ProgramFiles', '')\n    ProgramFilesx86 = environ.get('ProgramFiles(x86)', ProgramFiles)\n\n    def __init__(self, registry_info, vc_ver=None) -> None:\n        self.ri = registry_info\n        self.pi = self.ri.pi\n\n        self.known_vs_paths = self.find_programdata_vs_vers()\n\n        # Except for VS15+, VC version is aligned with VS version\n        self.vs_ver = self.vc_ver = vc_ver or self._find_latest_available_vs_ver()\n\n    def _find_latest_available_vs_ver(self):\n        \"\"\"\n        Find the latest VC version\n\n        Return\n        ------\n        float\n            version\n        \"\"\"\n        reg_vc_vers = self.find_reg_vs_vers()\n\n        if not (reg_vc_vers or self.known_vs_paths):\n            raise distutils.errors.DistutilsPlatformError(\n                'No Microsoft Visual C++ version found'\n            )\n\n        vc_vers = set(reg_vc_vers)\n        vc_vers.update(self.known_vs_paths)\n        return sorted(vc_vers)[-1]\n\n    def find_reg_vs_vers(self):\n        \"\"\"\n        Find Microsoft Visual Studio versions available in registry.\n\n        Return\n        ------\n        list of float\n            Versions\n        \"\"\"\n        ms = self.ri.microsoft\n        vckeys = (self.ri.vc, self.ri.vc_for_python, self.ri.vs)\n        vs_vers = []\n        for hkey, key in itertools.product(self.ri.HKEYS, vckeys):\n            try:\n                bkey = winreg.OpenKey(hkey, ms(key), 0, winreg.KEY_READ)\n            except OSError:\n                continue\n            with bkey:\n                subkeys, values, _ = winreg.QueryInfoKey(bkey)\n                for i in range(values):\n                    with contextlib.suppress(ValueError):\n                        ver = float(winreg.EnumValue(bkey, i)[0])\n                        if ver not in vs_vers:\n                            vs_vers.append(ver)\n                for i in range(subkeys):\n                    with contextlib.suppress(ValueError):\n                        ver = float(winreg.EnumKey(bkey, i))\n                        if ver not in vs_vers:\n                            vs_vers.append(ver)\n        return sorted(vs_vers)\n\n    def find_programdata_vs_vers(self) -> dict[float, str]:\n        r\"\"\"\n        Find Visual studio 2017+ versions from information in\n        \"C:\\ProgramData\\Microsoft\\VisualStudio\\Packages\\_Instances\".\n\n        Return\n        ------\n        dict\n            float version as key, path as value.\n        \"\"\"\n        vs_versions: dict[float, str] = {}\n        instances_dir = r'C:\\ProgramData\\Microsoft\\VisualStudio\\Packages\\_Instances'\n\n        try:\n            hashed_names = os.listdir(instances_dir)\n\n        except OSError:\n            # Directory not exists with all Visual Studio versions\n            return vs_versions\n\n        for name in hashed_names:\n            try:\n                # Get VS installation path from \"state.json\" file\n                state_path = os.path.join(instances_dir, name, 'state.json')\n                with open(state_path, 'rt', encoding='utf-8') as state_file:\n                    state = json.load(state_file)\n                vs_path = state['installationPath']\n\n                # Raises OSError if this VS installation does not contain VC\n                os.listdir(os.path.join(vs_path, r'VC\\Tools\\MSVC'))\n\n                # Store version and path\n                vs_versions[self._as_float_version(state['installationVersion'])] = (\n                    vs_path\n                )\n\n            except (OSError, KeyError):\n                # Skip if \"state.json\" file is missing or bad format\n                continue\n\n        return vs_versions\n\n    @staticmethod\n    def _as_float_version(version):\n        \"\"\"\n        Return a string version as a simplified float version (major.minor)\n\n        Parameters\n        ----------\n        version: str\n            Version.\n\n        Return\n        ------\n        float\n            version\n        \"\"\"\n        return float('.'.join(version.split('.')[:2]))\n\n    @property\n    def VSInstallDir(self):\n        \"\"\"\n        Microsoft Visual Studio directory.\n\n        Return\n        ------\n        str\n            path\n        \"\"\"\n        # Default path\n        default = os.path.join(\n            self.ProgramFilesx86, f'Microsoft Visual Studio {self.vs_ver:0.1f}'\n        )\n\n        # Try to get path from registry, if fail use default path\n        return self.ri.lookup(self.ri.vs, f'{self.vs_ver:0.1f}') or default\n\n    @property\n    def VCInstallDir(self):\n        \"\"\"\n        Microsoft Visual C++ directory.\n\n        Return\n        ------\n        str\n            path\n        \"\"\"\n        path = self._guess_vc() or self._guess_vc_legacy()\n\n        if not os.path.isdir(path):\n            msg = 'Microsoft Visual C++ directory not found'\n            raise distutils.errors.DistutilsPlatformError(msg)\n\n        return path\n\n    def _guess_vc(self):\n        \"\"\"\n        Locate Visual C++ for VS2017+.\n\n        Return\n        ------\n        str\n            path\n        \"\"\"\n        if self.vs_ver <= 14.0:\n            return ''\n\n        try:\n            # First search in known VS paths\n            vs_dir = self.known_vs_paths[self.vs_ver]\n        except KeyError:\n            # Else, search with path from registry\n            vs_dir = self.VSInstallDir\n\n        guess_vc = os.path.join(vs_dir, r'VC\\Tools\\MSVC')\n\n        # Subdir with VC exact version as name\n        try:\n            # Update the VC version with real one instead of VS version\n            vc_ver = os.listdir(guess_vc)[-1]\n            self.vc_ver = self._as_float_version(vc_ver)\n            return os.path.join(guess_vc, vc_ver)\n        except (OSError, IndexError):\n            return ''\n\n    def _guess_vc_legacy(self):\n        \"\"\"\n        Locate Visual C++ for versions prior to 2017.\n\n        Return\n        ------\n        str\n            path\n        \"\"\"\n        default = os.path.join(\n            self.ProgramFilesx86,\n            rf'Microsoft Visual Studio {self.vs_ver:0.1f}\\VC',\n        )\n\n        # Try to get \"VC++ for Python\" path from registry as default path\n        reg_path = os.path.join(self.ri.vc_for_python, f'{self.vs_ver:0.1f}')\n        python_vc = self.ri.lookup(reg_path, 'installdir')\n        default_vc = os.path.join(python_vc, 'VC') if python_vc else default\n\n        # Try to get path from registry, if fail use default path\n        return self.ri.lookup(self.ri.vc, f'{self.vs_ver:0.1f}') or default_vc\n\n    @property\n    def WindowsSdkVersion(self) -> tuple[LiteralString, ...]:\n        \"\"\"\n        Microsoft Windows SDK versions for specified MSVC++ version.\n\n        Return\n        ------\n        tuple of str\n            versions\n        \"\"\"\n        if self.vs_ver <= 9.0:\n            return '7.0', '6.1', '6.0a'\n        elif self.vs_ver == 10.0:\n            return '7.1', '7.0a'\n        elif self.vs_ver == 11.0:\n            return '8.0', '8.0a'\n        elif self.vs_ver == 12.0:\n            return '8.1', '8.1a'\n        elif self.vs_ver >= 14.0:\n            return '10.0', '8.1'\n        return ()\n\n    @property\n    def WindowsSdkLastVersion(self):\n        \"\"\"\n        Microsoft Windows SDK last version.\n\n        Return\n        ------\n        str\n            version\n        \"\"\"\n        return self._use_last_dir_name(os.path.join(self.WindowsSdkDir, 'lib'))\n\n    @property\n    def WindowsSdkDir(self) -> str | None:  # noqa: C901  # is too complex (12)  # FIXME\n        \"\"\"\n        Microsoft Windows SDK directory.\n\n        Return\n        ------\n        str\n            path\n        \"\"\"\n        sdkdir: str | None = ''\n        for ver in self.WindowsSdkVersion:\n            # Try to get it from registry\n            loc = os.path.join(self.ri.windows_sdk, f'v{ver}')\n            sdkdir = self.ri.lookup(loc, 'installationfolder')\n            if sdkdir:\n                break\n        if not sdkdir or not os.path.isdir(sdkdir):\n            # Try to get \"VC++ for Python\" version from registry\n            path = os.path.join(self.ri.vc_for_python, f'{self.vc_ver:0.1f}')\n            install_base = self.ri.lookup(path, 'installdir')\n            if install_base:\n                sdkdir = os.path.join(install_base, 'WinSDK')\n        if not sdkdir or not os.path.isdir(sdkdir):\n            # If fail, use default new path\n            for ver in self.WindowsSdkVersion:\n                intver = ver[: ver.rfind('.')]\n                path = rf'Microsoft SDKs\\Windows Kits\\{intver}'\n                d = os.path.join(self.ProgramFiles, path)\n                if os.path.isdir(d):\n                    sdkdir = d\n        if not sdkdir or not os.path.isdir(sdkdir):\n            # If fail, use default old path\n            for ver in self.WindowsSdkVersion:\n                path = rf'Microsoft SDKs\\Windows\\v{ver}'\n                d = os.path.join(self.ProgramFiles, path)\n                if os.path.isdir(d):\n                    sdkdir = d\n        if not sdkdir:\n            # If fail, use Platform SDK\n            sdkdir = os.path.join(self.VCInstallDir, 'PlatformSDK')\n        return sdkdir\n\n    @property\n    def WindowsSDKExecutablePath(self):\n        \"\"\"\n        Microsoft Windows SDK executable directory.\n\n        Return\n        ------\n        str\n            path\n        \"\"\"\n        # Find WinSDK NetFx Tools registry dir name\n        if self.vs_ver <= 11.0:\n            netfxver = 35\n            arch = ''\n        else:\n            netfxver = 40\n            hidex86 = True if self.vs_ver <= 12.0 else False\n            arch = self.pi.current_dir(x64=True, hidex86=hidex86).replace('\\\\', '-')\n        fx = f'WinSDK-NetFx{netfxver}Tools{arch}'\n\n        # list all possibles registry paths\n        regpaths = []\n        if self.vs_ver >= 14.0:\n            for ver in self.NetFxSdkVersion:\n                regpaths += [os.path.join(self.ri.netfx_sdk, ver, fx)]\n\n        for ver in self.WindowsSdkVersion:\n            regpaths += [os.path.join(self.ri.windows_sdk, f'v{ver}A', fx)]\n\n        # Return installation folder from the more recent path\n        for path in regpaths:\n            execpath = self.ri.lookup(path, 'installationfolder')\n            if execpath:\n                return execpath\n\n        return None\n\n    @property\n    def FSharpInstallDir(self):\n        \"\"\"\n        Microsoft Visual F# directory.\n\n        Return\n        ------\n        str\n            path\n        \"\"\"\n        path = os.path.join(self.ri.visualstudio, rf'{self.vs_ver:0.1f}\\Setup\\F#')\n        return self.ri.lookup(path, 'productdir') or ''\n\n    @property\n    def UniversalCRTSdkDir(self):\n        \"\"\"\n        Microsoft Universal CRT SDK directory.\n\n        Return\n        ------\n        str\n            path\n        \"\"\"\n        # Set Kit Roots versions for specified MSVC++ version\n        vers = ('10', '81') if self.vs_ver >= 14.0 else ()\n\n        # Find path of the more recent Kit\n        for ver in vers:\n            sdkdir = self.ri.lookup(self.ri.windows_kits_roots, f'kitsroot{ver}')\n            if sdkdir:\n                return sdkdir or ''\n\n        return None\n\n    @property\n    def UniversalCRTSdkLastVersion(self):\n        \"\"\"\n        Microsoft Universal C Runtime SDK last version.\n\n        Return\n        ------\n        str\n            version\n        \"\"\"\n        return self._use_last_dir_name(os.path.join(self.UniversalCRTSdkDir, 'lib'))\n\n    @property\n    def NetFxSdkVersion(self):\n        \"\"\"\n        Microsoft .NET Framework SDK versions.\n\n        Return\n        ------\n        tuple of str\n            versions\n        \"\"\"\n        # Set FxSdk versions for specified VS version\n        return (\n            ('4.7.2', '4.7.1', '4.7', '4.6.2', '4.6.1', '4.6', '4.5.2', '4.5.1', '4.5')\n            if self.vs_ver >= 14.0\n            else ()\n        )\n\n    @property\n    def NetFxSdkDir(self):\n        \"\"\"\n        Microsoft .NET Framework SDK directory.\n\n        Return\n        ------\n        str\n            path\n        \"\"\"\n        sdkdir = ''\n        for ver in self.NetFxSdkVersion:\n            loc = os.path.join(self.ri.netfx_sdk, ver)\n            sdkdir = self.ri.lookup(loc, 'kitsinstallationfolder')\n            if sdkdir:\n                break\n        return sdkdir\n\n    @property\n    def FrameworkDir32(self):\n        \"\"\"\n        Microsoft .NET Framework 32bit directory.\n\n        Return\n        ------\n        str\n            path\n        \"\"\"\n        # Default path\n        guess_fw = os.path.join(self.WinDir, r'Microsoft.NET\\Framework')\n\n        # Try to get path from registry, if fail use default path\n        return self.ri.lookup(self.ri.vc, 'frameworkdir32') or guess_fw\n\n    @property\n    def FrameworkDir64(self):\n        \"\"\"\n        Microsoft .NET Framework 64bit directory.\n\n        Return\n        ------\n        str\n            path\n        \"\"\"\n        # Default path\n        guess_fw = os.path.join(self.WinDir, r'Microsoft.NET\\Framework64')\n\n        # Try to get path from registry, if fail use default path\n        return self.ri.lookup(self.ri.vc, 'frameworkdir64') or guess_fw\n\n    @property\n    def FrameworkVersion32(self) -> tuple[str, ...]:\n        \"\"\"\n        Microsoft .NET Framework 32bit versions.\n\n        Return\n        ------\n        tuple of str\n            versions\n        \"\"\"\n        return self._find_dot_net_versions(32)\n\n    @property\n    def FrameworkVersion64(self) -> tuple[str, ...]:\n        \"\"\"\n        Microsoft .NET Framework 64bit versions.\n\n        Return\n        ------\n        tuple of str\n            versions\n        \"\"\"\n        return self._find_dot_net_versions(64)\n\n    def _find_dot_net_versions(self, bits) -> tuple[str, ...]:\n        \"\"\"\n        Find Microsoft .NET Framework versions.\n\n        Parameters\n        ----------\n        bits: int\n            Platform number of bits: 32 or 64.\n\n        Return\n        ------\n        tuple of str\n            versions\n        \"\"\"\n        # Find actual .NET version in registry\n        reg_ver = self.ri.lookup(self.ri.vc, f'frameworkver{bits}')\n        dot_net_dir = getattr(self, f'FrameworkDir{bits}')\n        ver = reg_ver or self._use_last_dir_name(dot_net_dir, 'v') or ''\n\n        # Set .NET versions for specified MSVC++ version\n        if self.vs_ver >= 12.0:\n            return ver, 'v4.0'\n        elif self.vs_ver >= 10.0:\n            return 'v4.0.30319' if ver.lower()[:2] != 'v4' else ver, 'v3.5'\n        elif self.vs_ver == 9.0:\n            return 'v3.5', 'v2.0.50727'\n        elif self.vs_ver == 8.0:\n            return 'v3.0', 'v2.0.50727'\n        return ()\n\n    @staticmethod\n    def _use_last_dir_name(path, prefix=''):\n        \"\"\"\n        Return name of the last dir in path or '' if no dir found.\n\n        Parameters\n        ----------\n        path: str\n            Use dirs in this path\n        prefix: str\n            Use only dirs starting by this prefix\n\n        Return\n        ------\n        str\n            name\n        \"\"\"\n        matching_dirs = (\n            dir_name\n            for dir_name in reversed(os.listdir(path))\n            if os.path.isdir(os.path.join(path, dir_name))\n            and dir_name.startswith(prefix)\n        )\n        return next(matching_dirs, None) or ''\n\n\nclass _EnvironmentDict(TypedDict):\n    include: str\n    lib: str\n    libpath: str\n    path: str\n    py_vcruntime_redist: NotRequired[str | None]\n\n\nclass EnvironmentInfo:\n    \"\"\"\n    Return environment variables for specified Microsoft Visual C++ version\n    and platform : Lib, Include, Path and libpath.\n\n    This function is compatible with Microsoft Visual C++ 9.0 to 14.X.\n\n    Script created by analysing Microsoft environment configuration files like\n    \"vcvars[...].bat\", \"SetEnv.Cmd\", \"vcbuildtools.bat\", ...\n\n    Parameters\n    ----------\n    arch: str\n        Target architecture.\n    vc_ver: float\n        Required Microsoft Visual C++ version. If not set, autodetect the last\n        version.\n    vc_min_ver: float\n        Minimum Microsoft Visual C++ version.\n    \"\"\"\n\n    # Variables and properties in this class use originals CamelCase variables\n    # names from Microsoft source files for more easy comparison.\n\n    def __init__(self, arch, vc_ver=None, vc_min_ver=0) -> None:\n        self.pi = PlatformInfo(arch)\n        self.ri = RegistryInfo(self.pi)\n        self.si = SystemInfo(self.ri, vc_ver)\n\n        if self.vc_ver < vc_min_ver:\n            err = 'No suitable Microsoft Visual C++ version found'\n            raise distutils.errors.DistutilsPlatformError(err)\n\n    @property\n    def vs_ver(self):\n        \"\"\"\n        Microsoft Visual Studio.\n\n        Return\n        ------\n        float\n            version\n        \"\"\"\n        return self.si.vs_ver\n\n    @property\n    def vc_ver(self):\n        \"\"\"\n        Microsoft Visual C++ version.\n\n        Return\n        ------\n        float\n            version\n        \"\"\"\n        return self.si.vc_ver\n\n    @property\n    def VSTools(self):\n        \"\"\"\n        Microsoft Visual Studio Tools.\n\n        Return\n        ------\n        list of str\n            paths\n        \"\"\"\n        paths = [r'Common7\\IDE', r'Common7\\Tools']\n\n        if self.vs_ver >= 14.0:\n            arch_subdir = self.pi.current_dir(hidex86=True, x64=True)\n            paths += [r'Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow']\n            paths += [r'Team Tools\\Performance Tools']\n            paths += [rf'Team Tools\\Performance Tools{arch_subdir}']\n\n        return [os.path.join(self.si.VSInstallDir, path) for path in paths]\n\n    @property\n    def VCIncludes(self):\n        \"\"\"\n        Microsoft Visual C++ & Microsoft Foundation Class Includes.\n\n        Return\n        ------\n        list of str\n            paths\n        \"\"\"\n        return [\n            os.path.join(self.si.VCInstallDir, 'Include'),\n            os.path.join(self.si.VCInstallDir, r'ATLMFC\\Include'),\n        ]\n\n    @property\n    def VCLibraries(self):\n        \"\"\"\n        Microsoft Visual C++ & Microsoft Foundation Class Libraries.\n\n        Return\n        ------\n        list of str\n            paths\n        \"\"\"\n        if self.vs_ver >= 15.0:\n            arch_subdir = self.pi.target_dir(x64=True)\n        else:\n            arch_subdir = self.pi.target_dir(hidex86=True)\n        paths = [f'Lib{arch_subdir}', rf'ATLMFC\\Lib{arch_subdir}']\n\n        if self.vs_ver >= 14.0:\n            paths += [rf'Lib\\store{arch_subdir}']\n\n        return [os.path.join(self.si.VCInstallDir, path) for path in paths]\n\n    @property\n    def VCStoreRefs(self):\n        \"\"\"\n        Microsoft Visual C++ store references Libraries.\n\n        Return\n        ------\n        list of str\n            paths\n        \"\"\"\n        if self.vs_ver < 14.0:\n            return []\n        return [os.path.join(self.si.VCInstallDir, r'Lib\\store\\references')]\n\n    @property\n    def VCTools(self):\n        \"\"\"\n        Microsoft Visual C++ Tools.\n\n        Return\n        ------\n        list of str\n            paths\n        \"\"\"\n        si = self.si\n        tools = [os.path.join(si.VCInstallDir, 'VCPackages')]\n\n        forcex86 = True if self.vs_ver <= 10.0 else False\n        arch_subdir = self.pi.cross_dir(forcex86)\n        if arch_subdir:\n            tools += [os.path.join(si.VCInstallDir, f'Bin{arch_subdir}')]\n\n        if self.vs_ver == 14.0:\n            path = f'Bin{self.pi.current_dir(hidex86=True)}'\n            tools += [os.path.join(si.VCInstallDir, path)]\n\n        elif self.vs_ver >= 15.0:\n            host_dir = (\n                r'bin\\HostX86%s' if self.pi.current_is_x86() else r'bin\\HostX64%s'\n            )\n            tools += [\n                os.path.join(si.VCInstallDir, host_dir % self.pi.target_dir(x64=True))\n            ]\n\n            if self.pi.current_cpu != self.pi.target_cpu:\n                tools += [\n                    os.path.join(\n                        si.VCInstallDir, host_dir % self.pi.current_dir(x64=True)\n                    )\n                ]\n\n        else:\n            tools += [os.path.join(si.VCInstallDir, 'Bin')]\n\n        return tools\n\n    @property\n    def OSLibraries(self):\n        \"\"\"\n        Microsoft Windows SDK Libraries.\n\n        Return\n        ------\n        list of str\n            paths\n        \"\"\"\n        if self.vs_ver <= 10.0:\n            arch_subdir = self.pi.target_dir(hidex86=True, x64=True)\n            return [os.path.join(self.si.WindowsSdkDir, f'Lib{arch_subdir}')]\n\n        else:\n            arch_subdir = self.pi.target_dir(x64=True)\n            lib = os.path.join(self.si.WindowsSdkDir, 'lib')\n            libver = self._sdk_subdir\n            return [os.path.join(lib, f'{libver}um{arch_subdir}')]\n\n    @property\n    def OSIncludes(self):\n        \"\"\"\n        Microsoft Windows SDK Include.\n\n        Return\n        ------\n        list of str\n            paths\n        \"\"\"\n        include = os.path.join(self.si.WindowsSdkDir, 'include')\n\n        if self.vs_ver <= 10.0:\n            return [include, os.path.join(include, 'gl')]\n\n        else:\n            if self.vs_ver >= 14.0:\n                sdkver = self._sdk_subdir\n            else:\n                sdkver = ''\n            return [\n                os.path.join(include, f'{sdkver}shared'),\n                os.path.join(include, f'{sdkver}um'),\n                os.path.join(include, f'{sdkver}winrt'),\n            ]\n\n    @property\n    def OSLibpath(self):\n        \"\"\"\n        Microsoft Windows SDK Libraries Paths.\n\n        Return\n        ------\n        list of str\n            paths\n        \"\"\"\n        ref = os.path.join(self.si.WindowsSdkDir, 'References')\n        libpath = []\n\n        if self.vs_ver <= 9.0:\n            libpath += self.OSLibraries\n\n        if self.vs_ver >= 11.0:\n            libpath += [os.path.join(ref, r'CommonConfiguration\\Neutral')]\n\n        if self.vs_ver >= 14.0:\n            libpath += [\n                ref,\n                os.path.join(self.si.WindowsSdkDir, 'UnionMetadata'),\n                os.path.join(ref, 'Windows.Foundation.UniversalApiContract', '1.0.0.0'),\n                os.path.join(ref, 'Windows.Foundation.FoundationContract', '1.0.0.0'),\n                os.path.join(\n                    ref, 'Windows.Networking.Connectivity.WwanContract', '1.0.0.0'\n                ),\n                os.path.join(\n                    self.si.WindowsSdkDir,\n                    'ExtensionSDKs',\n                    'Microsoft.VCLibs',\n                    f'{self.vs_ver:0.1f}',\n                    'References',\n                    'CommonConfiguration',\n                    'neutral',\n                ),\n            ]\n        return libpath\n\n    @property\n    def SdkTools(self):\n        \"\"\"\n        Microsoft Windows SDK Tools.\n\n        Return\n        ------\n        list of str\n            paths\n        \"\"\"\n        return list(self._sdk_tools())\n\n    def _sdk_tools(self):\n        \"\"\"\n        Microsoft Windows SDK Tools paths generator.\n\n        Return\n        ------\n        generator of str\n            paths\n        \"\"\"\n        if self.vs_ver < 15.0:\n            bin_dir = 'Bin' if self.vs_ver <= 11.0 else r'Bin\\x86'\n            yield os.path.join(self.si.WindowsSdkDir, bin_dir)\n\n        if not self.pi.current_is_x86():\n            arch_subdir = self.pi.current_dir(x64=True)\n            path = f'Bin{arch_subdir}'\n            yield os.path.join(self.si.WindowsSdkDir, path)\n\n        if self.vs_ver in (10.0, 11.0):\n            if self.pi.target_is_x86():\n                arch_subdir = ''\n            else:\n                arch_subdir = self.pi.current_dir(hidex86=True, x64=True)\n            path = rf'Bin\\NETFX 4.0 Tools{arch_subdir}'\n            yield os.path.join(self.si.WindowsSdkDir, path)\n\n        elif self.vs_ver >= 15.0:\n            path = os.path.join(self.si.WindowsSdkDir, 'Bin')\n            arch_subdir = self.pi.current_dir(x64=True)\n            sdkver = self.si.WindowsSdkLastVersion\n            yield os.path.join(path, f'{sdkver}{arch_subdir}')\n\n        if self.si.WindowsSDKExecutablePath:\n            yield self.si.WindowsSDKExecutablePath\n\n    @property\n    def _sdk_subdir(self):\n        \"\"\"\n        Microsoft Windows SDK version subdir.\n\n        Return\n        ------\n        str\n            subdir\n        \"\"\"\n        ucrtver = self.si.WindowsSdkLastVersion\n        return (f'{ucrtver}\\\\') if ucrtver else ''\n\n    @property\n    def SdkSetup(self):\n        \"\"\"\n        Microsoft Windows SDK Setup.\n\n        Return\n        ------\n        list of str\n            paths\n        \"\"\"\n        if self.vs_ver > 9.0:\n            return []\n\n        return [os.path.join(self.si.WindowsSdkDir, 'Setup')]\n\n    @property\n    def FxTools(self):\n        \"\"\"\n        Microsoft .NET Framework Tools.\n\n        Return\n        ------\n        list of str\n            paths\n        \"\"\"\n        pi = self.pi\n        si = self.si\n\n        if self.vs_ver <= 10.0:\n            include32 = True\n            include64 = not pi.target_is_x86() and not pi.current_is_x86()\n        else:\n            include32 = pi.target_is_x86() or pi.current_is_x86()\n            include64 = pi.current_cpu == 'amd64' or pi.target_cpu == 'amd64'\n\n        tools = []\n        if include32:\n            tools += [\n                os.path.join(si.FrameworkDir32, ver) for ver in si.FrameworkVersion32\n            ]\n        if include64:\n            tools += [\n                os.path.join(si.FrameworkDir64, ver) for ver in si.FrameworkVersion64\n            ]\n        return tools\n\n    @property\n    def NetFxSDKLibraries(self):\n        \"\"\"\n        Microsoft .Net Framework SDK Libraries.\n\n        Return\n        ------\n        list of str\n            paths\n        \"\"\"\n        if self.vs_ver < 14.0 or not self.si.NetFxSdkDir:\n            return []\n\n        arch_subdir = self.pi.target_dir(x64=True)\n        return [os.path.join(self.si.NetFxSdkDir, rf'lib\\um{arch_subdir}')]\n\n    @property\n    def NetFxSDKIncludes(self):\n        \"\"\"\n        Microsoft .Net Framework SDK Includes.\n\n        Return\n        ------\n        list of str\n            paths\n        \"\"\"\n        if self.vs_ver < 14.0 or not self.si.NetFxSdkDir:\n            return []\n\n        return [os.path.join(self.si.NetFxSdkDir, r'include\\um')]\n\n    @property\n    def VsTDb(self):\n        \"\"\"\n        Microsoft Visual Studio Team System Database.\n\n        Return\n        ------\n        list of str\n            paths\n        \"\"\"\n        return [os.path.join(self.si.VSInstallDir, r'VSTSDB\\Deploy')]\n\n    @property\n    def MSBuild(self):\n        \"\"\"\n        Microsoft Build Engine.\n\n        Return\n        ------\n        list of str\n            paths\n        \"\"\"\n        if self.vs_ver < 12.0:\n            return []\n        elif self.vs_ver < 15.0:\n            base_path = self.si.ProgramFilesx86\n            arch_subdir = self.pi.current_dir(hidex86=True)\n        else:\n            base_path = self.si.VSInstallDir\n            arch_subdir = ''\n\n        path = rf'MSBuild\\{self.vs_ver:0.1f}\\bin{arch_subdir}'\n        build = [os.path.join(base_path, path)]\n\n        if self.vs_ver >= 15.0:\n            # Add Roslyn C# & Visual Basic Compiler\n            build += [os.path.join(base_path, path, 'Roslyn')]\n\n        return build\n\n    @property\n    def HTMLHelpWorkshop(self):\n        \"\"\"\n        Microsoft HTML Help Workshop.\n\n        Return\n        ------\n        list of str\n            paths\n        \"\"\"\n        if self.vs_ver < 11.0:\n            return []\n\n        return [os.path.join(self.si.ProgramFilesx86, 'HTML Help Workshop')]\n\n    @property\n    def UCRTLibraries(self):\n        \"\"\"\n        Microsoft Universal C Runtime SDK Libraries.\n\n        Return\n        ------\n        list of str\n            paths\n        \"\"\"\n        if self.vs_ver < 14.0:\n            return []\n\n        arch_subdir = self.pi.target_dir(x64=True)\n        lib = os.path.join(self.si.UniversalCRTSdkDir, 'lib')\n        ucrtver = self._ucrt_subdir\n        return [os.path.join(lib, f'{ucrtver}ucrt{arch_subdir}')]\n\n    @property\n    def UCRTIncludes(self):\n        \"\"\"\n        Microsoft Universal C Runtime SDK Include.\n\n        Return\n        ------\n        list of str\n            paths\n        \"\"\"\n        if self.vs_ver < 14.0:\n            return []\n\n        include = os.path.join(self.si.UniversalCRTSdkDir, 'include')\n        return [os.path.join(include, f'{self._ucrt_subdir}ucrt')]\n\n    @property\n    def _ucrt_subdir(self):\n        \"\"\"\n        Microsoft Universal C Runtime SDK version subdir.\n\n        Return\n        ------\n        str\n            subdir\n        \"\"\"\n        ucrtver = self.si.UniversalCRTSdkLastVersion\n        return (f'{ucrtver}\\\\') if ucrtver else ''\n\n    @property\n    def FSharp(self):\n        \"\"\"\n        Microsoft Visual F#.\n\n        Return\n        ------\n        list of str\n            paths\n        \"\"\"\n        if 11.0 > self.vs_ver > 12.0:\n            return []\n\n        return [self.si.FSharpInstallDir]\n\n    @property\n    def VCRuntimeRedist(self) -> str | None:\n        \"\"\"\n        Microsoft Visual C++ runtime redistributable dll.\n\n        Returns the first suitable path found or None.\n        \"\"\"\n        vcruntime = f'vcruntime{self.vc_ver}0.dll'\n        arch_subdir = self.pi.target_dir(x64=True).strip('\\\\')\n\n        # Installation prefixes candidates\n        prefixes = []\n        tools_path = self.si.VCInstallDir\n        redist_path = os.path.dirname(tools_path.replace(r'\\Tools', r'\\Redist'))\n        if os.path.isdir(redist_path):\n            # Redist version may not be exactly the same as tools\n            redist_path = os.path.join(redist_path, os.listdir(redist_path)[-1])\n            prefixes += [redist_path, os.path.join(redist_path, 'onecore')]\n\n        prefixes += [os.path.join(tools_path, 'redist')]  # VS14 legacy path\n\n        # CRT directory\n        crt_dirs = (\n            f'Microsoft.VC{self.vc_ver * 10}.CRT',\n            # Sometime store in directory with VS version instead of VC\n            f'Microsoft.VC{int(self.vs_ver) * 10}.CRT',\n        )\n\n        # vcruntime path\n        candidate_paths = (\n            os.path.join(prefix, arch_subdir, crt_dir, vcruntime)\n            for (prefix, crt_dir) in itertools.product(prefixes, crt_dirs)\n        )\n        return next(filter(os.path.isfile, candidate_paths), None)  # type: ignore[arg-type] #python/mypy#12682\n\n    def return_env(self, exists: bool = True) -> _EnvironmentDict:\n        \"\"\"\n        Return environment dict.\n\n        Parameters\n        ----------\n        exists: bool\n            It True, only return existing paths.\n\n        Return\n        ------\n        dict\n            environment\n        \"\"\"\n        env = _EnvironmentDict(\n            include=self._build_paths(\n                'include',\n                [\n                    self.VCIncludes,\n                    self.OSIncludes,\n                    self.UCRTIncludes,\n                    self.NetFxSDKIncludes,\n                ],\n                exists,\n            ),\n            lib=self._build_paths(\n                'lib',\n                [\n                    self.VCLibraries,\n                    self.OSLibraries,\n                    self.FxTools,\n                    self.UCRTLibraries,\n                    self.NetFxSDKLibraries,\n                ],\n                exists,\n            ),\n            libpath=self._build_paths(\n                'libpath',\n                [self.VCLibraries, self.FxTools, self.VCStoreRefs, self.OSLibpath],\n                exists,\n            ),\n            path=self._build_paths(\n                'path',\n                [\n                    self.VCTools,\n                    self.VSTools,\n                    self.VsTDb,\n                    self.SdkTools,\n                    self.SdkSetup,\n                    self.FxTools,\n                    self.MSBuild,\n                    self.HTMLHelpWorkshop,\n                    self.FSharp,\n                ],\n                exists,\n            ),\n        )\n        if self.vs_ver >= 14 and self.VCRuntimeRedist:\n            env['py_vcruntime_redist'] = self.VCRuntimeRedist\n        return env\n\n    def _build_paths(self, name, spec_path_lists, exists):\n        \"\"\"\n        Given an environment variable name and specified paths,\n        return a pathsep-separated string of paths containing\n        unique, extant, directories from those paths and from\n        the environment variable. Raise an error if no paths\n        are resolved.\n\n        Parameters\n        ----------\n        name: str\n            Environment variable name\n        spec_path_lists: list of str\n            Paths\n        exists: bool\n            It True, only return existing paths.\n\n        Return\n        ------\n        str\n            Pathsep-separated paths\n        \"\"\"\n        # flatten spec_path_lists\n        spec_paths = itertools.chain.from_iterable(spec_path_lists)\n        env_paths = environ.get(name, '').split(os.pathsep)\n        paths = itertools.chain(spec_paths, env_paths)\n        extant_paths = list(filter(os.path.isdir, paths)) if exists else paths\n        if not extant_paths:\n            msg = f\"{name.upper()} environment variable is empty\"\n            raise distutils.errors.DistutilsPlatformError(msg)\n        unique_paths = unique_everseen(extant_paths)\n        return os.pathsep.join(unique_paths)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/namespaces.py","size":3171,"sha1":"b4be52b70d6911195497fbc9f24eb1969fc479c3","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import itertools\nimport os\n\nfrom .compat import py312\n\nfrom distutils import log\n\nflatten = itertools.chain.from_iterable\n\n\nclass Installer:\n    nspkg_ext = '-nspkg.pth'\n\n    def install_namespaces(self) -> None:\n        nsp = self._get_all_ns_packages()\n        if not nsp:\n            return\n        filename = self._get_nspkg_file()\n        self.outputs.append(filename)\n        log.info(\"Installing %s\", filename)\n        lines = map(self._gen_nspkg_line, nsp)\n\n        if self.dry_run:\n            # always generate the lines, even in dry run\n            list(lines)\n            return\n\n        with open(filename, 'wt', encoding=py312.PTH_ENCODING) as f:\n            # Python<3.13 requires encoding=\"locale\" instead of \"utf-8\"\n            # See: python/cpython#77102\n            f.writelines(lines)\n\n    def uninstall_namespaces(self) -> None:\n        filename = self._get_nspkg_file()\n        if not os.path.exists(filename):\n            return\n        log.info(\"Removing %s\", filename)\n        os.remove(filename)\n\n    def _get_nspkg_file(self):\n        filename, _ = os.path.splitext(self._get_target())\n        return filename + self.nspkg_ext\n\n    def _get_target(self):\n        return self.target\n\n    _nspkg_tmpl = (\n        \"import sys, types, os\",\n        \"p = os.path.join(%(root)s, *%(pth)r)\",\n        \"importlib = __import__('importlib.util')\",\n        \"__import__('importlib.machinery')\",\n        (\n            \"m = \"\n            \"sys.modules.setdefault(%(pkg)r, \"\n            \"importlib.util.module_from_spec(\"\n            \"importlib.machinery.PathFinder.find_spec(%(pkg)r, \"\n            \"[os.path.dirname(p)])))\"\n        ),\n        (\"m = m or sys.modules.setdefault(%(pkg)r, types.ModuleType(%(pkg)r))\"),\n        \"mp = (m or []) and m.__dict__.setdefault('__path__',[])\",\n        \"(p not in mp) and mp.append(p)\",\n    )\n    \"lines for the namespace installer\"\n\n    _nspkg_tmpl_multi = ('m and setattr(sys.modules[%(parent)r], %(child)r, m)',)\n    \"additional line(s) when a parent package is indicated\"\n\n    def _get_root(self):\n        return \"sys._getframe(1).f_locals['sitedir']\"\n\n    def _gen_nspkg_line(self, pkg):\n        pth = tuple(pkg.split('.'))\n        root = self._get_root()\n        tmpl_lines = self._nspkg_tmpl\n        parent, sep, child = pkg.rpartition('.')\n        if parent:\n            tmpl_lines += self._nspkg_tmpl_multi\n        return ';'.join(tmpl_lines) % locals() + '\\n'\n\n    def _get_all_ns_packages(self):\n        \"\"\"Return sorted list of all package namespaces\"\"\"\n        pkgs = self.distribution.namespace_packages or []\n        return sorted(set(flatten(map(self._pkg_names, pkgs))))\n\n    @staticmethod\n    def _pkg_names(pkg):\n        \"\"\"\n        Given a namespace package, yield the components of that\n        package.\n\n        >>> names = Installer._pkg_names('a.b.c')\n        >>> set(names) == set(['a', 'a.b', 'a.b.c'])\n        True\n        \"\"\"\n        parts = pkg.split('.')\n        while parts:\n            yield '.'.join(parts)\n            parts.pop()\n\n\nclass DevelopInstaller(Installer):\n    def _get_root(self):\n        return repr(str(self.egg_path))\n\n    def _get_target(self):\n        return self.egg_link\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/package_index.py","size":39095,"sha1":"de774642dae4701471710c6b95b6dcd389229000","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"PyPI and direct package downloading.\"\"\"\n\nfrom __future__ import annotations\n\nimport base64\nimport configparser\nimport hashlib\nimport html\nimport http.client\nimport io\nimport itertools\nimport os\nimport re\nimport shutil\nimport socket\nimport subprocess\nimport sys\nimport urllib.error\nimport urllib.parse\nimport urllib.request\nfrom fnmatch import translate\nfrom functools import wraps\nfrom typing import NamedTuple\n\nfrom more_itertools import unique_everseen\n\nimport setuptools\nfrom pkg_resources import (\n    BINARY_DIST,\n    CHECKOUT_DIST,\n    DEVELOP_DIST,\n    EGG_DIST,\n    SOURCE_DIST,\n    Distribution,\n    Environment,\n    Requirement,\n    find_distributions,\n    normalize_path,\n    parse_version,\n    safe_name,\n    safe_version,\n    to_filename,\n)\nfrom setuptools.wheel import Wheel\n\nfrom .unicode_utils import _cfg_read_utf8_with_fallback, _read_utf8_with_fallback\n\nfrom distutils import log\nfrom distutils.errors import DistutilsError\n\nEGG_FRAGMENT = re.compile(r'^egg=([-A-Za-z0-9_.+!]+)$')\nHREF = re.compile(r\"\"\"href\\s*=\\s*['\"]?([^'\"> ]+)\"\"\", re.I)\nPYPI_MD5 = re.compile(\n    r'<a href=\"([^\"#]+)\">([^<]+)</a>\\n\\s+\\(<a (?:title=\"MD5 hash\"\\n\\s+)'\n    r'href=\"[^?]+\\?:action=show_md5&amp;digest=([0-9a-f]{32})\">md5</a>\\)'\n)\nURL_SCHEME = re.compile('([-+.a-z0-9]{2,}):', re.I).match\nEXTENSIONS = \".tar.gz .tar.bz2 .tar .zip .tgz\".split()\n\n__all__ = [\n    'PackageIndex',\n    'distros_for_url',\n    'parse_bdist_wininst',\n    'interpret_distro_name',\n]\n\n_SOCKET_TIMEOUT = 15\n\nuser_agent = f\"setuptools/{setuptools.__version__} Python-urllib/{sys.version_info.major}.{sys.version_info.minor}\"\n\n\ndef parse_requirement_arg(spec):\n    try:\n        return Requirement.parse(spec)\n    except ValueError as e:\n        raise DistutilsError(\n            f\"Not a URL, existing file, or requirement spec: {spec!r}\"\n        ) from e\n\n\ndef parse_bdist_wininst(name):\n    \"\"\"Return (base,pyversion) or (None,None) for possible .exe name\"\"\"\n\n    lower = name.lower()\n    base, py_ver, plat = None, None, None\n\n    if lower.endswith('.exe'):\n        if lower.endswith('.win32.exe'):\n            base = name[:-10]\n            plat = 'win32'\n        elif lower.startswith('.win32-py', -16):\n            py_ver = name[-7:-4]\n            base = name[:-16]\n            plat = 'win32'\n        elif lower.endswith('.win-amd64.exe'):\n            base = name[:-14]\n            plat = 'win-amd64'\n        elif lower.startswith('.win-amd64-py', -20):\n            py_ver = name[-7:-4]\n            base = name[:-20]\n            plat = 'win-amd64'\n    return base, py_ver, plat\n\n\ndef egg_info_for_url(url):\n    parts = urllib.parse.urlparse(url)\n    _scheme, server, path, _parameters, _query, fragment = parts\n    base = urllib.parse.unquote(path.split('/')[-1])\n    if server == 'sourceforge.net' and base == 'download':  # XXX Yuck\n        base = urllib.parse.unquote(path.split('/')[-2])\n    if '#' in base:\n        base, fragment = base.split('#', 1)\n    return base, fragment\n\n\ndef distros_for_url(url, metadata=None):\n    \"\"\"Yield egg or source distribution objects that might be found at a URL\"\"\"\n    base, fragment = egg_info_for_url(url)\n    yield from distros_for_location(url, base, metadata)\n    if fragment:\n        match = EGG_FRAGMENT.match(fragment)\n        if match:\n            yield from interpret_distro_name(\n                url, match.group(1), metadata, precedence=CHECKOUT_DIST\n            )\n\n\ndef distros_for_location(location, basename, metadata=None):\n    \"\"\"Yield egg or source distribution objects based on basename\"\"\"\n    if basename.endswith('.egg.zip'):\n        basename = basename[:-4]  # strip the .zip\n    if basename.endswith('.egg') and '-' in basename:\n        # only one, unambiguous interpretation\n        return [Distribution.from_location(location, basename, metadata)]\n    if basename.endswith('.whl') and '-' in basename:\n        wheel = Wheel(basename)\n        if not wheel.is_compatible():\n            return []\n        return [\n            Distribution(\n                location=location,\n                project_name=wheel.project_name,\n                version=wheel.version,\n                # Increase priority over eggs.\n                precedence=EGG_DIST + 1,\n            )\n        ]\n    if basename.endswith('.exe'):\n        win_base, py_ver, platform = parse_bdist_wininst(basename)\n        if win_base is not None:\n            return interpret_distro_name(\n                location, win_base, metadata, py_ver, BINARY_DIST, platform\n            )\n    # Try source distro extensions (.zip, .tgz, etc.)\n    #\n    for ext in EXTENSIONS:\n        if basename.endswith(ext):\n            basename = basename[: -len(ext)]\n            return interpret_distro_name(location, basename, metadata)\n    return []  # no extension matched\n\n\ndef distros_for_filename(filename, metadata=None):\n    \"\"\"Yield possible egg or source distribution objects based on a filename\"\"\"\n    return distros_for_location(\n        normalize_path(filename), os.path.basename(filename), metadata\n    )\n\n\ndef interpret_distro_name(\n    location, basename, metadata, py_version=None, precedence=SOURCE_DIST, platform=None\n):\n    \"\"\"Generate the interpretation of a source distro name\n\n    Note: if `location` is a filesystem filename, you should call\n    ``pkg_resources.normalize_path()`` on it before passing it to this\n    routine!\n    \"\"\"\n\n    parts = basename.split('-')\n    if not py_version and any(re.match(r'py\\d\\.\\d$', p) for p in parts[2:]):\n        # it is a bdist_dumb, not an sdist -- bail out\n        return\n\n    # find the pivot (p) that splits the name from the version.\n    # infer the version as the first item that has a digit.\n    for p in range(len(parts)):\n        if parts[p][:1].isdigit():\n            break\n    else:\n        p = len(parts)\n\n    yield Distribution(\n        location,\n        metadata,\n        '-'.join(parts[:p]),\n        '-'.join(parts[p:]),\n        py_version=py_version,\n        precedence=precedence,\n        platform=platform,\n    )\n\n\ndef unique_values(func):\n    \"\"\"\n    Wrap a function returning an iterable such that the resulting iterable\n    only ever yields unique items.\n    \"\"\"\n\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        return unique_everseen(func(*args, **kwargs))\n\n    return wrapper\n\n\nREL = re.compile(r\"\"\"<([^>]*\\srel\\s{0,10}=\\s{0,10}['\"]?([^'\" >]+)[^>]*)>\"\"\", re.I)\n\"\"\"\nRegex for an HTML tag with 'rel=\"val\"' attributes.\n\"\"\"\n\n\n@unique_values\ndef find_external_links(url, page):\n    \"\"\"Find rel=\"homepage\" and rel=\"download\" links in `page`, yielding URLs\"\"\"\n\n    for match in REL.finditer(page):\n        tag, rel = match.groups()\n        rels = set(map(str.strip, rel.lower().split(',')))\n        if 'homepage' in rels or 'download' in rels:\n            for match in HREF.finditer(tag):\n                yield urllib.parse.urljoin(url, htmldecode(match.group(1)))\n\n    for tag in (\"<th>Home Page\", \"<th>Download URL\"):\n        pos = page.find(tag)\n        if pos != -1:\n            match = HREF.search(page, pos)\n            if match:\n                yield urllib.parse.urljoin(url, htmldecode(match.group(1)))\n\n\nclass ContentChecker:\n    \"\"\"\n    A null content checker that defines the interface for checking content\n    \"\"\"\n\n    def feed(self, block):\n        \"\"\"\n        Feed a block of data to the hash.\n        \"\"\"\n        return\n\n    def is_valid(self):\n        \"\"\"\n        Check the hash. Return False if validation fails.\n        \"\"\"\n        return True\n\n    def report(self, reporter, template):\n        \"\"\"\n        Call reporter with information about the checker (hash name)\n        substituted into the template.\n        \"\"\"\n        return\n\n\nclass HashChecker(ContentChecker):\n    pattern = re.compile(\n        r'(?P<hash_name>sha1|sha224|sha384|sha256|sha512|md5)='\n        r'(?P<expected>[a-f0-9]+)'\n    )\n\n    def __init__(self, hash_name, expected) -> None:\n        self.hash_name = hash_name\n        self.hash = hashlib.new(hash_name)\n        self.expected = expected\n\n    @classmethod\n    def from_url(cls, url):\n        \"Construct a (possibly null) ContentChecker from a URL\"\n        fragment = urllib.parse.urlparse(url)[-1]\n        if not fragment:\n            return ContentChecker()\n        match = cls.pattern.search(fragment)\n        if not match:\n            return ContentChecker()\n        return cls(**match.groupdict())\n\n    def feed(self, block):\n        self.hash.update(block)\n\n    def is_valid(self):\n        return self.hash.hexdigest() == self.expected\n\n    def report(self, reporter, template):\n        msg = template % self.hash_name\n        return reporter(msg)\n\n\nclass PackageIndex(Environment):\n    \"\"\"A distribution index that scans web pages for download URLs\"\"\"\n\n    def __init__(\n        self,\n        index_url: str = \"https://pypi.org/simple/\",\n        hosts=('*',),\n        ca_bundle=None,\n        verify_ssl: bool = True,\n        *args,\n        **kw,\n    ) -> None:\n        super().__init__(*args, **kw)\n        self.index_url = index_url + \"/\"[: not index_url.endswith('/')]\n        self.scanned_urls: dict = {}\n        self.fetched_urls: dict = {}\n        self.package_pages: dict = {}\n        self.allows = re.compile('|'.join(map(translate, hosts))).match\n        self.to_scan: list = []\n        self.opener = urllib.request.urlopen\n\n    def add(self, dist):\n        # ignore invalid versions\n        try:\n            parse_version(dist.version)\n        except Exception:\n            return None\n        return super().add(dist)\n\n    # FIXME: 'PackageIndex.process_url' is too complex (14)\n    def process_url(self, url, retrieve: bool = False) -> None:  # noqa: C901\n        \"\"\"Evaluate a URL as a possible download, and maybe retrieve it\"\"\"\n        if url in self.scanned_urls and not retrieve:\n            return\n        self.scanned_urls[url] = True\n        if not URL_SCHEME(url):\n            self.process_filename(url)\n            return\n        else:\n            dists = list(distros_for_url(url))\n            if dists:\n                if not self.url_ok(url):\n                    return\n                self.debug(\"Found link: %s\", url)\n\n        if dists or not retrieve or url in self.fetched_urls:\n            list(map(self.add, dists))\n            return  # don't need the actual page\n\n        if not self.url_ok(url):\n            self.fetched_urls[url] = True\n            return\n\n        self.info(\"Reading %s\", url)\n        self.fetched_urls[url] = True  # prevent multiple fetch attempts\n        tmpl = \"Download error on %s: %%s -- Some packages may not be found!\"\n        f = self.open_url(url, tmpl % url)\n        if f is None:\n            return\n        if isinstance(f, urllib.error.HTTPError) and f.code == 401:\n            self.info(f\"Authentication error: {f.msg}\")\n        self.fetched_urls[f.url] = True\n        if 'html' not in f.headers.get('content-type', '').lower():\n            f.close()  # not html, we can't process it\n            return\n\n        base = f.url  # handle redirects\n        page = f.read()\n        if not isinstance(page, str):\n            # In Python 3 and got bytes but want str.\n            if isinstance(f, urllib.error.HTTPError):\n                # Errors have no charset, assume latin1:\n                charset = 'latin-1'\n            else:\n                charset = f.headers.get_param('charset') or 'latin-1'\n            page = page.decode(charset, \"ignore\")\n        f.close()\n        for match in HREF.finditer(page):\n            link = urllib.parse.urljoin(base, htmldecode(match.group(1)))\n            self.process_url(link)\n        if url.startswith(self.index_url) and getattr(f, 'code', None) != 404:\n            page = self.process_index(url, page)\n\n    def process_filename(self, fn, nested: bool = False) -> None:\n        # process filenames or directories\n        if not os.path.exists(fn):\n            self.warn(\"Not found: %s\", fn)\n            return\n\n        if os.path.isdir(fn) and not nested:\n            path = os.path.realpath(fn)\n            for item in os.listdir(path):\n                self.process_filename(os.path.join(path, item), True)\n\n        dists = distros_for_filename(fn)\n        if dists:\n            self.debug(\"Found: %s\", fn)\n            list(map(self.add, dists))\n\n    def url_ok(self, url, fatal: bool = False) -> bool:\n        s = URL_SCHEME(url)\n        is_file = s and s.group(1).lower() == 'file'\n        if is_file or self.allows(urllib.parse.urlparse(url)[1]):\n            return True\n        msg = (\n            \"\\nNote: Bypassing %s (disallowed host; see \"\n            \"https://setuptools.pypa.io/en/latest/deprecated/\"\n            \"easy_install.html#restricting-downloads-with-allow-hosts for details).\\n\"\n        )\n        if fatal:\n            raise DistutilsError(msg % url)\n        else:\n            self.warn(msg, url)\n            return False\n\n    def scan_egg_links(self, search_path) -> None:\n        dirs = filter(os.path.isdir, search_path)\n        egg_links = (\n            (path, entry)\n            for path in dirs\n            for entry in os.listdir(path)\n            if entry.endswith('.egg-link')\n        )\n        list(itertools.starmap(self.scan_egg_link, egg_links))\n\n    def scan_egg_link(self, path, entry) -> None:\n        content = _read_utf8_with_fallback(os.path.join(path, entry))\n        # filter non-empty lines\n        lines = list(filter(None, map(str.strip, content.splitlines())))\n\n        if len(lines) != 2:\n            # format is not recognized; punt\n            return\n\n        egg_path, _setup_path = lines\n\n        for dist in find_distributions(os.path.join(path, egg_path)):\n            dist.location = os.path.join(path, *lines)\n            dist.precedence = SOURCE_DIST\n            self.add(dist)\n\n    def _scan(self, link):\n        # Process a URL to see if it's for a package page\n        NO_MATCH_SENTINEL = None, None\n        if not link.startswith(self.index_url):\n            return NO_MATCH_SENTINEL\n\n        parts = list(map(urllib.parse.unquote, link[len(self.index_url) :].split('/')))\n        if len(parts) != 2 or '#' in parts[1]:\n            return NO_MATCH_SENTINEL\n\n        # it's a package page, sanitize and index it\n        pkg = safe_name(parts[0])\n        ver = safe_version(parts[1])\n        self.package_pages.setdefault(pkg.lower(), {})[link] = True\n        return to_filename(pkg), to_filename(ver)\n\n    def process_index(self, url, page):\n        \"\"\"Process the contents of a PyPI page\"\"\"\n\n        # process an index page into the package-page index\n        for match in HREF.finditer(page):\n            try:\n                self._scan(urllib.parse.urljoin(url, htmldecode(match.group(1))))\n            except ValueError:\n                pass\n\n        pkg, ver = self._scan(url)  # ensure this page is in the page index\n        if not pkg:\n            return \"\"  # no sense double-scanning non-package pages\n\n        # process individual package page\n        for new_url in find_external_links(url, page):\n            # Process the found URL\n            base, frag = egg_info_for_url(new_url)\n            if base.endswith('.py') and not frag:\n                if ver:\n                    new_url += f'#egg={pkg}-{ver}'\n                else:\n                    self.need_version_info(url)\n            self.scan_url(new_url)\n\n        return PYPI_MD5.sub(\n            lambda m: '<a href=\"{}#md5={}\">{}</a>'.format(*m.group(1, 3, 2)), page\n        )\n\n    def need_version_info(self, url) -> None:\n        self.scan_all(\n            \"Page at %s links to .py file(s) without version info; an index \"\n            \"scan is required.\",\n            url,\n        )\n\n    def scan_all(self, msg=None, *args) -> None:\n        if self.index_url not in self.fetched_urls:\n            if msg:\n                self.warn(msg, *args)\n            self.info(\"Scanning index of all packages (this may take a while)\")\n        self.scan_url(self.index_url)\n\n    def find_packages(self, requirement) -> None:\n        self.scan_url(self.index_url + requirement.unsafe_name + '/')\n\n        if not self.package_pages.get(requirement.key):\n            # Fall back to safe version of the name\n            self.scan_url(self.index_url + requirement.project_name + '/')\n\n        if not self.package_pages.get(requirement.key):\n            # We couldn't find the target package, so search the index page too\n            self.not_found_in_index(requirement)\n\n        for url in list(self.package_pages.get(requirement.key, ())):\n            # scan each page that might be related to the desired package\n            self.scan_url(url)\n\n    def obtain(self, requirement, installer=None):\n        self.prescan()\n        self.find_packages(requirement)\n        for dist in self[requirement.key]:\n            if dist in requirement:\n                return dist\n            self.debug(\"%s does not match %s\", requirement, dist)\n        return super().obtain(requirement, installer)\n\n    def check_hash(self, checker, filename, tfp) -> None:\n        \"\"\"\n        checker is a ContentChecker\n        \"\"\"\n        checker.report(self.debug, f\"Validating %s checksum for {filename}\")\n        if not checker.is_valid():\n            tfp.close()\n            os.unlink(filename)\n            raise DistutilsError(\n                f\"{checker.hash.name} validation failed for {os.path.basename(filename)}; \"\n                \"possible download problem?\"\n            )\n\n    def add_find_links(self, urls) -> None:\n        \"\"\"Add `urls` to the list that will be prescanned for searches\"\"\"\n        for url in urls:\n            if (\n                self.to_scan is None  # if we have already \"gone online\"\n                or not URL_SCHEME(url)  # or it's a local file/directory\n                or url.startswith('file:')\n                or list(distros_for_url(url))  # or a direct package link\n            ):\n                # then go ahead and process it now\n                self.scan_url(url)\n            else:\n                # otherwise, defer retrieval till later\n                self.to_scan.append(url)\n\n    def prescan(self):\n        \"\"\"Scan urls scheduled for prescanning (e.g. --find-links)\"\"\"\n        if self.to_scan:\n            list(map(self.scan_url, self.to_scan))\n        self.to_scan = None  # from now on, go ahead and process immediately\n\n    def not_found_in_index(self, requirement) -> None:\n        if self[requirement.key]:  # we've seen at least one distro\n            meth, msg = self.info, \"Couldn't retrieve index page for %r\"\n        else:  # no distros seen for this name, might be misspelled\n            meth, msg = self.warn, \"Couldn't find index page for %r (maybe misspelled?)\"\n        meth(msg, requirement.unsafe_name)\n        self.scan_all()\n\n    def download(self, spec, tmpdir):\n        \"\"\"Locate and/or download `spec` to `tmpdir`, returning a local path\n\n        `spec` may be a ``Requirement`` object, or a string containing a URL,\n        an existing local filename, or a project/version requirement spec\n        (i.e. the string form of a ``Requirement`` object).  If it is the URL\n        of a .py file with an unambiguous ``#egg=name-version`` tag (i.e., one\n        that escapes ``-`` as ``_`` throughout), a trivial ``setup.py`` is\n        automatically created alongside the downloaded file.\n\n        If `spec` is a ``Requirement`` object or a string containing a\n        project/version requirement spec, this method returns the location of\n        a matching distribution (possibly after downloading it to `tmpdir`).\n        If `spec` is a locally existing file or directory name, it is simply\n        returned unchanged.  If `spec` is a URL, it is downloaded to a subpath\n        of `tmpdir`, and the local filename is returned.  Various errors may be\n        raised if a problem occurs during downloading.\n        \"\"\"\n        if not isinstance(spec, Requirement):\n            scheme = URL_SCHEME(spec)\n            if scheme:\n                # It's a url, download it to tmpdir\n                found = self._download_url(spec, tmpdir)\n                base, fragment = egg_info_for_url(spec)\n                if base.endswith('.py'):\n                    found = self.gen_setup(found, fragment, tmpdir)\n                return found\n            elif os.path.exists(spec):\n                # Existing file or directory, just return it\n                return spec\n            else:\n                spec = parse_requirement_arg(spec)\n        return getattr(self.fetch_distribution(spec, tmpdir), 'location', None)\n\n    def fetch_distribution(  # noqa: C901  # is too complex (14)  # FIXME\n        self,\n        requirement,\n        tmpdir,\n        force_scan: bool = False,\n        source: bool = False,\n        develop_ok: bool = False,\n        local_index=None,\n    ) -> Distribution | None:\n        \"\"\"Obtain a distribution suitable for fulfilling `requirement`\n\n        `requirement` must be a ``pkg_resources.Requirement`` instance.\n        If necessary, or if the `force_scan` flag is set, the requirement is\n        searched for in the (online) package index as well as the locally\n        installed packages.  If a distribution matching `requirement` is found,\n        the returned distribution's ``location`` is the value you would have\n        gotten from calling the ``download()`` method with the matching\n        distribution's URL or filename.  If no matching distribution is found,\n        ``None`` is returned.\n\n        If the `source` flag is set, only source distributions and source\n        checkout links will be considered.  Unless the `develop_ok` flag is\n        set, development and system eggs (i.e., those using the ``.egg-info``\n        format) will be ignored.\n        \"\"\"\n        # process a Requirement\n        self.info(\"Searching for %s\", requirement)\n        skipped = set()\n        dist = None\n\n        def find(req, env: Environment | None = None):\n            if env is None:\n                env = self\n            # Find a matching distribution; may be called more than once\n\n            for dist in env[req.key]:\n                if dist.precedence == DEVELOP_DIST and not develop_ok:\n                    if dist not in skipped:\n                        self.warn(\n                            \"Skipping development or system egg: %s\",\n                            dist,\n                        )\n                        skipped.add(dist)\n                    continue\n\n                test = dist in req and (dist.precedence <= SOURCE_DIST or not source)\n                if test:\n                    loc = self.download(dist.location, tmpdir)\n                    dist.download_location = loc\n                    if os.path.exists(dist.download_location):\n                        return dist\n\n            return None\n\n        if force_scan:\n            self.prescan()\n            self.find_packages(requirement)\n            dist = find(requirement)\n\n        if not dist and local_index is not None:\n            dist = find(requirement, local_index)\n\n        if dist is None:\n            if self.to_scan is not None:\n                self.prescan()\n            dist = find(requirement)\n\n        if dist is None and not force_scan:\n            self.find_packages(requirement)\n            dist = find(requirement)\n\n        if dist is None:\n            self.warn(\n                \"No local packages or working download links found for %s%s\",\n                (source and \"a source distribution of \" or \"\"),\n                requirement,\n            )\n            return None\n        else:\n            self.info(\"Best match: %s\", dist)\n            return dist.clone(location=dist.download_location)\n\n    def fetch(\n        self, requirement, tmpdir, force_scan: bool = False, source: bool = False\n    ) -> str | None:\n        \"\"\"Obtain a file suitable for fulfilling `requirement`\n\n        DEPRECATED; use the ``fetch_distribution()`` method now instead.  For\n        backward compatibility, this routine is identical but returns the\n        ``location`` of the downloaded distribution instead of a distribution\n        object.\n        \"\"\"\n        dist = self.fetch_distribution(requirement, tmpdir, force_scan, source)\n        if dist is not None:\n            return dist.location\n        return None\n\n    def gen_setup(self, filename, fragment, tmpdir):\n        match = EGG_FRAGMENT.match(fragment)\n        dists = (\n            match\n            and [\n                d\n                for d in interpret_distro_name(filename, match.group(1), None)\n                if d.version\n            ]\n            or []\n        )\n\n        if len(dists) == 1:  # unambiguous ``#egg`` fragment\n            basename = os.path.basename(filename)\n\n            # Make sure the file has been downloaded to the temp dir.\n            if os.path.dirname(filename) != tmpdir:\n                dst = os.path.join(tmpdir, basename)\n                if not (os.path.exists(dst) and os.path.samefile(filename, dst)):\n                    shutil.copy2(filename, dst)\n                    filename = dst\n\n            with open(os.path.join(tmpdir, 'setup.py'), 'w', encoding=\"utf-8\") as file:\n                file.write(\n                    \"from setuptools import setup\\n\"\n                    f\"setup(name={dists[0].project_name!r}, version={dists[0].version!r}, py_modules=[{os.path.splitext(basename)[0]!r}])\\n\"\n                )\n            return filename\n\n        elif match:\n            raise DistutilsError(\n                f\"Can't unambiguously interpret project/version identifier {fragment!r}; \"\n                \"any dashes in the name or version should be escaped using \"\n                f\"underscores. {dists!r}\"\n            )\n        else:\n            raise DistutilsError(\n                \"Can't process plain .py files without an '#egg=name-version'\"\n                \" suffix to enable automatic setup script generation.\"\n            )\n\n    dl_blocksize = 8192\n\n    def _download_to(self, url, filename):\n        self.info(\"Downloading %s\", url)\n        # Download the file\n        fp = None\n        try:\n            checker = HashChecker.from_url(url)\n            fp = self.open_url(url)\n            if isinstance(fp, urllib.error.HTTPError):\n                raise DistutilsError(f\"Can't download {url}: {fp.code} {fp.msg}\")\n            headers = fp.info()\n            blocknum = 0\n            bs = self.dl_blocksize\n            size = -1\n            if \"content-length\" in headers:\n                # Some servers return multiple Content-Length headers :(\n                sizes = headers.get_all('Content-Length')\n                size = max(map(int, sizes))\n                self.reporthook(url, filename, blocknum, bs, size)\n            with open(filename, 'wb') as tfp:\n                while True:\n                    block = fp.read(bs)\n                    if block:\n                        checker.feed(block)\n                        tfp.write(block)\n                        blocknum += 1\n                        self.reporthook(url, filename, blocknum, bs, size)\n                    else:\n                        break\n                self.check_hash(checker, filename, tfp)\n            return headers\n        finally:\n            if fp:\n                fp.close()\n\n    def reporthook(self, url, filename, blocknum, blksize, size) -> None:\n        pass  # no-op\n\n    # FIXME:\n    def open_url(self, url, warning=None):  # noqa: C901  # is too complex (12)\n        if url.startswith('file:'):\n            return local_open(url)\n        try:\n            return open_with_auth(url, self.opener)\n        except (ValueError, http.client.InvalidURL) as v:\n            msg = ' '.join([str(arg) for arg in v.args])\n            if warning:\n                self.warn(warning, msg)\n            else:\n                raise DistutilsError(f'{url} {msg}') from v\n        except urllib.error.HTTPError as v:\n            return v\n        except urllib.error.URLError as v:\n            if warning:\n                self.warn(warning, v.reason)\n            else:\n                raise DistutilsError(f\"Download error for {url}: {v.reason}\") from v\n        except http.client.BadStatusLine as v:\n            if warning:\n                self.warn(warning, v.line)\n            else:\n                raise DistutilsError(\n                    f'{url} returned a bad status line. The server might be '\n                    f'down, {v.line}'\n                ) from v\n        except (http.client.HTTPException, OSError) as v:\n            if warning:\n                self.warn(warning, v)\n            else:\n                raise DistutilsError(f\"Download error for {url}: {v}\") from v\n\n    def _download_url(self, url, tmpdir):\n        # Determine download filename\n        #\n        name, _fragment = egg_info_for_url(url)\n        if name:\n            while '..' in name:\n                name = name.replace('..', '.').replace('\\\\', '_')\n        else:\n            name = \"__downloaded__\"  # default if URL has no path contents\n\n        if name.endswith('.egg.zip'):\n            name = name[:-4]  # strip the extra .zip before download\n\n        filename = os.path.join(tmpdir, name)\n\n        return self._download_vcs(url, filename) or self._download_other(url, filename)\n\n    @staticmethod\n    def _resolve_vcs(url):\n        \"\"\"\n        >>> rvcs = PackageIndex._resolve_vcs\n        >>> rvcs('git+http://foo/bar')\n        'git'\n        >>> rvcs('hg+https://foo/bar')\n        'hg'\n        >>> rvcs('git:myhost')\n        'git'\n        >>> rvcs('hg:myhost')\n        >>> rvcs('http://foo/bar')\n        \"\"\"\n        scheme = urllib.parse.urlsplit(url).scheme\n        pre, sep, _post = scheme.partition('+')\n        # svn and git have their own protocol; hg does not\n        allowed = set(['svn', 'git'] + ['hg'] * bool(sep))\n        return next(iter({pre} & allowed), None)\n\n    def _download_vcs(self, url, spec_filename):\n        vcs = self._resolve_vcs(url)\n        if not vcs:\n            return None\n        if vcs == 'svn':\n            raise DistutilsError(\n                f\"Invalid config, SVN download is not supported: {url}\"\n            )\n\n        filename, _, _ = spec_filename.partition('#')\n        url, rev = self._vcs_split_rev_from_url(url)\n\n        self.info(f\"Doing {vcs} clone from {url} to {filename}\")\n        subprocess.check_call([vcs, 'clone', '--quiet', url, filename])\n\n        co_commands = dict(\n            git=[vcs, '-C', filename, 'checkout', '--quiet', rev],\n            hg=[vcs, '--cwd', filename, 'up', '-C', '-r', rev, '-q'],\n        )\n        if rev is not None:\n            self.info(f\"Checking out {rev}\")\n            subprocess.check_call(co_commands[vcs])\n\n        return filename\n\n    def _download_other(self, url, filename):\n        scheme = urllib.parse.urlsplit(url).scheme\n        if scheme == 'file':  # pragma: no cover\n            return urllib.request.url2pathname(urllib.parse.urlparse(url).path)\n        # raise error if not allowed\n        self.url_ok(url, True)\n        return self._attempt_download(url, filename)\n\n    def scan_url(self, url) -> None:\n        self.process_url(url, True)\n\n    def _attempt_download(self, url, filename):\n        headers = self._download_to(url, filename)\n        if 'html' in headers.get('content-type', '').lower():\n            return self._invalid_download_html(url, headers, filename)\n        else:\n            return filename\n\n    def _invalid_download_html(self, url, headers, filename):\n        os.unlink(filename)\n        raise DistutilsError(f\"Unexpected HTML page found at {url}\")\n\n    @staticmethod\n    def _vcs_split_rev_from_url(url):\n        \"\"\"\n        Given a possible VCS URL, return a clean URL and resolved revision if any.\n\n        >>> vsrfu = PackageIndex._vcs_split_rev_from_url\n        >>> vsrfu('git+https://github.com/pypa/setuptools@v69.0.0#egg-info=setuptools')\n        ('https://github.com/pypa/setuptools', 'v69.0.0')\n        >>> vsrfu('git+https://github.com/pypa/setuptools#egg-info=setuptools')\n        ('https://github.com/pypa/setuptools', None)\n        >>> vsrfu('http://foo/bar')\n        ('http://foo/bar', None)\n        \"\"\"\n        parts = urllib.parse.urlsplit(url)\n\n        clean_scheme = parts.scheme.split('+', 1)[-1]\n\n        # Some fragment identification fails\n        no_fragment_path, _, _ = parts.path.partition('#')\n\n        pre, sep, post = no_fragment_path.rpartition('@')\n        clean_path, rev = (pre, post) if sep else (post, None)\n\n        resolved = parts._replace(\n            scheme=clean_scheme,\n            path=clean_path,\n            # discard the fragment\n            fragment='',\n        ).geturl()\n\n        return resolved, rev\n\n    def debug(self, msg, *args) -> None:\n        log.debug(msg, *args)\n\n    def info(self, msg, *args) -> None:\n        log.info(msg, *args)\n\n    def warn(self, msg, *args) -> None:\n        log.warn(msg, *args)\n\n\n# This pattern matches a character entity reference (a decimal numeric\n# references, a hexadecimal numeric reference, or a named reference).\nentity_sub = re.compile(r'&(#(\\d+|x[\\da-fA-F]+)|[\\w.:-]+);?').sub\n\n\ndef decode_entity(match):\n    what = match.group(0)\n    return html.unescape(what)\n\n\ndef htmldecode(text):\n    \"\"\"\n    Decode HTML entities in the given text.\n\n    >>> htmldecode(\n    ...     'https://../package_name-0.1.2.tar.gz'\n    ...     '?tokena=A&amp;tokenb=B\">package_name-0.1.2.tar.gz')\n    'https://../package_name-0.1.2.tar.gz?tokena=A&tokenb=B\">package_name-0.1.2.tar.gz'\n    \"\"\"\n    return entity_sub(decode_entity, text)\n\n\ndef socket_timeout(timeout=15):\n    def _socket_timeout(func):\n        def _socket_timeout(*args, **kwargs):\n            old_timeout = socket.getdefaulttimeout()\n            socket.setdefaulttimeout(timeout)\n            try:\n                return func(*args, **kwargs)\n            finally:\n                socket.setdefaulttimeout(old_timeout)\n\n        return _socket_timeout\n\n    return _socket_timeout\n\n\ndef _encode_auth(auth):\n    \"\"\"\n    Encode auth from a URL suitable for an HTTP header.\n    >>> str(_encode_auth('username%3Apassword'))\n    'dXNlcm5hbWU6cGFzc3dvcmQ='\n\n    Long auth strings should not cause a newline to be inserted.\n    >>> long_auth = 'username:' + 'password'*10\n    >>> chr(10) in str(_encode_auth(long_auth))\n    False\n    \"\"\"\n    auth_s = urllib.parse.unquote(auth)\n    # convert to bytes\n    auth_bytes = auth_s.encode()\n    encoded_bytes = base64.b64encode(auth_bytes)\n    # convert back to a string\n    encoded = encoded_bytes.decode()\n    # strip the trailing carriage return\n    return encoded.replace('\\n', '')\n\n\nclass Credential(NamedTuple):\n    \"\"\"\n    A username/password pair.\n\n    Displayed separated by `:`.\n    >>> str(Credential('username', 'password'))\n    'username:password'\n    \"\"\"\n\n    username: str\n    password: str\n\n    def __str__(self) -> str:\n        return f'{self.username}:{self.password}'\n\n\nclass PyPIConfig(configparser.RawConfigParser):\n    def __init__(self):\n        \"\"\"\n        Load from ~/.pypirc\n        \"\"\"\n        defaults = dict.fromkeys(['username', 'password', 'repository'], '')\n        super().__init__(defaults)\n\n        rc = os.path.join(os.path.expanduser('~'), '.pypirc')\n        if os.path.exists(rc):\n            _cfg_read_utf8_with_fallback(self, rc)\n\n    @property\n    def creds_by_repository(self):\n        sections_with_repositories = [\n            section\n            for section in self.sections()\n            if self.get(section, 'repository').strip()\n        ]\n\n        return dict(map(self._get_repo_cred, sections_with_repositories))\n\n    def _get_repo_cred(self, section):\n        repo = self.get(section, 'repository').strip()\n        return repo, Credential(\n            self.get(section, 'username').strip(),\n            self.get(section, 'password').strip(),\n        )\n\n    def find_credential(self, url):\n        \"\"\"\n        If the URL indicated appears to be a repository defined in this\n        config, return the credential for that repository.\n        \"\"\"\n        for repository, cred in self.creds_by_repository.items():\n            if url.startswith(repository):\n                return cred\n        return None\n\n\ndef open_with_auth(url, opener=urllib.request.urlopen):\n    \"\"\"Open a urllib2 request, handling HTTP authentication\"\"\"\n\n    parsed = urllib.parse.urlparse(url)\n    scheme, netloc, path, params, query, frag = parsed\n\n    # Double scheme does not raise on macOS as revealed by a\n    # failing test. We would expect \"nonnumeric port\". Refs #20.\n    if netloc.endswith(':'):\n        raise http.client.InvalidURL(\"nonnumeric port: ''\")\n\n    if scheme in ('http', 'https'):\n        auth, address = _splituser(netloc)\n    else:\n        auth, address = (None, None)\n\n    if not auth:\n        cred = PyPIConfig().find_credential(url)\n        if cred:\n            auth = str(cred)\n            info = cred.username, url\n            log.info('Authenticating as %s for %s (from .pypirc)', *info)\n\n    if auth:\n        auth = \"Basic \" + _encode_auth(auth)\n        parts = scheme, address, path, params, query, frag\n        new_url = urllib.parse.urlunparse(parts)\n        request = urllib.request.Request(new_url)\n        request.add_header(\"Authorization\", auth)\n    else:\n        request = urllib.request.Request(url)\n\n    request.add_header('User-Agent', user_agent)\n    fp = opener(request)\n\n    if auth:\n        # Put authentication info back into request URL if same host,\n        # so that links found on the page will work\n        s2, h2, path2, param2, query2, frag2 = urllib.parse.urlparse(fp.url)\n        if s2 == scheme and h2 == address:\n            parts = s2, netloc, path2, param2, query2, frag2\n            fp.url = urllib.parse.urlunparse(parts)\n\n    return fp\n\n\n# copy of urllib.parse._splituser from Python 3.8\n# See https://github.com/python/cpython/issues/80072.\ndef _splituser(host):\n    \"\"\"splituser('user[:passwd]@host[:port]')\n    --> 'user[:passwd]', 'host[:port]'.\"\"\"\n    user, delim, host = host.rpartition('@')\n    return (user if delim else None), host\n\n\n# adding a timeout to avoid freezing package_index\nopen_with_auth = socket_timeout(_SOCKET_TIMEOUT)(open_with_auth)\n\n\ndef fix_sf_url(url):\n    return url  # backward compatibility\n\n\ndef local_open(url):\n    \"\"\"Read a local path, with special support for directories\"\"\"\n    _scheme, _server, path, _param, _query, _frag = urllib.parse.urlparse(url)\n    filename = urllib.request.url2pathname(path)\n    if os.path.isfile(filename):\n        return urllib.request.urlopen(url)\n    elif path.endswith('/') and os.path.isdir(filename):\n        files = []\n        for f in os.listdir(filename):\n            filepath = os.path.join(filename, f)\n            if f == 'index.html':\n                body = _read_utf8_with_fallback(filepath)\n                break\n            elif os.path.isdir(filepath):\n                f += '/'\n            files.append(f'<a href=\"{f}\">{f}</a>')\n        else:\n            tmpl = \"<html><head><title>{url}</title></head><body>{files}</body></html>\"\n            body = tmpl.format(url=url, files='\\n'.join(files))\n        status, message = 200, \"OK\"\n    else:\n        status, message, body = 404, \"Path not found\", \"Not found\"\n\n    headers = {'content-type': 'text/html'}\n    body_stream = io.StringIO(body)\n    return urllib.error.HTTPError(url, status, message, headers, body_stream)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/sandbox.py","size":14906,"sha1":"e025b7a0071c76bdf31efb4992094b10d83d8012","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"from __future__ import annotations\n\nimport builtins\nimport contextlib\nimport functools\nimport itertools\nimport operator\nimport os\nimport pickle\nimport re\nimport sys\nimport tempfile\nimport textwrap\nfrom types import TracebackType\nfrom typing import TYPE_CHECKING, Any, ClassVar\n\nimport pkg_resources\nfrom pkg_resources import working_set\n\nfrom distutils.errors import DistutilsError\n\nif TYPE_CHECKING:\n    import os as _os\nelif sys.platform.startswith('java'):\n    import org.python.modules.posix.PosixModule as _os  # pyright: ignore[reportMissingImports]\nelse:\n    _os = sys.modules[os.name]\n_open = open\n\n\nif TYPE_CHECKING:\n    from typing_extensions import Self\n\n__all__ = [\n    \"AbstractSandbox\",\n    \"DirectorySandbox\",\n    \"SandboxViolation\",\n    \"run_setup\",\n]\n\n\ndef _execfile(filename, globals, locals=None):\n    \"\"\"\n    Python 3 implementation of execfile.\n    \"\"\"\n    mode = 'rb'\n    with open(filename, mode) as stream:\n        script = stream.read()\n    if locals is None:\n        locals = globals\n    code = compile(script, filename, 'exec')\n    exec(code, globals, locals)\n\n\n@contextlib.contextmanager\ndef save_argv(repl=None):\n    saved = sys.argv[:]\n    if repl is not None:\n        sys.argv[:] = repl\n    try:\n        yield saved\n    finally:\n        sys.argv[:] = saved\n\n\n@contextlib.contextmanager\ndef save_path():\n    saved = sys.path[:]\n    try:\n        yield saved\n    finally:\n        sys.path[:] = saved\n\n\n@contextlib.contextmanager\ndef override_temp(replacement):\n    \"\"\"\n    Monkey-patch tempfile.tempdir with replacement, ensuring it exists\n    \"\"\"\n    os.makedirs(replacement, exist_ok=True)\n\n    saved = tempfile.tempdir\n\n    tempfile.tempdir = replacement\n\n    try:\n        yield\n    finally:\n        tempfile.tempdir = saved\n\n\n@contextlib.contextmanager\ndef pushd(target):\n    saved = os.getcwd()\n    os.chdir(target)\n    try:\n        yield saved\n    finally:\n        os.chdir(saved)\n\n\nclass UnpickleableException(Exception):\n    \"\"\"\n    An exception representing another Exception that could not be pickled.\n    \"\"\"\n\n    @staticmethod\n    def dump(type, exc):\n        \"\"\"\n        Always return a dumped (pickled) type and exc. If exc can't be pickled,\n        wrap it in UnpickleableException first.\n        \"\"\"\n        try:\n            return pickle.dumps(type), pickle.dumps(exc)\n        except Exception:\n            # get UnpickleableException inside the sandbox\n            from setuptools.sandbox import UnpickleableException as cls\n\n            return cls.dump(cls, cls(repr(exc)))\n\n\nclass ExceptionSaver:\n    \"\"\"\n    A Context Manager that will save an exception, serialize, and restore it\n    later.\n    \"\"\"\n\n    def __enter__(self) -> Self:\n        return self\n\n    def __exit__(\n        self,\n        type: type[BaseException] | None,\n        exc: BaseException | None,\n        tb: TracebackType | None,\n    ) -> bool:\n        if not exc:\n            return False\n\n        # dump the exception\n        self._saved = UnpickleableException.dump(type, exc)\n        self._tb = tb\n\n        # suppress the exception\n        return True\n\n    def resume(self):\n        \"restore and re-raise any exception\"\n\n        if '_saved' not in vars(self):\n            return\n\n        _type, exc = map(pickle.loads, self._saved)\n        raise exc.with_traceback(self._tb)\n\n\n@contextlib.contextmanager\ndef save_modules():\n    \"\"\"\n    Context in which imported modules are saved.\n\n    Translates exceptions internal to the context into the equivalent exception\n    outside the context.\n    \"\"\"\n    saved = sys.modules.copy()\n    with ExceptionSaver() as saved_exc:\n        yield saved\n\n    sys.modules.update(saved)\n    # remove any modules imported since\n    del_modules = (\n        mod_name\n        for mod_name in sys.modules\n        if mod_name not in saved\n        # exclude any encodings modules. See #285\n        and not mod_name.startswith('encodings.')\n    )\n    _clear_modules(del_modules)\n\n    saved_exc.resume()\n\n\ndef _clear_modules(module_names):\n    for mod_name in list(module_names):\n        del sys.modules[mod_name]\n\n\n@contextlib.contextmanager\ndef save_pkg_resources_state():\n    saved = pkg_resources.__getstate__()\n    try:\n        yield saved\n    finally:\n        pkg_resources.__setstate__(saved)\n\n\n@contextlib.contextmanager\ndef setup_context(setup_dir):\n    temp_dir = os.path.join(setup_dir, 'temp')\n    with save_pkg_resources_state():\n        with save_modules():\n            with save_path():\n                hide_setuptools()\n                with save_argv():\n                    with override_temp(temp_dir):\n                        with pushd(setup_dir):\n                            # ensure setuptools commands are available\n                            __import__('setuptools')\n                            yield\n\n\n_MODULES_TO_HIDE = {\n    'setuptools',\n    'distutils',\n    'pkg_resources',\n    'Cython',\n    '_distutils_hack',\n}\n\n\ndef _needs_hiding(mod_name):\n    \"\"\"\n    >>> _needs_hiding('setuptools')\n    True\n    >>> _needs_hiding('pkg_resources')\n    True\n    >>> _needs_hiding('setuptools_plugin')\n    False\n    >>> _needs_hiding('setuptools.__init__')\n    True\n    >>> _needs_hiding('distutils')\n    True\n    >>> _needs_hiding('os')\n    False\n    >>> _needs_hiding('Cython')\n    True\n    \"\"\"\n    base_module = mod_name.split('.', 1)[0]\n    return base_module in _MODULES_TO_HIDE\n\n\ndef hide_setuptools():\n    \"\"\"\n    Remove references to setuptools' modules from sys.modules to allow the\n    invocation to import the most appropriate setuptools. This technique is\n    necessary to avoid issues such as #315 where setuptools upgrading itself\n    would fail to find a function declared in the metadata.\n    \"\"\"\n    _distutils_hack = sys.modules.get('_distutils_hack', None)\n    if _distutils_hack is not None:\n        _distutils_hack._remove_shim()\n\n    modules = filter(_needs_hiding, sys.modules)\n    _clear_modules(modules)\n\n\ndef run_setup(setup_script, args):\n    \"\"\"Run a distutils setup script, sandboxed in its directory\"\"\"\n    setup_dir = os.path.abspath(os.path.dirname(setup_script))\n    with setup_context(setup_dir):\n        try:\n            sys.argv[:] = [setup_script] + list(args)\n            sys.path.insert(0, setup_dir)\n            # reset to include setup dir, w/clean callback list\n            working_set.__init__()\n            working_set.callbacks.append(lambda dist: dist.activate())\n\n            with DirectorySandbox(setup_dir):\n                ns = dict(__file__=setup_script, __name__='__main__')\n                _execfile(setup_script, ns)\n        except SystemExit as v:\n            if v.args and v.args[0]:\n                raise\n            # Normal exit, just return\n\n\nclass AbstractSandbox:\n    \"\"\"Wrap 'os' module and 'open()' builtin for virtualizing setup scripts\"\"\"\n\n    _active = False\n\n    def __init__(self) -> None:\n        self._attrs = [\n            name\n            for name in dir(_os)\n            if not name.startswith('_') and hasattr(self, name)\n        ]\n\n    def _copy(self, source):\n        for name in self._attrs:\n            setattr(os, name, getattr(source, name))\n\n    def __enter__(self) -> None:\n        self._copy(self)\n        builtins.open = self._open\n        self._active = True\n\n    def __exit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_value: BaseException | None,\n        traceback: TracebackType | None,\n    ):\n        self._active = False\n        builtins.open = _open\n        self._copy(_os)\n\n    def run(self, func):\n        \"\"\"Run 'func' under os sandboxing\"\"\"\n        with self:\n            return func()\n\n    def _mk_dual_path_wrapper(name: str):  # type: ignore[misc] # https://github.com/pypa/setuptools/pull/4099\n        original = getattr(_os, name)\n\n        def wrap(self, src, dst, *args, **kw):\n            if self._active:\n                src, dst = self._remap_pair(name, src, dst, *args, **kw)\n            return original(src, dst, *args, **kw)\n\n        return wrap\n\n    for __name in [\"rename\", \"link\", \"symlink\"]:\n        if hasattr(_os, __name):\n            locals()[__name] = _mk_dual_path_wrapper(__name)\n\n    def _mk_single_path_wrapper(name: str, original=None):  # type: ignore[misc] # https://github.com/pypa/setuptools/pull/4099\n        original = original or getattr(_os, name)\n\n        def wrap(self, path, *args, **kw):\n            if self._active:\n                path = self._remap_input(name, path, *args, **kw)\n            return original(path, *args, **kw)\n\n        return wrap\n\n    _open = _mk_single_path_wrapper('open', _open)\n    for __name in [\n        \"stat\",\n        \"listdir\",\n        \"chdir\",\n        \"open\",\n        \"chmod\",\n        \"chown\",\n        \"mkdir\",\n        \"remove\",\n        \"unlink\",\n        \"rmdir\",\n        \"utime\",\n        \"lchown\",\n        \"chroot\",\n        \"lstat\",\n        \"startfile\",\n        \"mkfifo\",\n        \"mknod\",\n        \"pathconf\",\n        \"access\",\n    ]:\n        if hasattr(_os, __name):\n            locals()[__name] = _mk_single_path_wrapper(__name)\n\n    def _mk_single_with_return(name: str):  # type: ignore[misc] # https://github.com/pypa/setuptools/pull/4099\n        original = getattr(_os, name)\n\n        def wrap(self, path, *args, **kw):\n            if self._active:\n                path = self._remap_input(name, path, *args, **kw)\n                return self._remap_output(name, original(path, *args, **kw))\n            return original(path, *args, **kw)\n\n        return wrap\n\n    for __name in ['readlink', 'tempnam']:\n        if hasattr(_os, __name):\n            locals()[__name] = _mk_single_with_return(__name)\n\n    def _mk_query(name: str):  # type: ignore[misc] # https://github.com/pypa/setuptools/pull/4099\n        original = getattr(_os, name)\n\n        def wrap(self, *args, **kw):\n            retval = original(*args, **kw)\n            if self._active:\n                return self._remap_output(name, retval)\n            return retval\n\n        return wrap\n\n    for __name in ['getcwd', 'tmpnam']:\n        if hasattr(_os, __name):\n            locals()[__name] = _mk_query(__name)\n\n    def _validate_path(self, path):\n        \"\"\"Called to remap or validate any path, whether input or output\"\"\"\n        return path\n\n    def _remap_input(self, operation, path, *args, **kw):\n        \"\"\"Called for path inputs\"\"\"\n        return self._validate_path(path)\n\n    def _remap_output(self, operation, path):\n        \"\"\"Called for path outputs\"\"\"\n        return self._validate_path(path)\n\n    def _remap_pair(self, operation, src, dst, *args, **kw):\n        \"\"\"Called for path pairs like rename, link, and symlink operations\"\"\"\n        return (\n            self._remap_input(operation + '-from', src, *args, **kw),\n            self._remap_input(operation + '-to', dst, *args, **kw),\n        )\n\n    if TYPE_CHECKING:\n        # This is a catch-all for all the dynamically created attributes.\n        # This isn't public API anyway\n        def __getattribute__(self, name: str) -> Any: ...\n\n\nif hasattr(os, 'devnull'):\n    _EXCEPTIONS = [os.devnull]\nelse:\n    _EXCEPTIONS = []\n\n\nclass DirectorySandbox(AbstractSandbox):\n    \"\"\"Restrict operations to a single subdirectory - pseudo-chroot\"\"\"\n\n    write_ops: ClassVar[dict[str, None]] = dict.fromkeys([\n        \"open\",\n        \"chmod\",\n        \"chown\",\n        \"mkdir\",\n        \"remove\",\n        \"unlink\",\n        \"rmdir\",\n        \"utime\",\n        \"lchown\",\n        \"chroot\",\n        \"mkfifo\",\n        \"mknod\",\n        \"tempnam\",\n    ])\n\n    _exception_patterns: list[str | re.Pattern] = []\n    \"exempt writing to paths that match the pattern\"\n\n    def __init__(self, sandbox, exceptions=_EXCEPTIONS) -> None:\n        self._sandbox = os.path.normcase(os.path.realpath(sandbox))\n        self._prefix = os.path.join(self._sandbox, '')\n        self._exceptions = [\n            os.path.normcase(os.path.realpath(path)) for path in exceptions\n        ]\n        AbstractSandbox.__init__(self)\n\n    def _violation(self, operation, *args, **kw):\n        from setuptools.sandbox import SandboxViolation\n\n        raise SandboxViolation(operation, args, kw)\n\n    def _open(self, path, mode='r', *args, **kw):\n        if mode not in ('r', 'rt', 'rb', 'rU', 'U') and not self._ok(path):\n            self._violation(\"open\", path, mode, *args, **kw)\n        return _open(path, mode, *args, **kw)\n\n    def tmpnam(self) -> None:\n        self._violation(\"tmpnam\")\n\n    def _ok(self, path):\n        active = self._active\n        try:\n            self._active = False\n            realpath = os.path.normcase(os.path.realpath(path))\n            return (\n                self._exempted(realpath)\n                or realpath == self._sandbox\n                or realpath.startswith(self._prefix)\n            )\n        finally:\n            self._active = active\n\n    def _exempted(self, filepath):\n        start_matches = (\n            filepath.startswith(exception) for exception in self._exceptions\n        )\n        pattern_matches = (\n            re.match(pattern, filepath) for pattern in self._exception_patterns\n        )\n        candidates = itertools.chain(start_matches, pattern_matches)\n        return any(candidates)\n\n    def _remap_input(self, operation, path, *args, **kw):\n        \"\"\"Called for path inputs\"\"\"\n        if operation in self.write_ops and not self._ok(path):\n            self._violation(operation, os.path.realpath(path), *args, **kw)\n        return path\n\n    def _remap_pair(self, operation, src, dst, *args, **kw):\n        \"\"\"Called for path pairs like rename, link, and symlink operations\"\"\"\n        if not self._ok(src) or not self._ok(dst):\n            self._violation(operation, src, dst, *args, **kw)\n        return (src, dst)\n\n    def open(self, file, flags, mode: int = 0o777, *args, **kw) -> int:\n        \"\"\"Called for low-level os.open()\"\"\"\n        if flags & WRITE_FLAGS and not self._ok(file):\n            self._violation(\"os.open\", file, flags, mode, *args, **kw)\n        return _os.open(file, flags, mode, *args, **kw)\n\n\nWRITE_FLAGS = functools.reduce(\n    operator.or_,\n    [\n        getattr(_os, a, 0)\n        for a in \"O_WRONLY O_RDWR O_APPEND O_CREAT O_TRUNC O_TEMPORARY\".split()\n    ],\n)\n\n\nclass SandboxViolation(DistutilsError):\n    \"\"\"A setup script attempted to modify the filesystem outside the sandbox\"\"\"\n\n    tmpl = textwrap.dedent(\n        \"\"\"\n        SandboxViolation: {cmd}{args!r} {kwargs}\n\n        The package setup script has attempted to modify files on your system\n        that are not within the EasyInstall build area, and has been aborted.\n\n        This package cannot be safely installed by EasyInstall, and may not\n        support alternate installation locations even if you run its setup\n        script by hand.  Please inform the package's author and the EasyInstall\n        maintainers to find out if a fix or workaround is available.\n        \"\"\"\n    ).lstrip()\n\n    def __str__(self) -> str:\n        cmd, args, kwargs = self.args\n        return self.tmpl.format(**locals())\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/tests/__init__.py","size":335,"sha1":"0d561c95521cd8e369ca11721f886ca164e574aa","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import locale\nimport sys\n\nimport pytest\n\n__all__ = ['fail_on_ascii']\n\nif sys.version_info >= (3, 11):\n    locale_encoding = locale.getencoding()\nelse:\n    locale_encoding = locale.getpreferredencoding(False)\nis_ascii = locale_encoding == 'ANSI_X3.4-1968'\nfail_on_ascii = pytest.mark.xfail(is_ascii, reason=\"Test fails in this locale\")\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/tests/compat/__init__.py","size":0,"sha1":"da39a3ee5e6b4b0d3255bfef95601890afd80709","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":""},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/tests/compat/py39.py","size":135,"sha1":"a5243bc3e703aac3609b9066f42fa392d6312e0a","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"from jaraco.test.cpython import from_test_support, try_import\n\nos_helper = try_import('os_helper') or from_test_support('can_symlink')\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/tests/config/__init__.py","size":0,"sha1":"da39a3ee5e6b4b0d3255bfef95601890afd80709","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":""},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/tests/config/downloads/__init__.py","size":1827,"sha1":"4bab6697f2ebd0e38af7f7ab1da8311a78f608fe","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"from __future__ import annotations\n\nimport re\nimport time\nfrom pathlib import Path\nfrom urllib.error import HTTPError\nfrom urllib.request import urlopen\n\n__all__ = [\"DOWNLOAD_DIR\", \"retrieve_file\", \"output_file\", \"urls_from_file\"]\n\n\nNAME_REMOVE = (\"http://\", \"https://\", \"github.com/\", \"/raw/\")\nDOWNLOAD_DIR = Path(__file__).parent\n\n\n# ----------------------------------------------------------------------\n# Please update ./preload.py accordingly when modifying this file\n# ----------------------------------------------------------------------\n\n\ndef output_file(url: str, download_dir: Path = DOWNLOAD_DIR) -> Path:\n    file_name = url.strip()\n    for part in NAME_REMOVE:\n        file_name = file_name.replace(part, '').strip().strip('/:').strip()\n    return Path(download_dir, re.sub(r\"[^\\-_\\.\\w\\d]+\", \"_\", file_name))\n\n\ndef retrieve_file(url: str, download_dir: Path = DOWNLOAD_DIR, wait: float = 5) -> Path:\n    path = output_file(url, download_dir)\n    if path.exists():\n        print(f\"Skipping {url} (already exists: {path})\")\n    else:\n        download_dir.mkdir(exist_ok=True, parents=True)\n        print(f\"Downloading {url} to {path}\")\n        try:\n            download(url, path)\n        except HTTPError:\n            time.sleep(wait)  # wait a few seconds and try again.\n            download(url, path)\n    return path\n\n\ndef urls_from_file(list_file: Path) -> list[str]:\n    \"\"\"``list_file`` should be a text file where each line corresponds to a URL to\n    download.\n    \"\"\"\n    print(f\"file: {list_file}\")\n    content = list_file.read_text(encoding=\"utf-8\")\n    return [url for url in content.splitlines() if not url.startswith(\"#\")]\n\n\ndef download(url: str, dest: Path):\n    with urlopen(url) as f:\n        data = f.read()\n\n    with open(dest, \"wb\") as f:\n        f.write(data)\n\n    assert Path(dest).exists()\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/tests/config/downloads/preload.py","size":450,"sha1":"f5d6ea34e6e05b505e779758e538adcbb029917f","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"This file can be used to preload files needed for testing.\n\nFor example you can use::\n\n    cd setuptools/tests/config\n    python -m downloads.preload setupcfg_examples.txt\n\nto make sure the `setup.cfg` examples are downloaded before starting the tests.\n\"\"\"\n\nimport sys\nfrom pathlib import Path\n\nfrom . import retrieve_file, urls_from_file\n\nif __name__ == \"__main__\":\n    urls = urls_from_file(Path(sys.argv[1]))\n    list(map(retrieve_file, urls))\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/tests/config/test_apply_pyprojecttoml.py","size":20286,"sha1":"eb13e7e735169bb25715977f05650e02c7bf2270","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"Make sure that applying the configuration from pyproject.toml is equivalent to\napplying a similar configuration from setup.cfg\n\nTo run these tests offline, please have a look on ``./downloads/preload.py``\n\"\"\"\n\nfrom __future__ import annotations\n\nimport io\nimport re\nimport tarfile\nfrom inspect import cleandoc\nfrom pathlib import Path\nfrom unittest.mock import Mock\n\nimport pytest\nfrom ini2toml.api import LiteTranslator\nfrom packaging.metadata import Metadata\n\nimport setuptools  # noqa: F401 # ensure monkey patch to metadata\nfrom setuptools._static import is_static\nfrom setuptools.command.egg_info import write_requirements\nfrom setuptools.config import expand, pyprojecttoml, setupcfg\nfrom setuptools.config._apply_pyprojecttoml import _MissingDynamic, _some_attrgetter\nfrom setuptools.dist import Distribution\nfrom setuptools.errors import RemovedConfigError\n\nfrom .downloads import retrieve_file, urls_from_file\n\nHERE = Path(__file__).parent\nEXAMPLES_FILE = \"setupcfg_examples.txt\"\n\n\ndef makedist(path, **attrs):\n    return Distribution({\"src_root\": path, **attrs})\n\n\n@pytest.mark.parametrize(\"url\", urls_from_file(HERE / EXAMPLES_FILE))\n@pytest.mark.filterwarnings(\"ignore\")\n@pytest.mark.uses_network\ndef test_apply_pyproject_equivalent_to_setupcfg(url, monkeypatch, tmp_path):\n    monkeypatch.setattr(expand, \"read_attr\", Mock(return_value=\"0.0.1\"))\n    setupcfg_example = retrieve_file(url)\n    pyproject_example = Path(tmp_path, \"pyproject.toml\")\n    setupcfg_text = setupcfg_example.read_text(encoding=\"utf-8\")\n    toml_config = LiteTranslator().translate(setupcfg_text, \"setup.cfg\")\n    pyproject_example.write_text(toml_config, encoding=\"utf-8\")\n\n    dist_toml = pyprojecttoml.apply_configuration(makedist(tmp_path), pyproject_example)\n    dist_cfg = setupcfg.apply_configuration(makedist(tmp_path), setupcfg_example)\n\n    pkg_info_toml = core_metadata(dist_toml)\n    pkg_info_cfg = core_metadata(dist_cfg)\n    assert pkg_info_toml == pkg_info_cfg\n\n    if any(getattr(d, \"license_files\", None) for d in (dist_toml, dist_cfg)):\n        assert set(dist_toml.license_files) == set(dist_cfg.license_files)\n\n    if any(getattr(d, \"entry_points\", None) for d in (dist_toml, dist_cfg)):\n        print(dist_cfg.entry_points)\n        ep_toml = {\n            (k, *sorted(i.replace(\" \", \"\") for i in v))\n            for k, v in dist_toml.entry_points.items()\n        }\n        ep_cfg = {\n            (k, *sorted(i.replace(\" \", \"\") for i in v))\n            for k, v in dist_cfg.entry_points.items()\n        }\n        assert ep_toml == ep_cfg\n\n    if any(getattr(d, \"package_data\", None) for d in (dist_toml, dist_cfg)):\n        pkg_data_toml = {(k, *sorted(v)) for k, v in dist_toml.package_data.items()}\n        pkg_data_cfg = {(k, *sorted(v)) for k, v in dist_cfg.package_data.items()}\n        assert pkg_data_toml == pkg_data_cfg\n\n    if any(getattr(d, \"data_files\", None) for d in (dist_toml, dist_cfg)):\n        data_files_toml = {(k, *sorted(v)) for k, v in dist_toml.data_files}\n        data_files_cfg = {(k, *sorted(v)) for k, v in dist_cfg.data_files}\n        assert data_files_toml == data_files_cfg\n\n    assert set(dist_toml.install_requires) == set(dist_cfg.install_requires)\n    if any(getattr(d, \"extras_require\", None) for d in (dist_toml, dist_cfg)):\n        extra_req_toml = {(k, *sorted(v)) for k, v in dist_toml.extras_require.items()}\n        extra_req_cfg = {(k, *sorted(v)) for k, v in dist_cfg.extras_require.items()}\n        assert extra_req_toml == extra_req_cfg\n\n\nPEP621_EXAMPLE = \"\"\"\\\n[project]\nname = \"spam\"\nversion = \"2020.0.0\"\ndescription = \"Lovely Spam! Wonderful Spam!\"\nreadme = \"README.rst\"\nrequires-python = \">=3.8\"\nlicense = {file = \"LICENSE.txt\"}\nkeywords = [\"egg\", \"bacon\", \"sausage\", \"tomatoes\", \"Lobster Thermidor\"]\nauthors = [\n  {email = \"hi@pradyunsg.me\"},\n  {name = \"Tzu-Ping Chung\"}\n]\nmaintainers = [\n  {name = \"Brett Cannon\", email = \"brett@python.org\"},\n  {name = \"John X. re\", email = \"john@utf8.org\"},\n  {name = \"  \", email = \"gama@utf8.org\"},\n]\nclassifiers = [\n  \"Development Status :: 4 - Beta\",\n  \"Programming Language :: Python\"\n]\n\ndependencies = [\n  \"httpx\",\n  \"gidgethub[httpx]>4.0.0\",\n  \"django>2.1; os_name != 'nt'\",\n  \"django>2.0; os_name == 'nt'\"\n]\n\n[project.optional-dependencies]\ntest = [\n  \"pytest < 5.0.0\",\n  \"pytest-cov[all]\"\n]\n\n[project.urls]\nhomepage = \"http://example.com\"\ndocumentation = \"http://readthedocs.org\"\nrepository = \"http://github.com\"\nchangelog = \"http://github.com/me/spam/blob/master/CHANGELOG.md\"\n\n[project.scripts]\nspam-cli = \"spam:main_cli\"\n\n[project.gui-scripts]\nspam-gui = \"spam:main_gui\"\n\n[project.entry-points.\"spam.magical\"]\ntomatoes = \"spam:main_tomatoes\"\n\"\"\"\n\nPEP621_INTERNATIONAL_EMAIL_EXAMPLE = \"\"\"\\\n[project]\nname = \"spam\"\nversion = \"2020.0.0\"\nauthors = [\n  {email = \"hi@pradyunsg.me\"},\n  {name = \"Tzu-Ping Chung\"}\n]\nmaintainers = [\n  {name = \" \", email = \"@-.\"},\n]\n\"\"\"\n\nPEP621_EXAMPLE_SCRIPT = \"\"\"\ndef main_cli(): pass\ndef main_gui(): pass\ndef main_tomatoes(): pass\n\"\"\"\n\n\ndef _pep621_example_project(\n    tmp_path,\n    readme=\"README.rst\",\n    pyproject_text=PEP621_EXAMPLE,\n):\n    pyproject = tmp_path / \"pyproject.toml\"\n    text = pyproject_text\n    replacements = {'readme = \"README.rst\"': f'readme = \"{readme}\"'}\n    for orig, subst in replacements.items():\n        text = text.replace(orig, subst)\n    pyproject.write_text(text, encoding=\"utf-8\")\n\n    (tmp_path / readme).write_text(\"hello world\", encoding=\"utf-8\")\n    (tmp_path / \"LICENSE.txt\").write_text(\"--- LICENSE stub ---\", encoding=\"utf-8\")\n    (tmp_path / \"spam.py\").write_text(PEP621_EXAMPLE_SCRIPT, encoding=\"utf-8\")\n    return pyproject\n\n\ndef test_pep621_example(tmp_path):\n    \"\"\"Make sure the example in PEP 621 works\"\"\"\n    pyproject = _pep621_example_project(tmp_path)\n    dist = pyprojecttoml.apply_configuration(makedist(tmp_path), pyproject)\n    assert dist.metadata.license == \"--- LICENSE stub ---\"\n    assert set(dist.metadata.license_files) == {\"LICENSE.txt\"}\n\n\n@pytest.mark.parametrize(\n    (\"readme\", \"ctype\"),\n    [\n        (\"Readme.txt\", \"text/plain\"),\n        (\"readme.md\", \"text/markdown\"),\n        (\"text.rst\", \"text/x-rst\"),\n    ],\n)\ndef test_readme_content_type(tmp_path, readme, ctype):\n    pyproject = _pep621_example_project(tmp_path, readme)\n    dist = pyprojecttoml.apply_configuration(makedist(tmp_path), pyproject)\n    assert dist.metadata.long_description_content_type == ctype\n\n\ndef test_undefined_content_type(tmp_path):\n    pyproject = _pep621_example_project(tmp_path, \"README.tex\")\n    with pytest.raises(ValueError, match=\"Undefined content type for README.tex\"):\n        pyprojecttoml.apply_configuration(makedist(tmp_path), pyproject)\n\n\ndef test_no_explicit_content_type_for_missing_extension(tmp_path):\n    pyproject = _pep621_example_project(tmp_path, \"README\")\n    dist = pyprojecttoml.apply_configuration(makedist(tmp_path), pyproject)\n    assert dist.metadata.long_description_content_type is None\n\n\n@pytest.mark.parametrize(\n    (\"pyproject_text\", \"expected_maintainers_meta_value\"),\n    (\n        pytest.param(\n            PEP621_EXAMPLE,\n            (\n                'Brett Cannon <brett@python.org>, \"John X. re\" <john@utf8.org>, '\n                '   <gama@utf8.org>'\n            ),\n            id='non-international-emails',\n        ),\n        pytest.param(\n            PEP621_INTERNATIONAL_EMAIL_EXAMPLE,\n            '  <@-.>',\n            marks=pytest.mark.xfail(\n                reason=\"CPython's `email.headerregistry.Address` only supports \"\n                'RFC 5322, as of Nov 10, 2022 and latest Python 3.11.0',\n                strict=True,\n            ),\n            id='international-email',\n        ),\n    ),\n)\ndef test_utf8_maintainer_in_metadata(  # issue-3663\n    expected_maintainers_meta_value,\n    pyproject_text,\n    tmp_path,\n):\n    pyproject = _pep621_example_project(\n        tmp_path,\n        \"README\",\n        pyproject_text=pyproject_text,\n    )\n    dist = pyprojecttoml.apply_configuration(makedist(tmp_path), pyproject)\n    assert dist.metadata.maintainer_email == expected_maintainers_meta_value\n    pkg_file = tmp_path / \"PKG-FILE\"\n    with open(pkg_file, \"w\", encoding=\"utf-8\") as fh:\n        dist.metadata.write_pkg_file(fh)\n    content = pkg_file.read_text(encoding=\"utf-8\")\n    assert f\"Maintainer-email: {expected_maintainers_meta_value}\" in content\n\n\nclass TestLicenseFiles:\n    # TODO: After PEP 639 is accepted, we have to move the license-files\n    #       to the `project` table instead of `tool.setuptools`\n\n    def base_pyproject(self, tmp_path, additional_text):\n        pyproject = _pep621_example_project(tmp_path, \"README\")\n        text = pyproject.read_text(encoding=\"utf-8\")\n\n        # Sanity-check\n        assert 'license = {file = \"LICENSE.txt\"}' in text\n        assert \"[tool.setuptools]\" not in text\n\n        text = f\"{text}\\n{additional_text}\\n\"\n        pyproject.write_text(text, encoding=\"utf-8\")\n        return pyproject\n\n    def test_both_license_and_license_files_defined(self, tmp_path):\n        setuptools_config = '[tool.setuptools]\\nlicense-files = [\"_FILE*\"]'\n        pyproject = self.base_pyproject(tmp_path, setuptools_config)\n\n        (tmp_path / \"_FILE.txt\").touch()\n        (tmp_path / \"_FILE.rst\").touch()\n\n        # Would normally match the `license_files` patterns, but we want to exclude it\n        # by being explicit. On the other hand, contents should be added to `license`\n        license = tmp_path / \"LICENSE.txt\"\n        license.write_text(\"LicenseRef-Proprietary\\n\", encoding=\"utf-8\")\n\n        dist = pyprojecttoml.apply_configuration(makedist(tmp_path), pyproject)\n        assert set(dist.metadata.license_files) == {\"_FILE.rst\", \"_FILE.txt\"}\n        assert dist.metadata.license == \"LicenseRef-Proprietary\\n\"\n\n    def test_default_patterns(self, tmp_path):\n        setuptools_config = '[tool.setuptools]\\nzip-safe = false'\n        # ^ used just to trigger section validation\n        pyproject = self.base_pyproject(tmp_path, setuptools_config)\n\n        license_files = \"LICENCE-a.html COPYING-abc.txt AUTHORS-xyz NOTICE,def\".split()\n\n        for fname in license_files:\n            (tmp_path / fname).write_text(f\"{fname}\\n\", encoding=\"utf-8\")\n\n        dist = pyprojecttoml.apply_configuration(makedist(tmp_path), pyproject)\n        assert (tmp_path / \"LICENSE.txt\").exists()  # from base example\n        assert set(dist.metadata.license_files) == {*license_files, \"LICENSE.txt\"}\n\n\nclass TestPyModules:\n    # https://github.com/pypa/setuptools/issues/4316\n\n    def dist(self, name):\n        toml_config = f\"\"\"\n        [project]\n        name = \"test\"\n        version = \"42.0\"\n        [tool.setuptools]\n        py-modules = [{name!r}]\n        \"\"\"\n        pyproject = Path(\"pyproject.toml\")\n        pyproject.write_text(cleandoc(toml_config), encoding=\"utf-8\")\n        return pyprojecttoml.apply_configuration(Distribution({}), pyproject)\n\n    @pytest.mark.parametrize(\"module\", [\"pip-run\", \"abc-d.-xyz-e\"])\n    def test_valid_module_name(self, tmp_path, monkeypatch, module):\n        monkeypatch.chdir(tmp_path)\n        assert module in self.dist(module).py_modules\n\n    @pytest.mark.parametrize(\"module\", [\"pip run\", \"-pip-run\", \"pip-run-stubs\"])\n    def test_invalid_module_name(self, tmp_path, monkeypatch, module):\n        monkeypatch.chdir(tmp_path)\n        with pytest.raises(ValueError, match=\"py-modules\"):\n            self.dist(module).py_modules\n\n\nclass TestExtModules:\n    def test_pyproject_sets_attribute(self, tmp_path, monkeypatch):\n        monkeypatch.chdir(tmp_path)\n        pyproject = Path(\"pyproject.toml\")\n        toml_config = \"\"\"\n        [project]\n        name = \"test\"\n        version = \"42.0\"\n        [tool.setuptools]\n        ext-modules = [\n          {name = \"my.ext\", sources = [\"hello.c\", \"world.c\"]}\n        ]\n        \"\"\"\n        pyproject.write_text(cleandoc(toml_config), encoding=\"utf-8\")\n        with pytest.warns(pyprojecttoml._ExperimentalConfiguration):\n            dist = pyprojecttoml.apply_configuration(Distribution({}), pyproject)\n        assert len(dist.ext_modules) == 1\n        assert dist.ext_modules[0].name == \"my.ext\"\n        assert set(dist.ext_modules[0].sources) == {\"hello.c\", \"world.c\"}\n\n\nclass TestDeprecatedFields:\n    def test_namespace_packages(self, tmp_path):\n        pyproject = tmp_path / \"pyproject.toml\"\n        config = \"\"\"\n        [project]\n        name = \"myproj\"\n        version = \"42\"\n        [tool.setuptools]\n        namespace-packages = [\"myproj.pkg\"]\n        \"\"\"\n        pyproject.write_text(cleandoc(config), encoding=\"utf-8\")\n        with pytest.raises(RemovedConfigError, match=\"namespace-packages\"):\n            pyprojecttoml.apply_configuration(makedist(tmp_path), pyproject)\n\n\nclass TestPresetField:\n    def pyproject(self, tmp_path, dynamic, extra_content=\"\"):\n        content = f\"[project]\\nname = 'proj'\\ndynamic = {dynamic!r}\\n\"\n        if \"version\" not in dynamic:\n            content += \"version = '42'\\n\"\n        file = tmp_path / \"pyproject.toml\"\n        file.write_text(content + extra_content, encoding=\"utf-8\")\n        return file\n\n    @pytest.mark.parametrize(\n        (\"attr\", \"field\", \"value\"),\n        [\n            (\"classifiers\", \"classifiers\", [\"Private :: Classifier\"]),\n            (\"entry_points\", \"scripts\", {\"console_scripts\": [\"foobar=foobar:main\"]}),\n            (\"entry_points\", \"gui-scripts\", {\"gui_scripts\": [\"bazquux=bazquux:main\"]}),\n            pytest.param(\n                *(\"install_requires\", \"dependencies\", [\"six\"]),\n                marks=[\n                    pytest.mark.filterwarnings(\"ignore:.*install_requires. overwritten\")\n                ],\n            ),\n        ],\n    )\n    def test_not_listed_in_dynamic(self, tmp_path, attr, field, value):\n        \"\"\"Setuptools cannot set a field if not listed in ``dynamic``\"\"\"\n        pyproject = self.pyproject(tmp_path, [])\n        dist = makedist(tmp_path, **{attr: value})\n        msg = re.compile(f\"defined outside of `pyproject.toml`:.*{field}\", re.S)\n        with pytest.warns(_MissingDynamic, match=msg):\n            dist = pyprojecttoml.apply_configuration(dist, pyproject)\n\n        dist_value = _some_attrgetter(f\"metadata.{attr}\", attr)(dist)\n        assert not dist_value\n\n    @pytest.mark.parametrize(\n        (\"attr\", \"field\", \"value\"),\n        [\n            (\"install_requires\", \"dependencies\", []),\n            (\"extras_require\", \"optional-dependencies\", {}),\n            (\"install_requires\", \"dependencies\", [\"six\"]),\n            (\"classifiers\", \"classifiers\", [\"Private :: Classifier\"]),\n        ],\n    )\n    def test_listed_in_dynamic(self, tmp_path, attr, field, value):\n        pyproject = self.pyproject(tmp_path, [field])\n        dist = makedist(tmp_path, **{attr: value})\n        dist = pyprojecttoml.apply_configuration(dist, pyproject)\n        dist_value = _some_attrgetter(f\"metadata.{attr}\", attr)(dist)\n        assert dist_value == value\n\n    def test_warning_overwritten_dependencies(self, tmp_path):\n        src = \"[project]\\nname='pkg'\\nversion='0.1'\\ndependencies=['click']\\n\"\n        pyproject = tmp_path / \"pyproject.toml\"\n        pyproject.write_text(src, encoding=\"utf-8\")\n        dist = makedist(tmp_path, install_requires=[\"wheel\"])\n        with pytest.warns(match=\"`install_requires` overwritten\"):\n            dist = pyprojecttoml.apply_configuration(dist, pyproject)\n        assert \"wheel\" not in dist.install_requires\n\n    def test_optional_dependencies_dont_remove_env_markers(self, tmp_path):\n        \"\"\"\n        Internally setuptools converts dependencies with markers to \"extras\".\n        If ``install_requires`` is given by ``setup.py``, we have to ensure that\n        applying ``optional-dependencies`` does not overwrite the mandatory\n        dependencies with markers (see #3204).\n        \"\"\"\n        # If setuptools replace its internal mechanism that uses `requires.txt`\n        # this test has to be rewritten to adapt accordingly\n        extra = \"\\n[project.optional-dependencies]\\nfoo = ['bar>1']\\n\"\n        pyproject = self.pyproject(tmp_path, [\"dependencies\"], extra)\n        install_req = ['importlib-resources (>=3.0.0) ; python_version < \"3.7\"']\n        dist = makedist(tmp_path, install_requires=install_req)\n        dist = pyprojecttoml.apply_configuration(dist, pyproject)\n        assert \"foo\" in dist.extras_require\n        egg_info = dist.get_command_obj(\"egg_info\")\n        write_requirements(egg_info, tmp_path, tmp_path / \"requires.txt\")\n        reqs = (tmp_path / \"requires.txt\").read_text(encoding=\"utf-8\")\n        assert \"importlib-resources\" in reqs\n        assert \"bar\" in reqs\n        assert ':python_version < \"3.7\"' in reqs\n\n    @pytest.mark.parametrize(\n        (\"field\", \"group\"),\n        [(\"scripts\", \"console_scripts\"), (\"gui-scripts\", \"gui_scripts\")],\n    )\n    @pytest.mark.filterwarnings(\"error\")\n    def test_scripts_dont_require_dynamic_entry_points(self, tmp_path, field, group):\n        # Issue 3862\n        pyproject = self.pyproject(tmp_path, [field])\n        dist = makedist(tmp_path, entry_points={group: [\"foobar=foobar:main\"]})\n        dist = pyprojecttoml.apply_configuration(dist, pyproject)\n        assert group in dist.entry_points\n\n\nclass TestMeta:\n    def test_example_file_in_sdist(self, setuptools_sdist):\n        \"\"\"Meta test to ensure tests can run from sdist\"\"\"\n        with tarfile.open(setuptools_sdist) as tar:\n            assert any(name.endswith(EXAMPLES_FILE) for name in tar.getnames())\n\n\nclass TestInteropCommandLineParsing:\n    def test_version(self, tmp_path, monkeypatch, capsys):\n        # See pypa/setuptools#4047\n        # This test can be removed once the CLI interface of setup.py is removed\n        monkeypatch.chdir(tmp_path)\n        toml_config = \"\"\"\n        [project]\n        name = \"test\"\n        version = \"42.0\"\n        \"\"\"\n        pyproject = Path(tmp_path, \"pyproject.toml\")\n        pyproject.write_text(cleandoc(toml_config), encoding=\"utf-8\")\n        opts = {\"script_args\": [\"--version\"]}\n        dist = pyprojecttoml.apply_configuration(Distribution(opts), pyproject)\n        dist.parse_command_line()  # <-- there should be no exception here.\n        captured = capsys.readouterr()\n        assert \"42.0\" in captured.out\n\n\nclass TestStaticConfig:\n    def test_mark_static_fields(self, tmp_path, monkeypatch):\n        monkeypatch.chdir(tmp_path)\n        toml_config = \"\"\"\n        [project]\n        name = \"test\"\n        version = \"42.0\"\n        dependencies = [\"hello\"]\n        keywords = [\"world\"]\n        classifiers = [\"private :: hello world\"]\n        [tool.setuptools]\n        obsoletes = [\"abcd\"]\n        provides = [\"abcd\"]\n        platforms = [\"abcd\"]\n        \"\"\"\n        pyproject = Path(tmp_path, \"pyproject.toml\")\n        pyproject.write_text(cleandoc(toml_config), encoding=\"utf-8\")\n        dist = pyprojecttoml.apply_configuration(Distribution({}), pyproject)\n        assert is_static(dist.install_requires)\n        assert is_static(dist.metadata.keywords)\n        assert is_static(dist.metadata.classifiers)\n        assert is_static(dist.metadata.obsoletes)\n        assert is_static(dist.metadata.provides)\n        assert is_static(dist.metadata.platforms)\n\n\n# --- Auxiliary Functions ---\n\n\ndef core_metadata(dist) -> str:\n    with io.StringIO() as buffer:\n        dist.metadata.write_pkg_file(buffer)\n        pkg_file_txt = buffer.getvalue()\n\n    # Make sure core metadata is valid\n    Metadata.from_email(pkg_file_txt, validate=True)  # can raise exceptions\n\n    skip_prefixes: tuple[str, ...] = ()\n    skip_lines = set()\n    # ---- DIFF NORMALISATION ----\n    # PEP 621 is very particular about author/maintainer metadata conversion, so skip\n    skip_prefixes += (\"Author:\", \"Author-email:\", \"Maintainer:\", \"Maintainer-email:\")\n    # May be redundant with Home-page\n    skip_prefixes += (\"Project-URL: Homepage,\", \"Home-page:\")\n    # May be missing in original (relying on default) but backfilled in the TOML\n    skip_prefixes += (\"Description-Content-Type:\",)\n    # Remove empty lines\n    skip_lines.add(\"\")\n\n    result = []\n    for line in pkg_file_txt.splitlines():\n        if line.startswith(skip_prefixes) or line in skip_lines:\n            continue\n        result.append(line + \"\\n\")\n\n    return \"\".join(result)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/tests/config/test_expand.py","size":8933,"sha1":"7289606d22e5079b0cfcf91d0f41081f47c5ee58","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import os\nimport sys\nfrom pathlib import Path\n\nimport pytest\n\nfrom setuptools._static import is_static\nfrom setuptools.config import expand\nfrom setuptools.discovery import find_package_path\n\nfrom distutils.errors import DistutilsOptionError\n\n\ndef write_files(files, root_dir):\n    for file, content in files.items():\n        path = root_dir / file\n        path.parent.mkdir(exist_ok=True, parents=True)\n        path.write_text(content, encoding=\"utf-8\")\n\n\ndef test_glob_relative(tmp_path, monkeypatch):\n    files = {\n        \"dir1/dir2/dir3/file1.txt\",\n        \"dir1/dir2/file2.txt\",\n        \"dir1/file3.txt\",\n        \"a.ini\",\n        \"b.ini\",\n        \"dir1/c.ini\",\n        \"dir1/dir2/a.ini\",\n    }\n\n    write_files({k: \"\" for k in files}, tmp_path)\n    patterns = [\"**/*.txt\", \"[ab].*\", \"**/[ac].ini\"]\n    monkeypatch.chdir(tmp_path)\n    assert set(expand.glob_relative(patterns)) == files\n    # Make sure the same APIs work outside cwd\n    assert set(expand.glob_relative(patterns, tmp_path)) == files\n\n\ndef test_read_files(tmp_path, monkeypatch):\n    dir_ = tmp_path / \"dir_\"\n    (tmp_path / \"_dir\").mkdir(exist_ok=True)\n    (tmp_path / \"a.txt\").touch()\n    files = {\"a.txt\": \"a\", \"dir1/b.txt\": \"b\", \"dir1/dir2/c.txt\": \"c\"}\n    write_files(files, dir_)\n\n    secrets = Path(str(dir_) + \"secrets\")\n    secrets.mkdir(exist_ok=True)\n    write_files({\"secrets.txt\": \"secret keys\"}, secrets)\n\n    with monkeypatch.context() as m:\n        m.chdir(dir_)\n        assert expand.read_files(list(files)) == \"a\\nb\\nc\"\n\n        cannot_access_msg = r\"Cannot access '.*\\.\\..a\\.txt'\"\n        with pytest.raises(DistutilsOptionError, match=cannot_access_msg):\n            expand.read_files([\"../a.txt\"])\n\n        cannot_access_secrets_msg = r\"Cannot access '.*secrets\\.txt'\"\n        with pytest.raises(DistutilsOptionError, match=cannot_access_secrets_msg):\n            expand.read_files([\"../dir_secrets/secrets.txt\"])\n\n    # Make sure the same APIs work outside cwd\n    assert expand.read_files(list(files), dir_) == \"a\\nb\\nc\"\n    with pytest.raises(DistutilsOptionError, match=cannot_access_msg):\n        expand.read_files([\"../a.txt\"], dir_)\n\n\nclass TestReadAttr:\n    @pytest.mark.parametrize(\n        \"example\",\n        [\n            # No cookie means UTF-8:\n            b\"__version__ = '\\xc3\\xa9'\\nraise SystemExit(1)\\n\",\n            # If a cookie is present, honor it:\n            b\"# -*- coding: utf-8 -*-\\n__version__ = '\\xc3\\xa9'\\nraise SystemExit(1)\\n\",\n            b\"# -*- coding: latin1 -*-\\n__version__ = '\\xe9'\\nraise SystemExit(1)\\n\",\n        ],\n    )\n    def test_read_attr_encoding_cookie(self, example, tmp_path):\n        (tmp_path / \"mod.py\").write_bytes(example)\n        assert expand.read_attr('mod.__version__', root_dir=tmp_path) == ''\n\n    def test_read_attr(self, tmp_path, monkeypatch):\n        files = {\n            \"pkg/__init__.py\": \"\",\n            \"pkg/sub/__init__.py\": \"VERSION = '0.1.1'\",\n            \"pkg/sub/mod.py\": (\n                \"VALUES = {'a': 0, 'b': {42}, 'c': (0, 1, 1)}\\nraise SystemExit(1)\"\n            ),\n        }\n        write_files(files, tmp_path)\n\n        with monkeypatch.context() as m:\n            m.chdir(tmp_path)\n            # Make sure it can read the attr statically without evaluating the module\n            version = expand.read_attr('pkg.sub.VERSION')\n            values = expand.read_attr('lib.mod.VALUES', {'lib': 'pkg/sub'})\n\n        assert version == '0.1.1'\n        assert is_static(values)\n\n        assert values['a'] == 0\n        assert values['b'] == {42}\n        assert is_static(values)\n\n        # Make sure the same APIs work outside cwd\n        assert expand.read_attr('pkg.sub.VERSION', root_dir=tmp_path) == '0.1.1'\n        values = expand.read_attr('lib.mod.VALUES', {'lib': 'pkg/sub'}, tmp_path)\n        assert values['c'] == (0, 1, 1)\n\n    @pytest.mark.parametrize(\n        \"example\",\n        [\n            \"VERSION: str\\nVERSION = '0.1.1'\\nraise SystemExit(1)\\n\",\n            \"VERSION: str = '0.1.1'\\nraise SystemExit(1)\\n\",\n        ],\n    )\n    def test_read_annotated_attr(self, tmp_path, example):\n        files = {\n            \"pkg/__init__.py\": \"\",\n            \"pkg/sub/__init__.py\": example,\n        }\n        write_files(files, tmp_path)\n        # Make sure this attribute can be read statically\n        version = expand.read_attr('pkg.sub.VERSION', root_dir=tmp_path)\n        assert version == '0.1.1'\n        assert is_static(version)\n\n    @pytest.mark.parametrize(\n        \"example\",\n        [\n            \"VERSION = (lambda: '0.1.1')()\\n\",\n            \"def fn(): return '0.1.1'\\nVERSION = fn()\\n\",\n            \"VERSION: str = (lambda: '0.1.1')()\\n\",\n        ],\n    )\n    def test_read_dynamic_attr(self, tmp_path, monkeypatch, example):\n        files = {\n            \"pkg/__init__.py\": \"\",\n            \"pkg/sub/__init__.py\": example,\n        }\n        write_files(files, tmp_path)\n        monkeypatch.chdir(tmp_path)\n        version = expand.read_attr('pkg.sub.VERSION')\n        assert version == '0.1.1'\n        assert not is_static(version)\n\n    def test_import_order(self, tmp_path):\n        \"\"\"\n        Sometimes the import machinery will import the parent package of a nested\n        module, which triggers side-effects and might create problems (see issue #3176)\n\n        ``read_attr`` should bypass these limitations by resolving modules statically\n        (via ast.literal_eval).\n        \"\"\"\n        files = {\n            \"src/pkg/__init__.py\": \"from .main import func\\nfrom .about import version\",\n            \"src/pkg/main.py\": \"import super_complicated_dep\\ndef func(): return 42\",\n            \"src/pkg/about.py\": \"version = '42'\",\n        }\n        write_files(files, tmp_path)\n        attr_desc = \"pkg.about.version\"\n        package_dir = {\"\": \"src\"}\n        # `import super_complicated_dep` should not run, otherwise the build fails\n        assert expand.read_attr(attr_desc, package_dir, tmp_path) == \"42\"\n\n\n@pytest.mark.parametrize(\n    (\"package_dir\", \"file\", \"module\", \"return_value\"),\n    [\n        ({\"\": \"src\"}, \"src/pkg/main.py\", \"pkg.main\", 42),\n        ({\"pkg\": \"lib\"}, \"lib/main.py\", \"pkg.main\", 13),\n        ({}, \"single_module.py\", \"single_module\", 70),\n        ({}, \"flat_layout/pkg.py\", \"flat_layout.pkg\", 836),\n    ],\n)\ndef test_resolve_class(monkeypatch, tmp_path, package_dir, file, module, return_value):\n    monkeypatch.setattr(sys, \"modules\", {})  # reproducibility\n    files = {file: f\"class Custom:\\n    def testing(self): return {return_value}\"}\n    write_files(files, tmp_path)\n    cls = expand.resolve_class(f\"{module}.Custom\", package_dir, tmp_path)\n    assert cls().testing() == return_value\n\n\n@pytest.mark.parametrize(\n    (\"args\", \"pkgs\"),\n    [\n        ({\"where\": [\".\"], \"namespaces\": False}, {\"pkg\", \"other\"}),\n        ({\"where\": [\".\", \"dir1\"], \"namespaces\": False}, {\"pkg\", \"other\", \"dir2\"}),\n        ({\"namespaces\": True}, {\"pkg\", \"other\", \"dir1\", \"dir1.dir2\"}),\n        ({}, {\"pkg\", \"other\", \"dir1\", \"dir1.dir2\"}),  # default value for `namespaces`\n    ],\n)\ndef test_find_packages(tmp_path, args, pkgs):\n    files = {\n        \"pkg/__init__.py\",\n        \"other/__init__.py\",\n        \"dir1/dir2/__init__.py\",\n    }\n    write_files({k: \"\" for k in files}, tmp_path)\n\n    package_dir = {}\n    kwargs = {\"root_dir\": tmp_path, \"fill_package_dir\": package_dir, **args}\n    where = kwargs.get(\"where\", [\".\"])\n    assert set(expand.find_packages(**kwargs)) == pkgs\n    for pkg in pkgs:\n        pkg_path = find_package_path(pkg, package_dir, tmp_path)\n        assert os.path.exists(pkg_path)\n\n    # Make sure the same APIs work outside cwd\n    where = [\n        str((tmp_path / p).resolve()).replace(os.sep, \"/\")  # ensure posix-style paths\n        for p in args.pop(\"where\", [\".\"])\n    ]\n\n    assert set(expand.find_packages(where=where, **args)) == pkgs\n\n\n@pytest.mark.parametrize(\n    (\"files\", \"where\", \"expected_package_dir\"),\n    [\n        ([\"pkg1/__init__.py\", \"pkg1/other.py\"], [\".\"], {}),\n        ([\"pkg1/__init__.py\", \"pkg2/__init__.py\"], [\".\"], {}),\n        ([\"src/pkg1/__init__.py\", \"src/pkg1/other.py\"], [\"src\"], {\"\": \"src\"}),\n        ([\"src/pkg1/__init__.py\", \"src/pkg2/__init__.py\"], [\"src\"], {\"\": \"src\"}),\n        (\n            [\"src1/pkg1/__init__.py\", \"src2/pkg2/__init__.py\"],\n            [\"src1\", \"src2\"],\n            {\"pkg1\": \"src1/pkg1\", \"pkg2\": \"src2/pkg2\"},\n        ),\n        (\n            [\"src/pkg1/__init__.py\", \"pkg2/__init__.py\"],\n            [\"src\", \".\"],\n            {\"pkg1\": \"src/pkg1\"},\n        ),\n    ],\n)\ndef test_fill_package_dir(tmp_path, files, where, expected_package_dir):\n    write_files({k: \"\" for k in files}, tmp_path)\n    pkg_dir = {}\n    kwargs = {\"root_dir\": tmp_path, \"fill_package_dir\": pkg_dir, \"namespaces\": False}\n    pkgs = expand.find_packages(where=where, **kwargs)\n    assert set(pkg_dir.items()) == set(expected_package_dir.items())\n    for pkg in pkgs:\n        pkg_path = find_package_path(pkg, pkg_dir, tmp_path)\n        assert os.path.exists(pkg_path)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/tests/config/test_pyprojecttoml.py","size":12406,"sha1":"b40ab0105faf7d8f8db858a916ae9ef314e4dea4","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import re\nfrom configparser import ConfigParser\nfrom inspect import cleandoc\n\nimport jaraco.path\nimport pytest\nimport tomli_w\nfrom path import Path\n\nimport setuptools  # noqa: F401 # force distutils.core to be patched\nfrom setuptools.config.pyprojecttoml import (\n    _ToolsTypoInMetadata,\n    apply_configuration,\n    expand_configuration,\n    read_configuration,\n    validate,\n)\nfrom setuptools.dist import Distribution\nfrom setuptools.errors import OptionError\n\nimport distutils.core\n\nEXAMPLE = \"\"\"\n[project]\nname = \"myproj\"\nkeywords = [\"some\", \"key\", \"words\"]\ndynamic = [\"version\", \"readme\"]\nrequires-python = \">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*, !=3.4.*\"\ndependencies = [\n    'importlib-metadata>=0.12;python_version<\"3.8\"',\n    'importlib-resources>=1.0;python_version<\"3.7\"',\n    'pathlib2>=2.3.3,<3;python_version < \"3.4\" and sys.platform != \"win32\"',\n]\n\n[project.optional-dependencies]\ndocs = [\n    \"sphinx>=3\",\n    \"sphinx-argparse>=0.2.5\",\n    \"sphinx-rtd-theme>=0.4.3\",\n]\ntesting = [\n    \"pytest>=1\",\n    \"coverage>=3,<5\",\n]\n\n[project.scripts]\nexec = \"pkg.__main__:exec\"\n\n[build-system]\nrequires = [\"setuptools\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[tool.setuptools]\npackage-dir = {\"\" = \"src\"}\nzip-safe = true\nplatforms = [\"any\"]\n\n[tool.setuptools.packages.find]\nwhere = [\"src\"]\n\n[tool.setuptools.cmdclass]\nsdist = \"pkg.mod.CustomSdist\"\n\n[tool.setuptools.dynamic.version]\nattr = \"pkg.__version__.VERSION\"\n\n[tool.setuptools.dynamic.readme]\nfile = [\"README.md\"]\ncontent-type = \"text/markdown\"\n\n[tool.setuptools.package-data]\n\"*\" = [\"*.txt\"]\n\n[tool.setuptools.data-files]\n\"data\" = [\"_files/*.txt\"]\n\n[tool.distutils.sdist]\nformats = \"gztar\"\n\n[tool.distutils.bdist_wheel]\nuniversal = true\n\"\"\"\n\n\ndef create_example(path, pkg_root):\n    files = {\n        \"pyproject.toml\": EXAMPLE,\n        \"README.md\": \"hello world\",\n        \"_files\": {\n            \"file.txt\": \"\",\n        },\n    }\n    packages = {\n        \"pkg\": {\n            \"__init__.py\": \"\",\n            \"mod.py\": \"class CustomSdist: pass\",\n            \"__version__.py\": \"VERSION = (3, 10)\",\n            \"__main__.py\": \"def exec(): print('hello')\",\n        },\n    }\n\n    assert pkg_root  # Meta-test: cannot be empty string.\n\n    if pkg_root == \".\":\n        files = {**files, **packages}\n        # skip other files: flat-layout will raise error for multi-package dist\n    else:\n        # Use this opportunity to ensure namespaces are discovered\n        files[pkg_root] = {**packages, \"other\": {\"nested\": {\"__init__.py\": \"\"}}}\n\n    jaraco.path.build(files, prefix=path)\n\n\ndef verify_example(config, path, pkg_root):\n    pyproject = path / \"pyproject.toml\"\n    pyproject.write_text(tomli_w.dumps(config), encoding=\"utf-8\")\n    expanded = expand_configuration(config, path)\n    expanded_project = expanded[\"project\"]\n    assert read_configuration(pyproject, expand=True) == expanded\n    assert expanded_project[\"version\"] == \"3.10\"\n    assert expanded_project[\"readme\"][\"text\"] == \"hello world\"\n    assert \"packages\" in expanded[\"tool\"][\"setuptools\"]\n    if pkg_root == \".\":\n        # Auto-discovery will raise error for multi-package dist\n        assert set(expanded[\"tool\"][\"setuptools\"][\"packages\"]) == {\"pkg\"}\n    else:\n        assert set(expanded[\"tool\"][\"setuptools\"][\"packages\"]) == {\n            \"pkg\",\n            \"other\",\n            \"other.nested\",\n        }\n    assert expanded[\"tool\"][\"setuptools\"][\"include-package-data\"] is True\n    assert \"\" in expanded[\"tool\"][\"setuptools\"][\"package-data\"]\n    assert \"*\" not in expanded[\"tool\"][\"setuptools\"][\"package-data\"]\n    assert expanded[\"tool\"][\"setuptools\"][\"data-files\"] == [\n        (\"data\", [\"_files/file.txt\"])\n    ]\n\n\ndef test_read_configuration(tmp_path):\n    create_example(tmp_path, \"src\")\n    pyproject = tmp_path / \"pyproject.toml\"\n\n    config = read_configuration(pyproject, expand=False)\n    assert config[\"project\"].get(\"version\") is None\n    assert config[\"project\"].get(\"readme\") is None\n\n    verify_example(config, tmp_path, \"src\")\n\n\n@pytest.mark.parametrize(\n    (\"pkg_root\", \"opts\"),\n    [\n        (\".\", {}),\n        (\"src\", {}),\n        (\"lib\", {\"packages\": {\"find\": {\"where\": [\"lib\"]}}}),\n    ],\n)\ndef test_discovered_package_dir_with_attr_directive_in_config(tmp_path, pkg_root, opts):\n    create_example(tmp_path, pkg_root)\n\n    pyproject = tmp_path / \"pyproject.toml\"\n\n    config = read_configuration(pyproject, expand=False)\n    assert config[\"project\"].get(\"version\") is None\n    assert config[\"project\"].get(\"readme\") is None\n    config[\"tool\"][\"setuptools\"].pop(\"packages\", None)\n    config[\"tool\"][\"setuptools\"].pop(\"package-dir\", None)\n\n    config[\"tool\"][\"setuptools\"].update(opts)\n    verify_example(config, tmp_path, pkg_root)\n\n\nENTRY_POINTS = {\n    \"console_scripts\": {\"a\": \"mod.a:func\"},\n    \"gui_scripts\": {\"b\": \"mod.b:func\"},\n    \"other\": {\"c\": \"mod.c:func [extra]\"},\n}\n\n\nclass TestEntryPoints:\n    def write_entry_points(self, tmp_path):\n        entry_points = ConfigParser()\n        entry_points.read_dict(ENTRY_POINTS)\n        with open(tmp_path / \"entry-points.txt\", \"w\", encoding=\"utf-8\") as f:\n            entry_points.write(f)\n\n    def pyproject(self, dynamic=None):\n        project = {\"dynamic\": dynamic or [\"scripts\", \"gui-scripts\", \"entry-points\"]}\n        tool = {\"dynamic\": {\"entry-points\": {\"file\": \"entry-points.txt\"}}}\n        return {\"project\": project, \"tool\": {\"setuptools\": tool}}\n\n    def test_all_listed_in_dynamic(self, tmp_path):\n        self.write_entry_points(tmp_path)\n        expanded = expand_configuration(self.pyproject(), tmp_path)\n        expanded_project = expanded[\"project\"]\n        assert len(expanded_project[\"scripts\"]) == 1\n        assert expanded_project[\"scripts\"][\"a\"] == \"mod.a:func\"\n        assert len(expanded_project[\"gui-scripts\"]) == 1\n        assert expanded_project[\"gui-scripts\"][\"b\"] == \"mod.b:func\"\n        assert len(expanded_project[\"entry-points\"]) == 1\n        assert expanded_project[\"entry-points\"][\"other\"][\"c\"] == \"mod.c:func [extra]\"\n\n    @pytest.mark.parametrize(\"missing_dynamic\", (\"scripts\", \"gui-scripts\"))\n    def test_scripts_not_listed_in_dynamic(self, tmp_path, missing_dynamic):\n        self.write_entry_points(tmp_path)\n        dynamic = {\"scripts\", \"gui-scripts\", \"entry-points\"} - {missing_dynamic}\n\n        msg = f\"defined outside of `pyproject.toml`:.*{missing_dynamic}\"\n        with pytest.raises(OptionError, match=re.compile(msg, re.S)):\n            expand_configuration(self.pyproject(dynamic), tmp_path)\n\n\nclass TestClassifiers:\n    def test_dynamic(self, tmp_path):\n        # Let's create a project example that has dynamic classifiers\n        # coming from a txt file.\n        create_example(tmp_path, \"src\")\n        classifiers = cleandoc(\n            \"\"\"\n            Framework :: Flask\n            Programming Language :: Haskell\n            \"\"\"\n        )\n        (tmp_path / \"classifiers.txt\").write_text(classifiers, encoding=\"utf-8\")\n\n        pyproject = tmp_path / \"pyproject.toml\"\n        config = read_configuration(pyproject, expand=False)\n        dynamic = config[\"project\"][\"dynamic\"]\n        config[\"project\"][\"dynamic\"] = list({*dynamic, \"classifiers\"})\n        dynamic_config = config[\"tool\"][\"setuptools\"][\"dynamic\"]\n        dynamic_config[\"classifiers\"] = {\"file\": \"classifiers.txt\"}\n\n        # When the configuration is expanded,\n        # each line of the file should be an different classifier.\n        validate(config, pyproject)\n        expanded = expand_configuration(config, tmp_path)\n\n        assert set(expanded[\"project\"][\"classifiers\"]) == {\n            \"Framework :: Flask\",\n            \"Programming Language :: Haskell\",\n        }\n\n    def test_dynamic_without_config(self, tmp_path):\n        config = \"\"\"\n        [project]\n        name = \"myproj\"\n        version = '42'\n        dynamic = [\"classifiers\"]\n        \"\"\"\n\n        pyproject = tmp_path / \"pyproject.toml\"\n        pyproject.write_text(cleandoc(config), encoding=\"utf-8\")\n        with pytest.raises(OptionError, match=\"No configuration .* .classifiers.\"):\n            read_configuration(pyproject)\n\n    def test_dynamic_readme_from_setup_script_args(self, tmp_path):\n        config = \"\"\"\n        [project]\n        name = \"myproj\"\n        version = '42'\n        dynamic = [\"readme\"]\n        \"\"\"\n        pyproject = tmp_path / \"pyproject.toml\"\n        pyproject.write_text(cleandoc(config), encoding=\"utf-8\")\n        dist = Distribution(attrs={\"long_description\": \"42\"})\n        # No error should occur because of missing `readme`\n        dist = apply_configuration(dist, pyproject)\n        assert dist.metadata.long_description == \"42\"\n\n    def test_dynamic_without_file(self, tmp_path):\n        config = \"\"\"\n        [project]\n        name = \"myproj\"\n        version = '42'\n        dynamic = [\"classifiers\"]\n\n        [tool.setuptools.dynamic]\n        classifiers = {file = [\"classifiers.txt\"]}\n        \"\"\"\n\n        pyproject = tmp_path / \"pyproject.toml\"\n        pyproject.write_text(cleandoc(config), encoding=\"utf-8\")\n        with pytest.warns(UserWarning, match=\"File .*classifiers.txt. cannot be found\"):\n            expanded = read_configuration(pyproject)\n        assert \"classifiers\" not in expanded[\"project\"]\n\n\n@pytest.mark.parametrize(\n    \"example\",\n    (\n        \"\"\"\n        [project]\n        name = \"myproj\"\n        version = \"1.2\"\n\n        [my-tool.that-disrespect.pep518]\n        value = 42\n        \"\"\",\n    ),\n)\ndef test_ignore_unrelated_config(tmp_path, example):\n    pyproject = tmp_path / \"pyproject.toml\"\n    pyproject.write_text(cleandoc(example), encoding=\"utf-8\")\n\n    # Make sure no error is raised due to 3rd party configs in pyproject.toml\n    assert read_configuration(pyproject) is not None\n\n\n@pytest.mark.parametrize(\n    (\"example\", \"error_msg\"),\n    [\n        (\n            \"\"\"\n            [project]\n            name = \"myproj\"\n            version = \"1.2\"\n            requires = ['pywin32; platform_system==\"Windows\"' ]\n            \"\"\",\n            \"configuration error: .project. must not contain ..requires.. properties\",\n        ),\n    ],\n)\ndef test_invalid_example(tmp_path, example, error_msg):\n    pyproject = tmp_path / \"pyproject.toml\"\n    pyproject.write_text(cleandoc(example), encoding=\"utf-8\")\n\n    pattern = re.compile(f\"invalid pyproject.toml.*{error_msg}.*\", re.M | re.S)\n    with pytest.raises(ValueError, match=pattern):\n        read_configuration(pyproject)\n\n\n@pytest.mark.parametrize(\"config\", (\"\", \"[tool.something]\\nvalue = 42\"))\ndef test_empty(tmp_path, config):\n    pyproject = tmp_path / \"pyproject.toml\"\n    pyproject.write_text(config, encoding=\"utf-8\")\n\n    # Make sure no error is raised\n    assert read_configuration(pyproject) == {}\n\n\n@pytest.mark.parametrize(\"config\", (\"[project]\\nname = 'myproj'\\nversion='42'\\n\",))\ndef test_include_package_data_by_default(tmp_path, config):\n    \"\"\"Builds with ``pyproject.toml`` should consider ``include-package-data=True`` as\n    default.\n    \"\"\"\n    pyproject = tmp_path / \"pyproject.toml\"\n    pyproject.write_text(config, encoding=\"utf-8\")\n\n    config = read_configuration(pyproject)\n    assert config[\"tool\"][\"setuptools\"][\"include-package-data\"] is True\n\n\ndef test_include_package_data_in_setuppy(tmp_path):\n    \"\"\"Builds with ``pyproject.toml`` should consider ``include_package_data`` set in\n    ``setup.py``.\n\n    See https://github.com/pypa/setuptools/issues/3197#issuecomment-1079023889\n    \"\"\"\n    files = {\n        \"pyproject.toml\": \"[project]\\nname = 'myproj'\\nversion='42'\\n\",\n        \"setup.py\": \"__import__('setuptools').setup(include_package_data=False)\",\n    }\n    jaraco.path.build(files, prefix=tmp_path)\n\n    with Path(tmp_path):\n        dist = distutils.core.run_setup(\"setup.py\", {}, stop_after=\"config\")\n\n    assert dist.get_name() == \"myproj\"\n    assert dist.get_version() == \"42\"\n    assert dist.include_package_data is False\n\n\ndef test_warn_tools_typo(tmp_path):\n    \"\"\"Test that the common ``tools.setuptools`` typo in ``pyproject.toml`` issues a warning\n\n    See https://github.com/pypa/setuptools/issues/4150\n    \"\"\"\n    config = \"\"\"\n    [build-system]\n    requires = [\"setuptools\"]\n    build-backend = \"setuptools.build_meta\"\n\n    [project]\n    name = \"myproj\"\n    version = '42'\n\n    [tools.setuptools]\n    packages = [\"package\"]\n    \"\"\"\n\n    pyproject = tmp_path / \"pyproject.toml\"\n    pyproject.write_text(cleandoc(config), encoding=\"utf-8\")\n\n    with pytest.warns(_ToolsTypoInMetadata):\n        read_configuration(pyproject)\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/tests/config/test_pyprojecttoml_dynamic_deps.py","size":3271,"sha1":"7b428b9c25419fa18d4407997cbc6137aec3c53b","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"from inspect import cleandoc\n\nimport pytest\nfrom jaraco import path\n\nfrom setuptools.config.pyprojecttoml import apply_configuration\nfrom setuptools.dist import Distribution\nfrom setuptools.warnings import SetuptoolsWarning\n\n\ndef test_dynamic_dependencies(tmp_path):\n    files = {\n        \"requirements.txt\": \"six\\n  # comment\\n\",\n        \"pyproject.toml\": cleandoc(\n            \"\"\"\n            [project]\n            name = \"myproj\"\n            version = \"1.0\"\n            dynamic = [\"dependencies\"]\n\n            [build-system]\n            requires = [\"setuptools\", \"wheel\"]\n            build-backend = \"setuptools.build_meta\"\n\n            [tool.setuptools.dynamic.dependencies]\n            file = [\"requirements.txt\"]\n            \"\"\"\n        ),\n    }\n    path.build(files, prefix=tmp_path)\n    dist = Distribution()\n    dist = apply_configuration(dist, tmp_path / \"pyproject.toml\")\n    assert dist.install_requires == [\"six\"]\n\n\ndef test_dynamic_optional_dependencies(tmp_path):\n    files = {\n        \"requirements-docs.txt\": \"sphinx\\n  # comment\\n\",\n        \"pyproject.toml\": cleandoc(\n            \"\"\"\n            [project]\n            name = \"myproj\"\n            version = \"1.0\"\n            dynamic = [\"optional-dependencies\"]\n\n            [tool.setuptools.dynamic.optional-dependencies.docs]\n            file = [\"requirements-docs.txt\"]\n\n            [build-system]\n            requires = [\"setuptools\", \"wheel\"]\n            build-backend = \"setuptools.build_meta\"\n            \"\"\"\n        ),\n    }\n    path.build(files, prefix=tmp_path)\n    dist = Distribution()\n    dist = apply_configuration(dist, tmp_path / \"pyproject.toml\")\n    assert dist.extras_require == {\"docs\": [\"sphinx\"]}\n\n\ndef test_mixed_dynamic_optional_dependencies(tmp_path):\n    \"\"\"\n    Test that if PEP 621 was loosened to allow mixing of dynamic and static\n    configurations in the case of fields containing sub-fields (groups),\n    things would work out.\n    \"\"\"\n    files = {\n        \"requirements-images.txt\": \"pillow~=42.0\\n  # comment\\n\",\n        \"pyproject.toml\": cleandoc(\n            \"\"\"\n            [project]\n            name = \"myproj\"\n            version = \"1.0\"\n            dynamic = [\"optional-dependencies\"]\n\n            [project.optional-dependencies]\n            docs = [\"sphinx\"]\n\n            [tool.setuptools.dynamic.optional-dependencies.images]\n            file = [\"requirements-images.txt\"]\n            \"\"\"\n        ),\n    }\n\n    path.build(files, prefix=tmp_path)\n    pyproject = tmp_path / \"pyproject.toml\"\n    with pytest.raises(ValueError, match=\"project.optional-dependencies\"):\n        apply_configuration(Distribution(), pyproject)\n\n\ndef test_mixed_extras_require_optional_dependencies(tmp_path):\n    files = {\n        \"pyproject.toml\": cleandoc(\n            \"\"\"\n            [project]\n            name = \"myproj\"\n            version = \"1.0\"\n            optional-dependencies.docs = [\"sphinx\"]\n            \"\"\"\n        ),\n    }\n\n    path.build(files, prefix=tmp_path)\n    pyproject = tmp_path / \"pyproject.toml\"\n\n    with pytest.warns(SetuptoolsWarning, match=\".extras_require. overwritten\"):\n        dist = Distribution({\"extras_require\": {\"hello\": [\"world\"]}})\n        dist = apply_configuration(dist, pyproject)\n        assert dist.extras_require == {\"docs\": [\"sphinx\"]}\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/tests/config/test_setupcfg.py","size":33189,"sha1":"5526566d03d33528d7fbec884dd0eb337c9ae098","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import configparser\nimport contextlib\nimport inspect\nfrom pathlib import Path\nfrom unittest.mock import Mock, patch\n\nimport pytest\nfrom packaging.requirements import InvalidRequirement\n\nfrom setuptools.config.setupcfg import ConfigHandler, Target, read_configuration\nfrom setuptools.dist import Distribution, _Distribution\nfrom setuptools.warnings import SetuptoolsDeprecationWarning\n\nfrom ..textwrap import DALS\n\nfrom distutils.errors import DistutilsFileError, DistutilsOptionError\n\n\nclass ErrConfigHandler(ConfigHandler[Target]):\n    \"\"\"Erroneous handler. Fails to implement required methods.\"\"\"\n\n    section_prefix = \"**err**\"\n\n\ndef make_package_dir(name, base_dir, ns=False):\n    dir_package = base_dir\n    for dir_name in name.split('/'):\n        dir_package = dir_package.mkdir(dir_name)\n    init_file = None\n    if not ns:\n        init_file = dir_package.join('__init__.py')\n        init_file.write('')\n    return dir_package, init_file\n\n\ndef fake_env(\n    tmpdir, setup_cfg, setup_py=None, encoding='ascii', package_path='fake_package'\n):\n    if setup_py is None:\n        setup_py = 'from setuptools import setup\\nsetup()\\n'\n\n    tmpdir.join('setup.py').write(setup_py)\n    config = tmpdir.join('setup.cfg')\n    config.write(setup_cfg.encode(encoding), mode='wb')\n\n    package_dir, init_file = make_package_dir(package_path, tmpdir)\n\n    init_file.write(\n        'VERSION = (1, 2, 3)\\n'\n        '\\n'\n        'VERSION_MAJOR = 1'\n        '\\n'\n        'def get_version():\\n'\n        '    return [3, 4, 5, \"dev\"]\\n'\n        '\\n'\n    )\n\n    return package_dir, config\n\n\n@contextlib.contextmanager\ndef get_dist(tmpdir, kwargs_initial=None, parse=True):\n    kwargs_initial = kwargs_initial or {}\n\n    with tmpdir.as_cwd():\n        dist = Distribution(kwargs_initial)\n        dist.script_name = 'setup.py'\n        parse and dist.parse_config_files()\n\n        yield dist\n\n\ndef test_parsers_implemented():\n    with pytest.raises(NotImplementedError):\n        handler = ErrConfigHandler(None, {}, False, Mock())\n        handler.parsers\n\n\nclass TestConfigurationReader:\n    def test_basic(self, tmpdir):\n        _, config = fake_env(\n            tmpdir,\n            '[metadata]\\n'\n            'version = 10.1.1\\n'\n            'keywords = one, two\\n'\n            '\\n'\n            '[options]\\n'\n            'scripts = bin/a.py, bin/b.py\\n',\n        )\n        config_dict = read_configuration(str(config))\n        assert config_dict['metadata']['version'] == '10.1.1'\n        assert config_dict['metadata']['keywords'] == ['one', 'two']\n        assert config_dict['options']['scripts'] == ['bin/a.py', 'bin/b.py']\n\n    def test_no_config(self, tmpdir):\n        with pytest.raises(DistutilsFileError):\n            read_configuration(str(tmpdir.join('setup.cfg')))\n\n    def test_ignore_errors(self, tmpdir):\n        _, config = fake_env(\n            tmpdir,\n            '[metadata]\\nversion = attr: none.VERSION\\nkeywords = one, two\\n',\n        )\n        with pytest.raises(ImportError):\n            read_configuration(str(config))\n\n        config_dict = read_configuration(str(config), ignore_option_errors=True)\n\n        assert config_dict['metadata']['keywords'] == ['one', 'two']\n        assert 'version' not in config_dict['metadata']\n\n        config.remove()\n\n\nclass TestMetadata:\n    def test_basic(self, tmpdir):\n        fake_env(\n            tmpdir,\n            '[metadata]\\n'\n            'version = 10.1.1\\n'\n            'description = Some description\\n'\n            'long_description_content_type = text/something\\n'\n            'long_description = file: README\\n'\n            'name = fake_name\\n'\n            'keywords = one, two\\n'\n            'provides = package, package.sub\\n'\n            'license = otherlic\\n'\n            'download_url = http://test.test.com/test/\\n'\n            'maintainer_email = test@test.com\\n',\n        )\n\n        tmpdir.join('README').write('readme contents\\nline2')\n\n        meta_initial = {\n            # This will be used so `otherlic` won't replace it.\n            'license': 'BSD 3-Clause License',\n        }\n\n        with get_dist(tmpdir, meta_initial) as dist:\n            metadata = dist.metadata\n\n            assert metadata.version == '10.1.1'\n            assert metadata.description == 'Some description'\n            assert metadata.long_description_content_type == 'text/something'\n            assert metadata.long_description == 'readme contents\\nline2'\n            assert metadata.provides == ['package', 'package.sub']\n            assert metadata.license == 'BSD 3-Clause License'\n            assert metadata.name == 'fake_name'\n            assert metadata.keywords == ['one', 'two']\n            assert metadata.download_url == 'http://test.test.com/test/'\n            assert metadata.maintainer_email == 'test@test.com'\n\n    def test_license_cfg(self, tmpdir):\n        fake_env(\n            tmpdir,\n            DALS(\n                \"\"\"\n            [metadata]\n            name=foo\n            version=0.0.1\n            license=Apache 2.0\n            \"\"\"\n            ),\n        )\n\n        with get_dist(tmpdir) as dist:\n            metadata = dist.metadata\n\n            assert metadata.name == \"foo\"\n            assert metadata.version == \"0.0.1\"\n            assert metadata.license == \"Apache 2.0\"\n\n    def test_file_mixed(self, tmpdir):\n        fake_env(\n            tmpdir,\n            '[metadata]\\nlong_description = file: README.rst, CHANGES.rst\\n\\n',\n        )\n\n        tmpdir.join('README.rst').write('readme contents\\nline2')\n        tmpdir.join('CHANGES.rst').write('changelog contents\\nand stuff')\n\n        with get_dist(tmpdir) as dist:\n            assert dist.metadata.long_description == (\n                'readme contents\\nline2\\nchangelog contents\\nand stuff'\n            )\n\n    def test_file_sandboxed(self, tmpdir):\n        tmpdir.ensure(\"README\")\n        project = tmpdir.join('depth1', 'depth2')\n        project.ensure(dir=True)\n        fake_env(project, '[metadata]\\nlong_description = file: ../../README\\n')\n\n        with get_dist(project, parse=False) as dist:\n            with pytest.raises(DistutilsOptionError):\n                dist.parse_config_files()  # file: out of sandbox\n\n    def test_aliases(self, tmpdir):\n        fake_env(\n            tmpdir,\n            '[metadata]\\n'\n            'author_email = test@test.com\\n'\n            'home_page = http://test.test.com/test/\\n'\n            'summary = Short summary\\n'\n            'platform = a, b\\n'\n            'classifier =\\n'\n            '  Framework :: Django\\n'\n            '  Programming Language :: Python :: 3.5\\n',\n        )\n\n        with get_dist(tmpdir) as dist:\n            metadata = dist.metadata\n            assert metadata.author_email == 'test@test.com'\n            assert metadata.url == 'http://test.test.com/test/'\n            assert metadata.description == 'Short summary'\n            assert metadata.platforms == ['a', 'b']\n            assert metadata.classifiers == [\n                'Framework :: Django',\n                'Programming Language :: Python :: 3.5',\n            ]\n\n    def test_multiline(self, tmpdir):\n        fake_env(\n            tmpdir,\n            '[metadata]\\n'\n            'name = fake_name\\n'\n            'keywords =\\n'\n            '  one\\n'\n            '  two\\n'\n            'classifiers =\\n'\n            '  Framework :: Django\\n'\n            '  Programming Language :: Python :: 3.5\\n',\n        )\n        with get_dist(tmpdir) as dist:\n            metadata = dist.metadata\n            assert metadata.keywords == ['one', 'two']\n            assert metadata.classifiers == [\n                'Framework :: Django',\n                'Programming Language :: Python :: 3.5',\n            ]\n\n    def test_dict(self, tmpdir):\n        fake_env(\n            tmpdir,\n            '[metadata]\\n'\n            'project_urls =\\n'\n            '  Link One = https://example.com/one/\\n'\n            '  Link Two = https://example.com/two/\\n',\n        )\n        with get_dist(tmpdir) as dist:\n            metadata = dist.metadata\n            assert metadata.project_urls == {\n                'Link One': 'https://example.com/one/',\n                'Link Two': 'https://example.com/two/',\n            }\n\n    def test_version(self, tmpdir):\n        package_dir, config = fake_env(\n            tmpdir, '[metadata]\\nversion = attr: fake_package.VERSION\\n'\n        )\n\n        sub_a = package_dir.mkdir('subpkg_a')\n        sub_a.join('__init__.py').write('')\n        sub_a.join('mod.py').write('VERSION = (2016, 11, 26)')\n\n        sub_b = package_dir.mkdir('subpkg_b')\n        sub_b.join('__init__.py').write('')\n        sub_b.join('mod.py').write(\n            'import third_party_module\\nVERSION = (2016, 11, 26)'\n        )\n\n        with get_dist(tmpdir) as dist:\n            assert dist.metadata.version == '1.2.3'\n\n        config.write('[metadata]\\nversion = attr: fake_package.get_version\\n')\n        with get_dist(tmpdir) as dist:\n            assert dist.metadata.version == '3.4.5.dev'\n\n        config.write('[metadata]\\nversion = attr: fake_package.VERSION_MAJOR\\n')\n        with get_dist(tmpdir) as dist:\n            assert dist.metadata.version == '1'\n\n        config.write('[metadata]\\nversion = attr: fake_package.subpkg_a.mod.VERSION\\n')\n        with get_dist(tmpdir) as dist:\n            assert dist.metadata.version == '2016.11.26'\n\n        config.write('[metadata]\\nversion = attr: fake_package.subpkg_b.mod.VERSION\\n')\n        with get_dist(tmpdir) as dist:\n            assert dist.metadata.version == '2016.11.26'\n\n    def test_version_file(self, tmpdir):\n        fake_env(tmpdir, '[metadata]\\nversion = file: fake_package/version.txt\\n')\n        tmpdir.join('fake_package', 'version.txt').write('1.2.3\\n')\n\n        with get_dist(tmpdir) as dist:\n            assert dist.metadata.version == '1.2.3'\n\n        tmpdir.join('fake_package', 'version.txt').write('1.2.3\\n4.5.6\\n')\n        with pytest.raises(DistutilsOptionError):\n            with get_dist(tmpdir) as dist:\n                dist.metadata.version\n\n    def test_version_with_package_dir_simple(self, tmpdir):\n        fake_env(\n            tmpdir,\n            '[metadata]\\n'\n            'version = attr: fake_package_simple.VERSION\\n'\n            '[options]\\n'\n            'package_dir =\\n'\n            '    = src\\n',\n            package_path='src/fake_package_simple',\n        )\n\n        with get_dist(tmpdir) as dist:\n            assert dist.metadata.version == '1.2.3'\n\n    def test_version_with_package_dir_rename(self, tmpdir):\n        fake_env(\n            tmpdir,\n            '[metadata]\\n'\n            'version = attr: fake_package_rename.VERSION\\n'\n            '[options]\\n'\n            'package_dir =\\n'\n            '    fake_package_rename = fake_dir\\n',\n            package_path='fake_dir',\n        )\n\n        with get_dist(tmpdir) as dist:\n            assert dist.metadata.version == '1.2.3'\n\n    def test_version_with_package_dir_complex(self, tmpdir):\n        fake_env(\n            tmpdir,\n            '[metadata]\\n'\n            'version = attr: fake_package_complex.VERSION\\n'\n            '[options]\\n'\n            'package_dir =\\n'\n            '    fake_package_complex = src/fake_dir\\n',\n            package_path='src/fake_dir',\n        )\n\n        with get_dist(tmpdir) as dist:\n            assert dist.metadata.version == '1.2.3'\n\n    def test_unknown_meta_item(self, tmpdir):\n        fake_env(tmpdir, '[metadata]\\nname = fake_name\\nunknown = some\\n')\n        with get_dist(tmpdir, parse=False) as dist:\n            dist.parse_config_files()  # Skip unknown.\n\n    def test_usupported_section(self, tmpdir):\n        fake_env(tmpdir, '[metadata.some]\\nkey = val\\n')\n        with get_dist(tmpdir, parse=False) as dist:\n            with pytest.raises(DistutilsOptionError):\n                dist.parse_config_files()\n\n    def test_classifiers(self, tmpdir):\n        expected = set([\n            'Framework :: Django',\n            'Programming Language :: Python :: 3',\n            'Programming Language :: Python :: 3.5',\n        ])\n\n        # From file.\n        _, config = fake_env(tmpdir, '[metadata]\\nclassifiers = file: classifiers\\n')\n\n        tmpdir.join('classifiers').write(\n            'Framework :: Django\\n'\n            'Programming Language :: Python :: 3\\n'\n            'Programming Language :: Python :: 3.5\\n'\n        )\n\n        with get_dist(tmpdir) as dist:\n            assert set(dist.metadata.classifiers) == expected\n\n        # From list notation\n        config.write(\n            '[metadata]\\n'\n            'classifiers =\\n'\n            '    Framework :: Django\\n'\n            '    Programming Language :: Python :: 3\\n'\n            '    Programming Language :: Python :: 3.5\\n'\n        )\n        with get_dist(tmpdir) as dist:\n            assert set(dist.metadata.classifiers) == expected\n\n    def test_interpolation(self, tmpdir):\n        fake_env(tmpdir, '[metadata]\\ndescription = %(message)s\\n')\n        with pytest.raises(configparser.InterpolationMissingOptionError):\n            with get_dist(tmpdir):\n                pass\n\n    def test_non_ascii_1(self, tmpdir):\n        fake_env(tmpdir, '[metadata]\\ndescription = \\n', encoding='utf-8')\n        with get_dist(tmpdir):\n            pass\n\n    def test_non_ascii_3(self, tmpdir):\n        fake_env(tmpdir, '\\n# -*- coding: invalid\\n')\n        with get_dist(tmpdir):\n            pass\n\n    def test_non_ascii_4(self, tmpdir):\n        fake_env(\n            tmpdir,\n            '# -*- coding: utf-8\\n[metadata]\\ndescription = \\n',\n            encoding='utf-8',\n        )\n        with get_dist(tmpdir) as dist:\n            assert dist.metadata.description == ''\n\n    def test_not_utf8(self, tmpdir):\n        \"\"\"\n        Config files encoded not in UTF-8 will fail\n        \"\"\"\n        fake_env(\n            tmpdir,\n            '# vim: set fileencoding=iso-8859-15 :\\n[metadata]\\ndescription = \\n',\n            encoding='iso-8859-15',\n        )\n        with pytest.raises(UnicodeDecodeError):\n            with get_dist(tmpdir):\n                pass\n\n    def test_warn_dash_deprecation(self, tmpdir):\n        # warn_dash_deprecation() is a method in setuptools.dist\n        # remove this test and the method when no longer needed\n        fake_env(\n            tmpdir,\n            '[metadata]\\n'\n            'author-email = test@test.com\\n'\n            'maintainer_email = foo@foo.com\\n',\n        )\n        msg = \"Usage of dash-separated 'author-email' will not be supported\"\n        with pytest.warns(SetuptoolsDeprecationWarning, match=msg):\n            with get_dist(tmpdir) as dist:\n                metadata = dist.metadata\n\n        assert metadata.author_email == 'test@test.com'\n        assert metadata.maintainer_email == 'foo@foo.com'\n\n    def test_make_option_lowercase(self, tmpdir):\n        # remove this test and the method make_option_lowercase() in setuptools.dist\n        # when no longer needed\n        fake_env(tmpdir, '[metadata]\\nName = foo\\ndescription = Some description\\n')\n        msg = \"Usage of uppercase key 'Name' in 'metadata' will not be supported\"\n        with pytest.warns(SetuptoolsDeprecationWarning, match=msg):\n            with get_dist(tmpdir) as dist:\n                metadata = dist.metadata\n\n        assert metadata.name == 'foo'\n        assert metadata.description == 'Some description'\n\n\nclass TestOptions:\n    def test_basic(self, tmpdir):\n        fake_env(\n            tmpdir,\n            '[options]\\n'\n            'zip_safe = True\\n'\n            'include_package_data = yes\\n'\n            'package_dir = b=c, =src\\n'\n            'packages = pack_a, pack_b.subpack\\n'\n            'namespace_packages = pack1, pack2\\n'\n            'scripts = bin/one.py, bin/two.py\\n'\n            'eager_resources = bin/one.py, bin/two.py\\n'\n            'install_requires = docutils>=0.3; pack ==1.1, ==1.3; hey\\n'\n            'setup_requires = docutils>=0.3; spack ==1.1, ==1.3; there\\n'\n            'dependency_links = http://some.com/here/1, '\n            'http://some.com/there/2\\n'\n            'python_requires = >=1.0, !=2.8\\n'\n            'py_modules = module1, module2\\n',\n        )\n        deprec = pytest.warns(SetuptoolsDeprecationWarning, match=\"namespace_packages\")\n        with deprec, get_dist(tmpdir) as dist:\n            assert dist.zip_safe\n            assert dist.include_package_data\n            assert dist.package_dir == {'': 'src', 'b': 'c'}\n            assert dist.packages == ['pack_a', 'pack_b.subpack']\n            assert dist.namespace_packages == ['pack1', 'pack2']\n            assert dist.scripts == ['bin/one.py', 'bin/two.py']\n            assert dist.dependency_links == ([\n                'http://some.com/here/1',\n                'http://some.com/there/2',\n            ])\n            assert dist.install_requires == ([\n                'docutils>=0.3',\n                'pack==1.1,==1.3',\n                'hey',\n            ])\n            assert dist.setup_requires == ([\n                'docutils>=0.3',\n                'spack ==1.1, ==1.3',\n                'there',\n            ])\n            assert dist.python_requires == '>=1.0, !=2.8'\n            assert dist.py_modules == ['module1', 'module2']\n\n    def test_multiline(self, tmpdir):\n        fake_env(\n            tmpdir,\n            '[options]\\n'\n            'package_dir = \\n'\n            '  b=c\\n'\n            '  =src\\n'\n            'packages = \\n'\n            '  pack_a\\n'\n            '  pack_b.subpack\\n'\n            'namespace_packages = \\n'\n            '  pack1\\n'\n            '  pack2\\n'\n            'scripts = \\n'\n            '  bin/one.py\\n'\n            '  bin/two.py\\n'\n            'eager_resources = \\n'\n            '  bin/one.py\\n'\n            '  bin/two.py\\n'\n            'install_requires = \\n'\n            '  docutils>=0.3\\n'\n            '  pack ==1.1, ==1.3\\n'\n            '  hey\\n'\n            'setup_requires = \\n'\n            '  docutils>=0.3\\n'\n            '  spack ==1.1, ==1.3\\n'\n            '  there\\n'\n            'dependency_links = \\n'\n            '  http://some.com/here/1\\n'\n            '  http://some.com/there/2\\n',\n        )\n        deprec = pytest.warns(SetuptoolsDeprecationWarning, match=\"namespace_packages\")\n        with deprec, get_dist(tmpdir) as dist:\n            assert dist.package_dir == {'': 'src', 'b': 'c'}\n            assert dist.packages == ['pack_a', 'pack_b.subpack']\n            assert dist.namespace_packages == ['pack1', 'pack2']\n            assert dist.scripts == ['bin/one.py', 'bin/two.py']\n            assert dist.dependency_links == ([\n                'http://some.com/here/1',\n                'http://some.com/there/2',\n            ])\n            assert dist.install_requires == ([\n                'docutils>=0.3',\n                'pack==1.1,==1.3',\n                'hey',\n            ])\n            assert dist.setup_requires == ([\n                'docutils>=0.3',\n                'spack ==1.1, ==1.3',\n                'there',\n            ])\n\n    def test_package_dir_fail(self, tmpdir):\n        fake_env(tmpdir, '[options]\\npackage_dir = a b\\n')\n        with get_dist(tmpdir, parse=False) as dist:\n            with pytest.raises(DistutilsOptionError):\n                dist.parse_config_files()\n\n    def test_package_data(self, tmpdir):\n        fake_env(\n            tmpdir,\n            '[options.package_data]\\n'\n            '* = *.txt, *.rst\\n'\n            'hello = *.msg\\n'\n            '\\n'\n            '[options.exclude_package_data]\\n'\n            '* = fake1.txt, fake2.txt\\n'\n            'hello = *.dat\\n',\n        )\n\n        with get_dist(tmpdir) as dist:\n            assert dist.package_data == {\n                '': ['*.txt', '*.rst'],\n                'hello': ['*.msg'],\n            }\n            assert dist.exclude_package_data == {\n                '': ['fake1.txt', 'fake2.txt'],\n                'hello': ['*.dat'],\n            }\n\n    def test_packages(self, tmpdir):\n        fake_env(tmpdir, '[options]\\npackages = find:\\n')\n\n        with get_dist(tmpdir) as dist:\n            assert dist.packages == ['fake_package']\n\n    def test_find_directive(self, tmpdir):\n        dir_package, config = fake_env(tmpdir, '[options]\\npackages = find:\\n')\n\n        make_package_dir('sub_one', dir_package)\n        make_package_dir('sub_two', dir_package)\n\n        with get_dist(tmpdir) as dist:\n            assert set(dist.packages) == set([\n                'fake_package',\n                'fake_package.sub_two',\n                'fake_package.sub_one',\n            ])\n\n        config.write(\n            '[options]\\n'\n            'packages = find:\\n'\n            '\\n'\n            '[options.packages.find]\\n'\n            'where = .\\n'\n            'include =\\n'\n            '    fake_package.sub_one\\n'\n            '    two\\n'\n        )\n        with get_dist(tmpdir) as dist:\n            assert dist.packages == ['fake_package.sub_one']\n\n        config.write(\n            '[options]\\n'\n            'packages = find:\\n'\n            '\\n'\n            '[options.packages.find]\\n'\n            'exclude =\\n'\n            '    fake_package.sub_one\\n'\n        )\n        with get_dist(tmpdir) as dist:\n            assert set(dist.packages) == set(['fake_package', 'fake_package.sub_two'])\n\n    def test_find_namespace_directive(self, tmpdir):\n        dir_package, config = fake_env(\n            tmpdir, '[options]\\npackages = find_namespace:\\n'\n        )\n\n        make_package_dir('sub_one', dir_package)\n        make_package_dir('sub_two', dir_package, ns=True)\n\n        with get_dist(tmpdir) as dist:\n            assert set(dist.packages) == {\n                'fake_package',\n                'fake_package.sub_two',\n                'fake_package.sub_one',\n            }\n\n        config.write(\n            '[options]\\n'\n            'packages = find_namespace:\\n'\n            '\\n'\n            '[options.packages.find]\\n'\n            'where = .\\n'\n            'include =\\n'\n            '    fake_package.sub_one\\n'\n            '    two\\n'\n        )\n        with get_dist(tmpdir) as dist:\n            assert dist.packages == ['fake_package.sub_one']\n\n        config.write(\n            '[options]\\n'\n            'packages = find_namespace:\\n'\n            '\\n'\n            '[options.packages.find]\\n'\n            'exclude =\\n'\n            '    fake_package.sub_one\\n'\n        )\n        with get_dist(tmpdir) as dist:\n            assert set(dist.packages) == {'fake_package', 'fake_package.sub_two'}\n\n    def test_extras_require(self, tmpdir):\n        fake_env(\n            tmpdir,\n            '[options.extras_require]\\n'\n            'pdf = ReportLab>=1.2; RXP\\n'\n            'rest = \\n'\n            '  docutils>=0.3\\n'\n            '  pack ==1.1, ==1.3\\n',\n        )\n\n        with get_dist(tmpdir) as dist:\n            assert dist.extras_require == {\n                'pdf': ['ReportLab>=1.2', 'RXP'],\n                'rest': ['docutils>=0.3', 'pack==1.1,==1.3'],\n            }\n            assert set(dist.metadata.provides_extras) == {'pdf', 'rest'}\n\n    @pytest.mark.parametrize(\n        \"config\",\n        [\n            \"[options.extras_require]\\nfoo = bar;python_version<'3'\",\n            \"[options.extras_require]\\nfoo = bar;os_name=='linux'\",\n            \"[options.extras_require]\\nfoo = bar;python_version<'3'\\n\",\n            \"[options.extras_require]\\nfoo = bar;os_name=='linux'\\n\",\n            \"[options]\\ninstall_requires = bar;python_version<'3'\",\n            \"[options]\\ninstall_requires = bar;os_name=='linux'\",\n            \"[options]\\ninstall_requires = bar;python_version<'3'\\n\",\n            \"[options]\\ninstall_requires = bar;os_name=='linux'\\n\",\n        ],\n    )\n    def test_raises_accidental_env_marker_misconfig(self, config, tmpdir):\n        fake_env(tmpdir, config)\n        match = (\n            r\"One of the parsed requirements in `(install_requires|extras_require.+)` \"\n            \"looks like a valid environment marker.*\"\n        )\n        with pytest.raises(InvalidRequirement, match=match):\n            with get_dist(tmpdir) as _:\n                pass\n\n    @pytest.mark.parametrize(\n        \"config\",\n        [\n            \"[options.extras_require]\\nfoo = bar;python_version<3\",\n            \"[options.extras_require]\\nfoo = bar;python_version<3\\n\",\n            \"[options]\\ninstall_requires = bar;python_version<3\",\n            \"[options]\\ninstall_requires = bar;python_version<3\\n\",\n        ],\n    )\n    def test_warn_accidental_env_marker_misconfig(self, config, tmpdir):\n        fake_env(tmpdir, config)\n        match = (\n            r\"One of the parsed requirements in `(install_requires|extras_require.+)` \"\n            \"looks like a valid environment marker.*\"\n        )\n        with pytest.warns(SetuptoolsDeprecationWarning, match=match):\n            with get_dist(tmpdir) as _:\n                pass\n\n    @pytest.mark.parametrize(\n        \"config\",\n        [\n            \"[options.extras_require]\\nfoo =\\n    bar;python_version<'3'\",\n            \"[options.extras_require]\\nfoo = bar;baz\\nboo = xxx;yyy\",\n            \"[options.extras_require]\\nfoo =\\n    bar;python_version<'3'\\n\",\n            \"[options.extras_require]\\nfoo = bar;baz\\nboo = xxx;yyy\\n\",\n            \"[options.extras_require]\\nfoo =\\n    bar\\n    python_version<3\\n\",\n            \"[options]\\ninstall_requires =\\n    bar;python_version<'3'\",\n            \"[options]\\ninstall_requires = bar;baz\\nboo = xxx;yyy\",\n            \"[options]\\ninstall_requires =\\n    bar;python_version<'3'\\n\",\n            \"[options]\\ninstall_requires = bar;baz\\nboo = xxx;yyy\\n\",\n            \"[options]\\ninstall_requires =\\n    bar\\n    python_version<3\\n\",\n        ],\n    )\n    @pytest.mark.filterwarnings(\"error::setuptools.SetuptoolsDeprecationWarning\")\n    def test_nowarn_accidental_env_marker_misconfig(self, config, tmpdir, recwarn):\n        fake_env(tmpdir, config)\n        num_warnings = len(recwarn)\n        with get_dist(tmpdir) as _:\n            pass\n        # The examples are valid, no warnings shown\n        assert len(recwarn) == num_warnings\n\n    def test_dash_preserved_extras_require(self, tmpdir):\n        fake_env(tmpdir, '[options.extras_require]\\nfoo-a = foo\\nfoo_b = test\\n')\n\n        with get_dist(tmpdir) as dist:\n            assert dist.extras_require == {'foo-a': ['foo'], 'foo_b': ['test']}\n\n    def test_entry_points(self, tmpdir):\n        _, config = fake_env(\n            tmpdir,\n            '[options.entry_points]\\n'\n            'group1 = point1 = pack.module:func, '\n            '.point2 = pack.module2:func_rest [rest]\\n'\n            'group2 = point3 = pack.module:func2\\n',\n        )\n\n        with get_dist(tmpdir) as dist:\n            assert dist.entry_points == {\n                'group1': [\n                    'point1 = pack.module:func',\n                    '.point2 = pack.module2:func_rest [rest]',\n                ],\n                'group2': ['point3 = pack.module:func2'],\n            }\n\n        expected = (\n            '[blogtool.parsers]\\n'\n            '.rst = some.nested.module:SomeClass.some_classmethod[reST]\\n'\n        )\n\n        tmpdir.join('entry_points').write(expected)\n\n        # From file.\n        config.write('[options]\\nentry_points = file: entry_points\\n')\n\n        with get_dist(tmpdir) as dist:\n            assert dist.entry_points == expected\n\n    def test_case_sensitive_entry_points(self, tmpdir):\n        fake_env(\n            tmpdir,\n            '[options.entry_points]\\n'\n            'GROUP1 = point1 = pack.module:func, '\n            '.point2 = pack.module2:func_rest [rest]\\n'\n            'group2 = point3 = pack.module:func2\\n',\n        )\n\n        with get_dist(tmpdir) as dist:\n            assert dist.entry_points == {\n                'GROUP1': [\n                    'point1 = pack.module:func',\n                    '.point2 = pack.module2:func_rest [rest]',\n                ],\n                'group2': ['point3 = pack.module:func2'],\n            }\n\n    def test_data_files(self, tmpdir):\n        fake_env(\n            tmpdir,\n            '[options.data_files]\\n'\n            'cfg =\\n'\n            '      a/b.conf\\n'\n            '      c/d.conf\\n'\n            'data = e/f.dat, g/h.dat\\n',\n        )\n\n        with get_dist(tmpdir) as dist:\n            expected = [\n                ('cfg', ['a/b.conf', 'c/d.conf']),\n                ('data', ['e/f.dat', 'g/h.dat']),\n            ]\n            assert sorted(dist.data_files) == sorted(expected)\n\n    def test_data_files_globby(self, tmpdir):\n        fake_env(\n            tmpdir,\n            '[options.data_files]\\n'\n            'cfg =\\n'\n            '      a/b.conf\\n'\n            '      c/d.conf\\n'\n            'data = *.dat\\n'\n            'icons = \\n'\n            '      *.ico\\n'\n            'audio = \\n'\n            '      *.wav\\n'\n            '      sounds.db\\n',\n        )\n\n        # Create dummy files for glob()'s sake:\n        tmpdir.join('a.dat').write('')\n        tmpdir.join('b.dat').write('')\n        tmpdir.join('c.dat').write('')\n        tmpdir.join('a.ico').write('')\n        tmpdir.join('b.ico').write('')\n        tmpdir.join('c.ico').write('')\n        tmpdir.join('beep.wav').write('')\n        tmpdir.join('boop.wav').write('')\n        tmpdir.join('sounds.db').write('')\n\n        with get_dist(tmpdir) as dist:\n            expected = [\n                ('cfg', ['a/b.conf', 'c/d.conf']),\n                ('data', ['a.dat', 'b.dat', 'c.dat']),\n                ('icons', ['a.ico', 'b.ico', 'c.ico']),\n                ('audio', ['beep.wav', 'boop.wav', 'sounds.db']),\n            ]\n            assert sorted(dist.data_files) == sorted(expected)\n\n    def test_python_requires_simple(self, tmpdir):\n        fake_env(\n            tmpdir,\n            DALS(\n                \"\"\"\n            [options]\n            python_requires=>=2.7\n            \"\"\"\n            ),\n        )\n        with get_dist(tmpdir) as dist:\n            dist.parse_config_files()\n\n    def test_python_requires_compound(self, tmpdir):\n        fake_env(\n            tmpdir,\n            DALS(\n                \"\"\"\n            [options]\n            python_requires=>=2.7,!=3.0.*\n            \"\"\"\n            ),\n        )\n        with get_dist(tmpdir) as dist:\n            dist.parse_config_files()\n\n    def test_python_requires_invalid(self, tmpdir):\n        fake_env(\n            tmpdir,\n            DALS(\n                \"\"\"\n            [options]\n            python_requires=invalid\n            \"\"\"\n            ),\n        )\n        with pytest.raises(Exception):\n            with get_dist(tmpdir) as dist:\n                dist.parse_config_files()\n\n    def test_cmdclass(self, tmpdir):\n        module_path = Path(tmpdir, \"src/custom_build.py\")  # auto discovery for src\n        module_path.parent.mkdir(parents=True, exist_ok=True)\n        module_path.write_text(\n            \"from distutils.core import Command\\nclass CustomCmd(Command): pass\\n\",\n            encoding=\"utf-8\",\n        )\n\n        setup_cfg = \"\"\"\n            [options]\n            cmdclass =\n                customcmd = custom_build.CustomCmd\n        \"\"\"\n        fake_env(tmpdir, inspect.cleandoc(setup_cfg))\n\n        with get_dist(tmpdir) as dist:\n            cmdclass = dist.cmdclass['customcmd']\n            assert cmdclass.__name__ == \"CustomCmd\"\n            assert cmdclass.__module__ == \"custom_build\"\n            assert module_path.samefile(inspect.getfile(cmdclass))\n\n    def test_requirements_file(self, tmpdir):\n        fake_env(\n            tmpdir,\n            DALS(\n                \"\"\"\n            [options]\n            install_requires = file:requirements.txt\n            [options.extras_require]\n            colors = file:requirements-extra.txt\n            \"\"\"\n            ),\n        )\n\n        tmpdir.join('requirements.txt').write('\\ndocutils>=0.3\\n\\n')\n        tmpdir.join('requirements-extra.txt').write('colorama')\n\n        with get_dist(tmpdir) as dist:\n            assert dist.install_requires == ['docutils>=0.3']\n            assert dist.extras_require == {'colors': ['colorama']}\n\n\nsaved_dist_init = _Distribution.__init__\n\n\nclass TestExternalSetters:\n    # During creation of the setuptools Distribution() object, we call\n    # the init of the parent distutils Distribution object via\n    # _Distribution.__init__ ().\n    #\n    # It's possible distutils calls out to various keyword\n    # implementations (i.e. distutils.setup_keywords entry points)\n    # that may set a range of variables.\n    #\n    # This wraps distutil's Distribution.__init__ and simulates\n    # pbr or something else setting these values.\n    def _fake_distribution_init(self, dist, attrs):\n        saved_dist_init(dist, attrs)\n        # see self._DISTUTILS_UNSUPPORTED_METADATA\n        dist.metadata.long_description_content_type = 'text/something'\n        # Test overwrite setup() args\n        dist.metadata.project_urls = {\n            'Link One': 'https://example.com/one/',\n            'Link Two': 'https://example.com/two/',\n        }\n\n    @patch.object(_Distribution, '__init__', autospec=True)\n    def test_external_setters(self, mock_parent_init, tmpdir):\n        mock_parent_init.side_effect = self._fake_distribution_init\n\n        dist = Distribution(attrs={'project_urls': {'will_be': 'ignored'}})\n\n        assert dist.metadata.long_description_content_type == 'text/something'\n        assert dist.metadata.project_urls == {\n            'Link One': 'https://example.com/one/',\n            'Link Two': 'https://example.com/two/',\n        }\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/tests/contexts.py","size":3480,"sha1":"e28c5008e633be55d597bcec96306f6e661828f5","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import contextlib\nimport io\nimport os\nimport shutil\nimport site\nimport sys\nimport tempfile\n\nfrom filelock import FileLock\n\n\n@contextlib.contextmanager\ndef tempdir(cd=lambda dir: None, **kwargs):\n    temp_dir = tempfile.mkdtemp(**kwargs)\n    orig_dir = os.getcwd()\n    try:\n        cd(temp_dir)\n        yield temp_dir\n    finally:\n        cd(orig_dir)\n        shutil.rmtree(temp_dir)\n\n\n@contextlib.contextmanager\ndef environment(**replacements):\n    \"\"\"\n    In a context, patch the environment with replacements. Pass None values\n    to clear the values.\n    \"\"\"\n    saved = dict((key, os.environ[key]) for key in replacements if key in os.environ)\n\n    # remove values that are null\n    remove = (key for (key, value) in replacements.items() if value is None)\n    for key in list(remove):\n        os.environ.pop(key, None)\n        replacements.pop(key)\n\n    os.environ.update(replacements)\n\n    try:\n        yield saved\n    finally:\n        for key in replacements:\n            os.environ.pop(key, None)\n        os.environ.update(saved)\n\n\n@contextlib.contextmanager\ndef quiet():\n    \"\"\"\n    Redirect stdout/stderr to StringIO objects to prevent console output from\n    distutils commands.\n    \"\"\"\n\n    old_stdout = sys.stdout\n    old_stderr = sys.stderr\n    new_stdout = sys.stdout = io.StringIO()\n    new_stderr = sys.stderr = io.StringIO()\n    try:\n        yield new_stdout, new_stderr\n    finally:\n        new_stdout.seek(0)\n        new_stderr.seek(0)\n        sys.stdout = old_stdout\n        sys.stderr = old_stderr\n\n\n@contextlib.contextmanager\ndef save_user_site_setting():\n    saved = site.ENABLE_USER_SITE\n    try:\n        yield saved\n    finally:\n        site.ENABLE_USER_SITE = saved\n\n\n@contextlib.contextmanager\ndef save_pkg_resources_state():\n    import pkg_resources\n\n    pr_state = pkg_resources.__getstate__()\n    # also save sys.path\n    sys_path = sys.path[:]\n    try:\n        yield pr_state, sys_path\n    finally:\n        sys.path[:] = sys_path\n        pkg_resources.__setstate__(pr_state)\n\n\n@contextlib.contextmanager\ndef suppress_exceptions(*excs):\n    try:\n        yield\n    except excs:\n        pass\n\n\ndef multiproc(request):\n    \"\"\"\n    Return True if running under xdist and multiple\n    workers are used.\n    \"\"\"\n    try:\n        worker_id = request.getfixturevalue('worker_id')\n    except Exception:\n        return False\n    return worker_id != 'master'\n\n\n@contextlib.contextmanager\ndef session_locked_tmp_dir(request, tmp_path_factory, name):\n    \"\"\"Uses a file lock to guarantee only one worker can access a temp dir\"\"\"\n    # get the temp directory shared by all workers\n    base = tmp_path_factory.getbasetemp()\n    shared_dir = base.parent if multiproc(request) else base\n\n    locked_dir = shared_dir / name\n    with FileLock(locked_dir.with_suffix(\".lock\")):\n        # ^-- prevent multiple workers to access the directory at once\n        locked_dir.mkdir(exist_ok=True, parents=True)\n        yield locked_dir\n\n\n@contextlib.contextmanager\ndef save_paths():\n    \"\"\"Make sure ``sys.path``, ``sys.meta_path`` and ``sys.path_hooks`` are preserved\"\"\"\n    prev = sys.path[:], sys.meta_path[:], sys.path_hooks[:]\n\n    try:\n        yield\n    finally:\n        sys.path, sys.meta_path, sys.path_hooks = prev\n\n\n@contextlib.contextmanager\ndef save_sys_modules():\n    \"\"\"Make sure initial ``sys.modules`` is preserved\"\"\"\n    prev_modules = sys.modules\n\n    try:\n        sys.modules = sys.modules.copy()\n        yield\n    finally:\n        sys.modules = prev_modules\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/tests/environment.py","size":3102,"sha1":"0b5349c8f40ab5666bbbcfdb12806d500855a8a0","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import os\nimport subprocess\nimport sys\nimport unicodedata\nfrom subprocess import PIPE as _PIPE, Popen as _Popen\n\nimport jaraco.envs\n\n\nclass VirtualEnv(jaraco.envs.VirtualEnv):\n    name = '.env'\n    # Some version of PyPy will import distutils on startup, implicitly\n    # importing setuptools, and thus leading to BackendInvalid errors\n    # when upgrading Setuptools. Bypass this behavior by avoiding the\n    # early availability and need to upgrade.\n    create_opts = ['--no-setuptools']\n\n    def run(self, cmd, *args, **kwargs):\n        cmd = [self.exe(cmd[0])] + cmd[1:]\n        kwargs = {\"cwd\": self.root, \"encoding\": \"utf-8\", **kwargs}  # Allow overriding\n        # In some environments (eg. downstream distro packaging), where:\n        # - tox isn't used to run tests and\n        # - PYTHONPATH is set to point to a specific setuptools codebase and\n        # - no custom env is explicitly set by a test\n        # PYTHONPATH will leak into the spawned processes.\n        # In that case tests look for module in the wrong place (on PYTHONPATH).\n        # Unless the test sets its own special env, pass a copy of the existing\n        # environment with removed PYTHONPATH to the subprocesses.\n        if \"env\" not in kwargs:\n            env = dict(os.environ)\n            if \"PYTHONPATH\" in env:\n                del env[\"PYTHONPATH\"]\n            kwargs[\"env\"] = env\n        return subprocess.check_output(cmd, *args, **kwargs)\n\n\ndef _which_dirs(cmd):\n    result = set()\n    for path in os.environ.get('PATH', '').split(os.pathsep):\n        filename = os.path.join(path, cmd)\n        if os.access(filename, os.X_OK):\n            result.add(path)\n    return result\n\n\ndef run_setup_py(cmd, pypath=None, path=None, data_stream=0, env=None):\n    \"\"\"\n    Execution command for tests, separate from those used by the\n    code directly to prevent accidental behavior issues\n    \"\"\"\n    if env is None:\n        env = dict()\n        for envname in os.environ:\n            env[envname] = os.environ[envname]\n\n    # override the python path if needed\n    if pypath is not None:\n        env[\"PYTHONPATH\"] = pypath\n\n    # override the execution path if needed\n    if path is not None:\n        env[\"PATH\"] = path\n    if not env.get(\"PATH\", \"\"):\n        env[\"PATH\"] = _which_dirs(\"tar\").union(_which_dirs(\"gzip\"))\n        env[\"PATH\"] = os.pathsep.join(env[\"PATH\"])\n\n    cmd = [sys.executable, \"setup.py\"] + list(cmd)\n\n    # https://bugs.python.org/issue8557\n    shell = sys.platform == 'win32'\n\n    try:\n        proc = _Popen(\n            cmd,\n            stdout=_PIPE,\n            stderr=_PIPE,\n            shell=shell,\n            env=env,\n            encoding=\"utf-8\",\n        )\n\n        if isinstance(data_stream, tuple):\n            data_stream = slice(*data_stream)\n        data = proc.communicate()[data_stream]\n    except OSError:\n        return 1, ''\n\n    # decode the console string if needed\n    if hasattr(data, \"decode\"):\n        # use the default encoding\n        data = data.decode()\n        data = unicodedata.normalize('NFC', data)\n\n    # communicate calls wait()\n    return proc.returncode, data\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/tests/fixtures.py","size":5197,"sha1":"6d8b0624a0ac83559a82efcc54666ace4458dc86","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import contextlib\nimport os\nimport subprocess\nimport sys\nfrom pathlib import Path\n\nimport path\nimport pytest\n\nfrom . import contexts, environment\n\n\n@pytest.fixture\ndef user_override(monkeypatch):\n    \"\"\"\n    Override site.USER_BASE and site.USER_SITE with temporary directories in\n    a context.\n    \"\"\"\n    with contexts.tempdir() as user_base:\n        monkeypatch.setattr('site.USER_BASE', user_base)\n        with contexts.tempdir() as user_site:\n            monkeypatch.setattr('site.USER_SITE', user_site)\n            with contexts.save_user_site_setting():\n                yield\n\n\n@pytest.fixture\ndef tmpdir_cwd(tmpdir):\n    with tmpdir.as_cwd() as orig:\n        yield orig\n\n\n@pytest.fixture(autouse=True, scope=\"session\")\ndef workaround_xdist_376(request):\n    \"\"\"\n    Workaround pytest-dev/pytest-xdist#376\n\n    ``pytest-xdist`` tends to inject '' into ``sys.path``,\n    which may break certain isolation expectations.\n    Remove the entry so the import\n    machinery behaves the same irrespective of xdist.\n    \"\"\"\n    if not request.config.pluginmanager.has_plugin('xdist'):\n        return\n\n    with contextlib.suppress(ValueError):\n        sys.path.remove('')\n\n\n@pytest.fixture\ndef sample_project(tmp_path):\n    \"\"\"\n    Clone the 'sampleproject' and return a path to it.\n    \"\"\"\n    cmd = ['git', 'clone', 'https://github.com/pypa/sampleproject']\n    try:\n        subprocess.check_call(cmd, cwd=str(tmp_path))\n    except Exception:\n        pytest.skip(\"Unable to clone sampleproject\")\n    return tmp_path / 'sampleproject'\n\n\n# sdist and wheel artifacts should be stable across a round of tests\n# so we can build them once per session and use the files as \"readonly\"\n\n# In the case of setuptools, building the wheel without sdist may cause\n# it to contain the `build` directory, and therefore create situations with\n# `setuptools/build/lib/build/lib/...`. To avoid that, build both artifacts at once.\n\n\ndef _build_distributions(tmp_path_factory, request):\n    with contexts.session_locked_tmp_dir(\n        request, tmp_path_factory, \"dist_build\"\n    ) as tmp:  # pragma: no cover\n        sdist = next(tmp.glob(\"*.tar.gz\"), None)\n        wheel = next(tmp.glob(\"*.whl\"), None)\n        if sdist and wheel:\n            return (sdist, wheel)\n\n        # Sanity check: should not create recursive setuptools/build/lib/build/lib/...\n        assert not Path(request.config.rootdir, \"build/lib/build\").exists()\n\n        subprocess.check_output([\n            sys.executable,\n            \"-m\",\n            \"build\",\n            \"--outdir\",\n            str(tmp),\n            str(request.config.rootdir),\n        ])\n\n        # Sanity check: should not create recursive setuptools/build/lib/build/lib/...\n        assert not Path(request.config.rootdir, \"build/lib/build\").exists()\n\n        return next(tmp.glob(\"*.tar.gz\")), next(tmp.glob(\"*.whl\"))\n\n\n@pytest.fixture(scope=\"session\")\ndef setuptools_sdist(tmp_path_factory, request):\n    prebuilt = os.getenv(\"PRE_BUILT_SETUPTOOLS_SDIST\")\n    if prebuilt and os.path.exists(prebuilt):  # pragma: no cover\n        return Path(prebuilt).resolve()\n\n    sdist, _ = _build_distributions(tmp_path_factory, request)\n    return sdist\n\n\n@pytest.fixture(scope=\"session\")\ndef setuptools_wheel(tmp_path_factory, request):\n    prebuilt = os.getenv(\"PRE_BUILT_SETUPTOOLS_WHEEL\")\n    if prebuilt and os.path.exists(prebuilt):  # pragma: no cover\n        return Path(prebuilt).resolve()\n\n    _, wheel = _build_distributions(tmp_path_factory, request)\n    return wheel\n\n\n@pytest.fixture\ndef venv(tmp_path, setuptools_wheel):\n    \"\"\"Virtual env with the version of setuptools under test installed\"\"\"\n    env = environment.VirtualEnv()\n    env.root = path.Path(tmp_path / 'venv')\n    env.create_opts = ['--no-setuptools', '--wheel=bundle']\n    # TODO: Use `--no-wheel` when setuptools implements its own bdist_wheel\n    env.req = str(setuptools_wheel)\n    # In some environments (eg. downstream distro packaging),\n    # where tox isn't used to run tests and PYTHONPATH is set to point to\n    # a specific setuptools codebase, PYTHONPATH will leak into the spawned\n    # processes.\n    # env.create() should install the just created setuptools\n    # wheel, but it doesn't if it finds another existing matching setuptools\n    # installation present on PYTHONPATH:\n    # `setuptools is already installed with the same version as the provided\n    # wheel. Use --force-reinstall to force an installation of the wheel.`\n    # This prevents leaking PYTHONPATH to the created environment.\n    with contexts.environment(PYTHONPATH=None):\n        return env.create()\n\n\n@pytest.fixture\ndef venv_without_setuptools(tmp_path):\n    \"\"\"Virtual env without any version of setuptools installed\"\"\"\n    env = environment.VirtualEnv()\n    env.root = path.Path(tmp_path / 'venv_without_setuptools')\n    env.create_opts = ['--no-setuptools', '--no-wheel']\n    env.ensure_env()\n    return env\n\n\n@pytest.fixture\ndef bare_venv(tmp_path):\n    \"\"\"Virtual env without any common packages installed\"\"\"\n    env = environment.VirtualEnv()\n    env.root = path.Path(tmp_path / 'bare_venv')\n    env.create_opts = ['--no-setuptools', '--no-pip', '--no-wheel', '--no-seed']\n    env.ensure_env()\n    return env\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/tests/indexes/test_links_priority/external.html","size":92,"sha1":"0a0091cbce99adffb0b768a1c613b753b90a5570","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"<html><body>\n<a href=\"/foobar-0.1.tar.gz#md5=1__bad_md5___\">bad old link</a>\n</body></html>\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/tests/indexes/test_links_priority/simple/foobar/index.html","size":174,"sha1":"f856eaf3b778915a665463baac007f9ad9a3b196","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"<html><body>\n<a href=\"/foobar-0.1.tar.gz#md5=0_correct_md5\">foobar-0.1.tar.gz</a><br/>\n<a href=\"../../external.html\" rel=\"homepage\">external homepage</a><br/>\n</body></html>\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/tests/integration/__init__.py","size":0,"sha1":"da39a3ee5e6b4b0d3255bfef95601890afd80709","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":""},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/tests/integration/helpers.py","size":2522,"sha1":"c8b7eff32a0fe54c3f67c9fa3ca27a1f11be7485","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"Reusable functions and classes for different types of integration tests.\n\nFor example ``Archive`` can be used to check the contents of distribution built\nwith setuptools, and ``run`` will always try to be as verbose as possible to\nfacilitate debugging.\n\"\"\"\n\nimport os\nimport subprocess\nimport tarfile\nfrom pathlib import Path\nfrom zipfile import ZipFile\n\n\ndef run(cmd, env=None):\n    r = subprocess.run(\n        cmd,\n        capture_output=True,\n        text=True,\n        encoding=\"utf-8\",\n        env={**os.environ, **(env or {})},\n        # ^-- allow overwriting instead of discarding the current env\n    )\n\n    out = r.stdout + \"\\n\" + r.stderr\n    # pytest omits stdout/err by default, if the test fails they help debugging\n    print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n    print(f\"Command: {cmd}\\nreturn code: {r.returncode}\\n\\n{out}\")\n\n    if r.returncode == 0:\n        return out\n    raise subprocess.CalledProcessError(r.returncode, cmd, r.stdout, r.stderr)\n\n\nclass Archive:\n    \"\"\"Compatibility layer for ZipFile/Info and TarFile/Info\"\"\"\n\n    def __init__(self, filename):\n        self._filename = filename\n        if filename.endswith(\"tar.gz\"):\n            self._obj = tarfile.open(filename, \"r:gz\")\n        elif filename.endswith(\"zip\"):\n            self._obj = ZipFile(filename)\n        else:\n            raise ValueError(f\"{filename} doesn't seem to be a zip or tar.gz\")\n\n    def __iter__(self):\n        if hasattr(self._obj, \"infolist\"):\n            return iter(self._obj.infolist())\n        return iter(self._obj)\n\n    def get_name(self, zip_or_tar_info):\n        if hasattr(zip_or_tar_info, \"filename\"):\n            return zip_or_tar_info.filename\n        return zip_or_tar_info.name\n\n    def get_content(self, zip_or_tar_info):\n        if hasattr(self._obj, \"extractfile\"):\n            content = self._obj.extractfile(zip_or_tar_info)\n            if content is None:\n                msg = f\"Invalid {zip_or_tar_info.name} in {self._filename}\"\n                raise ValueError(msg)\n            return str(content.read(), \"utf-8\")\n        return str(self._obj.read(zip_or_tar_info), \"utf-8\")\n\n\ndef get_sdist_members(sdist_path):\n    with tarfile.open(sdist_path, \"r:gz\") as tar:\n        files = [Path(f) for f in tar.getnames()]\n    # remove root folder\n    relative_files = (\"/\".join(f.parts[1:]) for f in files)\n    return {f for f in relative_files if f}\n\n\ndef get_wheel_members(wheel_path):\n    with ZipFile(wheel_path) as zipfile:\n        return set(zipfile.namelist())\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/tests/integration/test_pip_install_sdist.py","size":8204,"sha1":"39292253d5b3a644d9c7a2f1ac3705db157f7e45","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"# https://github.com/python/mypy/issues/16936\n# mypy: disable-error-code=\"has-type\"\n\"\"\"Integration tests for setuptools that focus on building packages via pip.\n\nThe idea behind these tests is not to exhaustively check all the possible\ncombinations of packages, operating systems, supporting libraries, etc, but\nrather check a limited number of popular packages and how they interact with\nthe exposed public API. This way if any change in API is introduced, we hope to\nidentify backward compatibility problems before publishing a release.\n\nThe number of tested packages is purposefully kept small, to minimise duration\nand the associated maintenance cost (changes in the way these packages define\ntheir build process may require changes in the tests).\n\"\"\"\n\nimport json\nimport os\nimport shutil\nimport sys\nfrom enum import Enum\nfrom glob import glob\nfrom hashlib import md5\nfrom urllib.request import urlopen\n\nimport pytest\nfrom packaging.requirements import Requirement\n\nfrom .helpers import Archive, run\n\npytestmark = pytest.mark.integration\n\n\n(LATEST,) = Enum(\"v\", \"LATEST\")  # type: ignore[misc] # https://github.com/python/mypy/issues/16936\n\"\"\"Default version to be checked\"\"\"\n# There are positive and negative aspects of checking the latest version of the\n# packages.\n# The main positive aspect is that the latest version might have already\n# removed the use of APIs deprecated in previous releases of setuptools.\n\n\n# Packages to be tested:\n# (Please notice the test environment cannot support EVERY library required for\n# compiling binary extensions. In Ubuntu/Debian nomenclature, we only assume\n# that `build-essential`, `gfortran` and `libopenblas-dev` are installed,\n# due to their relevance to the numerical/scientific programming ecosystem)\nEXAMPLES = [\n    (\"pip\", LATEST),  # just in case...\n    (\"pytest\", LATEST),  # uses setuptools_scm\n    (\"mypy\", LATEST),  # custom build_py + ext_modules\n    # --- Popular packages: https://hugovk.github.io/top-pypi-packages/ ---\n    (\"botocore\", LATEST),\n    (\"kiwisolver\", LATEST),  # build_ext\n    (\"brotli\", LATEST),  # not in the list but used by urllib3\n    (\"pyyaml\", LATEST),  # cython + custom build_ext + custom distclass\n    (\"charset-normalizer\", LATEST),  # uses mypyc, used by aiohttp\n    (\"protobuf\", LATEST),\n    (\"requests\", LATEST),\n    (\"celery\", LATEST),\n    # When adding packages to this list, make sure they expose a `__version__`\n    # attribute, or modify the tests below\n]\n\n\n# Some packages have \"optional\" dependencies that modify their build behaviour\n# and are not listed in pyproject.toml, others still use `setup_requires`\nEXTRA_BUILD_DEPS = {\n    \"pyyaml\": (\"Cython<3.0\",),  # constraint to avoid errors\n    \"charset-normalizer\": (\"mypy>=1.4.1\",),  # no pyproject.toml available\n}\n\nEXTRA_ENV_VARS = {\n    \"pyyaml\": {\"PYYAML_FORCE_CYTHON\": \"1\"},\n    \"charset-normalizer\": {\"CHARSET_NORMALIZER_USE_MYPYC\": \"1\"},\n}\n\nIMPORT_NAME = {\n    \"pyyaml\": \"yaml\",\n    \"protobuf\": \"google.protobuf\",\n}\n\n\nVIRTUALENV = (sys.executable, \"-m\", \"virtualenv\")\n\n\n# By default, pip will try to build packages in isolation (PEP 517), which\n# means it will download the previous stable version of setuptools.\n# `pip` flags can avoid that (the version of setuptools under test\n# should be the one to be used)\nINSTALL_OPTIONS = (\n    \"--ignore-installed\",\n    \"--no-build-isolation\",\n    # Omit \"--no-binary :all:\" the sdist is supplied directly.\n    # Allows dependencies as wheels.\n)\n# The downside of `--no-build-isolation` is that pip will not download build\n# dependencies. The test script will have to also handle that.\n\n\n@pytest.fixture\ndef venv_python(tmp_path):\n    run([*VIRTUALENV, str(tmp_path / \".venv\")])\n    possible_path = (str(p.parent) for p in tmp_path.glob(\".venv/*/python*\"))\n    return shutil.which(\"python\", path=os.pathsep.join(possible_path))\n\n\n@pytest.fixture(autouse=True)\ndef _prepare(tmp_path, venv_python, monkeypatch):\n    download_path = os.getenv(\"DOWNLOAD_PATH\", str(tmp_path))\n    os.makedirs(download_path, exist_ok=True)\n\n    # Environment vars used for building some of the packages\n    monkeypatch.setenv(\"USE_MYPYC\", \"1\")\n\n    yield\n\n    # Let's provide the maximum amount of information possible in the case\n    # it is necessary to debug the tests directly from the CI logs.\n    print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n    print(\"Temporary directory:\")\n    map(print, tmp_path.glob(\"*\"))\n    print(\"Virtual environment:\")\n    run([venv_python, \"-m\", \"pip\", \"freeze\"])\n\n\n@pytest.mark.parametrize((\"package\", \"version\"), EXAMPLES)\n@pytest.mark.uses_network\ndef test_install_sdist(package, version, tmp_path, venv_python, setuptools_wheel):\n    venv_pip = (venv_python, \"-m\", \"pip\")\n    sdist = retrieve_sdist(package, version, tmp_path)\n    deps = build_deps(package, sdist)\n    if deps:\n        print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n        print(\"Dependencies:\", deps)\n        run([*venv_pip, \"install\", *deps])\n\n    # Use a virtualenv to simulate PEP 517 isolation\n    # but install fresh setuptools wheel to ensure the version under development\n    env = EXTRA_ENV_VARS.get(package, {})\n    run([*venv_pip, \"install\", \"--force-reinstall\", setuptools_wheel])\n    run([*venv_pip, \"install\", *INSTALL_OPTIONS, sdist], env)\n\n    # Execute a simple script to make sure the package was installed correctly\n    pkg = IMPORT_NAME.get(package, package).replace(\"-\", \"_\")\n    script = f\"import {pkg}; print(getattr({pkg}, '__version__', 0))\"\n    run([venv_python, \"-c\", script])\n\n\n# ---- Helper Functions ----\n\n\ndef retrieve_sdist(package, version, tmp_path):\n    \"\"\"Either use cached sdist file or download it from PyPI\"\"\"\n    # `pip download` cannot be used due to\n    # https://github.com/pypa/pip/issues/1884\n    # https://discuss.python.org/t/pep-625-file-name-of-a-source-distribution/4686\n    # We have to find the correct distribution file and download it\n    download_path = os.getenv(\"DOWNLOAD_PATH\", str(tmp_path))\n    dist = retrieve_pypi_sdist_metadata(package, version)\n\n    # Remove old files to prevent cache to grow indefinitely\n    for file in glob(os.path.join(download_path, f\"{package}*\")):\n        if dist[\"filename\"] != file:\n            os.unlink(file)\n\n    dist_file = os.path.join(download_path, dist[\"filename\"])\n    if not os.path.exists(dist_file):\n        download(dist[\"url\"], dist_file, dist[\"md5_digest\"])\n    return dist_file\n\n\ndef retrieve_pypi_sdist_metadata(package, version):\n    # https://warehouse.pypa.io/api-reference/json.html\n    id_ = package if version is LATEST else f\"{package}/{version}\"\n    with urlopen(f\"https://pypi.org/pypi/{id_}/json\") as f:\n        metadata = json.load(f)\n\n    if metadata[\"info\"][\"yanked\"]:\n        raise ValueError(f\"Release for {package} {version} was yanked\")\n\n    version = metadata[\"info\"][\"version\"]\n    release = metadata[\"releases\"][version] if version is LATEST else metadata[\"urls\"]\n    (sdist,) = filter(lambda d: d[\"packagetype\"] == \"sdist\", release)\n    return sdist\n\n\ndef download(url, dest, md5_digest):\n    with urlopen(url) as f:\n        data = f.read()\n\n    assert md5(data).hexdigest() == md5_digest\n\n    with open(dest, \"wb\") as f:\n        f.write(data)\n\n    assert os.path.exists(dest)\n\n\ndef build_deps(package, sdist_file):\n    \"\"\"Find out what are the build dependencies for a package.\n\n    \"Manually\" install them, since pip will not install build\n    deps with `--no-build-isolation`.\n    \"\"\"\n    # delay importing, since pytest discovery phase may hit this file from a\n    # testenv without tomli\n    from setuptools.compat.py310 import tomllib\n\n    archive = Archive(sdist_file)\n    info = tomllib.loads(_read_pyproject(archive))\n    deps = info.get(\"build-system\", {}).get(\"requires\", [])\n    deps += EXTRA_BUILD_DEPS.get(package, [])\n    # Remove setuptools from requirements (and deduplicate)\n    requirements = {Requirement(d).name: d for d in deps}\n    return [v for k, v in requirements.items() if k != \"setuptools\"]\n\n\ndef _read_pyproject(archive):\n    contents = (\n        archive.get_content(member)\n        for member in archive\n        if os.path.basename(archive.get_name(member)) == \"pyproject.toml\"\n    )\n    return next(contents, \"\")\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/tests/mod_with_constant.py","size":22,"sha1":"9cf7de6a1bc806ce8ac8c6d04bdf52d0004ca6a1","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"value = 'three, sir!'\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/tests/namespaces.py","size":2774,"sha1":"faec9e3af88471d466e33ec01c6843c3edf2f1d7","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import ast\nimport json\nimport textwrap\nfrom pathlib import Path\n\n\ndef iter_namespace_pkgs(namespace):\n    parts = namespace.split(\".\")\n    for i in range(len(parts)):\n        yield \".\".join(parts[: i + 1])\n\n\ndef build_namespace_package(tmpdir, name, version=\"1.0\", impl=\"pkg_resources\"):\n    src_dir = tmpdir / name\n    src_dir.mkdir()\n    setup_py = src_dir / 'setup.py'\n    namespace, _, rest = name.rpartition('.')\n    namespaces = list(iter_namespace_pkgs(namespace))\n    setup_args = {\n        \"name\": name,\n        \"version\": version,\n        \"packages\": namespaces,\n    }\n\n    if impl == \"pkg_resources\":\n        tmpl = '__import__(\"pkg_resources\").declare_namespace(__name__)'\n        setup_args[\"namespace_packages\"] = namespaces\n    elif impl == \"pkgutil\":\n        tmpl = '__path__ = __import__(\"pkgutil\").extend_path(__path__, __name__)'\n    else:\n        raise ValueError(f\"Cannot recognise {impl=} when creating namespaces\")\n\n    args = json.dumps(setup_args, indent=4)\n    assert ast.literal_eval(args)  # ensure it is valid Python\n\n    script = textwrap.dedent(\n        \"\"\"\\\n        import setuptools\n        args = {args}\n        setuptools.setup(**args)\n        \"\"\"\n    ).format(args=args)\n    setup_py.write_text(script, encoding='utf-8')\n\n    ns_pkg_dir = Path(src_dir, namespace.replace(\".\", \"/\"))\n    ns_pkg_dir.mkdir(parents=True)\n\n    for ns in namespaces:\n        pkg_init = src_dir / ns.replace(\".\", \"/\") / '__init__.py'\n        pkg_init.write_text(tmpl, encoding='utf-8')\n\n    pkg_mod = ns_pkg_dir / (rest + '.py')\n    some_functionality = 'name = {rest!r}'.format(**locals())\n    pkg_mod.write_text(some_functionality, encoding='utf-8')\n    return src_dir\n\n\ndef build_pep420_namespace_package(tmpdir, name):\n    src_dir = tmpdir / name\n    src_dir.mkdir()\n    pyproject = src_dir / \"pyproject.toml\"\n    namespace, _, rest = name.rpartition(\".\")\n    script = f\"\"\"\\\n        [build-system]\n        requires = [\"setuptools\"]\n        build-backend = \"setuptools.build_meta\"\n\n        [project]\n        name = \"{name}\"\n        version = \"3.14159\"\n        \"\"\"\n    pyproject.write_text(textwrap.dedent(script), encoding='utf-8')\n    ns_pkg_dir = Path(src_dir, namespace.replace(\".\", \"/\"))\n    ns_pkg_dir.mkdir(parents=True)\n    pkg_mod = ns_pkg_dir / (rest + \".py\")\n    some_functionality = f\"name = {rest!r}\"\n    pkg_mod.write_text(some_functionality, encoding='utf-8')\n    return src_dir\n\n\ndef make_site_dir(target):\n    \"\"\"\n    Add a sitecustomize.py module in target to cause\n    target to be added to site dirs such that .pth files\n    are processed there.\n    \"\"\"\n    sc = target / 'sitecustomize.py'\n    target_str = str(target)\n    tmpl = '__import__(\"site\").addsitedir({target_str!r})'\n    sc.write_text(tmpl.format(**locals()), encoding='utf-8')\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/tests/script-with-bom.py","size":18,"sha1":"8974a5aa344d37cfb7634ffb42c5fd49315db611","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"result = 'passed'\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/tests/server.py","size":2397,"sha1":"e181a4a05bf841d71e8d94e63fcdcf829deadf7b","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"Basic http server for tests to simulate PyPI or custom indexes\"\"\"\n\nimport http.server\nimport os\nimport threading\nimport time\nimport urllib.parse\nimport urllib.request\n\n\nclass IndexServer(http.server.HTTPServer):\n    \"\"\"Basic single-threaded http server simulating a package index\n\n    You can use this server in unittest like this::\n        s = IndexServer()\n        s.start()\n        index_url = s.base_url() + 'mytestindex'\n        # do some test requests to the index\n        # The index files should be located in setuptools/tests/indexes\n        s.stop()\n    \"\"\"\n\n    def __init__(\n        self,\n        server_address=('', 0),\n        RequestHandlerClass=http.server.SimpleHTTPRequestHandler,\n    ):\n        http.server.HTTPServer.__init__(self, server_address, RequestHandlerClass)\n        self._run = True\n\n    def start(self):\n        self.thread = threading.Thread(target=self.serve_forever)\n        self.thread.start()\n\n    def stop(self):\n        \"Stop the server\"\n\n        # Let the server finish the last request and wait for a new one.\n        time.sleep(0.1)\n\n        self.shutdown()\n        self.thread.join()\n        self.socket.close()\n\n    def base_url(self):\n        port = self.server_port\n        return f'http://127.0.0.1:{port}/setuptools/tests/indexes/'\n\n\nclass RequestRecorder(http.server.BaseHTTPRequestHandler):\n    def do_GET(self):\n        requests = vars(self.server).setdefault('requests', [])\n        requests.append(self)\n        self.send_response(200, 'OK')\n\n\nclass MockServer(http.server.HTTPServer, threading.Thread):\n    \"\"\"\n    A simple HTTP Server that records the requests made to it.\n    \"\"\"\n\n    def __init__(self, server_address=('', 0), RequestHandlerClass=RequestRecorder):\n        http.server.HTTPServer.__init__(self, server_address, RequestHandlerClass)\n        threading.Thread.__init__(self)\n        self.daemon = True\n        self.requests = []\n\n    def run(self):\n        self.serve_forever()\n\n    @property\n    def netloc(self):\n        return f'localhost:{self.server_port}'\n\n    @property\n    def url(self):\n        return f'http://{self.netloc}/'\n\n\ndef path_to_url(path, authority=None):\n    \"\"\"Convert a path to a file: URL.\"\"\"\n    path = os.path.normpath(os.path.abspath(path))\n    base = 'file:'\n    if authority is not None:\n        base += '//' + authority\n    return urllib.parse.urljoin(base, urllib.request.pathname2url(path))\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/tests/test_archive_util.py","size":845,"sha1":"f422702034d42dcc73b6a22df24d37c99f747fbb","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import io\nimport tarfile\n\nimport pytest\n\nfrom setuptools import archive_util\n\n\n@pytest.fixture\ndef tarfile_with_unicode(tmpdir):\n    \"\"\"\n    Create a tarfile containing only a file whose name is\n    a zero byte file called testimage.png.\n    \"\"\"\n    tarobj = io.BytesIO()\n\n    with tarfile.open(fileobj=tarobj, mode=\"w:gz\") as tgz:\n        data = b\"\"\n\n        filename = \"testimage.png\"\n\n        t = tarfile.TarInfo(filename)\n        t.size = len(data)\n\n        tgz.addfile(t, io.BytesIO(data))\n\n    target = tmpdir / 'unicode-pkg-1.0.tar.gz'\n    with open(str(target), mode='wb') as tf:\n        tf.write(tarobj.getvalue())\n    return str(target)\n\n\n@pytest.mark.xfail(reason=\"#710 and #712\")\ndef test_unicode_files(tarfile_with_unicode, tmpdir):\n    target = tmpdir / 'out'\n    archive_util.unpack_archive(tarfile_with_unicode, str(target))\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/tests/test_bdist_deprecations.py","size":775,"sha1":"995314d5a85d9219d87ff6eea505a433d5e38286","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"develop tests\"\"\"\n\nimport sys\nfrom unittest import mock\n\nimport pytest\n\nfrom setuptools import SetuptoolsDeprecationWarning\nfrom setuptools.dist import Distribution\n\n\n@pytest.mark.skipif(sys.platform == 'win32', reason='non-Windows only')\n@pytest.mark.xfail(reason=\"bdist_rpm is long deprecated, should we remove it? #1988\")\n@mock.patch('distutils.command.bdist_rpm.bdist_rpm')\ndef test_bdist_rpm_warning(distutils_cmd, tmpdir_cwd):\n    dist = Distribution(\n        dict(\n            script_name='setup.py',\n            script_args=['bdist_rpm'],\n            name='foo',\n            py_modules=['hi'],\n        )\n    )\n    dist.parse_command_line()\n    with pytest.warns(SetuptoolsDeprecationWarning):\n        dist.run_commands()\n\n    distutils_cmd.run.assert_called_once()\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/tests/test_bdist_egg.py","size":1957,"sha1":"384561090b6894da1ff113780c5bc437e2a96479","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"\"\"\"develop tests\"\"\"\n\nimport os\nimport re\nimport zipfile\n\nimport pytest\n\nfrom setuptools.dist import Distribution\n\nfrom . import contexts\n\nSETUP_PY = \"\"\"\\\nfrom setuptools import setup\n\nsetup(py_modules=['hi'])\n\"\"\"\n\n\n@pytest.fixture\ndef setup_context(tmpdir):\n    with (tmpdir / 'setup.py').open('w') as f:\n        f.write(SETUP_PY)\n    with (tmpdir / 'hi.py').open('w') as f:\n        f.write('1\\n')\n    with tmpdir.as_cwd():\n        yield tmpdir\n\n\nclass Test:\n    @pytest.mark.usefixtures(\"user_override\")\n    @pytest.mark.usefixtures(\"setup_context\")\n    def test_bdist_egg(self):\n        dist = Distribution(\n            dict(\n                script_name='setup.py',\n                script_args=['bdist_egg'],\n                name='foo',\n                py_modules=['hi'],\n            )\n        )\n        os.makedirs(os.path.join('build', 'src'))\n        with contexts.quiet():\n            dist.parse_command_line()\n            dist.run_commands()\n\n        # let's see if we got our egg link at the right place\n        [content] = os.listdir('dist')\n        assert re.match(r'foo-0.0.0-py[23].\\d+.egg$', content)\n\n    @pytest.mark.xfail(\n        os.environ.get('PYTHONDONTWRITEBYTECODE', False),\n        reason=\"Byte code disabled\",\n    )\n    @pytest.mark.usefixtures(\"user_override\")\n    @pytest.mark.usefixtures(\"setup_context\")\n    def test_exclude_source_files(self):\n        dist = Distribution(\n            dict(\n                script_name='setup.py',\n                script_args=['bdist_egg', '--exclude-source-files'],\n                py_modules=['hi'],\n            )\n        )\n        with contexts.quiet():\n            dist.parse_command_line()\n            dist.run_commands()\n        [dist_name] = os.listdir('dist')\n        dist_filename = os.path.join('dist', dist_name)\n        zip = zipfile.ZipFile(dist_filename)\n        names = list(zi.filename for zi in zip.filelist)\n        assert 'hi.pyc' in names\n        assert 'hi.py' not in names\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/tests/test_bdist_wheel.py","size":19906,"sha1":"a95c48f9db01c22e4b6c5653a42d1fa20599c143","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"from __future__ import annotations\n\nimport builtins\nimport importlib\nimport os.path\nimport platform\nimport shutil\nimport stat\nimport struct\nimport sys\nimport sysconfig\nfrom contextlib import suppress\nfrom inspect import cleandoc\nfrom zipfile import ZipFile\n\nimport jaraco.path\nimport pytest\nfrom packaging import tags\n\nimport setuptools\nfrom setuptools.command.bdist_wheel import bdist_wheel, get_abi_tag\nfrom setuptools.dist import Distribution\nfrom setuptools.warnings import SetuptoolsDeprecationWarning\n\nfrom distutils.core import run_setup\n\nDEFAULT_FILES = {\n    \"dummy_dist-1.0.dist-info/top_level.txt\",\n    \"dummy_dist-1.0.dist-info/METADATA\",\n    \"dummy_dist-1.0.dist-info/WHEEL\",\n    \"dummy_dist-1.0.dist-info/RECORD\",\n}\nDEFAULT_LICENSE_FILES = {\n    \"LICENSE\",\n    \"LICENSE.txt\",\n    \"LICENCE\",\n    \"LICENCE.txt\",\n    \"COPYING\",\n    \"COPYING.md\",\n    \"NOTICE\",\n    \"NOTICE.rst\",\n    \"AUTHORS\",\n    \"AUTHORS.txt\",\n}\nOTHER_IGNORED_FILES = {\n    \"LICENSE~\",\n    \"AUTHORS~\",\n}\nSETUPPY_EXAMPLE = \"\"\"\\\nfrom setuptools import setup\n\nsetup(\n    name='dummy_dist',\n    version='1.0',\n)\n\"\"\"\n\n\nEXAMPLES = {\n    \"dummy-dist\": {\n        \"setup.py\": SETUPPY_EXAMPLE,\n        \"licenses\": {\"DUMMYFILE\": \"\"},\n        **dict.fromkeys(DEFAULT_LICENSE_FILES | OTHER_IGNORED_FILES, \"\"),\n    },\n    \"simple-dist\": {\n        \"setup.py\": cleandoc(\n            \"\"\"\n            from setuptools import setup\n\n            setup(\n                name=\"simple.dist\",\n                version=\"0.1\",\n                description=\"A testing distribution \\N{SNOWMAN}\",\n                extras_require={\"voting\": [\"beaglevote\"]},\n            )\n            \"\"\"\n        ),\n        \"simpledist\": \"\",\n    },\n    \"complex-dist\": {\n        \"setup.py\": cleandoc(\n            \"\"\"\n            from setuptools import setup\n\n            setup(\n                name=\"complex-dist\",\n                version=\"0.1\",\n                description=\"Another testing distribution \\N{SNOWMAN}\",\n                long_description=\"Another testing distribution \\N{SNOWMAN}\",\n                author=\"Illustrious Author\",\n                author_email=\"illustrious@example.org\",\n                url=\"http://example.org/exemplary\",\n                packages=[\"complexdist\"],\n                setup_requires=[\"setuptools\"],\n                install_requires=[\"quux\", \"splort\"],\n                extras_require={\"simple\": [\"simple.dist\"]},\n                entry_points={\n                    \"console_scripts\": [\n                        \"complex-dist=complexdist:main\",\n                        \"complex-dist2=complexdist:main\",\n                    ],\n                },\n            )\n            \"\"\"\n        ),\n        \"complexdist\": {\"__init__.py\": \"def main(): return\"},\n    },\n    \"headers-dist\": {\n        \"setup.py\": cleandoc(\n            \"\"\"\n            from setuptools import setup\n\n            setup(\n                name=\"headers.dist\",\n                version=\"0.1\",\n                description=\"A distribution with headers\",\n                headers=[\"header.h\"],\n            )\n            \"\"\"\n        ),\n        \"headersdist.py\": \"\",\n        \"header.h\": \"\",\n    },\n    \"commasinfilenames-dist\": {\n        \"setup.py\": cleandoc(\n            \"\"\"\n            from setuptools import setup\n\n            setup(\n                name=\"testrepo\",\n                version=\"0.1\",\n                packages=[\"mypackage\"],\n                description=\"A test package with commas in file names\",\n                include_package_data=True,\n                package_data={\"mypackage.data\": [\"*\"]},\n            )\n            \"\"\"\n        ),\n        \"mypackage\": {\n            \"__init__.py\": \"\",\n            \"data\": {\"__init__.py\": \"\", \"1,2,3.txt\": \"\"},\n        },\n        \"testrepo-0.1.0\": {\n            \"mypackage\": {\"__init__.py\": \"\"},\n        },\n    },\n    \"unicode-dist\": {\n        \"setup.py\": cleandoc(\n            \"\"\"\n            from setuptools import setup\n\n            setup(\n                name=\"unicode.dist\",\n                version=\"0.1\",\n                description=\"A testing distribution \\N{SNOWMAN}\",\n                packages=[\"unicodedist\"],\n                zip_safe=True,\n            )\n            \"\"\"\n        ),\n        \"unicodedist\": {\"__init__.py\": \"\", \"_.py\": \"\"},\n    },\n    \"utf8-metadata-dist\": {\n        \"setup.cfg\": cleandoc(\n            \"\"\"\n            [metadata]\n            name = utf8-metadata-dist\n            version = 42\n            author_email = \"John X. re\" <john@utf8.org>,    <gama@utf8.org>\n            long_description = file: README.rst\n            \"\"\"\n        ),\n        \"README.rst\": \"UTF-8  \",\n    },\n}\n\n\nif sys.platform != \"win32\":\n    # ABI3 extensions don't really work on Windows\n    EXAMPLES[\"abi3extension-dist\"] = {\n        \"setup.py\": cleandoc(\n            \"\"\"\n            from setuptools import Extension, setup\n\n            setup(\n                name=\"extension.dist\",\n                version=\"0.1\",\n                description=\"A testing distribution \\N{SNOWMAN}\",\n                ext_modules=[\n                    Extension(\n                        name=\"extension\", sources=[\"extension.c\"], py_limited_api=True\n                    )\n                ],\n            )\n            \"\"\"\n        ),\n        \"setup.cfg\": \"[bdist_wheel]\\npy_limited_api=cp32\",\n        \"extension.c\": \"#define Py_LIMITED_API 0x03020000\\n#include <Python.h>\",\n    }\n\n\ndef bdist_wheel_cmd(**kwargs):\n    \"\"\"Run command in the same process so that it is easier to collect coverage\"\"\"\n    dist_obj = (\n        run_setup(\"setup.py\", stop_after=\"init\")\n        if os.path.exists(\"setup.py\")\n        else Distribution({\"script_name\": \"%%build_meta%%\"})\n    )\n    dist_obj.parse_config_files()\n    cmd = bdist_wheel(dist_obj)\n    for attr, value in kwargs.items():\n        setattr(cmd, attr, value)\n    cmd.finalize_options()\n    return cmd\n\n\ndef mkexample(tmp_path_factory, name):\n    basedir = tmp_path_factory.mktemp(name)\n    jaraco.path.build(EXAMPLES[name], prefix=str(basedir))\n    return basedir\n\n\n@pytest.fixture(scope=\"session\")\ndef wheel_paths(tmp_path_factory):\n    build_base = tmp_path_factory.mktemp(\"build\")\n    dist_dir = tmp_path_factory.mktemp(\"dist\")\n    for name in EXAMPLES:\n        example_dir = mkexample(tmp_path_factory, name)\n        build_dir = build_base / name\n        with jaraco.path.DirectoryStack().context(example_dir):\n            bdist_wheel_cmd(bdist_dir=str(build_dir), dist_dir=str(dist_dir)).run()\n\n    return sorted(str(fname) for fname in dist_dir.glob(\"*.whl\"))\n\n\n@pytest.fixture\ndef dummy_dist(tmp_path_factory):\n    return mkexample(tmp_path_factory, \"dummy-dist\")\n\n\ndef test_no_scripts(wheel_paths):\n    \"\"\"Make sure entry point scripts are not generated.\"\"\"\n    path = next(path for path in wheel_paths if \"complex_dist\" in path)\n    for entry in ZipFile(path).infolist():\n        assert \".data/scripts/\" not in entry.filename\n\n\ndef test_unicode_record(wheel_paths):\n    path = next(path for path in wheel_paths if \"unicode.dist\" in path)\n    with ZipFile(path) as zf:\n        record = zf.read(\"unicode.dist-0.1.dist-info/RECORD\")\n\n    assert \"_.py\".encode() in record\n\n\nUTF8_PKG_INFO = \"\"\"\\\nMetadata-Version: 2.1\nName: helloworld\nVersion: 42\nAuthor-email: \"John X. re\" <john@utf8.org>,    <gama@utf8.org>\n\n\nUTF-8  \n\"\"\"\n\n\ndef test_preserve_unicode_metadata(monkeypatch, tmp_path):\n    monkeypatch.chdir(tmp_path)\n    egginfo = tmp_path / \"dummy_dist.egg-info\"\n    distinfo = tmp_path / \"dummy_dist.dist-info\"\n\n    egginfo.mkdir()\n    (egginfo / \"PKG-INFO\").write_text(UTF8_PKG_INFO, encoding=\"utf-8\")\n    (egginfo / \"dependency_links.txt\").touch()\n\n    class simpler_bdist_wheel(bdist_wheel):\n        \"\"\"Avoid messing with setuptools/distutils internals\"\"\"\n\n        def __init__(self):\n            pass\n\n        @property\n        def license_paths(self):\n            return []\n\n    cmd_obj = simpler_bdist_wheel()\n    cmd_obj.egg2dist(egginfo, distinfo)\n\n    metadata = (distinfo / \"METADATA\").read_text(encoding=\"utf-8\")\n    assert 'Author-email: \"John X. re\"' in metadata\n    assert \"   \" in metadata\n    assert \"UTF-8  \" in metadata\n\n\ndef test_licenses_default(dummy_dist, monkeypatch, tmp_path):\n    monkeypatch.chdir(dummy_dist)\n    bdist_wheel_cmd(bdist_dir=str(tmp_path)).run()\n    with ZipFile(\"dist/dummy_dist-1.0-py3-none-any.whl\") as wf:\n        license_files = {\n            \"dummy_dist-1.0.dist-info/\" + fname for fname in DEFAULT_LICENSE_FILES\n        }\n        assert set(wf.namelist()) == DEFAULT_FILES | license_files\n\n\ndef test_licenses_deprecated(dummy_dist, monkeypatch, tmp_path):\n    dummy_dist.joinpath(\"setup.cfg\").write_text(\n        \"[metadata]\\nlicense_file=licenses/DUMMYFILE\", encoding=\"utf-8\"\n    )\n    monkeypatch.chdir(dummy_dist)\n\n    bdist_wheel_cmd(bdist_dir=str(tmp_path)).run()\n\n    with ZipFile(\"dist/dummy_dist-1.0-py3-none-any.whl\") as wf:\n        license_files = {\"dummy_dist-1.0.dist-info/DUMMYFILE\"}\n        assert set(wf.namelist()) == DEFAULT_FILES | license_files\n\n\n@pytest.mark.parametrize(\n    (\"config_file\", \"config\"),\n    [\n        (\"setup.cfg\", \"[metadata]\\nlicense_files=licenses/*\\n  LICENSE\"),\n        (\"setup.cfg\", \"[metadata]\\nlicense_files=licenses/*, LICENSE\"),\n        (\n            \"setup.py\",\n            SETUPPY_EXAMPLE.replace(\n                \")\", \"  license_files=['licenses/DUMMYFILE', 'LICENSE'])\"\n            ),\n        ),\n    ],\n)\ndef test_licenses_override(dummy_dist, monkeypatch, tmp_path, config_file, config):\n    dummy_dist.joinpath(config_file).write_text(config, encoding=\"utf-8\")\n    monkeypatch.chdir(dummy_dist)\n    bdist_wheel_cmd(bdist_dir=str(tmp_path)).run()\n    with ZipFile(\"dist/dummy_dist-1.0-py3-none-any.whl\") as wf:\n        license_files = {\n            \"dummy_dist-1.0.dist-info/\" + fname for fname in {\"DUMMYFILE\", \"LICENSE\"}\n        }\n        assert set(wf.namelist()) == DEFAULT_FILES | license_files\n\n\ndef test_licenses_disabled(dummy_dist, monkeypatch, tmp_path):\n    dummy_dist.joinpath(\"setup.cfg\").write_text(\n        \"[metadata]\\nlicense_files=\\n\", encoding=\"utf-8\"\n    )\n    monkeypatch.chdir(dummy_dist)\n    bdist_wheel_cmd(bdist_dir=str(tmp_path)).run()\n    with ZipFile(\"dist/dummy_dist-1.0-py3-none-any.whl\") as wf:\n        assert set(wf.namelist()) == DEFAULT_FILES\n\n\ndef test_build_number(dummy_dist, monkeypatch, tmp_path):\n    monkeypatch.chdir(dummy_dist)\n    bdist_wheel_cmd(bdist_dir=str(tmp_path), build_number=\"2\").run()\n    with ZipFile(\"dist/dummy_dist-1.0-2-py3-none-any.whl\") as wf:\n        filenames = set(wf.namelist())\n        assert \"dummy_dist-1.0.dist-info/RECORD\" in filenames\n        assert \"dummy_dist-1.0.dist-info/METADATA\" in filenames\n\n\ndef test_universal_deprecated(dummy_dist, monkeypatch, tmp_path):\n    monkeypatch.chdir(dummy_dist)\n    with pytest.warns(SetuptoolsDeprecationWarning, match=\".*universal is deprecated\"):\n        bdist_wheel_cmd(bdist_dir=str(tmp_path), universal=True).run()\n\n    # For now we still respect the option\n    assert os.path.exists(\"dist/dummy_dist-1.0-py2.py3-none-any.whl\")\n\n\nEXTENSION_EXAMPLE = \"\"\"\\\n#include <Python.h>\n\nstatic PyMethodDef methods[] = {\n  { NULL, NULL, 0, NULL }\n};\n\nstatic struct PyModuleDef module_def = {\n  PyModuleDef_HEAD_INIT,\n  \"extension\",\n  \"Dummy extension module\",\n  -1,\n  methods\n};\n\nPyMODINIT_FUNC PyInit_extension(void) {\n  return PyModule_Create(&module_def);\n}\n\"\"\"\nEXTENSION_SETUPPY = \"\"\"\\\nfrom __future__ import annotations\n\nfrom setuptools import Extension, setup\n\nsetup(\n    name=\"extension.dist\",\n    version=\"0.1\",\n    description=\"A testing distribution \\N{SNOWMAN}\",\n    ext_modules=[Extension(name=\"extension\", sources=[\"extension.c\"])],\n)\n\"\"\"\n\n\n@pytest.mark.filterwarnings(\n    \"once:Config variable '.*' is unset.*, Python ABI tag may be incorrect\"\n)\ndef test_limited_abi(monkeypatch, tmp_path, tmp_path_factory):\n    \"\"\"Test that building a binary wheel with the limited ABI works.\"\"\"\n    source_dir = tmp_path_factory.mktemp(\"extension_dist\")\n    (source_dir / \"setup.py\").write_text(EXTENSION_SETUPPY, encoding=\"utf-8\")\n    (source_dir / \"extension.c\").write_text(EXTENSION_EXAMPLE, encoding=\"utf-8\")\n    build_dir = tmp_path.joinpath(\"build\")\n    dist_dir = tmp_path.joinpath(\"dist\")\n    monkeypatch.chdir(source_dir)\n    bdist_wheel_cmd(bdist_dir=str(build_dir), dist_dir=str(dist_dir)).run()\n\n\ndef test_build_from_readonly_tree(dummy_dist, monkeypatch, tmp_path):\n    basedir = str(tmp_path.joinpath(\"dummy\"))\n    shutil.copytree(str(dummy_dist), basedir)\n    monkeypatch.chdir(basedir)\n\n    # Make the tree read-only\n    for root, _dirs, files in os.walk(basedir):\n        for fname in files:\n            os.chmod(os.path.join(root, fname), stat.S_IREAD)\n\n    bdist_wheel_cmd().run()\n\n\n@pytest.mark.parametrize(\n    (\"option\", \"compress_type\"),\n    list(bdist_wheel.supported_compressions.items()),\n    ids=list(bdist_wheel.supported_compressions),\n)\ndef test_compression(dummy_dist, monkeypatch, tmp_path, option, compress_type):\n    monkeypatch.chdir(dummy_dist)\n    bdist_wheel_cmd(bdist_dir=str(tmp_path), compression=option).run()\n    with ZipFile(\"dist/dummy_dist-1.0-py3-none-any.whl\") as wf:\n        filenames = set(wf.namelist())\n        assert \"dummy_dist-1.0.dist-info/RECORD\" in filenames\n        assert \"dummy_dist-1.0.dist-info/METADATA\" in filenames\n        for zinfo in wf.filelist:\n            assert zinfo.compress_type == compress_type\n\n\ndef test_wheelfile_line_endings(wheel_paths):\n    for path in wheel_paths:\n        with ZipFile(path) as wf:\n            wheelfile = next(fn for fn in wf.filelist if fn.filename.endswith(\"WHEEL\"))\n            wheelfile_contents = wf.read(wheelfile)\n            assert b\"\\r\" not in wheelfile_contents\n\n\ndef test_unix_epoch_timestamps(dummy_dist, monkeypatch, tmp_path):\n    monkeypatch.setenv(\"SOURCE_DATE_EPOCH\", \"0\")\n    monkeypatch.chdir(dummy_dist)\n    bdist_wheel_cmd(bdist_dir=str(tmp_path), build_number=\"2a\").run()\n    with ZipFile(\"dist/dummy_dist-1.0-2a-py3-none-any.whl\") as wf:\n        for zinfo in wf.filelist:\n            assert zinfo.date_time >= (1980, 1, 1, 0, 0, 0)  # min epoch is used\n\n\ndef test_get_abi_tag_windows(monkeypatch):\n    monkeypatch.setattr(tags, \"interpreter_name\", lambda: \"cp\")\n    monkeypatch.setattr(sysconfig, \"get_config_var\", lambda x: \"cp313-win_amd64\")\n    assert get_abi_tag() == \"cp313\"\n    monkeypatch.setattr(sys, \"gettotalrefcount\", lambda: 1, False)\n    assert get_abi_tag() == \"cp313d\"\n    monkeypatch.setattr(sysconfig, \"get_config_var\", lambda x: \"cp313t-win_amd64\")\n    assert get_abi_tag() == \"cp313td\"\n    monkeypatch.delattr(sys, \"gettotalrefcount\")\n    assert get_abi_tag() == \"cp313t\"\n\n\ndef test_get_abi_tag_pypy_old(monkeypatch):\n    monkeypatch.setattr(tags, \"interpreter_name\", lambda: \"pp\")\n    monkeypatch.setattr(sysconfig, \"get_config_var\", lambda x: \"pypy36-pp73\")\n    assert get_abi_tag() == \"pypy36_pp73\"\n\n\ndef test_get_abi_tag_pypy_new(monkeypatch):\n    monkeypatch.setattr(sysconfig, \"get_config_var\", lambda x: \"pypy37-pp73-darwin\")\n    monkeypatch.setattr(tags, \"interpreter_name\", lambda: \"pp\")\n    assert get_abi_tag() == \"pypy37_pp73\"\n\n\ndef test_get_abi_tag_graalpy(monkeypatch):\n    monkeypatch.setattr(\n        sysconfig, \"get_config_var\", lambda x: \"graalpy231-310-native-x86_64-linux\"\n    )\n    monkeypatch.setattr(tags, \"interpreter_name\", lambda: \"graalpy\")\n    assert get_abi_tag() == \"graalpy231_310_native\"\n\n\ndef test_get_abi_tag_fallback(monkeypatch):\n    monkeypatch.setattr(sysconfig, \"get_config_var\", lambda x: \"unknown-python-310\")\n    monkeypatch.setattr(tags, \"interpreter_name\", lambda: \"unknown-python\")\n    assert get_abi_tag() == \"unknown_python_310\"\n\n\ndef test_platform_with_space(dummy_dist, monkeypatch):\n    \"\"\"Ensure building on platforms with a space in the name succeed.\"\"\"\n    monkeypatch.chdir(dummy_dist)\n    bdist_wheel_cmd(plat_name=\"isilon onefs\").run()\n\n\ndef test_data_dir_with_tag_build(monkeypatch, tmp_path):\n    \"\"\"\n    Setuptools allow authors to set PEP 440's local version segments\n    using ``egg_info.tag_build``. This should be reflected not only in the\n    ``.whl`` file name, but also in the ``.dist-info`` and ``.data`` dirs.\n    See pypa/setuptools#3997.\n    \"\"\"\n    monkeypatch.chdir(tmp_path)\n    files = {\n        \"setup.py\": \"\"\"\n            from setuptools import setup\n            setup(headers=[\"hello.h\"])\n            \"\"\",\n        \"setup.cfg\": \"\"\"\n            [metadata]\n            name = test\n            version = 1.0\n\n            [options.data_files]\n            hello/world = file.txt\n\n            [egg_info]\n            tag_build = +what\n            tag_date = 0\n            \"\"\",\n        \"file.txt\": \"\",\n        \"hello.h\": \"\",\n    }\n    for file, content in files.items():\n        with open(file, \"w\", encoding=\"utf-8\") as fh:\n            fh.write(cleandoc(content))\n\n    bdist_wheel_cmd().run()\n\n    # Ensure .whl, .dist-info and .data contain the local segment\n    wheel_path = \"dist/test-1.0+what-py3-none-any.whl\"\n    assert os.path.exists(wheel_path)\n    entries = set(ZipFile(wheel_path).namelist())\n    for expected in (\n        \"test-1.0+what.data/headers/hello.h\",\n        \"test-1.0+what.data/data/hello/world/file.txt\",\n        \"test-1.0+what.dist-info/METADATA\",\n        \"test-1.0+what.dist-info/WHEEL\",\n    ):\n        assert expected in entries\n\n    for not_expected in (\n        \"test.data/headers/hello.h\",\n        \"test-1.0.data/data/hello/world/file.txt\",\n        \"test.dist-info/METADATA\",\n        \"test-1.0.dist-info/WHEEL\",\n    ):\n        assert not_expected not in entries\n\n\n@pytest.mark.parametrize(\n    (\"reported\", \"expected\"),\n    [(\"linux-x86_64\", \"linux_i686\"), (\"linux-aarch64\", \"linux_armv7l\")],\n)\n@pytest.mark.skipif(\n    platform.system() != \"Linux\", reason=\"Only makes sense to test on Linux\"\n)\ndef test_platform_linux32(reported, expected, monkeypatch):\n    monkeypatch.setattr(struct, \"calcsize\", lambda x: 4)\n    dist = setuptools.Distribution()\n    cmd = bdist_wheel(dist)\n    cmd.plat_name = reported\n    cmd.root_is_pure = False\n    _, _, actual = cmd.get_tag()\n    assert actual == expected\n\n\ndef test_no_ctypes(monkeypatch) -> None:\n    def _fake_import(name: str, *args, **kwargs):\n        if name == \"ctypes\":\n            raise ModuleNotFoundError(f\"No module named {name}\")\n\n        return importlib.__import__(name, *args, **kwargs)\n\n    with suppress(KeyError):\n        monkeypatch.delitem(sys.modules, \"wheel.macosx_libfile\")\n\n    # Install an importer shim that refuses to load ctypes\n    monkeypatch.setattr(builtins, \"__import__\", _fake_import)\n    with pytest.raises(ModuleNotFoundError, match=\"No module named ctypes\"):\n        import wheel.macosx_libfile  # noqa: F401\n\n    # Unload and reimport the bdist_wheel command module to make sure it won't try to\n    # import ctypes\n    monkeypatch.delitem(sys.modules, \"setuptools.command.bdist_wheel\")\n\n    import setuptools.command.bdist_wheel  # noqa: F401\n\n\ndef test_dist_info_provided(dummy_dist, monkeypatch, tmp_path):\n    monkeypatch.chdir(dummy_dist)\n    distinfo = tmp_path / \"dummy_dist.dist-info\"\n\n    distinfo.mkdir()\n    (distinfo / \"METADATA\").write_text(\"name: helloworld\", encoding=\"utf-8\")\n\n    # We don't control the metadata. According to PEP-517, \"The hook MAY also\n    # create other files inside this directory, and a build frontend MUST\n    # preserve\".\n    (distinfo / \"FOO\").write_text(\"bar\", encoding=\"utf-8\")\n\n    bdist_wheel_cmd(bdist_dir=str(tmp_path), dist_info_dir=str(distinfo)).run()\n    expected = {\n        \"dummy_dist-1.0.dist-info/FOO\",\n        \"dummy_dist-1.0.dist-info/RECORD\",\n    }\n    with ZipFile(\"dist/dummy_dist-1.0-py3-none-any.whl\") as wf:\n        files_found = set(wf.namelist())\n    # Check that all expected files are there.\n    assert expected - files_found == set()\n    # Make sure there is no accidental egg-info bleeding into the wheel.\n    assert not [path for path in files_found if 'egg-info' in str(path)]\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/tests/test_build.py","size":798,"sha1":"ad86e006caf2b1e2604869e6200162ab4050a0a1","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"from setuptools import Command\nfrom setuptools.command.build import build\nfrom setuptools.dist import Distribution\n\n\ndef test_distribution_gives_setuptools_build_obj(tmpdir_cwd):\n    \"\"\"\n    Check that the setuptools Distribution uses the\n    setuptools specific build object.\n    \"\"\"\n\n    dist = Distribution(\n        dict(\n            script_name='setup.py',\n            script_args=['build'],\n            packages=[],\n            package_data={'': ['path/*']},\n        )\n    )\n    assert isinstance(dist.get_command_obj(\"build\"), build)\n\n\nclass Subcommand(Command):\n    \"\"\"Dummy command to be used in tests\"\"\"\n\n    def initialize_options(self):\n        pass\n\n    def finalize_options(self):\n        pass\n\n    def run(self):\n        raise NotImplementedError(\"just to check if the command runs\")\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/tests/test_build_clib.py","size":3123,"sha1":"f2a92dbec825d2cd2b9d8d951de6ee1b21dfa64d","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import random\nfrom unittest import mock\n\nimport pytest\n\nfrom setuptools.command.build_clib import build_clib\nfrom setuptools.dist import Distribution\n\nfrom distutils.errors import DistutilsSetupError\n\n\nclass TestBuildCLib:\n    @mock.patch('setuptools.command.build_clib.newer_pairwise_group')\n    def test_build_libraries(self, mock_newer):\n        dist = Distribution()\n        cmd = build_clib(dist)\n\n        # this will be a long section, just making sure all\n        # exceptions are properly raised\n        libs = [('example', {'sources': 'broken.c'})]\n        with pytest.raises(DistutilsSetupError):\n            cmd.build_libraries(libs)\n\n        obj_deps = 'some_string'\n        libs = [('example', {'sources': ['source.c'], 'obj_deps': obj_deps})]\n        with pytest.raises(DistutilsSetupError):\n            cmd.build_libraries(libs)\n\n        obj_deps = {'': ''}\n        libs = [('example', {'sources': ['source.c'], 'obj_deps': obj_deps})]\n        with pytest.raises(DistutilsSetupError):\n            cmd.build_libraries(libs)\n\n        obj_deps = {'source.c': ''}\n        libs = [('example', {'sources': ['source.c'], 'obj_deps': obj_deps})]\n        with pytest.raises(DistutilsSetupError):\n            cmd.build_libraries(libs)\n\n        # with that out of the way, let's see if the crude dependency\n        # system works\n        cmd.compiler = mock.MagicMock(spec=cmd.compiler)\n        mock_newer.return_value = ([], [])\n\n        obj_deps = {'': ('global.h',), 'example.c': ('example.h',)}\n        libs = [('example', {'sources': ['example.c'], 'obj_deps': obj_deps})]\n\n        cmd.build_libraries(libs)\n        assert [['example.c', 'global.h', 'example.h']] in mock_newer.call_args[0]\n        assert not cmd.compiler.compile.called\n        assert cmd.compiler.create_static_lib.call_count == 1\n\n        # reset the call numbers so we can test again\n        cmd.compiler.reset_mock()\n\n        mock_newer.return_value = ''  # anything as long as it's not ([],[])\n        cmd.build_libraries(libs)\n        assert cmd.compiler.compile.call_count == 1\n        assert cmd.compiler.create_static_lib.call_count == 1\n\n    @mock.patch('setuptools.command.build_clib.newer_pairwise_group')\n    def test_build_libraries_reproducible(self, mock_newer):\n        dist = Distribution()\n        cmd = build_clib(dist)\n\n        # with that out of the way, let's see if the crude dependency\n        # system works\n        cmd.compiler = mock.MagicMock(spec=cmd.compiler)\n        mock_newer.return_value = ([], [])\n\n        original_sources = ['a-example.c', 'example.c']\n        sources = original_sources\n\n        obj_deps = {'': ('global.h',), 'example.c': ('example.h',)}\n        libs = [('example', {'sources': sources, 'obj_deps': obj_deps})]\n\n        cmd.build_libraries(libs)\n        computed_call_args = mock_newer.call_args[0]\n\n        while sources == original_sources:\n            sources = random.sample(original_sources, len(original_sources))\n        libs = [('example', {'sources': sources, 'obj_deps': obj_deps})]\n\n        cmd.build_libraries(libs)\n        assert computed_call_args == mock_newer.call_args[0]\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/tests/test_build_ext.py","size":10099,"sha1":"249228c9040948ee3b24d3a162cb8d86b4a362dc","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"from __future__ import annotations\n\nimport os\nimport sys\nfrom importlib.util import cache_from_source as _compiled_file_name\n\nimport pytest\nfrom jaraco import path\n\nfrom setuptools.command.build_ext import build_ext, get_abi3_suffix\nfrom setuptools.dist import Distribution\nfrom setuptools.errors import CompileError\nfrom setuptools.extension import Extension\n\nfrom . import environment\nfrom .textwrap import DALS\n\nimport distutils.command.build_ext as orig\nfrom distutils.sysconfig import get_config_var\n\nIS_PYPY = '__pypy__' in sys.builtin_module_names\n\n\nclass TestBuildExt:\n    def test_get_ext_filename(self):\n        \"\"\"\n        Setuptools needs to give back the same\n        result as distutils, even if the fullname\n        is not in ext_map.\n        \"\"\"\n        dist = Distribution()\n        cmd = build_ext(dist)\n        cmd.ext_map['foo/bar'] = ''\n        res = cmd.get_ext_filename('foo')\n        wanted = orig.build_ext.get_ext_filename(cmd, 'foo')\n        assert res == wanted\n\n    def test_abi3_filename(self):\n        \"\"\"\n        Filename needs to be loadable by several versions\n        of Python 3 if 'is_abi3' is truthy on Extension()\n        \"\"\"\n        print(get_abi3_suffix())\n\n        extension = Extension('spam.eggs', ['eggs.c'], py_limited_api=True)\n        dist = Distribution(dict(ext_modules=[extension]))\n        cmd = build_ext(dist)\n        cmd.finalize_options()\n        assert 'spam.eggs' in cmd.ext_map\n        res = cmd.get_ext_filename('spam.eggs')\n\n        if not get_abi3_suffix():\n            assert res.endswith(get_config_var('EXT_SUFFIX'))\n        elif sys.platform == 'win32':\n            assert res.endswith('eggs.pyd')\n        else:\n            assert 'abi3' in res\n\n    def test_ext_suffix_override(self):\n        \"\"\"\n        SETUPTOOLS_EXT_SUFFIX variable always overrides\n        default extension options.\n        \"\"\"\n        dist = Distribution()\n        cmd = build_ext(dist)\n        cmd.ext_map['for_abi3'] = ext = Extension(\n            'for_abi3',\n            ['s.c'],\n            # Override shouldn't affect abi3 modules\n            py_limited_api=True,\n        )\n        # Mock value needed to pass tests\n        ext._links_to_dynamic = False\n\n        if not IS_PYPY:\n            expect = cmd.get_ext_filename('for_abi3')\n        else:\n            # PyPy builds do not use ABI3 tag, so they will\n            # also get the overridden suffix.\n            expect = 'for_abi3.test-suffix'\n\n        try:\n            os.environ['SETUPTOOLS_EXT_SUFFIX'] = '.test-suffix'\n            res = cmd.get_ext_filename('normal')\n            assert 'normal.test-suffix' == res\n            res = cmd.get_ext_filename('for_abi3')\n            assert expect == res\n        finally:\n            del os.environ['SETUPTOOLS_EXT_SUFFIX']\n\n    def dist_with_example(self):\n        files = {\n            \"src\": {\"mypkg\": {\"subpkg\": {\"ext2.c\": \"\"}}},\n            \"c-extensions\": {\"ext1\": {\"main.c\": \"\"}},\n        }\n\n        ext1 = Extension(\"mypkg.ext1\", [\"c-extensions/ext1/main.c\"])\n        ext2 = Extension(\"mypkg.subpkg.ext2\", [\"src/mypkg/subpkg/ext2.c\"])\n        ext3 = Extension(\"ext3\", [\"c-extension/ext3.c\"])\n\n        path.build(files)\n        return Distribution({\n            \"script_name\": \"%test%\",\n            \"ext_modules\": [ext1, ext2, ext3],\n            \"package_dir\": {\"\": \"src\"},\n        })\n\n    def test_get_outputs(self, tmpdir_cwd, monkeypatch):\n        monkeypatch.setenv('SETUPTOOLS_EXT_SUFFIX', '.mp3')  # make test OS-independent\n        monkeypatch.setattr('setuptools.command.build_ext.use_stubs', False)\n        dist = self.dist_with_example()\n\n        # Regular build: get_outputs not empty, but get_output_mappings is empty\n        build_ext = dist.get_command_obj(\"build_ext\")\n        build_ext.editable_mode = False\n        build_ext.ensure_finalized()\n        build_lib = build_ext.build_lib.replace(os.sep, \"/\")\n        outputs = [x.replace(os.sep, \"/\") for x in build_ext.get_outputs()]\n        assert outputs == [\n            f\"{build_lib}/ext3.mp3\",\n            f\"{build_lib}/mypkg/ext1.mp3\",\n            f\"{build_lib}/mypkg/subpkg/ext2.mp3\",\n        ]\n        assert build_ext.get_output_mapping() == {}\n\n        # Editable build: get_output_mappings should contain everything in get_outputs\n        dist.reinitialize_command(\"build_ext\")\n        build_ext.editable_mode = True\n        build_ext.ensure_finalized()\n        mapping = {\n            k.replace(os.sep, \"/\"): v.replace(os.sep, \"/\")\n            for k, v in build_ext.get_output_mapping().items()\n        }\n        assert mapping == {\n            f\"{build_lib}/ext3.mp3\": \"src/ext3.mp3\",\n            f\"{build_lib}/mypkg/ext1.mp3\": \"src/mypkg/ext1.mp3\",\n            f\"{build_lib}/mypkg/subpkg/ext2.mp3\": \"src/mypkg/subpkg/ext2.mp3\",\n        }\n\n    def test_get_output_mapping_with_stub(self, tmpdir_cwd, monkeypatch):\n        monkeypatch.setenv('SETUPTOOLS_EXT_SUFFIX', '.mp3')  # make test OS-independent\n        monkeypatch.setattr('setuptools.command.build_ext.use_stubs', True)\n        dist = self.dist_with_example()\n\n        # Editable build should create compiled stubs (.pyc files only, no .py)\n        build_ext = dist.get_command_obj(\"build_ext\")\n        build_ext.editable_mode = True\n        build_ext.ensure_finalized()\n        for ext in build_ext.extensions:\n            monkeypatch.setattr(ext, \"_needs_stub\", True)\n\n        build_lib = build_ext.build_lib.replace(os.sep, \"/\")\n        mapping = {\n            k.replace(os.sep, \"/\"): v.replace(os.sep, \"/\")\n            for k, v in build_ext.get_output_mapping().items()\n        }\n\n        def C(file):\n            \"\"\"Make it possible to do comparisons and tests in a OS-independent way\"\"\"\n            return _compiled_file_name(file).replace(os.sep, \"/\")\n\n        assert mapping == {\n            C(f\"{build_lib}/ext3.py\"): C(\"src/ext3.py\"),\n            f\"{build_lib}/ext3.mp3\": \"src/ext3.mp3\",\n            C(f\"{build_lib}/mypkg/ext1.py\"): C(\"src/mypkg/ext1.py\"),\n            f\"{build_lib}/mypkg/ext1.mp3\": \"src/mypkg/ext1.mp3\",\n            C(f\"{build_lib}/mypkg/subpkg/ext2.py\"): C(\"src/mypkg/subpkg/ext2.py\"),\n            f\"{build_lib}/mypkg/subpkg/ext2.mp3\": \"src/mypkg/subpkg/ext2.mp3\",\n        }\n\n        # Ensure only the compiled stubs are present not the raw .py stub\n        assert f\"{build_lib}/mypkg/ext1.py\" not in mapping\n        assert f\"{build_lib}/mypkg/subpkg/ext2.py\" not in mapping\n\n        # Visualize what the cached stub files look like\n        example_stub = C(f\"{build_lib}/mypkg/ext1.py\")\n        assert example_stub in mapping\n        assert example_stub.startswith(f\"{build_lib}/mypkg/__pycache__/ext1\")\n        assert example_stub.endswith(\".pyc\")\n\n\nclass TestBuildExtInplace:\n    def get_build_ext_cmd(self, optional: bool, **opts) -> build_ext:\n        files: dict[str, str | dict[str, dict[str, str]]] = {\n            \"eggs.c\": \"#include missingheader.h\\n\",\n            \".build\": {\"lib\": {}, \"tmp\": {}},\n        }\n        path.build(files)\n        extension = Extension('spam.eggs', ['eggs.c'], optional=optional)\n        dist = Distribution(dict(ext_modules=[extension]))\n        dist.script_name = 'setup.py'\n        cmd = build_ext(dist)\n        vars(cmd).update(build_lib=\".build/lib\", build_temp=\".build/tmp\", **opts)\n        cmd.ensure_finalized()\n        return cmd\n\n    def get_log_messages(self, caplog, capsys):\n        \"\"\"\n        Historically, distutils \"logged\" by printing to sys.std*.\n        Later versions adopted the logging framework. Grab\n        messages regardless of how they were captured.\n        \"\"\"\n        std = capsys.readouterr()\n        return std.out.splitlines() + std.err.splitlines() + caplog.messages\n\n    def test_optional(self, tmpdir_cwd, caplog, capsys):\n        \"\"\"\n        If optional extensions fail to build, setuptools should show the error\n        in the logs but not fail to build\n        \"\"\"\n        cmd = self.get_build_ext_cmd(optional=True, inplace=True)\n        cmd.run()\n        assert any(\n            'build_ext: building extension \"spam.eggs\" failed'\n            for msg in self.get_log_messages(caplog, capsys)\n        )\n        # No compile error exception should be raised\n\n    def test_non_optional(self, tmpdir_cwd):\n        # Non-optional extensions should raise an exception\n        cmd = self.get_build_ext_cmd(optional=False, inplace=True)\n        with pytest.raises(CompileError):\n            cmd.run()\n\n\ndef test_build_ext_config_handling(tmpdir_cwd):\n    files = {\n        'setup.py': DALS(\n            \"\"\"\n            from setuptools import Extension, setup\n            setup(\n                name='foo',\n                version='0.0.0',\n                ext_modules=[Extension('foo', ['foo.c'])],\n            )\n            \"\"\"\n        ),\n        'foo.c': DALS(\n            \"\"\"\n            #include \"Python.h\"\n\n            #if PY_MAJOR_VERSION >= 3\n\n            static struct PyModuleDef moduledef = {\n                    PyModuleDef_HEAD_INIT,\n                    \"foo\",\n                    NULL,\n                    0,\n                    NULL,\n                    NULL,\n                    NULL,\n                    NULL,\n                    NULL\n            };\n\n            #define INITERROR return NULL\n\n            PyMODINIT_FUNC PyInit_foo(void)\n\n            #else\n\n            #define INITERROR return\n\n            void initfoo(void)\n\n            #endif\n            {\n            #if PY_MAJOR_VERSION >= 3\n                PyObject *module = PyModule_Create(&moduledef);\n            #else\n                PyObject *module = Py_InitModule(\"extension\", NULL);\n            #endif\n                if (module == NULL)\n                    INITERROR;\n            #if PY_MAJOR_VERSION >= 3\n                return module;\n            #endif\n            }\n            \"\"\"\n        ),\n        'setup.cfg': DALS(\n            \"\"\"\n            [build]\n            build_base = foo_build\n            \"\"\"\n        ),\n    }\n    path.build(files)\n    code, (stdout, stderr) = environment.run_setup_py(\n        cmd=['build'],\n        data_stream=(0, 2),\n    )\n    assert code == 0, f'\\nSTDOUT:\\n{stdout}\\nSTDERR:\\n{stderr}'\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/tests/test_build_meta.py","size":33574,"sha1":"eeca6e4fe42c5dbc798c8ea9fb7dcbbce996dbab","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import contextlib\nimport importlib\nimport os\nimport re\nimport shutil\nimport signal\nimport sys\nimport tarfile\nfrom concurrent import futures\nfrom pathlib import Path\nfrom typing import Any, Callable\nfrom zipfile import ZipFile\n\nimport pytest\nfrom jaraco import path\nfrom packaging.requirements import Requirement\n\nfrom .textwrap import DALS\n\nSETUP_SCRIPT_STUB = \"__import__('setuptools').setup()\"\n\n\nTIMEOUT = int(os.getenv(\"TIMEOUT_BACKEND_TEST\", \"180\"))  # in seconds\nIS_PYPY = '__pypy__' in sys.builtin_module_names\n\n\npytestmark = pytest.mark.skipif(\n    sys.platform == \"win32\" and IS_PYPY,\n    reason=\"The combination of PyPy + Windows + pytest-xdist + ProcessPoolExecutor \"\n    \"is flaky and problematic\",\n)\n\n\nclass BuildBackendBase:\n    def __init__(self, cwd='.', env=None, backend_name='setuptools.build_meta'):\n        self.cwd = cwd\n        self.env = env or {}\n        self.backend_name = backend_name\n\n\nclass BuildBackend(BuildBackendBase):\n    \"\"\"PEP 517 Build Backend\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.pool = futures.ProcessPoolExecutor(max_workers=1)\n\n    def __getattr__(self, name: str) -> Callable[..., Any]:\n        \"\"\"Handles arbitrary function invocations on the build backend.\"\"\"\n\n        def method(*args, **kw):\n            root = os.path.abspath(self.cwd)\n            caller = BuildBackendCaller(root, self.env, self.backend_name)\n            pid = None\n            try:\n                pid = self.pool.submit(os.getpid).result(TIMEOUT)\n                return self.pool.submit(caller, name, *args, **kw).result(TIMEOUT)\n            except futures.TimeoutError:\n                self.pool.shutdown(wait=False)  # doesn't stop already running processes\n                self._kill(pid)\n                pytest.xfail(f\"Backend did not respond before timeout ({TIMEOUT} s)\")\n            except (futures.process.BrokenProcessPool, MemoryError, OSError):\n                if IS_PYPY:\n                    pytest.xfail(\"PyPy frequently fails tests with ProcessPoolExector\")\n                raise\n\n        return method\n\n    def _kill(self, pid):\n        if pid is None:\n            return\n        with contextlib.suppress(ProcessLookupError, OSError):\n            os.kill(pid, signal.SIGTERM if os.name == \"nt\" else signal.SIGKILL)\n\n\nclass BuildBackendCaller(BuildBackendBase):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        (self.backend_name, _, self.backend_obj) = self.backend_name.partition(':')\n\n    def __call__(self, name, *args, **kw):\n        \"\"\"Handles arbitrary function invocations on the build backend.\"\"\"\n        os.chdir(self.cwd)\n        os.environ.update(self.env)\n        mod = importlib.import_module(self.backend_name)\n\n        if self.backend_obj:\n            backend = getattr(mod, self.backend_obj)\n        else:\n            backend = mod\n\n        return getattr(backend, name)(*args, **kw)\n\n\ndefns = [\n    {  # simple setup.py script\n        'setup.py': DALS(\n            \"\"\"\n            __import__('setuptools').setup(\n                name='foo',\n                version='0.0.0',\n                py_modules=['hello'],\n                setup_requires=['six'],\n            )\n            \"\"\"\n        ),\n        'hello.py': DALS(\n            \"\"\"\n            def run():\n                print('hello')\n            \"\"\"\n        ),\n    },\n    {  # setup.py that relies on __name__\n        'setup.py': DALS(\n            \"\"\"\n            assert __name__ == '__main__'\n            __import__('setuptools').setup(\n                name='foo',\n                version='0.0.0',\n                py_modules=['hello'],\n                setup_requires=['six'],\n            )\n            \"\"\"\n        ),\n        'hello.py': DALS(\n            \"\"\"\n            def run():\n                print('hello')\n            \"\"\"\n        ),\n    },\n    {  # setup.py script that runs arbitrary code\n        'setup.py': DALS(\n            \"\"\"\n            variable = True\n            def function():\n                return variable\n            assert variable\n            __import__('setuptools').setup(\n                name='foo',\n                version='0.0.0',\n                py_modules=['hello'],\n                setup_requires=['six'],\n            )\n            \"\"\"\n        ),\n        'hello.py': DALS(\n            \"\"\"\n            def run():\n                print('hello')\n            \"\"\"\n        ),\n    },\n    {  # setup.py script that constructs temp files to be included in the distribution\n        'setup.py': DALS(\n            \"\"\"\n            # Some packages construct files on the fly, include them in the package,\n            # and immediately remove them after `setup()` (e.g. pybind11==2.9.1).\n            # Therefore, we cannot use `distutils.core.run_setup(..., stop_after=...)`\n            # to obtain a distribution object first, and then run the distutils\n            # commands later, because these files will be removed in the meantime.\n\n            with open('world.py', 'w', encoding=\"utf-8\") as f:\n                f.write('x = 42')\n\n            try:\n                __import__('setuptools').setup(\n                    name='foo',\n                    version='0.0.0',\n                    py_modules=['world'],\n                    setup_requires=['six'],\n                )\n            finally:\n                # Some packages will clean temporary files\n                __import__('os').unlink('world.py')\n            \"\"\"\n        ),\n    },\n    {  # setup.cfg only\n        'setup.cfg': DALS(\n            \"\"\"\n        [metadata]\n        name = foo\n        version = 0.0.0\n\n        [options]\n        py_modules=hello\n        setup_requires=six\n        \"\"\"\n        ),\n        'hello.py': DALS(\n            \"\"\"\n        def run():\n            print('hello')\n        \"\"\"\n        ),\n    },\n    {  # setup.cfg and setup.py\n        'setup.cfg': DALS(\n            \"\"\"\n        [metadata]\n        name = foo\n        version = 0.0.0\n\n        [options]\n        py_modules=hello\n        setup_requires=six\n        \"\"\"\n        ),\n        'setup.py': \"__import__('setuptools').setup()\",\n        'hello.py': DALS(\n            \"\"\"\n        def run():\n            print('hello')\n        \"\"\"\n        ),\n    },\n]\n\n\nclass TestBuildMetaBackend:\n    backend_name = 'setuptools.build_meta'\n\n    def get_build_backend(self):\n        return BuildBackend(backend_name=self.backend_name)\n\n    @pytest.fixture(params=defns)\n    def build_backend(self, tmpdir, request):\n        path.build(request.param, prefix=str(tmpdir))\n        with tmpdir.as_cwd():\n            yield self.get_build_backend()\n\n    def test_get_requires_for_build_wheel(self, build_backend):\n        actual = build_backend.get_requires_for_build_wheel()\n        expected = ['six']\n        assert sorted(actual) == sorted(expected)\n\n    def test_get_requires_for_build_sdist(self, build_backend):\n        actual = build_backend.get_requires_for_build_sdist()\n        expected = ['six']\n        assert sorted(actual) == sorted(expected)\n\n    def test_build_wheel(self, build_backend):\n        dist_dir = os.path.abspath('pip-wheel')\n        os.makedirs(dist_dir)\n        wheel_name = build_backend.build_wheel(dist_dir)\n\n        wheel_file = os.path.join(dist_dir, wheel_name)\n        assert os.path.isfile(wheel_file)\n\n        # Temporary files should be removed\n        assert not os.path.isfile('world.py')\n\n        with ZipFile(wheel_file) as zipfile:\n            wheel_contents = set(zipfile.namelist())\n\n        # Each one of the examples have a single module\n        # that should be included in the distribution\n        python_scripts = (f for f in wheel_contents if f.endswith('.py'))\n        modules = [f for f in python_scripts if not f.endswith('setup.py')]\n        assert len(modules) == 1\n\n    @pytest.mark.parametrize('build_type', ('wheel', 'sdist'))\n    def test_build_with_existing_file_present(self, build_type, tmpdir_cwd):\n        # Building a sdist/wheel should still succeed if there's\n        # already a sdist/wheel in the destination directory.\n        files = {\n            'setup.py': \"from setuptools import setup\\nsetup()\",\n            'VERSION': \"0.0.1\",\n            'setup.cfg': DALS(\n                \"\"\"\n                [metadata]\n                name = foo\n                version = file: VERSION\n                \"\"\"\n            ),\n            'pyproject.toml': DALS(\n                \"\"\"\n                [build-system]\n                requires = [\"setuptools\", \"wheel\"]\n                build-backend = \"setuptools.build_meta\"\n                \"\"\"\n            ),\n        }\n\n        path.build(files)\n\n        dist_dir = os.path.abspath('preexisting-' + build_type)\n\n        build_backend = self.get_build_backend()\n        build_method = getattr(build_backend, 'build_' + build_type)\n\n        # Build a first sdist/wheel.\n        # Note: this also check the destination directory is\n        # successfully created if it does not exist already.\n        first_result = build_method(dist_dir)\n\n        # Change version.\n        with open(\"VERSION\", \"wt\", encoding=\"utf-8\") as version_file:\n            version_file.write(\"0.0.2\")\n\n        # Build a *second* sdist/wheel.\n        second_result = build_method(dist_dir)\n\n        assert os.path.isfile(os.path.join(dist_dir, first_result))\n        assert first_result != second_result\n\n        # And if rebuilding the exact same sdist/wheel?\n        open(os.path.join(dist_dir, second_result), 'wb').close()\n        third_result = build_method(dist_dir)\n        assert third_result == second_result\n        assert os.path.getsize(os.path.join(dist_dir, third_result)) > 0\n\n    @pytest.mark.parametrize(\"setup_script\", [None, SETUP_SCRIPT_STUB])\n    def test_build_with_pyproject_config(self, tmpdir, setup_script):\n        files = {\n            'pyproject.toml': DALS(\n                \"\"\"\n                [build-system]\n                requires = [\"setuptools\", \"wheel\"]\n                build-backend = \"setuptools.build_meta\"\n\n                [project]\n                name = \"foo\"\n                license = {text = \"MIT\"}\n                description = \"This is a Python package\"\n                dynamic = [\"version\", \"readme\"]\n                classifiers = [\n                    \"Development Status :: 5 - Production/Stable\",\n                    \"Intended Audience :: Developers\"\n                ]\n                urls = {Homepage = \"http://github.com\"}\n                dependencies = [\n                    \"appdirs\",\n                ]\n\n                [project.optional-dependencies]\n                all = [\n                    \"tomli>=1\",\n                    \"pyscaffold>=4,<5\",\n                    'importlib; python_version == \"2.6\"',\n                ]\n\n                [project.scripts]\n                foo = \"foo.cli:main\"\n\n                [tool.setuptools]\n                zip-safe = false\n                package-dir = {\"\" = \"src\"}\n                packages = {find = {where = [\"src\"]}}\n                license-files = [\"LICENSE*\"]\n\n                [tool.setuptools.dynamic]\n                version = {attr = \"foo.__version__\"}\n                readme = {file = \"README.rst\"}\n\n                [tool.distutils.sdist]\n                formats = \"gztar\"\n                \"\"\"\n            ),\n            \"MANIFEST.in\": DALS(\n                \"\"\"\n                global-include *.py *.txt\n                global-exclude *.py[cod]\n                \"\"\"\n            ),\n            \"README.rst\": \"This is a ``README``\",\n            \"LICENSE.txt\": \"---- placeholder MIT license ----\",\n            \"src\": {\n                \"foo\": {\n                    \"__init__.py\": \"__version__ = '0.1'\",\n                    \"__init__.pyi\": \"__version__: str\",\n                    \"cli.py\": \"def main(): print('hello world')\",\n                    \"data.txt\": \"def main(): print('hello world')\",\n                    \"py.typed\": \"\",\n                }\n            },\n        }\n        if setup_script:\n            files[\"setup.py\"] = setup_script\n\n        build_backend = self.get_build_backend()\n        with tmpdir.as_cwd():\n            path.build(files)\n            sdist_path = build_backend.build_sdist(\"temp\")\n            wheel_file = build_backend.build_wheel(\"temp\")\n\n        with tarfile.open(os.path.join(tmpdir, \"temp\", sdist_path)) as tar:\n            sdist_contents = set(tar.getnames())\n\n        with ZipFile(os.path.join(tmpdir, \"temp\", wheel_file)) as zipfile:\n            wheel_contents = set(zipfile.namelist())\n            metadata = str(zipfile.read(\"foo-0.1.dist-info/METADATA\"), \"utf-8\")\n            license = str(zipfile.read(\"foo-0.1.dist-info/LICENSE.txt\"), \"utf-8\")\n            epoints = str(zipfile.read(\"foo-0.1.dist-info/entry_points.txt\"), \"utf-8\")\n\n        assert sdist_contents - {\"foo-0.1/setup.py\"} == {\n            'foo-0.1',\n            'foo-0.1/LICENSE.txt',\n            'foo-0.1/MANIFEST.in',\n            'foo-0.1/PKG-INFO',\n            'foo-0.1/README.rst',\n            'foo-0.1/pyproject.toml',\n            'foo-0.1/setup.cfg',\n            'foo-0.1/src',\n            'foo-0.1/src/foo',\n            'foo-0.1/src/foo/__init__.py',\n            'foo-0.1/src/foo/__init__.pyi',\n            'foo-0.1/src/foo/cli.py',\n            'foo-0.1/src/foo/data.txt',\n            'foo-0.1/src/foo/py.typed',\n            'foo-0.1/src/foo.egg-info',\n            'foo-0.1/src/foo.egg-info/PKG-INFO',\n            'foo-0.1/src/foo.egg-info/SOURCES.txt',\n            'foo-0.1/src/foo.egg-info/dependency_links.txt',\n            'foo-0.1/src/foo.egg-info/entry_points.txt',\n            'foo-0.1/src/foo.egg-info/requires.txt',\n            'foo-0.1/src/foo.egg-info/top_level.txt',\n            'foo-0.1/src/foo.egg-info/not-zip-safe',\n        }\n        assert wheel_contents == {\n            \"foo/__init__.py\",\n            \"foo/__init__.pyi\",  # include type information by default\n            \"foo/cli.py\",\n            \"foo/data.txt\",  # include_package_data defaults to True\n            \"foo/py.typed\",  # include type information by default\n            \"foo-0.1.dist-info/LICENSE.txt\",\n            \"foo-0.1.dist-info/METADATA\",\n            \"foo-0.1.dist-info/WHEEL\",\n            \"foo-0.1.dist-info/entry_points.txt\",\n            \"foo-0.1.dist-info/top_level.txt\",\n            \"foo-0.1.dist-info/RECORD\",\n        }\n        assert license == \"---- placeholder MIT license ----\"\n\n        for line in (\n            \"Summary: This is a Python package\",\n            \"License: MIT\",\n            \"Classifier: Intended Audience :: Developers\",\n            \"Requires-Dist: appdirs\",\n            \"Requires-Dist: \" + str(Requirement('tomli>=1 ; extra == \"all\"')),\n            \"Requires-Dist: \"\n            + str(Requirement('importlib; python_version==\"2.6\" and extra ==\"all\"')),\n        ):\n            assert line in metadata, (line, metadata)\n\n        assert metadata.strip().endswith(\"This is a ``README``\")\n        assert epoints.strip() == \"[console_scripts]\\nfoo = foo.cli:main\"\n\n    def test_static_metadata_in_pyproject_config(self, tmpdir):\n        # Make sure static metadata in pyproject.toml is not overwritten by setup.py\n        # as required by PEP 621\n        files = {\n            'pyproject.toml': DALS(\n                \"\"\"\n                [build-system]\n                requires = [\"setuptools\", \"wheel\"]\n                build-backend = \"setuptools.build_meta\"\n\n                [project]\n                name = \"foo\"\n                description = \"This is a Python package\"\n                version = \"42\"\n                dependencies = [\"six\"]\n                \"\"\"\n            ),\n            'hello.py': DALS(\n                \"\"\"\n                def run():\n                    print('hello')\n                \"\"\"\n            ),\n            'setup.py': DALS(\n                \"\"\"\n                __import__('setuptools').setup(\n                    name='bar',\n                    version='13',\n                )\n                \"\"\"\n            ),\n        }\n        build_backend = self.get_build_backend()\n        with tmpdir.as_cwd():\n            path.build(files)\n            sdist_path = build_backend.build_sdist(\"temp\")\n            wheel_file = build_backend.build_wheel(\"temp\")\n\n        assert (tmpdir / \"temp/foo-42.tar.gz\").exists()\n        assert (tmpdir / \"temp/foo-42-py3-none-any.whl\").exists()\n        assert not (tmpdir / \"temp/bar-13.tar.gz\").exists()\n        assert not (tmpdir / \"temp/bar-42.tar.gz\").exists()\n        assert not (tmpdir / \"temp/foo-13.tar.gz\").exists()\n        assert not (tmpdir / \"temp/bar-13-py3-none-any.whl\").exists()\n        assert not (tmpdir / \"temp/bar-42-py3-none-any.whl\").exists()\n        assert not (tmpdir / \"temp/foo-13-py3-none-any.whl\").exists()\n\n        with tarfile.open(os.path.join(tmpdir, \"temp\", sdist_path)) as tar:\n            pkg_info = str(tar.extractfile('foo-42/PKG-INFO').read(), \"utf-8\")\n            members = tar.getnames()\n            assert \"bar-13/PKG-INFO\" not in members\n\n        with ZipFile(os.path.join(tmpdir, \"temp\", wheel_file)) as zipfile:\n            metadata = str(zipfile.read(\"foo-42.dist-info/METADATA\"), \"utf-8\")\n            members = zipfile.namelist()\n            assert \"bar-13.dist-info/METADATA\" not in members\n\n        for file in pkg_info, metadata:\n            for line in (\"Name: foo\", \"Version: 42\"):\n                assert line in file\n            for line in (\"Name: bar\", \"Version: 13\"):\n                assert line not in file\n\n    def test_build_sdist(self, build_backend):\n        dist_dir = os.path.abspath('pip-sdist')\n        os.makedirs(dist_dir)\n        sdist_name = build_backend.build_sdist(dist_dir)\n\n        assert os.path.isfile(os.path.join(dist_dir, sdist_name))\n\n    def test_prepare_metadata_for_build_wheel(self, build_backend):\n        dist_dir = os.path.abspath('pip-dist-info')\n        os.makedirs(dist_dir)\n\n        dist_info = build_backend.prepare_metadata_for_build_wheel(dist_dir)\n\n        assert os.path.isfile(os.path.join(dist_dir, dist_info, 'METADATA'))\n\n    def test_prepare_metadata_inplace(self, build_backend):\n        \"\"\"\n        Some users might pass metadata_directory pre-populated with `.tox` or `.venv`.\n        See issue #3523.\n        \"\"\"\n        for pre_existing in [\n            \".tox/python/lib/python3.10/site-packages/attrs-22.1.0.dist-info\",\n            \".tox/python/lib/python3.10/site-packages/autocommand-2.2.1.dist-info\",\n            \".nox/python/lib/python3.10/site-packages/build-0.8.0.dist-info\",\n            \".venv/python3.10/site-packages/click-8.1.3.dist-info\",\n            \"venv/python3.10/site-packages/distlib-0.3.5.dist-info\",\n            \"env/python3.10/site-packages/docutils-0.19.dist-info\",\n        ]:\n            os.makedirs(pre_existing, exist_ok=True)\n        dist_info = build_backend.prepare_metadata_for_build_wheel(\".\")\n        assert os.path.isfile(os.path.join(dist_info, 'METADATA'))\n\n    def test_build_sdist_explicit_dist(self, build_backend):\n        # explicitly specifying the dist folder should work\n        # the folder sdist_directory and the ``--dist-dir`` can be the same\n        dist_dir = os.path.abspath('dist')\n        sdist_name = build_backend.build_sdist(dist_dir)\n        assert os.path.isfile(os.path.join(dist_dir, sdist_name))\n\n    def test_build_sdist_version_change(self, build_backend):\n        sdist_into_directory = os.path.abspath(\"out_sdist\")\n        os.makedirs(sdist_into_directory)\n\n        sdist_name = build_backend.build_sdist(sdist_into_directory)\n        assert os.path.isfile(os.path.join(sdist_into_directory, sdist_name))\n\n        # if the setup.py changes subsequent call of the build meta\n        # should still succeed, given the\n        # sdist_directory the frontend specifies is empty\n        setup_loc = os.path.abspath(\"setup.py\")\n        if not os.path.exists(setup_loc):\n            setup_loc = os.path.abspath(\"setup.cfg\")\n\n        with open(setup_loc, 'rt', encoding=\"utf-8\") as file_handler:\n            content = file_handler.read()\n        with open(setup_loc, 'wt', encoding=\"utf-8\") as file_handler:\n            file_handler.write(content.replace(\"version='0.0.0'\", \"version='0.0.1'\"))\n\n        shutil.rmtree(sdist_into_directory)\n        os.makedirs(sdist_into_directory)\n\n        sdist_name = build_backend.build_sdist(\"out_sdist\")\n        assert os.path.isfile(os.path.join(os.path.abspath(\"out_sdist\"), sdist_name))\n\n    def test_build_sdist_pyproject_toml_exists(self, tmpdir_cwd):\n        files = {\n            'setup.py': DALS(\n                \"\"\"\n                __import__('setuptools').setup(\n                    name='foo',\n                    version='0.0.0',\n                    py_modules=['hello']\n                )\"\"\"\n            ),\n            'hello.py': '',\n            'pyproject.toml': DALS(\n                \"\"\"\n                [build-system]\n                requires = [\"setuptools\", \"wheel\"]\n                build-backend = \"setuptools.build_meta\"\n                \"\"\"\n            ),\n        }\n        path.build(files)\n        build_backend = self.get_build_backend()\n        targz_path = build_backend.build_sdist(\"temp\")\n        with tarfile.open(os.path.join(\"temp\", targz_path)) as tar:\n            assert any('pyproject.toml' in name for name in tar.getnames())\n\n    def test_build_sdist_setup_py_exists(self, tmpdir_cwd):\n        # If build_sdist is called from a script other than setup.py,\n        # ensure setup.py is included\n        path.build(defns[0])\n\n        build_backend = self.get_build_backend()\n        targz_path = build_backend.build_sdist(\"temp\")\n        with tarfile.open(os.path.join(\"temp\", targz_path)) as tar:\n            assert any('setup.py' in name for name in tar.getnames())\n\n    def test_build_sdist_setup_py_manifest_excluded(self, tmpdir_cwd):\n        # Ensure that MANIFEST.in can exclude setup.py\n        files = {\n            'setup.py': DALS(\n                \"\"\"\n        __import__('setuptools').setup(\n            name='foo',\n            version='0.0.0',\n            py_modules=['hello']\n        )\"\"\"\n            ),\n            'hello.py': '',\n            'MANIFEST.in': DALS(\n                \"\"\"\n        exclude setup.py\n        \"\"\"\n            ),\n        }\n\n        path.build(files)\n\n        build_backend = self.get_build_backend()\n        targz_path = build_backend.build_sdist(\"temp\")\n        with tarfile.open(os.path.join(\"temp\", targz_path)) as tar:\n            assert not any('setup.py' in name for name in tar.getnames())\n\n    def test_build_sdist_builds_targz_even_if_zip_indicated(self, tmpdir_cwd):\n        files = {\n            'setup.py': DALS(\n                \"\"\"\n                __import__('setuptools').setup(\n                    name='foo',\n                    version='0.0.0',\n                    py_modules=['hello']\n                )\"\"\"\n            ),\n            'hello.py': '',\n            'setup.cfg': DALS(\n                \"\"\"\n                [sdist]\n                formats=zip\n                \"\"\"\n            ),\n        }\n\n        path.build(files)\n\n        build_backend = self.get_build_backend()\n        build_backend.build_sdist(\"temp\")\n\n    _relative_path_import_files = {\n        'setup.py': DALS(\n            \"\"\"\n            __import__('setuptools').setup(\n                name='foo',\n                version=__import__('hello').__version__,\n                py_modules=['hello']\n            )\"\"\"\n        ),\n        'hello.py': '__version__ = \"0.0.0\"',\n        'setup.cfg': DALS(\n            \"\"\"\n            [sdist]\n            formats=zip\n            \"\"\"\n        ),\n    }\n\n    def test_build_sdist_relative_path_import(self, tmpdir_cwd):\n        path.build(self._relative_path_import_files)\n        build_backend = self.get_build_backend()\n        with pytest.raises(ImportError, match=\"^No module named 'hello'$\"):\n            build_backend.build_sdist(\"temp\")\n\n    _simple_pyproject_example = {\n        \"pyproject.toml\": DALS(\n            \"\"\"\n            [project]\n            name = \"proj\"\n            version = \"42\"\n            \"\"\"\n        ),\n        \"src\": {\"proj\": {\"__init__.py\": \"\"}},\n    }\n\n    def _assert_link_tree(self, parent_dir):\n        \"\"\"All files in the directory should be either links or hard links\"\"\"\n        files = list(Path(parent_dir).glob(\"**/*\"))\n        assert files  # Should not be empty\n        for file in files:\n            assert file.is_symlink() or os.stat(file).st_nlink > 0\n\n    def test_editable_without_config_settings(self, tmpdir_cwd):\n        \"\"\"\n        Sanity check to ensure tests with --mode=strict are different from the ones\n        without --mode.\n\n        --mode=strict should create a local directory with a package tree.\n        The directory should not get created otherwise.\n        \"\"\"\n        path.build(self._simple_pyproject_example)\n        build_backend = self.get_build_backend()\n        assert not Path(\"build\").exists()\n        build_backend.build_editable(\"temp\")\n        assert not Path(\"build\").exists()\n\n    def test_build_wheel_inplace(self, tmpdir_cwd):\n        config_settings = {\"--build-option\": [\"build_ext\", \"--inplace\"]}\n        path.build(self._simple_pyproject_example)\n        build_backend = self.get_build_backend()\n        assert not Path(\"build\").exists()\n        Path(\"build\").mkdir()\n        build_backend.prepare_metadata_for_build_wheel(\"build\", config_settings)\n        build_backend.build_wheel(\"build\", config_settings)\n        assert Path(\"build/proj-42-py3-none-any.whl\").exists()\n\n    @pytest.mark.parametrize(\"config_settings\", [{\"editable-mode\": \"strict\"}])\n    def test_editable_with_config_settings(self, tmpdir_cwd, config_settings):\n        path.build({**self._simple_pyproject_example, '_meta': {}})\n        assert not Path(\"build\").exists()\n        build_backend = self.get_build_backend()\n        build_backend.prepare_metadata_for_build_editable(\"_meta\", config_settings)\n        build_backend.build_editable(\"temp\", config_settings, \"_meta\")\n        self._assert_link_tree(next(Path(\"build\").glob(\"__editable__.*\")))\n\n    @pytest.mark.parametrize(\n        (\"setup_literal\", \"requirements\"),\n        [\n            (\"'foo'\", ['foo']),\n            (\"['foo']\", ['foo']),\n            (r\"'foo\\n'\", ['foo']),\n            (r\"'foo\\n\\n'\", ['foo']),\n            (\"['foo', 'bar']\", ['foo', 'bar']),\n            (r\"'# Has a comment line\\nfoo'\", ['foo']),\n            (r\"'foo # Has an inline comment'\", ['foo']),\n            (r\"'foo \\\\\\n >=3.0'\", ['foo>=3.0']),\n            (r\"'foo\\nbar'\", ['foo', 'bar']),\n            (r\"'foo\\nbar\\n'\", ['foo', 'bar']),\n            (r\"['foo\\n', 'bar\\n']\", ['foo', 'bar']),\n        ],\n    )\n    @pytest.mark.parametrize('use_wheel', [True, False])\n    def test_setup_requires(self, setup_literal, requirements, use_wheel, tmpdir_cwd):\n        files = {\n            'setup.py': DALS(\n                \"\"\"\n                from setuptools import setup\n\n                setup(\n                    name=\"qux\",\n                    version=\"0.0.0\",\n                    py_modules=[\"hello\"],\n                    setup_requires={setup_literal},\n                )\n            \"\"\"\n            ).format(setup_literal=setup_literal),\n            'hello.py': DALS(\n                \"\"\"\n            def run():\n                print('hello')\n            \"\"\"\n            ),\n        }\n\n        path.build(files)\n\n        build_backend = self.get_build_backend()\n\n        if use_wheel:\n            get_requires = build_backend.get_requires_for_build_wheel\n        else:\n            get_requires = build_backend.get_requires_for_build_sdist\n\n        # Ensure that the build requirements are properly parsed\n        expected = sorted(requirements)\n        actual = get_requires()\n\n        assert expected == sorted(actual)\n\n    def test_setup_requires_with_auto_discovery(self, tmpdir_cwd):\n        # Make sure patches introduced to retrieve setup_requires don't accidentally\n        # activate auto-discovery and cause problems due to the incomplete set of\n        # attributes passed to MinimalDistribution\n        files = {\n            'pyproject.toml': DALS(\n                \"\"\"\n                [project]\n                name = \"proj\"\n                version = \"42\"\n            \"\"\"\n            ),\n            \"setup.py\": DALS(\n                \"\"\"\n                __import__('setuptools').setup(\n                    setup_requires=[\"foo\"],\n                    py_modules = [\"hello\", \"world\"]\n                )\n            \"\"\"\n            ),\n            'hello.py': \"'hello'\",\n            'world.py': \"'world'\",\n        }\n        path.build(files)\n        build_backend = self.get_build_backend()\n        setup_requires = build_backend.get_requires_for_build_wheel()\n        assert setup_requires == [\"foo\"]\n\n    def test_dont_install_setup_requires(self, tmpdir_cwd):\n        files = {\n            'setup.py': DALS(\n                \"\"\"\n                        from setuptools import setup\n\n                        setup(\n                            name=\"qux\",\n                            version=\"0.0.0\",\n                            py_modules=[\"hello\"],\n                            setup_requires=[\"does-not-exist >99\"],\n                        )\n                    \"\"\"\n            ),\n            'hello.py': DALS(\n                \"\"\"\n                    def run():\n                        print('hello')\n                    \"\"\"\n            ),\n        }\n\n        path.build(files)\n\n        build_backend = self.get_build_backend()\n\n        dist_dir = os.path.abspath('pip-dist-info')\n        os.makedirs(dist_dir)\n\n        # does-not-exist can't be satisfied, so if it attempts to install\n        # setup_requires, it will fail.\n        build_backend.prepare_metadata_for_build_wheel(dist_dir)\n\n    _sys_argv_0_passthrough = {\n        'setup.py': DALS(\n            \"\"\"\n            import os\n            import sys\n\n            __import__('setuptools').setup(\n                name='foo',\n                version='0.0.0',\n            )\n\n            sys_argv = os.path.abspath(sys.argv[0])\n            file_path = os.path.abspath('setup.py')\n            assert sys_argv == file_path\n            \"\"\"\n        )\n    }\n\n    def test_sys_argv_passthrough(self, tmpdir_cwd):\n        path.build(self._sys_argv_0_passthrough)\n        build_backend = self.get_build_backend()\n        with pytest.raises(AssertionError):\n            build_backend.build_sdist(\"temp\")\n\n    _setup_py_file_abspath = {\n        'setup.py': DALS(\n            \"\"\"\n            import os\n            assert os.path.isabs(__file__)\n            __import__('setuptools').setup(\n                name='foo',\n                version='0.0.0',\n                py_modules=['hello'],\n                setup_requires=['six'],\n            )\n            \"\"\"\n        )\n    }\n\n    def test_setup_py_file_abspath(self, tmpdir_cwd):\n        path.build(self._setup_py_file_abspath)\n        build_backend = self.get_build_backend()\n        build_backend.build_sdist(\"temp\")\n\n    @pytest.mark.parametrize('build_hook', ('build_sdist', 'build_wheel'))\n    def test_build_with_empty_setuppy(self, build_backend, build_hook):\n        files = {'setup.py': ''}\n        path.build(files)\n\n        msg = re.escape('No distribution was found.')\n        with pytest.raises(ValueError, match=msg):\n            getattr(build_backend, build_hook)(\"temp\")\n\n\nclass TestBuildMetaLegacyBackend(TestBuildMetaBackend):\n    backend_name = 'setuptools.build_meta:__legacy__'\n\n    # build_meta_legacy-specific tests\n    def test_build_sdist_relative_path_import(self, tmpdir_cwd):\n        # This must fail in build_meta, but must pass in build_meta_legacy\n        path.build(self._relative_path_import_files)\n\n        build_backend = self.get_build_backend()\n        build_backend.build_sdist(\"temp\")\n\n    def test_sys_argv_passthrough(self, tmpdir_cwd):\n        path.build(self._sys_argv_0_passthrough)\n\n        build_backend = self.get_build_backend()\n        build_backend.build_sdist(\"temp\")\n\n\ndef test_legacy_editable_install(venv, tmpdir, tmpdir_cwd):\n    pyproject = \"\"\"\n    [build-system]\n    requires = [\"setuptools\"]\n    build-backend = \"setuptools.build_meta\"\n    [project]\n    name = \"myproj\"\n    version = \"42\"\n    \"\"\"\n    path.build({\"pyproject.toml\": DALS(pyproject), \"mymod.py\": \"\"})\n\n    # First: sanity check\n    cmd = [\"pip\", \"install\", \"--no-build-isolation\", \"-e\", \".\"]\n    output = venv.run(cmd, cwd=tmpdir).lower()\n    assert \"running setup.py develop for myproj\" not in output\n    assert \"created wheel for myproj\" in output\n\n    # Then: real test\n    env = {**os.environ, \"SETUPTOOLS_ENABLE_FEATURES\": \"legacy-editable\"}\n    cmd = [\"pip\", \"install\", \"--no-build-isolation\", \"-e\", \".\"]\n    output = venv.run(cmd, cwd=tmpdir, env=env).lower()\n    assert \"running setup.py develop for myproj\" in output\n\n\n@pytest.mark.filterwarnings(\"ignore::setuptools.SetuptoolsDeprecationWarning\")\ndef test_sys_exit_0_in_setuppy(monkeypatch, tmp_path):\n    \"\"\"Setuptools should be resilient to setup.py with ``sys.exit(0)`` (#3973).\"\"\"\n    monkeypatch.chdir(tmp_path)\n    setuppy = \"\"\"\n        import sys, setuptools\n        setuptools.setup(name='foo', version='0.0.0')\n        sys.exit(0)\n        \"\"\"\n    (tmp_path / \"setup.py\").write_text(DALS(setuppy), encoding=\"utf-8\")\n    backend = BuildBackend(backend_name=\"setuptools.build_meta\")\n    assert backend.get_requires_for_build_wheel() == []\n\n\ndef test_system_exit_in_setuppy(monkeypatch, tmp_path):\n    monkeypatch.chdir(tmp_path)\n    setuppy = \"import sys; sys.exit('some error')\"\n    (tmp_path / \"setup.py\").write_text(setuppy, encoding=\"utf-8\")\n    with pytest.raises(SystemExit, match=\"some error\"):\n        backend = BuildBackend(backend_name=\"setuptools.build_meta\")\n        backend.get_requires_for_build_wheel()\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/tests/test_build_py.py","size":14187,"sha1":"d3ef19cdf37e4c38215d32c14e1e4778671a9a59","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import os\nimport shutil\nimport stat\nimport warnings\nfrom pathlib import Path\nfrom unittest.mock import Mock\n\nimport jaraco.path\nimport pytest\n\nfrom setuptools import SetuptoolsDeprecationWarning\nfrom setuptools.dist import Distribution\n\nfrom .textwrap import DALS\n\n\ndef test_directories_in_package_data_glob(tmpdir_cwd):\n    \"\"\"\n    Directories matching the glob in package_data should\n    not be included in the package data.\n\n    Regression test for #261.\n    \"\"\"\n    dist = Distribution(\n        dict(\n            script_name='setup.py',\n            script_args=['build_py'],\n            packages=[''],\n            package_data={'': ['path/*']},\n        )\n    )\n    os.makedirs('path/subpath')\n    dist.parse_command_line()\n    dist.run_commands()\n\n\ndef test_recursive_in_package_data_glob(tmpdir_cwd):\n    \"\"\"\n    Files matching recursive globs (**) in package_data should\n    be included in the package data.\n\n    #1806\n    \"\"\"\n    dist = Distribution(\n        dict(\n            script_name='setup.py',\n            script_args=['build_py'],\n            packages=[''],\n            package_data={'': ['path/**/data']},\n        )\n    )\n    os.makedirs('path/subpath/subsubpath')\n    open('path/subpath/subsubpath/data', 'wb').close()\n\n    dist.parse_command_line()\n    dist.run_commands()\n\n    assert stat.S_ISREG(os.stat('build/lib/path/subpath/subsubpath/data').st_mode), (\n        \"File is not included\"\n    )\n\n\ndef test_read_only(tmpdir_cwd):\n    \"\"\"\n    Ensure read-only flag is not preserved in copy\n    for package modules and package data, as that\n    causes problems with deleting read-only files on\n    Windows.\n\n    #1451\n    \"\"\"\n    dist = Distribution(\n        dict(\n            script_name='setup.py',\n            script_args=['build_py'],\n            packages=['pkg'],\n            package_data={'pkg': ['data.dat']},\n        )\n    )\n    os.makedirs('pkg')\n    open('pkg/__init__.py', 'wb').close()\n    open('pkg/data.dat', 'wb').close()\n    os.chmod('pkg/__init__.py', stat.S_IREAD)\n    os.chmod('pkg/data.dat', stat.S_IREAD)\n    dist.parse_command_line()\n    dist.run_commands()\n    shutil.rmtree('build')\n\n\n@pytest.mark.xfail(\n    'platform.system() == \"Windows\"',\n    reason=\"On Windows, files do not have executable bits\",\n    raises=AssertionError,\n    strict=True,\n)\ndef test_executable_data(tmpdir_cwd):\n    \"\"\"\n    Ensure executable bit is preserved in copy for\n    package data, as users rely on it for scripts.\n\n    #2041\n    \"\"\"\n    dist = Distribution(\n        dict(\n            script_name='setup.py',\n            script_args=['build_py'],\n            packages=['pkg'],\n            package_data={'pkg': ['run-me']},\n        )\n    )\n    os.makedirs('pkg')\n    open('pkg/__init__.py', 'wb').close()\n    open('pkg/run-me', 'wb').close()\n    os.chmod('pkg/run-me', 0o700)\n\n    dist.parse_command_line()\n    dist.run_commands()\n\n    assert os.stat('build/lib/pkg/run-me').st_mode & stat.S_IEXEC, (\n        \"Script is not executable\"\n    )\n\n\nEXAMPLE_WITH_MANIFEST = {\n    \"setup.cfg\": DALS(\n        \"\"\"\n        [metadata]\n        name = mypkg\n        version = 42\n\n        [options]\n        include_package_data = True\n        packages = find:\n\n        [options.packages.find]\n        exclude = *.tests*\n        \"\"\"\n    ),\n    \"mypkg\": {\n        \"__init__.py\": \"\",\n        \"resource_file.txt\": \"\",\n        \"tests\": {\n            \"__init__.py\": \"\",\n            \"test_mypkg.py\": \"\",\n            \"test_file.txt\": \"\",\n        },\n    },\n    \"MANIFEST.in\": DALS(\n        \"\"\"\n        global-include *.py *.txt\n        global-exclude *.py[cod]\n        prune dist\n        prune build\n        prune *.egg-info\n        \"\"\"\n    ),\n}\n\n\ndef test_excluded_subpackages(tmpdir_cwd):\n    jaraco.path.build(EXAMPLE_WITH_MANIFEST)\n    dist = Distribution({\"script_name\": \"%PEP 517%\"})\n    dist.parse_config_files()\n\n    build_py = dist.get_command_obj(\"build_py\")\n\n    msg = r\"Python recognizes 'mypkg\\.tests' as an importable package\"\n    with pytest.warns(SetuptoolsDeprecationWarning, match=msg):\n        # TODO: To fix #3260 we need some transition period to deprecate the\n        # existing behavior of `include_package_data`. After the transition, we\n        # should remove the warning and fix the behaviour.\n\n        if os.getenv(\"SETUPTOOLS_USE_DISTUTILS\") == \"stdlib\":\n            # pytest.warns reset the warning filter temporarily\n            # https://github.com/pytest-dev/pytest/issues/4011#issuecomment-423494810\n            warnings.filterwarnings(\n                \"ignore\",\n                \"'encoding' argument not specified\",\n                module=\"distutils.text_file\",\n                # This warning is already fixed in pypa/distutils but not in stdlib\n            )\n\n        build_py.finalize_options()\n        build_py.run()\n\n    build_dir = Path(dist.get_command_obj(\"build_py\").build_lib)\n    assert (build_dir / \"mypkg/__init__.py\").exists()\n    assert (build_dir / \"mypkg/resource_file.txt\").exists()\n\n    # Setuptools is configured to ignore `mypkg.tests`, therefore the following\n    # files/dirs should not be included in the distribution.\n    for f in [\n        \"mypkg/tests/__init__.py\",\n        \"mypkg/tests/test_mypkg.py\",\n        \"mypkg/tests/test_file.txt\",\n        \"mypkg/tests\",\n    ]:\n        with pytest.raises(AssertionError):\n            # TODO: Enforce the following assertion once #3260 is fixed\n            # (remove context manager and the following xfail).\n            assert not (build_dir / f).exists()\n\n    pytest.xfail(\"#3260\")\n\n\n@pytest.mark.filterwarnings(\"ignore::setuptools.SetuptoolsDeprecationWarning\")\ndef test_existing_egg_info(tmpdir_cwd, monkeypatch):\n    \"\"\"When provided with the ``existing_egg_info_dir`` attribute, build_py should not\n    attempt to run egg_info again.\n    \"\"\"\n    # == Pre-condition ==\n    # Generate an egg-info dir\n    jaraco.path.build(EXAMPLE_WITH_MANIFEST)\n    dist = Distribution({\"script_name\": \"%PEP 517%\"})\n    dist.parse_config_files()\n    assert dist.include_package_data\n\n    egg_info = dist.get_command_obj(\"egg_info\")\n    dist.run_command(\"egg_info\")\n    egg_info_dir = next(Path(egg_info.egg_base).glob(\"*.egg-info\"))\n    assert egg_info_dir.is_dir()\n\n    # == Setup ==\n    build_py = dist.get_command_obj(\"build_py\")\n    build_py.finalize_options()\n    egg_info = dist.get_command_obj(\"egg_info\")\n    egg_info_run = Mock(side_effect=egg_info.run)\n    monkeypatch.setattr(egg_info, \"run\", egg_info_run)\n\n    # == Remove caches ==\n    # egg_info is called when build_py looks for data_files, which gets cached.\n    # We need to ensure it is not cached yet, otherwise it may impact on the tests\n    build_py.__dict__.pop('data_files', None)\n    dist.reinitialize_command(egg_info)\n\n    # == Sanity check ==\n    # Ensure that if existing_egg_info is not given, build_py attempts to run egg_info\n    build_py.existing_egg_info_dir = None\n    build_py.run()\n    egg_info_run.assert_called()\n\n    # == Remove caches ==\n    egg_info_run.reset_mock()\n    build_py.__dict__.pop('data_files', None)\n    dist.reinitialize_command(egg_info)\n\n    # == Actual test ==\n    # Ensure that if existing_egg_info_dir is given, egg_info doesn't run\n    build_py.existing_egg_info_dir = egg_info_dir\n    build_py.run()\n    egg_info_run.assert_not_called()\n    assert build_py.data_files\n\n    # Make sure the list of outputs is actually OK\n    outputs = map(lambda x: x.replace(os.sep, \"/\"), build_py.get_outputs())\n    assert outputs\n    example = str(Path(build_py.build_lib, \"mypkg/__init__.py\")).replace(os.sep, \"/\")\n    assert example in outputs\n\n\nEXAMPLE_ARBITRARY_MAPPING = {\n    \"pyproject.toml\": DALS(\n        \"\"\"\n        [project]\n        name = \"mypkg\"\n        version = \"42\"\n\n        [tool.setuptools]\n        packages = [\"mypkg\", \"mypkg.sub1\", \"mypkg.sub2\", \"mypkg.sub2.nested\"]\n\n        [tool.setuptools.package-dir]\n        \"\" = \"src\"\n        \"mypkg.sub2\" = \"src/mypkg/_sub2\"\n        \"mypkg.sub2.nested\" = \"other\"\n        \"\"\"\n    ),\n    \"src\": {\n        \"mypkg\": {\n            \"__init__.py\": \"\",\n            \"resource_file.txt\": \"\",\n            \"sub1\": {\n                \"__init__.py\": \"\",\n                \"mod1.py\": \"\",\n            },\n            \"_sub2\": {\n                \"mod2.py\": \"\",\n            },\n        },\n    },\n    \"other\": {\n        \"__init__.py\": \"\",\n        \"mod3.py\": \"\",\n    },\n    \"MANIFEST.in\": DALS(\n        \"\"\"\n        global-include *.py *.txt\n        global-exclude *.py[cod]\n        \"\"\"\n    ),\n}\n\n\ndef test_get_outputs(tmpdir_cwd):\n    jaraco.path.build(EXAMPLE_ARBITRARY_MAPPING)\n    dist = Distribution({\"script_name\": \"%test%\"})\n    dist.parse_config_files()\n\n    build_py = dist.get_command_obj(\"build_py\")\n    build_py.editable_mode = True\n    build_py.ensure_finalized()\n    build_lib = build_py.build_lib.replace(os.sep, \"/\")\n    outputs = {x.replace(os.sep, \"/\") for x in build_py.get_outputs()}\n    assert outputs == {\n        f\"{build_lib}/mypkg/__init__.py\",\n        f\"{build_lib}/mypkg/resource_file.txt\",\n        f\"{build_lib}/mypkg/sub1/__init__.py\",\n        f\"{build_lib}/mypkg/sub1/mod1.py\",\n        f\"{build_lib}/mypkg/sub2/mod2.py\",\n        f\"{build_lib}/mypkg/sub2/nested/__init__.py\",\n        f\"{build_lib}/mypkg/sub2/nested/mod3.py\",\n    }\n    mapping = {\n        k.replace(os.sep, \"/\"): v.replace(os.sep, \"/\")\n        for k, v in build_py.get_output_mapping().items()\n    }\n    assert mapping == {\n        f\"{build_lib}/mypkg/__init__.py\": \"src/mypkg/__init__.py\",\n        f\"{build_lib}/mypkg/resource_file.txt\": \"src/mypkg/resource_file.txt\",\n        f\"{build_lib}/mypkg/sub1/__init__.py\": \"src/mypkg/sub1/__init__.py\",\n        f\"{build_lib}/mypkg/sub1/mod1.py\": \"src/mypkg/sub1/mod1.py\",\n        f\"{build_lib}/mypkg/sub2/mod2.py\": \"src/mypkg/_sub2/mod2.py\",\n        f\"{build_lib}/mypkg/sub2/nested/__init__.py\": \"other/__init__.py\",\n        f\"{build_lib}/mypkg/sub2/nested/mod3.py\": \"other/mod3.py\",\n    }\n\n\nclass TestTypeInfoFiles:\n    PYPROJECTS = {\n        \"default_pyproject\": DALS(\n            \"\"\"\n            [project]\n            name = \"foo\"\n            version = \"1\"\n            \"\"\"\n        ),\n        \"dont_include_package_data\": DALS(\n            \"\"\"\n            [project]\n            name = \"foo\"\n            version = \"1\"\n\n            [tool.setuptools]\n            include-package-data = false\n            \"\"\"\n        ),\n        \"exclude_type_info\": DALS(\n            \"\"\"\n            [project]\n            name = \"foo\"\n            version = \"1\"\n\n            [tool.setuptools]\n            include-package-data = false\n\n            [tool.setuptools.exclude-package-data]\n            \"*\" = [\"py.typed\", \"*.pyi\"]\n            \"\"\"\n        ),\n    }\n\n    EXAMPLES = {\n        \"simple_namespace\": {\n            \"directory_structure\": {\n                \"foo\": {\n                    \"bar.pyi\": \"\",\n                    \"py.typed\": \"\",\n                    \"__init__.py\": \"\",\n                }\n            },\n            \"expected_type_files\": {\"foo/bar.pyi\", \"foo/py.typed\"},\n        },\n        \"nested_inside_namespace\": {\n            \"directory_structure\": {\n                \"foo\": {\n                    \"bar\": {\n                        \"py.typed\": \"\",\n                        \"mod.pyi\": \"\",\n                    }\n                }\n            },\n            \"expected_type_files\": {\"foo/bar/mod.pyi\", \"foo/bar/py.typed\"},\n        },\n        \"namespace_nested_inside_regular\": {\n            \"directory_structure\": {\n                \"foo\": {\n                    \"namespace\": {\n                        \"foo.pyi\": \"\",\n                    },\n                    \"__init__.pyi\": \"\",\n                    \"py.typed\": \"\",\n                }\n            },\n            \"expected_type_files\": {\n                \"foo/namespace/foo.pyi\",\n                \"foo/__init__.pyi\",\n                \"foo/py.typed\",\n            },\n        },\n    }\n\n    @pytest.mark.parametrize(\n        \"pyproject\",\n        [\n            \"default_pyproject\",\n            pytest.param(\n                \"dont_include_package_data\",\n                marks=pytest.mark.xfail(reason=\"pypa/setuptools#4350\"),\n            ),\n        ],\n    )\n    @pytest.mark.parametrize(\"example\", EXAMPLES.keys())\n    def test_type_files_included_by_default(self, tmpdir_cwd, pyproject, example):\n        structure = {\n            **self.EXAMPLES[example][\"directory_structure\"],\n            \"pyproject.toml\": self.PYPROJECTS[pyproject],\n        }\n        expected_type_files = self.EXAMPLES[example][\"expected_type_files\"]\n        jaraco.path.build(structure)\n\n        build_py = get_finalized_build_py()\n        outputs = get_outputs(build_py)\n        assert expected_type_files <= outputs\n\n    @pytest.mark.parametrize(\"pyproject\", [\"exclude_type_info\"])\n    @pytest.mark.parametrize(\"example\", EXAMPLES.keys())\n    def test_type_files_can_be_excluded(self, tmpdir_cwd, pyproject, example):\n        structure = {\n            **self.EXAMPLES[example][\"directory_structure\"],\n            \"pyproject.toml\": self.PYPROJECTS[pyproject],\n        }\n        expected_type_files = self.EXAMPLES[example][\"expected_type_files\"]\n        jaraco.path.build(structure)\n\n        build_py = get_finalized_build_py()\n        outputs = get_outputs(build_py)\n        assert expected_type_files.isdisjoint(outputs)\n\n    def test_stub_only_package(self, tmpdir_cwd):\n        structure = {\n            \"pyproject.toml\": DALS(\n                \"\"\"\n                [project]\n                name = \"foo-stubs\"\n                version = \"1\"\n                \"\"\"\n            ),\n            \"foo-stubs\": {\"__init__.pyi\": \"\", \"bar.pyi\": \"\"},\n        }\n        expected_type_files = {\"foo-stubs/__init__.pyi\", \"foo-stubs/bar.pyi\"}\n        jaraco.path.build(structure)\n\n        build_py = get_finalized_build_py()\n        outputs = get_outputs(build_py)\n        assert expected_type_files <= outputs\n\n\ndef get_finalized_build_py(script_name=\"%build_py-test%\"):\n    dist = Distribution({\"script_name\": script_name})\n    dist.parse_config_files()\n    build_py = dist.get_command_obj(\"build_py\")\n    build_py.finalize_options()\n    return build_py\n\n\ndef get_outputs(build_py):\n    build_dir = Path(build_py.build_lib)\n    return {\n        os.path.relpath(x, build_dir).replace(os.sep, \"/\")\n        for x in build_py.get_outputs()\n    }\n"},{"path":".local/share/virtualenv/wheel/3.11/image/1/CopyPipInstall/setuptools-75.8.0-py3-none-any/setuptools/tests/test_config_discovery.py","size":22580,"sha1":"2cf4df922cfd38c79696b5f35286a14dd7ffe756","mtime":1754403963,"is_binary":false,"encoding":"utf-8","content":"import os\nimport sys\nfrom configparser import ConfigParser\nfrom itertools import product\nfrom typing import cast\n\nimport jaraco.path\nimport pytest\nfrom path import Path\n\nimport setuptools  # noqa: F401 # force distutils.core to be patched\nfrom setuptools.command.sdist import sdist\nfrom setuptools.discovery import find_package_path, find_parent_package\nfrom setuptools.dist import Distribution\nfrom setuptools.errors import PackageDiscoveryError\n\nfrom .contexts import quiet\nfrom .integration.helpers import get_sdist_members, get_wheel_members, run\nfrom .textwrap import DALS\n\nimport distutils.core\n\n\nclass TestFindParentPackage:\n    def test_single_package(self, tmp_path):\n        # find_parent_package should find a non-namespace parent package\n        (tmp_path / \"src/namespace/pkg/nested\").mkdir(exist_ok=True, parents=True)\n        (tmp_path / \"src/namespace/pkg/nested/__init__.py\").touch()\n        (tmp_path / \"src/namespace/pkg/__init__.py\").touch()\n        packages = [\"namespace\", \"namespace.pkg\", \"namespace.pkg.nested\"]\n        assert find_parent_package(packages, {\"\": \"src\"}, tmp_path) == \"namespace.pkg\"\n\n    def test_multiple_toplevel(self, tmp_path):\n        # find_parent_package should return null if the given list of packages does not\n        # have a single parent package\n        multiple = [\"pkg\", \"pkg1\", \"pkg2\"]\n        for name in multiple:\n            (tmp_path / f\"src/{name}\").mkdir(exist_ok=True, parents=True)\n            (tmp_path / f\"src/{name}/__init__.py\").touch()\n        assert find_parent_package(multiple, {\"\": \"src\"}, tmp_path) is None\n\n\nclass TestDiscoverPackagesAndPyModules:\n    \"\"\"Make sure discovered values for ``packages`` and ``py_modules`` work\n    similarly to explicit configuration for the simple scenarios.\n    \"\"\"\n\n    OPTIONS = {\n        # Different options according to the circumstance being tested\n        \"explicit-src\": {\"package_dir\": {\"\": \"src\"}, \"packages\": [\"pkg\"]},\n        \"variation-lib\": {\n            \"package_dir\": {\"\": \"lib\"},  # variation of the source-layout\n        },\n        \"explicit-flat\": {\"packages\": [\"pkg\"]},\n        \"explicit-single_module\": {\"py_modules\": [\"pkg\"]},\n        \"explicit-namespace\": {\"packages\": [\"ns\", \"ns.pkg\"]},\n        \"automatic-src\": {},\n        \"automatic-flat\": {},\n        \"automatic-single_module\": {},\n        \"automatic-namespace\": {},\n    }\n    FILES = {\n        \"src\": [\"src/pkg/__init__.py\", \"src/pkg/main.py\"],\n        \"lib\": [\"lib/pkg/__init__.py\", \"lib/pkg/main.py\"],\n        \"flat\": [\"pkg/__init__.py\", \"pkg/main.py\"],\n        \"single_module\": [\"pkg.py\"],\n        \"namespace\": [\"ns/pkg/__init__.py\"],\n    }\n\n    def _get_info(self, circumstance):\n        _, _, layout = circumstance.partition(\"-\")\n        files = self.FILES[layout]\n        options = self.OPTIONS[circumstance]\n        return files, options\n\n    @pytest.mark.parametrize(\"circumstance\", OPTIONS.keys())\n    def test_sdist_filelist(self, tmp_path, circumstance):\n        files, options = self._get_info(circumstance)\n        _populate_project_dir(tmp_path, files, options)\n\n        _, cmd = _run_sdist_programatically(tmp_path, options)\n\n        manifest = [f.replace(os.sep, \"/\") for f in cmd.filelist.files]\n        for file in files:\n            assert any(f.endswith(file) for f in manifest)\n\n    @pytest.mark.parametrize(\"circumstance\", OPTIONS.keys())\n    def test_project(self, tmp_path, circumstance):\n        files, options = self._get_info(circumstance)\n        _populate_project_dir(tmp_path, files, options)\n\n        # Simulate a pre-existing `build` directory\n        (tmp_path / \"build\").mkdir()\n        (tmp_path / \"build/lib\").mkdir()\n        (tmp_path / \"build/bdist.linux-x86_64\").mkdir()\n        (tmp_path / \"build/bdist.linux-x86_64/file.py\").touch()\n        (tmp_path / \"build/lib/__init__.py\").touch()\n        (tmp_path / \"build/lib/file.py\").touch()\n        (tmp_path / \"dist\").mkdir()\n        (tmp_path / \"dist/file.py\").touch()\n\n        _run_build(tmp_path)\n\n        sdist_files = get_sdist_members(next(tmp_path.glob(\"dist/*.tar.gz\")))\n        print(\"~~~~~ sdist_members ~~~~~\")\n        print('\\n'.join(sdist_files))\n        assert sdist_files >= set(files)\n\n        wheel_files = get_wheel_members(next(tmp_path.glob(\"dist/*.whl\")))\n        print(\"~~~~~ wheel_members ~~~~~\")\n        print('\\n'.join(wheel_files))\n        orig_files = {f.replace(\"src/\", \"\").replace(\"lib/\", \"\") for f in files}\n        assert wheel_files >= orig_files\n\n        # Make sure build files are not included by mistake\n        for file in wheel_files:\n            assert \"build\" not in files\n            assert \"dist\" not in files\n\n    PURPOSEFULLY_EMPY = {\n        \"setup.cfg\": DALS(\n            \"\"\"\n            [metadata]\n            name = myproj\n            version = 0.0.0\n\n            [options]\n            {param} =\n            \"\"\"\n        ),\n        \"setup.py\": DALS(\n            \"\"\"\n            __import__('setuptools').setup(\n                name=\"myproj\",\n                version=\"0.0.0\",\n                {param}=[]\n            )\n            \"\"\"\n        ),\n        \"pyproject.toml\": DALS(\n            \"\"\"\n            [build-system]\n            requires = []\n            build-backend = 'setuptools.build_meta'\n\n            [project]\n            name = \"myproj\"\n            version = \"0.0.0\"\n\n            [tool.setuptools]\n            {param} = []\n            \"\"\"\n        ),\n        \"template-pyproject.toml\": DALS(\n            \"\"\"\n            [build-system]\n            requires = []\n            build-backend = 'setuptools.build_meta'\n            \"\"\"\n        ),\n    }\n\n    @pytest.mark.parametrize(\n        (\"config_file\", \"param\", \"circumstance\"),\n        product(\n            [\"setup.cfg\", \"setup.py\", \"pyproject.toml\"],\n            [\"packages\", \"py_modules\"],\n            FILES.keys(),\n        ),\n    )\n    def test_purposefully_empty(self, tmp_path, config_file, param, circumstance):\n        files = self.FILES[circumstance] + [\"mod.py\", \"other.py\", \"src/pkg/__init__.py\"]\n        _populate_project_dir(tmp_path, files, {})\n\n        if config_file == \"pyproject.toml\":\n            template_param = param.replace(\"_\", \"-\")\n        else:\n            # Make sure build works with or without setup.cfg\n            pyproject = self.PURPOSEFULLY_EMPY[\"template-pyproject.toml\"]\n            (tmp_path / \"pyproject.toml\").write_text(pyproject, encoding=\"utf-8\")\n            template_param = param\n\n        config = self.PURPOSEFULLY_EMPY[config_file].format(param=template_param)\n        (tmp_path / config_file).write_text(config, encoding=\"utf-8\")\n\n        dist = _get_dist(tmp_path, {})\n        # When either parameter package or py_modules is an empty list,\n        # then there should be no discovery\n        assert getattr(dist, param) == []\n        other = {\"py_modules\": \"packages\", \"packages\": \"py_modules\"}[param]\n        assert getattr(dist, other) is None\n\n    @pytest.mark.parametrize(\n        (\"extra_files\", \"pkgs\"),\n        [\n            ([\"venv/bin/simulate_venv\"], {\"pkg\"}),\n            ([\"pkg-stubs/__init__.pyi\"], {\"pkg\", \"pkg-stubs\"}),\n            ([\"other-stubs/__init__.pyi\"], {\"pkg\", \"other-stubs\"}),\n            (\n                # Type stubs can also be namespaced\n                [\"namespace-stubs/pkg/__init__.pyi\"],\n                {\"pkg\", \"namespace-stubs\", \"namespace-stubs.pkg\"},\n            ),\n            (\n                # Just the top-level package can have `-stubs`, ignore nested ones\n                [\"namespace-stubs/pkg-stubs/__init__.pyi\"],\n                {\"pkg\", \"namespace-stubs\"},\n            ),\n            ([\"_hidden/file.py\"], {\"pkg\"}),\n            ([\"news/finalize.py\"], {\"pkg\"}),\n        ],\n    )\n    def test_flat_layout_with_extra_files(self, tmp_path, extra_files, pkgs):\n        files = self.FILES[\"flat\"] + extra_files\n        _populate_project_dir(tmp_path, files, {})\n        dist = _get_dist(tmp_path, {})\n        assert set(dist.packages) == pkgs\n\n    @pytest.mark.parametrize(\n        \"extra_files\",\n        [\n            [\"other/__init__.py\"],\n            [\"other/finalize.py\"],\n        ],\n    )\n    def test_flat_layout_with_dangerous_extra_files(self, tmp_path, extra_files):\n        files = self.FILES[\"flat\"] + extra_files\n        _populate_project_dir(tmp_path, files, {})\n        with pytest.raises(PackageDiscoveryError, match=\"multiple (packages|modules)\"):\n            _get_dist(tmp_path, {})\n\n    def test_flat_layout_with_single_module(self, tmp_path):\n        files = self.FILES[\"single_module\"] + [\"invalid-module-name.py\"]\n        _populate_project_dir(tmp_path, files, {})\n        dist = _get_dist(tmp_path, {})\n        assert set(dist.py_modules) == {\"pkg\"}\n\n    def test_flat_layout_with_multiple_modules(self, tmp_path):\n        files = self.FILES[\"single_module\"] + [\"valid_module_name.py\"]\n        _populate_project_dir(tmp_path, files, {})\n        with pytest.raises(PackageDiscoveryError, match=\"multiple (packages|modules)\"):\n            _get_dist(tmp_path, {})\n\n    def test_py_modules_when_wheel_dir_is_cwd(self, tmp_path):\n        \"\"\"Regression for issue 3692\"\"\"\n        from setuptools import build_meta\n\n        pyproject = '[project]\\nname = \"test\"\\nversion = \"1\"'\n        (tmp_path / \"pyproject.toml\").write_text(DALS(pyproject), encoding=\"utf-8\")\n        (tmp_path / \"foo.py\").touch()\n        with jaraco.path.DirectoryStack().context(tmp_path):\n            build_meta.build_wheel(\".\")\n        # Ensure py_modules are found\n        wheel_files = get_wheel_members(next(tmp_path.glob(\"*.whl\")))\n        assert \"foo.py\" in wheel_files\n\n\nclass TestNoConfig:\n    DEFAULT_VERSION = \"0.0.0\"  # Default version given by setuptools\n\n    EXAMPLES = {\n        \"pkg1\": [\"src/pkg1.py\"],\n        \"pkg2\": [\"src/pkg2/__init__.py\"],\n        \"pkg3\": [\"src/pkg3/__init__.py\", \"src/pkg3-stubs/__init__.py\"],\n        \"pkg4\": [\"pkg4/__init__.py\", \"pkg4-stubs/__init__.py\"],\n        \"ns.nested.pkg1\": [\"src/ns/nested/pkg1/__init__.py\"],\n        \"ns.nested.pkg2\": [\"ns/nested/pkg2/__init__.py\"],\n    }\n\n    @pytest.mark.parametrize(\"example\", EXAMPLES.keys())\n    def test_discover_name(self, tmp_path, example):\n        _populate_project_dir(tmp_path, self.EXAMPLES[example], {})\n        dist = _get_dist(tmp_path, {})\n        assert dist.get_name() == example\n\n    def test_build_with_discovered_name(self, tmp_path):\n        files = [\"src/ns/nested/pkg/__init__.py\"]\n        _populate_project_dir(tmp_path, files, {})\n        _run_build(tmp_path, \"--sdist\")\n        # Expected distribution file\n        dist_file = tmp_path / f\"dist/ns_nested_pkg-{self.DEFAULT_VERSION}.tar.gz\"\n        assert dist_file.is_file()\n\n\nclass TestWithAttrDirective:\n    @pytest.mark.parametrize(\n        (\"folder\", \"opts\"),\n        [\n            (\"src\", {}),\n            (\"lib\", {\"packages\": \"find:\", \"packages.find\": {\"where\": \"lib\"}}),\n        ],\n    )\n    def test_setupcfg_metadata(self, tmp_path, folder, opts):\n        files = [f\"{folder}/pkg/__init__.py\", \"setup.cfg\"]\n        _populate_project_dir(tmp_path, files, opts)\n\n        config = (tmp_path / \"setup.cfg\").read_text(encoding=\"utf-8\")\n        overwrite = {\n            folder: {\"pkg\": {\"__init__.py\": \"version = 42\"}},\n            \"setup.cfg\": \"[metadata]\\nversion = attr: pkg.version\\n\" + config,\n        }\n        jaraco.path.build(overwrite, prefix=tmp_path)\n\n        dist = _get_dist(tmp_path, {})\n        assert dist.get_name() == \"pkg\"\n        assert dist.get_version() == \"42\"\n        assert dist.package_dir\n        package_path = find_package_path(\"pkg\", dist.package_dir, tmp_path)\n        assert os.path.exists(package_path)\n        assert folder in Path(package_path).parts()\n\n        _run_build(tmp_path, \"--sdist\")\n        dist_file = tmp_path / \"dist/pkg-42.tar.gz\"\n        assert dist_file.is_file()\n\n    def test_pyproject_metadata(self, tmp_path):\n        _populate_project_dir(tmp_path, [\"src/pkg/__init__.py\"], {})\n\n        overwrite = {\n            \"src\": {\"pkg\": {\"__init__.py\": \"version = 42\"}},\n            \"pyproject.toml\": (\n                \"[project]\\nname = 'pkg'\\ndynamic = ['version']\\n\"\n                \"[tool.setuptools.dynamic]\\nversion = {attr = 'pkg.version'}\\n\"\n            ),\n        }\n        jaraco.path.build(overwrite, prefix=tmp_path)\n\n        dist = _get_dist(tmp_path, {})\n        assert dist.get_version() == \"42\"\n        assert dist.package_dir == {\"\": \"src\"}\n\n\nclass TestWithCExtension:\n    def _simulate_package_with_extension(self, tmp_path):\n        # This example is based on: https://github.com/nucleic/kiwi/tree/1.4.0\n        files = [\n            \"benchmarks/file.py\",\n            \"docs/Makefile\",\n            \"docs/requirements.txt\",\n            \"docs/source/conf.py\",\n            \"proj/header.h\",\n            \"proj/file.py\",\n            \"py/proj.cpp\",\n            \"py/other.cpp\",\n            \"py/file.py\",\n            \"py/py.typed\",\n            \"py/tests/test_proj.py\",\n            \"README.rst\",\n        ]\n        _populate_project_dir(tmp_path, files, {})\n\n        setup_script = \"\"\"\n            from setuptools import Extension, setup\n\n            ext_modules = [\n                Extension(\n                    \"proj\",\n                    [\"py/proj.cpp\", \"py/other.cpp\"],\n                    include_dirs=[\".\"],\n                    language=\"c++\",\n                ),\n            ]\n            setup(ext_modules=ext_modules)\n        \"\"\"\n        (tmp_path / \"setup.py\").write_text(DALS(setup_script), encoding=\"utf-8\")\n\n    def test_skip_discovery_with_setupcfg_metadata(self, tmp_path):\n        \"\"\"Ensure that auto-discovery is not triggered when the project is based on\n        C-extensions only, for backward compatibility.\n        \"\"\"\n        self._simulate_package_with_extension(tmp_path)\n\n        pyproject = \"\"\"\n            [build-system]\n            requires = []\n            build-backend = 'setuptools.build_meta'\n        \"\"\"\n        (tmp_path / \"pyproject.toml\").write_text(DALS(pyproject), encoding=\"utf-8\")\n\n        setupcfg = \"\"\"\n            [metadata]\n            name = proj\n            version = 42\n        \"\"\"\n        (tmp_path / \"setup.cfg\").write_text(DALS(setupcfg), encoding=\"utf-8\")\n\n        dist = _get_dist(tmp_path, {})\n        assert dist.get_name() == \"proj\"\n        assert dist.get_version() == \"42\"\n        assert dist.py_modules is None\n        assert dist.packages is None\n        assert len(dist.ext_modules) == 1\n        assert dist.ext_modules[0].name == \"proj\"\n\n    def test_dont_skip_discovery_with_pyproject_metadata(self, tmp_path):\n        \"\"\"When opting-in to pyproject.toml metadata, auto-discovery will be active if\n        the package lists C-extensions, but does not configure py-modules or packages.\n\n        This way we ensure users with complex package layouts that would lead to the\n        discovery of multiple top-level modules/packages see errors and are forced to\n        explicitly set ``packages`` or ``py-modules``.\n        \"\"\"\n        self._simulate_package_with_extension(tmp_path)\n\n        pyproject = \"\"\"\n            [project]\n            name = 'proj'\n            version = '42'\n        \"\"\"\n        (tmp_path / \"pyproject.toml\").write_text(DALS(pyproject), encoding=\"utf-8\")\n        with pytest.raises(PackageDiscoveryError, match=\"multiple (packages|modules)\"):\n            _get_dist(tmp_path, {})\n\n\nclass TestWithPackageData:\n    def _simulate_package_with_data_files(self, tmp_path, src_root):\n        files = [\n            f\"{src_root}/proj/__init__.py\",\n            f\"{src_root}/proj/file1.txt\",\n            f\"{src_root}/proj/nested/file2.txt\",\n        ]\n        _populate_project_dir(tmp_path, files, {})\n\n        manifest = \"\"\"\n            global-include *.py *.txt\n        \"\"\"\n        (tmp_path / \"MANIFEST.in\").write_text(DALS(manifest), encoding=\"utf-8\")\n\n    EXAMPLE_SETUPCFG = \"\"\"\n    [metadata]\n    name = proj\n    version = 42\n\n    [options]\n    include_package_data = True\n    \"\"\"\n    EXAMPLE_PYPROJECT = \"\"\"\n    [project]\n    name = \"proj\"\n    version = \"42\"\n    \"\"\"\n\n    PYPROJECT_PACKAGE_DIR = \"\"\"\n    [tool.setuptools]\n    package-dir = {\"\" = \"src\"}\n    \"\"\"\n\n    @pytest.mark.parametrize(\n        (\"src_root\", \"files\"),\n        [\n            (\".\", {\"setup.cfg\": DALS(EXAMPLE_SETUPCFG)}),\n            (\".\", {\"pyproject.toml\": DALS(EXAMPLE_PYPROJECT)}),\n            (\"src\", {\"setup.cfg\": DALS(EXAMPLE_SETUPCFG)}),\n            (\"src\", {\"pyproject.toml\": DALS(EXAMPLE_PYPROJECT)}),\n            (\n                \"src\",\n                {\n                    \"setup.cfg\": DALS(EXAMPLE_SETUPCFG)\n                    + DALS(\n                        \"\"\"\n                        packages = find:\n                        package_dir =\n                            =src\n\n                        [options.packages.find]\n                        where = src\n                        \"\"\"\n                    )\n                },\n            ),\n            (\n                \"src\",\n                {\n                    \"pyproject.toml\": DALS(EXAMPLE_PYPROJECT)\n                    + DALS(\n                        \"\"\"\n                        [tool.setuptools]\n                        package-dir = {\"\" = \"src\"}\n                        \"\"\"\n                    )\n                },\n            ),\n        ],\n    )\n    def test_include_package_data(self, tmp_path, src_root, files):\n        \"\"\"\n        Make sure auto-discovery does not affect package include_package_data.\n        See issue #3196.\n        \"\"\"\n        jaraco.path.build(files, prefix=str(tmp_path))\n        self._simulate_package_with_data_files(tmp_path, src_root)\n\n        expected = {\n            os.path.normpath(f\"{src_root}/proj/file1.txt\").replace(os.sep, \"/\"),\n            os.path.normpath(f\"{src_root}/proj/nested/file2.txt\").replace(os.sep, \"/\"),\n        }\n\n        _run_build(tmp_path)\n\n        sdist_files = get_sdist_members(next(tmp_path.glob(\"dist/*.tar.gz\")))\n        print(\"~~~~~ sdist_members ~~~~~\")\n        print('\\n'.join(sdist_files))\n        assert sdist_files >= expected\n\n        wheel_files = get_wheel_members(next(tmp_path.glob(\"dist/*.whl\")))\n        print(\"~~~~~ wheel_members ~~~~~\")\n        print('\\n'.join(wheel_files))\n        orig_files = {f.replace(\"src/\", \"\").replace(\"lib/\", \"\") for f in expected}\n        assert wheel_files >= orig_files\n\n\ndef test_compatible_with_numpy_configuration(tmp_path):\n    files = [\n        \"dir1/__init__.py\",\n        \"dir2/__init__.py\",\n        \"file.py\",\n    ]\n    _populate_project_dir(tmp_path, files, {})\n    dist = Distribution({})\n    dist.configuration = object()\n    dist.set_defaults()\n    assert dist.py_modules is None\n    assert dist.packages is None\n\n\ndef test_name_discovery_doesnt_break_cli(tmpdir_cwd):\n    jaraco.path.build({\"pkg.py\": \"\"})\n    dist = Distribution({})\n    dist.script_args = [\"--name\"]\n    dist.set_defaults()\n    dist.parse_command_line()  # <-- no exception should be raised here.\n    assert dist.get_name() == \"pkg\"\n\n\ndef test_preserve_explicit_name_with_dynamic_version(tmpdir_cwd, monkeypatch):\n    \"\"\"According to #3545 it seems that ``name`` discovery is running,\n    even when the project already explicitly sets it.\n    This seems to be related to parsing of dynamic versions (via ``attr`` directive),\n    which requires the auto-discovery of ``package_dir``.\n    \"\"\"\n    files = {\n        \"src\": {\n            \"pkg\": {\"__init__.py\": \"__version__ = 42\\n\"},\n        },\n        \"pyproject.toml\": DALS(\n            \"\"\"\n            [project]\n            name = \"myproj\"  # purposefully different from package name\n            dynamic = [\"version\"]\n            [tool.setuptools.dynamic]\n            version = {\"attr\" = \"pkg.__version__\"}\n            \"\"\"\n        ),\n    }\n    jaraco.path.build(files)\n    dist = Distribution({})\n    orig_analyse_name = dist.set_defaults.analyse_name\n\n    def spy_analyse_name():\n        # We can check if name discovery was triggered by ensuring the original\n        # name remains instead of the package name.\n        orig_analyse_name()\n        assert dist.get_name() == \"myproj\"\n\n    monkeypatch.setattr(dist.set_defaults, \"analyse_name\", spy_analyse_name)\n    dist.parse_config_files()\n    assert dist.get_version() == \"42\"\n    assert set(dist.packages) == {\"pkg\"}\n\n\ndef _populate_project_dir(root, files, options):\n    # NOTE: Currently pypa/build will refuse to build the project if no\n    # `pyproject.toml` or `setup.py` is found. So it is impossible to do\n    # completely \"config-less\" projects.\n    basic = {\n        \"setup.py\": \"import setuptools\\nsetuptools.setup()\",\n        \"README.md\": \"# Example Package\",\n        \"LICENSE\": \"Copyright (c) 2018\",\n    }\n    jaraco.path.build(basic, prefix=root)\n    _write_setupcfg(root, options)\n    paths = (root / f for f in files)\n    for path in paths:\n        path.parent.mkdir(exist_ok=True, parents=True)\n        path.touch()\n\n\ndef _write_setupcfg(root, options):\n    if not options:\n        print(\"~~~~~ **NO** setup.cfg ~~~~~\")\n        return\n    setupcfg = ConfigParser()\n    setupcfg.add_section(\"options\")\n    for key, value in options.items():\n        if key == \"packages.find\":\n            setupcfg.add_section(f\"options.{key}\")\n            setupcfg[f\"options.{key}\"].update(value)\n        elif isinstance(value, list):\n            setupcfg[\"options\"][key] = \", \".join(value)\n        elif isinstance(value, dict):\n            str_value = \"\\n\".join(f\"\\t{k} = {v}\" for k, v in value.items())\n            setupcfg[\"options\"][key] = \"\\n\" + str_value\n        else:\n            setupcfg[\"options\"][key] = str(value)\n    with open(root / \"setup.cfg\", \"w\", encoding=\"utf-8\") as f:\n        setupcfg.write(f)\n    print(\"~~~~~ setup.cfg ~~~~~\")\n    print((root / \"setup.cfg\").read_text(encoding=\"utf-8\"))\n\n\ndef _run_build(path, *flags):\n    cmd = [sys.executable, \"-m\", \"build\", \"--no-isolation\", *flags, str(path)]\n    return run(cmd, env={'DISTUTILS_DEBUG': ''})\n\n\ndef _get_dist(dist_path, attrs):\n    root = \"/\".join(os.path.split(dist_path))  # POSIX-style\n\n    script = dist_path / 'setup.py'\n    if script.exists():\n        with Path(dist_path):\n            dist = cast(\n                Distribution,\n                distutils.core.run_setup(\"setup.py\", {}, stop_after=\"init\"),\n            )\n    else:\n        dist = Distribution(attrs)\n\n    dist.src_root = root\n    dist.script_name = \"setup.py\"\n    with Path(dist_path):\n        dist.parse_config_files()\n\n    dist.set_defaults()\n    return dist\n\n\ndef _run_sdist_programatically(dist_path, attrs):\n    dist = _get_dist(dist_path, attrs)\n    cmd = sdist(dist)\n    cmd.ensure_finalized()\n    assert cmd.distribution.packages or cmd.distribution.py_modules\n\n    with quiet(), Path(dist_path):\n        cmd.run()\n\n    return dist, cmd\n"}]}